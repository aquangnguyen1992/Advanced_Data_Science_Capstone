{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4e24a44e-3354-4554-83f0-a710b3dce779"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab7366be-72dc-471b-d601-90a63510d196"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d8458a75-2d81-4207-e266-7d9cb16a11f5"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)\n",
        "saved_columns= np.array(dataset_identity.columns)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5caeeaab-978f-4be4-acd7-9c430a90fa26"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ba94e6cc-01f7-4a7f-9b3f-830b180eaa94"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4MWdmZ8wBEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = dataset_identity.pop(column)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS60VEEMwgHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "582002f5-7e4f-45a0-a186-2a1488865880"
      },
      "source": [
        "merged_data = pd.merge(left=dataset_transaction, right=dataset_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "dataset_transaction = None\n",
        "dataset_identity = None\n",
        "merged_data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 430)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIAS8CbdwwET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1eb06523-beee-475c-9aea-bd9d6578a88c"
      },
      "source": [
        "merged_data.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a12994b-6a1c-45d0-d4e8-800ce5f0fabf"
      },
      "source": [
        "dataset_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "dataset_identity = None\n",
        "\n",
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "cache = dict()\n",
        "print(len(float_columns), len(int_columns), len(obj_columns))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e2fefba1-c1bd-4cca-b372-0f0216f89ded"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN column for every features\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "8ffa7e2a-5efd-4a21-e0b1-7df71803638b"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 829 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  id_26_NaN_Code  id_32_NaN_Code\n",
              "0        2987000        0  ...               1               1\n",
              "1        2987001        0  ...               1               1\n",
              "2        2987002        0  ...               1               1\n",
              "3        2987003        0  ...               1               1\n",
              "4        2987004        0  ...               1               0\n",
              "\n",
              "[5 rows x 829 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ae073d23-44f5-49d8-bf40-91f3a70118f5"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "      <th>TransactionDT_NaN_Code</th>\n",
              "      <th>card1_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 831 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  TransactionDT_NaN_Code  card1_NaN_Code\n",
              "0        2987000        0  ...                       0               0\n",
              "1        2987001        0  ...                       0               0\n",
              "2        2987002        0  ...                       0               0\n",
              "3        2987003        0  ...                       0               0\n",
              "4        2987004        0  ...                       0               0\n",
              "\n",
              "[5 rows x 831 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48a89e22-4834-49b6-fe16-383a898ff5ff"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoded_column = 0\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "  cache[column] = dataset_transaction[column].values.reshape(-1,1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuvQmMmLRnM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "87cf777e-f44b-44b4-a3de-8fbe52983f62"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1011 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 1011 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b4c60a9-dcc9-43c3-85b2-320733a7d0a7"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoc4TuIx1zoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7748ab31-6eec-4551-bf5a-fe90f71ed7b0"
      },
      "source": [
        "out = ['isFraud']\n",
        "for column in dataset_transaction.columns:\n",
        "  if column.find('R_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "  if column.find('P_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "print(out)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['isFraud', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'P_emaildomain_5', 'P_emaildomain_6', 'P_emaildomain_7', 'P_emaildomain_8', 'P_emaildomain_9', 'P_emaildomain_10', 'P_emaildomain_11', 'P_emaildomain_12', 'P_emaildomain_13', 'P_emaildomain_14', 'P_emaildomain_15', 'P_emaildomain_16', 'P_emaildomain_17', 'P_emaildomain_18', 'P_emaildomain_19', 'P_emaildomain_20', 'P_emaildomain_21', 'P_emaildomain_22', 'P_emaildomain_23', 'P_emaildomain_24', 'P_emaildomain_25', 'P_emaildomain_26', 'P_emaildomain_27', 'P_emaildomain_28', 'P_emaildomain_29', 'P_emaildomain_30', 'P_emaildomain_31', 'P_emaildomain_32', 'P_emaildomain_33', 'P_emaildomain_34', 'P_emaildomain_35', 'P_emaildomain_36', 'P_emaildomain_37', 'P_emaildomain_38', 'P_emaildomain_39', 'P_emaildomain_40', 'P_emaildomain_41', 'P_emaildomain_42', 'P_emaildomain_43', 'P_emaildomain_44', 'P_emaildomain_45', 'P_emaildomain_46', 'P_emaildomain_47', 'P_emaildomain_48', 'P_emaildomain_49', 'P_emaildomain_50', 'P_emaildomain_51', 'P_emaildomain_52', 'P_emaildomain_53', 'P_emaildomain_54', 'P_emaildomain_55', 'P_emaildomain_56', 'P_emaildomain_57', 'P_emaildomain_58', 'P_emaildomain_59', 'R_emaildomain_0', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'R_emaildomain_4', 'R_emaildomain_5', 'R_emaildomain_6', 'R_emaildomain_7', 'R_emaildomain_8', 'R_emaildomain_9', 'R_emaildomain_10', 'R_emaildomain_11', 'R_emaildomain_12', 'R_emaildomain_13', 'R_emaildomain_14', 'R_emaildomain_15', 'R_emaildomain_16', 'R_emaildomain_17', 'R_emaildomain_18', 'R_emaildomain_19', 'R_emaildomain_20', 'R_emaildomain_21', 'R_emaildomain_22', 'R_emaildomain_23', 'R_emaildomain_24', 'R_emaildomain_25', 'R_emaildomain_26', 'R_emaildomain_27', 'R_emaildomain_28', 'R_emaildomain_29', 'R_emaildomain_30', 'R_emaildomain_31', 'R_emaildomain_32', 'R_emaildomain_33', 'R_emaildomain_34', 'R_emaildomain_35', 'R_emaildomain_36', 'R_emaildomain_37', 'R_emaildomain_38', 'R_emaildomain_39', 'R_emaildomain_40', 'R_emaildomain_41', 'R_emaildomain_42', 'R_emaildomain_43', 'R_emaildomain_44', 'R_emaildomain_45', 'R_emaildomain_46', 'R_emaildomain_47', 'R_emaildomain_48', 'R_emaildomain_49', 'R_emaildomain_50', 'R_emaildomain_51', 'R_emaildomain_52', 'R_emaildomain_53', 'R_emaildomain_54', 'R_emaildomain_55', 'R_emaildomain_56', 'R_emaildomain_57', 'R_emaildomain_58', 'R_emaildomain_59', 'R_emaildomain_60']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#columns_to_analyze = ['isFraud', 'DeviceType_0', 'DeviceType_1', 'DeviceType_2', 'id_15_0', 'id_15_1', 'id_15_2', 'id_15_3']#, 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2', 'card1', 'card2', 'card3']\n",
        "columns_to_analyze = out\n",
        "\n",
        "analyzing_data = dataset_transaction[columns_to_analyze]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "to_display = False\n",
        "\n",
        "if to_display:\n",
        "  mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD5CKASq2rzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "b6b98fa4-51ab-476d-a5de-0f485bca0585"
      },
      "source": [
        "# Remove the weak correlation features\n",
        "col = corr.columns\n",
        "is_fraud = np.where(col=='isFraud')[0][0]\n",
        "col = col.to_list()\n",
        "col.pop(is_fraud)\n",
        "to_remove = []\n",
        "for each_col in col:\n",
        "  if abs(corr['isFraud'][each_col]) < 0.05: # Weak correlation\n",
        "    to_remove.append(each_col)\n",
        "    a = dataset_transaction.pop(each_col)\n",
        "print(len(to_remove))\n",
        "analyzing_data = None\n",
        "\n",
        "\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 893 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 893 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "145f6005-8a3f-4b01-9fc5-ea2bc3d2686e"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "a = dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 892 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  ...  DeviceType_1  DeviceType_2\n",
              "0        0      -0.463379  ...             0             0\n",
              "1        0      -0.463379  ...             0             0\n",
              "2        0      -0.463379  ...             0             0\n",
              "3        0      -0.463379  ...             0             0\n",
              "4        0      -0.463379  ...             0             1\n",
              "\n",
              "[5 rows x 892 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)\n",
        "X_train = X\n",
        "Y_train = Y\n",
        "\n",
        "#test_size = 20000\n",
        "#indices = np.random.randint(0, len(Y), size=(test_size,))\n",
        "#X_test = np.array(X_train)[indices]\n",
        "#Y_test = np.array(Y_train)[indices]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "455bb57d-1fee-4abc-e0db-d033d0384c7d"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXWElEQVR4nO3dbYwd1XnA8f9Tm5fmDQx2U2S7rFEtRaZqBbEIJahNQhWMabJUfZFR2pjUrZsGKiKqtqZITZUqKvlSEtQ0FQIUkKIAJWnjJlDqYqOqRTasKWAMNSyGFFs0OLYDQVFJoU8/zFkyvrpn9669d3bj/f+kq515zpk5j8+dvc/OzL3XkZlIktTPj812ApKkucsiIUmqskhIkqosEpKkKouEJKlq4WwnMNMWL16cIyMjs52GJP1I2blz53cyc0lv/LgrEiMjI4yNjc12GpL0IyUivtUv7uUmSVKVRUKSVGWRkCRVHXf3JI7FyKZvznYKOo49f/2ls52CNG2eSUiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaoauEhExIKI+I+I+EZZXxEROyJiPCLujIgTS/yksj5e2kda+7i2xPdExMWt+JoSG4+ITa143zEkSd2YzpnE1cBTrfXPAjdk5k8Dh4ENJb4BOFziN5R+RMQqYB1wNrAG+JtSeBYAXwAuAVYBl5e+k40hSerAQEUiIpYBlwI3l/UAPgDcXbrcBlxWlkfLOqX9otJ/FLgjM1/LzOeAceC88hjPzL2Z+QPgDmB0ijEkSR0Y9Ezic8AfA/9X1k8HvpuZr5f1fcDSsrwUeAGgtL9c+r8Z79mmFp9sjCNExMaIGIuIsQMHDgz4T5IkTWXKIhERvwy8lJk7O8jnqGTmTZm5OjNXL1myZLbTkaTjxsIB+rwX+HBErAVOBt4BfB44NSIWlr/0lwH7S//9wHJgX0QsBE4BDrbiE9rb9IsfnGQMSVIHpjyTyMxrM3NZZo7Q3HjempkfAbYBv1a6rQe+XpY3l3VK+9bMzBJfV979tAJYCTwEPAysLO9kOrGMsblsUxtDktSBY/mcxJ8A10TEOM39g1tK/Bbg9BK/BtgEkJm7gbuAJ4F/Aq7MzDfKWcJVwH007566q/SdbAxJUgcGudz0psx8AHigLO+leWdSb5//AX69sv1ngM/0id8D3NMn3ncMSVI3/MS1JKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkqimLREQsj4htEfFkROyOiKtL/LSI2BIRz5Sfi0o8IuLGiBiPiMcj4tzWvtaX/s9ExPpW/N0Rsatsc2NExGRjSJK6MciZxOvAH2bmKuB84MqIWAVsAu7PzJXA/WUd4BJgZXlsBL4IzQs+8CngPcB5wKdaL/pfBH63td2aEq+NIUnqwJRFIjNfzMxHyvL3gKeApcAocFvpdhtwWVkeBW7Pxnbg1Ig4A7gY2JKZhzLzMLAFWFPa3pGZ2zMzgdt79tVvDElSB6Z1TyIiRoBzgB3AOzPzxdL038A7y/JS4IXWZvtKbLL4vj5xJhmjN6+NETEWEWMHDhyYzj9JkjSJgYtERLwN+Crwycx8pd1WzgByhnM7wmRjZOZNmbk6M1cvWbJkmGlI0rwyUJGIiBNoCsSXM/NrJfztcqmI8vOlEt8PLG9tvqzEJosv6xOfbAxJUgcGeXdTALcAT2XmX7WaNgMT71BaD3y9Ff9oeZfT+cDL5ZLRfcAHI2JRuWH9QeC+0vZKRJxfxvpoz776jSFJ6sDCAfq8F/gtYFdEPFpifwpcD9wVERuAbwG/UdruAdYC48D3gY8BZOahiPgL4OHS79OZeagsfwL4EvDjwL3lwSRjSJI6MGWRyMx/A6LSfFGf/glcWdnXrcCtfeJjwM/0iR/sN4YkqRt+4lqSVGWRkCRVWSQkSVUWCUlSlUVCklRlkZAkVVkkJElVFglJUpVFQpJUZZGQJFVZJCRJVRYJSVKVRUKSVGWRkCRVWSQkSVUWCUlSlUVCklRlkZAkVVkkJElVFglJUpVFQpJUZZGQJFVZJCRJVRYJSVLVwtlOYCoRsQb4PLAAuDkzr5/llKSjMrLpm7Odgo5jz19/6VD2O6fPJCJiAfAF4BJgFXB5RKya3awkaf6Y00UCOA8Yz8y9mfkD4A5gdJZzkqR5Y65fbloKvNBa3we8p7dTRGwENpbVVyNiz1GOtxj4zlFuO0zmNT3mNT3mNT1zMq/47DHndWa/4FwvEgPJzJuAm451PxExlpmrZyClGWVe02Ne02Ne0zPf8prrl5v2A8tb68tKTJLUgbleJB4GVkbEiog4EVgHbJ7lnCRp3pjTl5sy8/WIuAq4j+YtsLdm5u4hDnnMl6yGxLymx7ymx7ymZ17lFZk5jP1Kko4Dc/1ykyRpFlkkJElV86ZIRMSaiNgTEeMRsalP+0kRcWdp3xERI622a0t8T0Rc3HFe10TEkxHxeETcHxFnttreiIhHy2NGb+gPkNcVEXGgNf7vtNrWR8Qz5bG+47xuaOX0dER8t9U2lPmKiFsj4qWIeKLSHhFxY8n58Yg4t9U2zLmaKq+PlHx2RcSDEfFzrbbnS/zRiBjrOK/3RcTLrefqz1ptkz7/Q87rj1o5PVGOp9NK2zDna3lEbCuvA7sj4uo+fYZ3jGXmcf+guen9LHAWcCLwGLCqp88ngL8ty+uAO8vyqtL/JGBF2c+CDvN6P/CWsvz7E3mV9Vdncb6uAP66z7anAXvLz0VleVFXefX0/wOaNzsMe75+ATgXeKLSvha4FwjgfGDHsOdqwLwumBiP5qtvdrTangcWz9J8vQ/4xrE+/zOdV0/fDwFbO5qvM4Bzy/Lbgaf7/D4O7RibL2cSg3y9xyhwW1m+G7goIqLE78jM1zLzOWC87K+TvDJzW2Z+v6xup/msyLAdy9ehXAxsycxDmXkY2AKsmaW8Lge+MkNjV2XmvwKHJukyCtyeje3AqRFxBsOdqynzyswHy7jQ3bE1yHzVDPVreqaZVyfHFkBmvpiZj5Tl7wFP0XwbRdvQjrH5UiT6fb1H7yS/2SczXwdeBk4fcNth5tW2geavhQknR8RYRGyPiMtmKKfp5PWr5dT27oiY+NDjnJivclluBbC1FR7WfE2llvcw52q6eo+tBP45InZG87U3Xfv5iHgsIu6NiLNLbE7MV0S8heaF9qutcCfzFc1l8HOAHT1NQzvG5vTnJPRDEfGbwGrgF1vhMzNzf0ScBWyNiF2Z+WxHKf0j8JXMfC0ifo/mLOwDHY09iHXA3Zn5Ris2m/M1Z0XE+2mKxIWt8IVlrn4C2BIR/1n+0u7CIzTP1asRsRb4B2BlR2MP4kPAv2dm+6xj6PMVEW+jKUyfzMxXZnLfk5kvZxKDfL3Hm30iYiFwCnBwwG2HmRcR8UvAdcCHM/O1iXhm7i8/9wIP0PyF0UlemXmwlcvNwLsH3XaYebWso+dywBDnayq1vGf9a2ci4mdpnr/RzDw4EW/N1UvA3zNzl1inlJmvZOarZfke4ISIWMwcmK9ismNrKPMVESfQFIgvZ+bX+nQZ3jE2jBstc+1Bc8a0l+byw8QNr7N7+lzJkTeu7yrLZ3Pkjeu9zNyN60HyOofmZt3Knvgi4KSyvBh4hhm6iTdgXme0ln8F2J4/vFH2XMlvUVk+rau8Sr930dxIjC7mq+xzhPqN2Es58qbiQ8OeqwHz+imae2wX9MTfCry9tfwgsKbDvH5y4rmjebH9rzJ3Az3/w8qrtJ9Cc9/irV3NV/m33w58bpI+QzvGZmxy5/qD5u7/0zQvuNeV2Kdp/joHOBn4u/JL8xBwVmvb68p2e4BLOs7rX4BvA4+Wx+YSvwDYVX5RdgEbOs7rL4HdZfxtwLta2/52mcdx4GNd5lXW/xy4vme7oc0XzV+VLwL/S3PNdwPwceDjpT1o/vOsZ8vYqzuaq6nyuhk43Dq2xkr8rDJPj5Xn+LqO87qqdWxtp1XE+j3/XeVV+lxB80aW9nbDnq8Lae55PN56rtZ2dYz5tRySpKr5ck9CknQULBKSpCqLhCSp6rj7nMTixYtzZGRkttOQpB8pO3fu/E5mLumNH3dFYmRkhLGxGf1+LUk67kXEt/rFvdwkSaqySEiSqiwSkqSq4+6exLEY2fTN2U5Bx7Hnr790tlOQps0zCUlSlUVCklRlkZAkVVkkJElVFglJUpVFQpJUZZGQJFVZJCRJVRYJSVKVRUKSVGWRkCRVWSQkSVUWCUlSlUVCklRlkZAkVVkkJElVFglJUpVFQpJUZZGQJFVZJCRJVRYJSVKVRUKSVGWRkCRVWSQkSVUDF4mIWBAR/xER3yjrKyJiR0SMR8SdEXFiiZ9U1sdL+0hrH9eW+J6IuLgVX1Ni4xGxqRXvO4YkqRvTOZO4Gniqtf5Z4IbM/GngMLChxDcAh0v8htKPiFgFrAPOBtYAf1MKzwLgC8AlwCrg8tJ3sjEkSR0YqEhExDLgUuDmsh7AB4C7S5fbgMvK8mhZp7RfVPqPAndk5muZ+RwwDpxXHuOZuTczfwDcAYxOMYYkqQODnkl8Dvhj4P/K+unAdzPz9bK+D1halpcCLwCU9pdL/zfjPdvU4pONcYSI2BgRYxExduDAgQH/SZKkqUxZJCLil4GXMnNnB/kclcy8KTNXZ+bqJUuWzHY6knTcWDhAn/cCH46ItcDJwDuAzwOnRsTC8pf+MmB/6b8fWA7si4iFwCnAwVZ8QnubfvGDk4whSerAlGcSmXltZi7LzBGaG89bM/MjwDbg10q39cDXy/Lmsk5p35qZWeLryrufVgArgYeAh4GV5Z1MJ5YxNpdtamNIkjpwLJ+T+BPgmogYp7l/cEuJ3wKcXuLXAJsAMnM3cBfwJPBPwJWZ+UY5S7gKuI/m3VN3lb6TjSFJ6sAgl5velJkPAA+U5b0070zq7fM/wK9Xtv8M8Jk+8XuAe/rE+44hSeqGn7iWJFVZJCRJVRYJSVKVRUKSVGWRkCRVWSQkSVUWCUlSlUVCklRlkZAkVVkkJElVFglJUpVFQpJUZZGQJFVZJCRJVRYJSVKVRUKSVGWRkCRVWSQkSVUWCUlSlUVCklRlkZAkVVkkJElVFglJUpVFQpJUNWWRiIjlEbEtIp6MiN0RcXWJnxYRWyLimfJzUYlHRNwYEeMR8XhEnNva1/rS/5mIWN+KvzsidpVtboyImGwMSVI3BjmTeB34w8xcBZwPXBkRq4BNwP2ZuRK4v6wDXAKsLI+NwBehecEHPgW8BzgP+FTrRf+LwO+2tltT4rUxJEkdmLJIZOaLmflIWf4e8BSwFBgFbivdbgMuK8ujwO3Z2A6cGhFnABcDWzLzUGYeBrYAa0rbOzJze2YmcHvPvvqNIUnqwLTuSUTECHAOsAN4Z2a+WJr+G3hnWV4KvNDabF+JTRbf1yfOJGP05rUxIsYiYuzAgQPT+SdJkiYxcJGIiLcBXwU+mZmvtNvKGUDOcG5HmGyMzLwpM1dn5uolS5YMMw1JmlcGKhIRcQJNgfhyZn6thL9dLhVRfr5U4vuB5a3Nl5XYZPFlfeKTjSFJ6sAg724K4Bbgqcz8q1bTZmDiHUrrga+34h8t73I6H3i5XDK6D/hgRCwqN6w/CNxX2l6JiPPLWB/t2Ve/MSRJHVg4QJ/3Ar8F7IqIR0vsT4HrgbsiYgPwLeA3Sts9wFpgHPg+8DGAzDwUEX8BPFz6fTozD5XlTwBfAn4cuLc8mGQMSVIHpiwSmflvQFSaL+rTP4ErK/u6Fbi1T3wM+Jk+8YP9xpAkdcNPXEuSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpauFsJyDNFyObvjnbKeg49vz1lw5lv3P+TCIi1kTEnogYj4hNs52PJM0nc7pIRMQC4AvAJcAq4PKIWDW7WUnS/DGniwRwHjCemXsz8wfAHcDoLOckSfPGXL8nsRR4obW+D3hPb6eI2AhsLKuvRsSeoxxvMfCdo9x2mMxresxresxreuZkXvHZY87rzH7BuV4kBpKZNwE3Het+ImIsM1fPQEozyrymx7ymx7ymZ77lNdcvN+0HlrfWl5WYJKkDc71IPAysjIgVEXEisA7YPMs5SdK8MacvN2Xm6xFxFXAfsAC4NTN3D3HIY75kNSTmNT3mNT3mNT3zKq/IzGHsV5J0HJjrl5skSbPIIiFJqpo3RWKqr/eIiJMi4s7SviMiRlpt15b4noi4uOO8romIJyPi8Yi4PyLObLW9ERGPlseM3tAfIK8rIuJAa/zfabWtj4hnymN9x3nd0Mrp6Yj4bqttKPMVEbdGxEsR8USlPSLixpLz4xFxbqttmHM1VV4fKfnsiogHI+LnWm3Pl/ijETHWcV7vi4iXW8/Vn7XahvY1PQPk9UetnJ4ox9NppW2Y87U8IraV14HdEXF1nz7DO8Yy87h/0Nz0fhY4CzgReAxY1dPnE8DfluV1wJ1leVXpfxKwouxnQYd5vR94S1n+/Ym8yvqrszhfVwB/3Wfb04C95eeisryoq7x6+v8BzZsdhj1fvwCcCzxRaV8L3AsEcD6wY9hzNWBeF0yMR/PVNztabc8Di2dpvt4HfONYn/+Zzqun74eArR3N1xnAuWX57cDTfX4fh3aMzZcziUG+3mMUuK0s3w1cFBFR4ndk5muZ+RwwXvbXSV6ZuS0zv19Wt9N8VmTYjuXrUC4GtmTmocw8DGwB1sxSXpcDX5mhsasy81+BQ5N0GQVuz8Z24NSIOIPhztWUeWXmg2Vc6O7YGmS+aob6NT3TzKuTYwsgM1/MzEfK8veAp2i+jaJtaMfYfCkS/b7eo3eS3+yTma8DLwOnD7jtMPNq20Dz18KEkyNiLCK2R8RlM5TTdPL61XJqe3dETHzocU7MV7kstwLY2goPa76mUst7mHM1Xb3HVgL/HBE7o/nam679fEQ8FhH3RsTZJTYn5isi3kLzQvvVVriT+YrmMvg5wI6epqEdY3P6cxL6oYj4TWA18Iut8JmZuT8izgK2RsSuzHy2o5T+EfhKZr4WEb9Hcxb2gY7GHsQ64O7MfKMVm835mrMi4v00ReLCVvjCMlc/AWyJiP8sf2l34RGa5+rViFgL/AOwsqOxB/Eh4N8zs33WMfT5ioi30RSmT2bmKzO578nMlzOJQb7e480+EbEQOAU4OOC2w8yLiPgl4Drgw5n52kQ8M/eXn3uBB2j+wugkr8w82MrlZuDdg247zLxa1tFzOWCI8zWVWt6z/rUzEfGzNM/faGYenIi35uol4O+ZuUusU8rMVzLz1bJ8D3BCRCxmDsxXMdmxNZT5iogTaArElzPza326DO8YG8aNlrn2oDlj2ktz+WHihtfZPX2u5Mgb13eV5bM58sb1XmbuxvUgeZ1Dc7NuZU98EXBSWV4MPMMM3cQbMK8zWsu/AmzPH94oe67kt6gsn9ZVXqXfu2huJEYX81X2OUL9RuylHHlT8aFhz9WAef0UzT22C3ribwXe3lp+EFjTYV4/OfHc0bzY/leZu4Ge/2HlVdpPoblv8dau5qv8228HPjdJn6EdYzM2uXP9QXP3/2maF9zrSuzTNH+dA5wM/F35pXkIOKu17XVluz3AJR3n9S/At4FHy2NziV8A7Cq/KLuADR3n9ZfA7jL+NuBdrW1/u8zjOPCxLvMq638OXN+z3dDmi+avyheB/6W55rsB+Djw8dIeNP951rNl7NUdzdVUed0MHG4dW2MlflaZp8fKc3xdx3ld1Tq2ttMqYv2e/67yKn2uoHkjS3u7Yc/XhTT3PB5vPVdruzrG/FoOSVLVfLknIUk6ChYJSVKVRUKSVGWRkCRVWSQkSVUWCUlSlUVCklT1/5pG/knD4lsqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f22271c3-aa82-44bc-b803-b7be1f8ef506"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 5\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_new = np.array(X_train)[indices_0_new]\n",
        "Y_new = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_new, test_size=0.05)\n",
        "\n",
        "X_to_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_train, axis=1)\n",
        "\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[2]))\n",
        "Y_test = np.squeeze(Y_test, axis=1)\n",
        "\n",
        "\n",
        "print(X_to_train.shape, X_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569877, 1)\n",
            "(659162, 891) (34693, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "dfd9f855-ad18-4c11-d197-cb4fefd33390"
      },
      "source": [
        "X_new = None\n",
        "Y_new = None\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 17.86%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  \n",
        "  out_model.add(Dense(dense1, activation='relu',\n",
        "                      input_shape=(X_to_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(dense2, activation='relu', \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(int(dense2/2), activation='relu', \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(int(dense2/2), activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[METRICS])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "21c81338-c9fc-451d-ffc1-28028269ba6d"
      },
      "source": [
        "#my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=1e-4, l2_rate=5e-4, init_std=0.1, lr=0.00008)\n",
        "my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=5e-5, l2_rate=1e-5, init_std=0.1, lr=0.0001)\n",
        "my_model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 477,825\n",
            "Trainable params: 476,545\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a6490ce-bf13-470f-dd7a-805cd6a314f3"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "NB_EPOCH = 2000\n",
        "PATIENCE = 20\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', patience=PATIENCE, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./best_model.h5', monitor='val_auc', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.01, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 1.3852 - tp: 45653.0000 - fp: 52195.0000 - tn: 483545.0000 - fn: 70895.0000 - accuracy: 0.8113 - precision: 0.4666 - recall: 0.3917 - auc: 0.7412\n",
            "Epoch 00001: val_auc improved from -inf to 0.83991, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 1.3851 - tp: 45670.0000 - fp: 52199.0000 - tn: 483773.0000 - fn: 70928.0000 - accuracy: 0.8113 - precision: 0.4666 - recall: 0.3917 - auc: 0.7413 - val_loss: 1.1267 - val_tp: 493.0000 - val_fp: 148.0000 - val_tn: 5308.0000 - val_fn: 643.0000 - val_accuracy: 0.8800 - val_precision: 0.7691 - val_recall: 0.4340 - val_auc: 0.8399\n",
            "Epoch 2/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 1.0195 - tp: 45654.0000 - fp: 12313.0000 - tn: 523637.0000 - fn: 70940.0000 - accuracy: 0.8724 - precision: 0.7876 - recall: 0.3916 - auc: 0.8386\n",
            "Epoch 00002: val_auc improved from 0.83991 to 0.85133, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 1.0195 - tp: 45657.0000 - fp: 12315.0000 - tn: 523657.0000 - fn: 70941.0000 - accuracy: 0.8724 - precision: 0.7876 - recall: 0.3916 - auc: 0.8386 - val_loss: 0.8990 - val_tp: 404.0000 - val_fp: 51.0000 - val_tn: 5405.0000 - val_fn: 732.0000 - val_accuracy: 0.8812 - val_precision: 0.8879 - val_recall: 0.3556 - val_auc: 0.8513\n",
            "Epoch 3/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.8157 - tp: 49498.0000 - fp: 12807.0000 - tn: 522724.0000 - fn: 67003.0000 - accuracy: 0.8776 - precision: 0.7944 - recall: 0.4249 - auc: 0.8527\n",
            "Epoch 00003: val_auc improved from 0.85133 to 0.85305, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.8156 - tp: 49538.0000 - fp: 12819.0000 - tn: 523153.0000 - fn: 67060.0000 - accuracy: 0.8776 - precision: 0.7944 - recall: 0.4249 - auc: 0.8527 - val_loss: 0.7288 - val_tp: 614.0000 - val_fp: 257.0000 - val_tn: 5199.0000 - val_fn: 522.0000 - val_accuracy: 0.8818 - val_precision: 0.7049 - val_recall: 0.5405 - val_auc: 0.8531\n",
            "Epoch 4/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.6510 - tp: 51914.0000 - fp: 13111.0000 - tn: 522840.0000 - fn: 64679.0000 - accuracy: 0.8808 - precision: 0.7984 - recall: 0.4453 - auc: 0.8599\n",
            "Epoch 00004: val_auc improved from 0.85305 to 0.86454, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.6510 - tp: 51917.0000 - fp: 13112.0000 - tn: 522860.0000 - fn: 64681.0000 - accuracy: 0.8808 - precision: 0.7984 - recall: 0.4453 - auc: 0.8599 - val_loss: 0.5775 - val_tp: 487.0000 - val_fp: 74.0000 - val_tn: 5382.0000 - val_fn: 649.0000 - val_accuracy: 0.8903 - val_precision: 0.8681 - val_recall: 0.4287 - val_auc: 0.8645\n",
            "Epoch 5/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.5421 - tp: 54052.0000 - fp: 13450.0000 - tn: 521878.0000 - fn: 62396.0000 - accuracy: 0.8836 - precision: 0.8007 - recall: 0.4642 - auc: 0.8662\n",
            "Epoch 00005: val_auc improved from 0.86454 to 0.87056, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.5421 - tp: 54121.0000 - fp: 13466.0000 - tn: 522506.0000 - fn: 62477.0000 - accuracy: 0.8836 - precision: 0.8008 - recall: 0.4642 - auc: 0.8662 - val_loss: 0.4957 - val_tp: 496.0000 - val_fp: 75.0000 - val_tn: 5381.0000 - val_fn: 640.0000 - val_accuracy: 0.8915 - val_precision: 0.8687 - val_recall: 0.4366 - val_auc: 0.8706\n",
            "Epoch 6/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.4784 - tp: 56076.0000 - fp: 13495.0000 - tn: 522477.0000 - fn: 60522.0000 - accuracy: 0.8866 - precision: 0.8060 - recall: 0.4809 - auc: 0.8713\n",
            "Epoch 00006: val_auc improved from 0.87056 to 0.87240, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.4784 - tp: 56076.0000 - fp: 13495.0000 - tn: 522477.0000 - fn: 60522.0000 - accuracy: 0.8866 - precision: 0.8060 - recall: 0.4809 - auc: 0.8713 - val_loss: 0.4616 - val_tp: 626.0000 - val_fp: 211.0000 - val_tn: 5245.0000 - val_fn: 510.0000 - val_accuracy: 0.8906 - val_precision: 0.7479 - val_recall: 0.5511 - val_auc: 0.8724\n",
            "Epoch 7/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.4400 - tp: 57106.0000 - fp: 13387.0000 - tn: 521924.0000 - fn: 59359.0000 - accuracy: 0.8884 - precision: 0.8101 - recall: 0.4903 - auc: 0.8757\n",
            "Epoch 00007: val_auc improved from 0.87240 to 0.87701, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.4400 - tp: 57173.0000 - fp: 13408.0000 - tn: 522564.0000 - fn: 59425.0000 - accuracy: 0.8884 - precision: 0.8100 - recall: 0.4903 - auc: 0.8757 - val_loss: 0.4202 - val_tp: 524.0000 - val_fp: 88.0000 - val_tn: 5368.0000 - val_fn: 612.0000 - val_accuracy: 0.8938 - val_precision: 0.8562 - val_recall: 0.4613 - val_auc: 0.8770\n",
            "Epoch 8/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.4149 - tp: 58598.0000 - fp: 13585.0000 - tn: 521733.0000 - fn: 57860.0000 - accuracy: 0.8904 - precision: 0.8118 - recall: 0.5032 - auc: 0.8789\n",
            "Epoch 00008: val_auc improved from 0.87701 to 0.87724, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.4148 - tp: 58674.0000 - fp: 13602.0000 - tn: 522370.0000 - fn: 57924.0000 - accuracy: 0.8904 - precision: 0.8118 - recall: 0.5032 - auc: 0.8790 - val_loss: 0.4040 - val_tp: 543.0000 - val_fp: 109.0000 - val_tn: 5347.0000 - val_fn: 593.0000 - val_accuracy: 0.8935 - val_precision: 0.8328 - val_recall: 0.4780 - val_auc: 0.8772\n",
            "Epoch 9/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.3961 - tp: 59932.0000 - fp: 13801.0000 - tn: 521510.0000 - fn: 56533.0000 - accuracy: 0.8921 - precision: 0.8128 - recall: 0.5146 - auc: 0.8829\n",
            "Epoch 00009: val_auc improved from 0.87724 to 0.88000, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.3961 - tp: 59991.0000 - fp: 13818.0000 - tn: 522154.0000 - fn: 56607.0000 - accuracy: 0.8921 - precision: 0.8128 - recall: 0.5145 - auc: 0.8829 - val_loss: 0.3913 - val_tp: 484.0000 - val_fp: 44.0000 - val_tn: 5412.0000 - val_fn: 652.0000 - val_accuracy: 0.8944 - val_precision: 0.9167 - val_recall: 0.4261 - val_auc: 0.8800\n",
            "Epoch 10/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3825 - tp: 60825.0000 - fp: 13664.0000 - tn: 522284.0000 - fn: 55771.0000 - accuracy: 0.8936 - precision: 0.8166 - recall: 0.5217 - auc: 0.8853\n",
            "Epoch 00010: val_auc improved from 0.88000 to 0.88661, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3825 - tp: 60826.0000 - fp: 13666.0000 - tn: 522306.0000 - fn: 55772.0000 - accuracy: 0.8936 - precision: 0.8165 - recall: 0.5217 - auc: 0.8853 - val_loss: 0.3716 - val_tp: 538.0000 - val_fp: 72.0000 - val_tn: 5384.0000 - val_fn: 598.0000 - val_accuracy: 0.8984 - val_precision: 0.8820 - val_recall: 0.4736 - val_auc: 0.8866\n",
            "Epoch 11/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.3715 - tp: 61710.0000 - fp: 13737.0000 - tn: 521797.0000 - fn: 54788.0000 - accuracy: 0.8949 - precision: 0.8179 - recall: 0.5297 - auc: 0.8882\n",
            "Epoch 00011: val_auc improved from 0.88661 to 0.88838, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.3715 - tp: 61765.0000 - fp: 13747.0000 - tn: 522225.0000 - fn: 54833.0000 - accuracy: 0.8949 - precision: 0.8179 - recall: 0.5297 - auc: 0.8882 - val_loss: 0.3632 - val_tp: 549.0000 - val_fp: 79.0000 - val_tn: 5377.0000 - val_fn: 587.0000 - val_accuracy: 0.8990 - val_precision: 0.8742 - val_recall: 0.4833 - val_auc: 0.8884\n",
            "Epoch 12/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.3619 - tp: 62486.0000 - fp: 13603.0000 - tn: 521501.0000 - fn: 53930.0000 - accuracy: 0.8963 - precision: 0.8212 - recall: 0.5367 - auc: 0.8909\n",
            "Epoch 00012: val_auc improved from 0.88838 to 0.88930, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3619 - tp: 62581.0000 - fp: 13624.0000 - tn: 522348.0000 - fn: 54017.0000 - accuracy: 0.8963 - precision: 0.8212 - recall: 0.5367 - auc: 0.8909 - val_loss: 0.3589 - val_tp: 667.0000 - val_fp: 165.0000 - val_tn: 5291.0000 - val_fn: 469.0000 - val_accuracy: 0.9038 - val_precision: 0.8017 - val_recall: 0.5871 - val_auc: 0.8893\n",
            "Epoch 13/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.3530 - tp: 63872.0000 - fp: 13869.0000 - tn: 522103.0000 - fn: 52726.0000 - accuracy: 0.8979 - precision: 0.8216 - recall: 0.5478 - auc: 0.8937\n",
            "Epoch 00013: val_auc improved from 0.88930 to 0.89588, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3530 - tp: 63872.0000 - fp: 13869.0000 - tn: 522103.0000 - fn: 52726.0000 - accuracy: 0.8979 - precision: 0.8216 - recall: 0.5478 - auc: 0.8937 - val_loss: 0.3403 - val_tp: 660.0000 - val_fp: 161.0000 - val_tn: 5295.0000 - val_fn: 476.0000 - val_accuracy: 0.9034 - val_precision: 0.8039 - val_recall: 0.5810 - val_auc: 0.8959\n",
            "Epoch 14/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.3446 - tp: 64594.0000 - fp: 13668.0000 - tn: 521866.0000 - fn: 51904.0000 - accuracy: 0.8994 - precision: 0.8254 - recall: 0.5545 - auc: 0.8963\n",
            "Epoch 00014: val_auc improved from 0.89588 to 0.89987, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3446 - tp: 64646.0000 - fp: 13677.0000 - tn: 522295.0000 - fn: 51952.0000 - accuracy: 0.8994 - precision: 0.8254 - recall: 0.5544 - auc: 0.8963 - val_loss: 0.3338 - val_tp: 581.0000 - val_fp: 76.0000 - val_tn: 5380.0000 - val_fn: 555.0000 - val_accuracy: 0.9043 - val_precision: 0.8843 - val_recall: 0.5114 - val_auc: 0.8999\n",
            "Epoch 15/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.3368 - tp: 65726.0000 - fp: 13755.0000 - tn: 522217.0000 - fn: 50872.0000 - accuracy: 0.9010 - precision: 0.8269 - recall: 0.5637 - auc: 0.8994\n",
            "Epoch 00015: val_auc improved from 0.89987 to 0.90178, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3368 - tp: 65726.0000 - fp: 13755.0000 - tn: 522217.0000 - fn: 50872.0000 - accuracy: 0.9010 - precision: 0.8269 - recall: 0.5637 - auc: 0.8994 - val_loss: 0.3265 - val_tp: 642.0000 - val_fp: 124.0000 - val_tn: 5332.0000 - val_fn: 494.0000 - val_accuracy: 0.9062 - val_precision: 0.8381 - val_recall: 0.5651 - val_auc: 0.9018\n",
            "Epoch 16/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3312 - tp: 66286.0000 - fp: 13879.0000 - tn: 521855.0000 - fn: 50268.0000 - accuracy: 0.9017 - precision: 0.8269 - recall: 0.5687 - auc: 0.9021\n",
            "Epoch 00016: val_auc improved from 0.90178 to 0.90348, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3312 - tp: 66313.0000 - fp: 13880.0000 - tn: 522092.0000 - fn: 50285.0000 - accuracy: 0.9017 - precision: 0.8269 - recall: 0.5687 - auc: 0.9021 - val_loss: 0.3212 - val_tp: 621.0000 - val_fp: 83.0000 - val_tn: 5373.0000 - val_fn: 515.0000 - val_accuracy: 0.9093 - val_precision: 0.8821 - val_recall: 0.5467 - val_auc: 0.9035\n",
            "Epoch 17/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3266 - tp: 67207.0000 - fp: 13978.0000 - tn: 521970.0000 - fn: 49389.0000 - accuracy: 0.9029 - precision: 0.8278 - recall: 0.5764 - auc: 0.9039\n",
            "Epoch 00017: val_auc improved from 0.90348 to 0.90541, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3266 - tp: 67209.0000 - fp: 13980.0000 - tn: 521992.0000 - fn: 49389.0000 - accuracy: 0.9029 - precision: 0.8278 - recall: 0.5764 - auc: 0.9039 - val_loss: 0.3188 - val_tp: 697.0000 - val_fp: 166.0000 - val_tn: 5290.0000 - val_fn: 439.0000 - val_accuracy: 0.9082 - val_precision: 0.8076 - val_recall: 0.6136 - val_auc: 0.9054\n",
            "Epoch 18/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.3226 - tp: 67653.0000 - fp: 13964.0000 - tn: 522008.0000 - fn: 48945.0000 - accuracy: 0.9036 - precision: 0.8289 - recall: 0.5802 - auc: 0.9059\n",
            "Epoch 00018: val_auc improved from 0.90541 to 0.90751, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3226 - tp: 67653.0000 - fp: 13964.0000 - tn: 522008.0000 - fn: 48945.0000 - accuracy: 0.9036 - precision: 0.8289 - recall: 0.5802 - auc: 0.9059 - val_loss: 0.3129 - val_tp: 640.0000 - val_fp: 92.0000 - val_tn: 5364.0000 - val_fn: 496.0000 - val_accuracy: 0.9108 - val_precision: 0.8743 - val_recall: 0.5634 - val_auc: 0.9075\n",
            "Epoch 19/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.3183 - tp: 68351.0000 - fp: 13799.0000 - tn: 521733.0000 - fn: 48149.0000 - accuracy: 0.9050 - precision: 0.8320 - recall: 0.5867 - auc: 0.9078\n",
            "Epoch 00019: val_auc did not improve from 0.90751\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3183 - tp: 68407.0000 - fp: 13810.0000 - tn: 522162.0000 - fn: 48191.0000 - accuracy: 0.9050 - precision: 0.8320 - recall: 0.5867 - auc: 0.9078 - val_loss: 0.3194 - val_tp: 756.0000 - val_fp: 240.0000 - val_tn: 5216.0000 - val_fn: 380.0000 - val_accuracy: 0.9059 - val_precision: 0.7590 - val_recall: 0.6655 - val_auc: 0.9069\n",
            "Epoch 20/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3137 - tp: 69467.0000 - fp: 13866.0000 - tn: 521883.0000 - fn: 47072.0000 - accuracy: 0.9066 - precision: 0.8336 - recall: 0.5961 - auc: 0.9104\n",
            "Epoch 00020: val_auc improved from 0.90751 to 0.90906, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3137 - tp: 69499.0000 - fp: 13869.0000 - tn: 522103.0000 - fn: 47099.0000 - accuracy: 0.9066 - precision: 0.8336 - recall: 0.5961 - auc: 0.9104 - val_loss: 0.3166 - val_tp: 758.0000 - val_fp: 233.0000 - val_tn: 5223.0000 - val_fn: 378.0000 - val_accuracy: 0.9073 - val_precision: 0.7649 - val_recall: 0.6673 - val_auc: 0.9091\n",
            "Epoch 21/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3109 - tp: 69637.0000 - fp: 13771.0000 - tn: 521970.0000 - fn: 46910.0000 - accuracy: 0.9070 - precision: 0.8349 - recall: 0.5975 - auc: 0.9120\n",
            "Epoch 00021: val_auc improved from 0.90906 to 0.91152, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3108 - tp: 69672.0000 - fp: 13774.0000 - tn: 522198.0000 - fn: 46926.0000 - accuracy: 0.9070 - precision: 0.8349 - recall: 0.5975 - auc: 0.9121 - val_loss: 0.3054 - val_tp: 708.0000 - val_fp: 152.0000 - val_tn: 5304.0000 - val_fn: 428.0000 - val_accuracy: 0.9120 - val_precision: 0.8233 - val_recall: 0.6232 - val_auc: 0.9115\n",
            "Epoch 22/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.3074 - tp: 70311.0000 - fp: 13876.0000 - tn: 521659.0000 - fn: 46186.0000 - accuracy: 0.9079 - precision: 0.8352 - recall: 0.6035 - auc: 0.9137\n",
            "Epoch 00022: val_auc improved from 0.91152 to 0.91472, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3074 - tp: 70371.0000 - fp: 13885.0000 - tn: 522087.0000 - fn: 46227.0000 - accuracy: 0.9079 - precision: 0.8352 - recall: 0.6035 - auc: 0.9137 - val_loss: 0.3024 - val_tp: 621.0000 - val_fp: 66.0000 - val_tn: 5390.0000 - val_fn: 515.0000 - val_accuracy: 0.9119 - val_precision: 0.9039 - val_recall: 0.5467 - val_auc: 0.9147\n",
            "Epoch 23/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3042 - tp: 70906.0000 - fp: 13787.0000 - tn: 521966.0000 - fn: 45629.0000 - accuracy: 0.9089 - precision: 0.8372 - recall: 0.6085 - auc: 0.9158\n",
            "Epoch 00023: val_auc did not improve from 0.91472\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3042 - tp: 70940.0000 - fp: 13788.0000 - tn: 522184.0000 - fn: 45658.0000 - accuracy: 0.9089 - precision: 0.8373 - recall: 0.6084 - auc: 0.9159 - val_loss: 0.3011 - val_tp: 722.0000 - val_fp: 155.0000 - val_tn: 5301.0000 - val_fn: 414.0000 - val_accuracy: 0.9137 - val_precision: 0.8233 - val_recall: 0.6356 - val_auc: 0.9146\n",
            "Epoch 24/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3020 - tp: 71343.0000 - fp: 13695.0000 - tn: 522257.0000 - fn: 45249.0000 - accuracy: 0.9097 - precision: 0.8390 - recall: 0.6119 - auc: 0.9170\n",
            "Epoch 00024: val_auc improved from 0.91472 to 0.91613, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3020 - tp: 71346.0000 - fp: 13695.0000 - tn: 522277.0000 - fn: 45252.0000 - accuracy: 0.9097 - precision: 0.8390 - recall: 0.6119 - auc: 0.9170 - val_loss: 0.3012 - val_tp: 718.0000 - val_fp: 177.0000 - val_tn: 5279.0000 - val_fn: 418.0000 - val_accuracy: 0.9097 - val_precision: 0.8022 - val_recall: 0.6320 - val_auc: 0.9161\n",
            "Epoch 25/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2993 - tp: 71967.0000 - fp: 13757.0000 - tn: 521993.0000 - fn: 44571.0000 - accuracy: 0.9106 - precision: 0.8395 - recall: 0.6175 - auc: 0.9188\n",
            "Epoch 00025: val_auc improved from 0.91613 to 0.91970, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2993 - tp: 72000.0000 - fp: 13761.0000 - tn: 522211.0000 - fn: 44598.0000 - accuracy: 0.9106 - precision: 0.8395 - recall: 0.6175 - auc: 0.9188 - val_loss: 0.2960 - val_tp: 651.0000 - val_fp: 108.0000 - val_tn: 5348.0000 - val_fn: 485.0000 - val_accuracy: 0.9100 - val_precision: 0.8577 - val_recall: 0.5731 - val_auc: 0.9197\n",
            "Epoch 26/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2967 - tp: 72474.0000 - fp: 13811.0000 - tn: 522161.0000 - fn: 44124.0000 - accuracy: 0.9112 - precision: 0.8399 - recall: 0.6216 - auc: 0.9206\n",
            "Epoch 00026: val_auc improved from 0.91970 to 0.92076, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2967 - tp: 72474.0000 - fp: 13811.0000 - tn: 522161.0000 - fn: 44124.0000 - accuracy: 0.9112 - precision: 0.8399 - recall: 0.6216 - auc: 0.9206 - val_loss: 0.2953 - val_tp: 699.0000 - val_fp: 155.0000 - val_tn: 5301.0000 - val_fn: 437.0000 - val_accuracy: 0.9102 - val_precision: 0.8185 - val_recall: 0.6153 - val_auc: 0.9208\n",
            "Epoch 27/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2941 - tp: 72927.0000 - fp: 13937.0000 - tn: 522011.0000 - fn: 43669.0000 - accuracy: 0.9117 - precision: 0.8396 - recall: 0.6255 - auc: 0.9223\n",
            "Epoch 00027: val_auc improved from 0.92076 to 0.92450, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2941 - tp: 72929.0000 - fp: 13938.0000 - tn: 522034.0000 - fn: 43669.0000 - accuracy: 0.9117 - precision: 0.8395 - recall: 0.6255 - auc: 0.9223 - val_loss: 0.2855 - val_tp: 691.0000 - val_fp: 111.0000 - val_tn: 5345.0000 - val_fn: 445.0000 - val_accuracy: 0.9157 - val_precision: 0.8616 - val_recall: 0.6083 - val_auc: 0.9245\n",
            "Epoch 28/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2919 - tp: 73462.0000 - fp: 13950.0000 - tn: 521790.0000 - fn: 43086.0000 - accuracy: 0.9126 - precision: 0.8404 - recall: 0.6303 - auc: 0.9235\n",
            "Epoch 00028: val_auc improved from 0.92450 to 0.92677, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2919 - tp: 73496.0000 - fp: 13955.0000 - tn: 522017.0000 - fn: 43102.0000 - accuracy: 0.9126 - precision: 0.8404 - recall: 0.6303 - auc: 0.9235 - val_loss: 0.2791 - val_tp: 730.0000 - val_fp: 117.0000 - val_tn: 5339.0000 - val_fn: 406.0000 - val_accuracy: 0.9207 - val_precision: 0.8619 - val_recall: 0.6426 - val_auc: 0.9268\n",
            "Epoch 29/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2903 - tp: 73657.0000 - fp: 13996.0000 - tn: 521526.0000 - fn: 42853.0000 - accuracy: 0.9128 - precision: 0.8403 - recall: 0.6322 - auc: 0.9246\n",
            "Epoch 00029: val_auc did not improve from 0.92677\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2903 - tp: 73704.0000 - fp: 14011.0000 - tn: 521961.0000 - fn: 42894.0000 - accuracy: 0.9128 - precision: 0.8403 - recall: 0.6321 - auc: 0.9246 - val_loss: 0.2892 - val_tp: 641.0000 - val_fp: 73.0000 - val_tn: 5383.0000 - val_fn: 495.0000 - val_accuracy: 0.9138 - val_precision: 0.8978 - val_recall: 0.5643 - val_auc: 0.9250\n",
            "Epoch 30/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2883 - tp: 74034.0000 - fp: 13851.0000 - tn: 521662.0000 - fn: 42485.0000 - accuracy: 0.9136 - precision: 0.8424 - recall: 0.6354 - auc: 0.9260\n",
            "Epoch 00030: val_auc improved from 0.92677 to 0.93204, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2883 - tp: 74084.0000 - fp: 13864.0000 - tn: 522108.0000 - fn: 42514.0000 - accuracy: 0.9136 - precision: 0.8424 - recall: 0.6354 - auc: 0.9260 - val_loss: 0.2763 - val_tp: 687.0000 - val_fp: 94.0000 - val_tn: 5362.0000 - val_fn: 449.0000 - val_accuracy: 0.9176 - val_precision: 0.8796 - val_recall: 0.6048 - val_auc: 0.9320\n",
            "Epoch 31/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2856 - tp: 74505.0000 - fp: 13989.0000 - tn: 521753.0000 - fn: 42041.0000 - accuracy: 0.9141 - precision: 0.8419 - recall: 0.6393 - auc: 0.9277\n",
            "Epoch 00031: val_auc did not improve from 0.93204\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2856 - tp: 74536.0000 - fp: 13993.0000 - tn: 521979.0000 - fn: 42062.0000 - accuracy: 0.9141 - precision: 0.8419 - recall: 0.6393 - auc: 0.9277 - val_loss: 0.2820 - val_tp: 646.0000 - val_fp: 70.0000 - val_tn: 5386.0000 - val_fn: 490.0000 - val_accuracy: 0.9150 - val_precision: 0.9022 - val_recall: 0.5687 - val_auc: 0.9313\n",
            "Epoch 32/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2844 - tp: 74624.0000 - fp: 13917.0000 - tn: 521394.0000 - fn: 41841.0000 - accuracy: 0.9145 - precision: 0.8428 - recall: 0.6407 - auc: 0.9286\n",
            "Epoch 00032: val_auc improved from 0.93204 to 0.93256, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2845 - tp: 74709.0000 - fp: 13934.0000 - tn: 522038.0000 - fn: 41889.0000 - accuracy: 0.9145 - precision: 0.8428 - recall: 0.6407 - auc: 0.9286 - val_loss: 0.2744 - val_tp: 699.0000 - val_fp: 91.0000 - val_tn: 5365.0000 - val_fn: 437.0000 - val_accuracy: 0.9199 - val_precision: 0.8848 - val_recall: 0.6153 - val_auc: 0.9326\n",
            "Epoch 33/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2818 - tp: 75371.0000 - fp: 13864.0000 - tn: 522092.0000 - fn: 41217.0000 - accuracy: 0.9156 - precision: 0.8446 - recall: 0.6465 - auc: 0.9303\n",
            "Epoch 00033: val_auc improved from 0.93256 to 0.93516, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2818 - tp: 75376.0000 - fp: 13864.0000 - tn: 522108.0000 - fn: 41222.0000 - accuracy: 0.9156 - precision: 0.8446 - recall: 0.6465 - auc: 0.9303 - val_loss: 0.2736 - val_tp: 780.0000 - val_fp: 174.0000 - val_tn: 5282.0000 - val_fn: 356.0000 - val_accuracy: 0.9196 - val_precision: 0.8176 - val_recall: 0.6866 - val_auc: 0.9352\n",
            "Epoch 34/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2808 - tp: 75370.0000 - fp: 13748.0000 - tn: 521780.0000 - fn: 41134.0000 - accuracy: 0.9158 - precision: 0.8457 - recall: 0.6469 - auc: 0.9309\n",
            "Epoch 00034: val_auc did not improve from 0.93516\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2808 - tp: 75431.0000 - fp: 13771.0000 - tn: 522201.0000 - fn: 41167.0000 - accuracy: 0.9158 - precision: 0.8456 - recall: 0.6469 - auc: 0.9309 - val_loss: 0.2719 - val_tp: 748.0000 - val_fp: 118.0000 - val_tn: 5338.0000 - val_fn: 388.0000 - val_accuracy: 0.9232 - val_precision: 0.8637 - val_recall: 0.6585 - val_auc: 0.9338\n",
            "Epoch 35/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2780 - tp: 75933.0000 - fp: 13686.0000 - tn: 522267.0000 - fn: 40658.0000 - accuracy: 0.9167 - precision: 0.8473 - recall: 0.6513 - auc: 0.9327\n",
            "Epoch 00035: val_auc improved from 0.93516 to 0.93858, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2780 - tp: 75939.0000 - fp: 13686.0000 - tn: 522286.0000 - fn: 40659.0000 - accuracy: 0.9167 - precision: 0.8473 - recall: 0.6513 - auc: 0.9327 - val_loss: 0.2654 - val_tp: 765.0000 - val_fp: 140.0000 - val_tn: 5316.0000 - val_fn: 371.0000 - val_accuracy: 0.9225 - val_precision: 0.8453 - val_recall: 0.6734 - val_auc: 0.9386\n",
            "Epoch 36/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2769 - tp: 76154.0000 - fp: 13786.0000 - tn: 521955.0000 - fn: 40393.0000 - accuracy: 0.9169 - precision: 0.8467 - recall: 0.6534 - auc: 0.9335\n",
            "Epoch 00036: val_auc did not improve from 0.93858\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2769 - tp: 76186.0000 - fp: 13796.0000 - tn: 522176.0000 - fn: 40412.0000 - accuracy: 0.9169 - precision: 0.8467 - recall: 0.6534 - auc: 0.9335 - val_loss: 0.2747 - val_tp: 795.0000 - val_fp: 172.0000 - val_tn: 5284.0000 - val_fn: 341.0000 - val_accuracy: 0.9222 - val_precision: 0.8221 - val_recall: 0.6998 - val_auc: 0.9341\n",
            "Epoch 37/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2753 - tp: 76410.0000 - fp: 13662.0000 - tn: 521664.0000 - fn: 40040.0000 - accuracy: 0.9176 - precision: 0.8483 - recall: 0.6562 - auc: 0.9345\n",
            "Epoch 00037: val_auc did not improve from 0.93858\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2753 - tp: 76500.0000 - fp: 13676.0000 - tn: 522296.0000 - fn: 40098.0000 - accuracy: 0.9176 - precision: 0.8483 - recall: 0.6561 - auc: 0.9345 - val_loss: 0.2655 - val_tp: 742.0000 - val_fp: 108.0000 - val_tn: 5348.0000 - val_fn: 394.0000 - val_accuracy: 0.9238 - val_precision: 0.8729 - val_recall: 0.6532 - val_auc: 0.9371\n",
            "Epoch 38/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2739 - tp: 76958.0000 - fp: 13782.0000 - tn: 522167.0000 - fn: 39637.0000 - accuracy: 0.9181 - precision: 0.8481 - recall: 0.6600 - auc: 0.9354\n",
            "Epoch 00038: val_auc did not improve from 0.93858\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2739 - tp: 76961.0000 - fp: 13782.0000 - tn: 522190.0000 - fn: 39637.0000 - accuracy: 0.9181 - precision: 0.8481 - recall: 0.6601 - auc: 0.9354 - val_loss: 0.2685 - val_tp: 713.0000 - val_fp: 89.0000 - val_tn: 5367.0000 - val_fn: 423.0000 - val_accuracy: 0.9223 - val_precision: 0.8890 - val_recall: 0.6276 - val_auc: 0.9381\n",
            "Epoch 39/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2730 - tp: 77156.0000 - fp: 13689.0000 - tn: 521838.0000 - fn: 39349.0000 - accuracy: 0.9187 - precision: 0.8493 - recall: 0.6623 - auc: 0.9358\n",
            "Epoch 00039: val_auc improved from 0.93858 to 0.93976, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2730 - tp: 77219.0000 - fp: 13699.0000 - tn: 522273.0000 - fn: 39379.0000 - accuracy: 0.9187 - precision: 0.8493 - recall: 0.6623 - auc: 0.9358 - val_loss: 0.2711 - val_tp: 844.0000 - val_fp: 225.0000 - val_tn: 5231.0000 - val_fn: 292.0000 - val_accuracy: 0.9216 - val_precision: 0.7895 - val_recall: 0.7430 - val_auc: 0.9398\n",
            "Epoch 40/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2715 - tp: 77304.0000 - fp: 13895.0000 - tn: 521397.0000 - fn: 39180.0000 - accuracy: 0.9186 - precision: 0.8476 - recall: 0.6636 - auc: 0.9368\n",
            "Epoch 00040: val_auc improved from 0.93976 to 0.94151, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2715 - tp: 77379.0000 - fp: 13914.0000 - tn: 522058.0000 - fn: 39219.0000 - accuracy: 0.9186 - precision: 0.8476 - recall: 0.6636 - auc: 0.9368 - val_loss: 0.2591 - val_tp: 763.0000 - val_fp: 123.0000 - val_tn: 5333.0000 - val_fn: 373.0000 - val_accuracy: 0.9248 - val_precision: 0.8612 - val_recall: 0.6717 - val_auc: 0.9415\n",
            "Epoch 41/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2705 - tp: 77648.0000 - fp: 13876.0000 - tn: 522075.0000 - fn: 38945.0000 - accuracy: 0.9191 - precision: 0.8484 - recall: 0.6660 - auc: 0.9375\n",
            "Epoch 00041: val_auc improved from 0.94151 to 0.94214, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2705 - tp: 77651.0000 - fp: 13876.0000 - tn: 522096.0000 - fn: 38947.0000 - accuracy: 0.9191 - precision: 0.8484 - recall: 0.6660 - auc: 0.9375 - val_loss: 0.2625 - val_tp: 733.0000 - val_fp: 102.0000 - val_tn: 5354.0000 - val_fn: 403.0000 - val_accuracy: 0.9234 - val_precision: 0.8778 - val_recall: 0.6452 - val_auc: 0.9421\n",
            "Epoch 42/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2691 - tp: 77992.0000 - fp: 13811.0000 - tn: 522161.0000 - fn: 38606.0000 - accuracy: 0.9197 - precision: 0.8496 - recall: 0.6689 - auc: 0.9384\n",
            "Epoch 00042: val_auc improved from 0.94214 to 0.94477, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2691 - tp: 77992.0000 - fp: 13811.0000 - tn: 522161.0000 - fn: 38606.0000 - accuracy: 0.9197 - precision: 0.8496 - recall: 0.6689 - auc: 0.9384 - val_loss: 0.2632 - val_tp: 679.0000 - val_fp: 63.0000 - val_tn: 5393.0000 - val_fn: 457.0000 - val_accuracy: 0.9211 - val_precision: 0.9151 - val_recall: 0.5977 - val_auc: 0.9448\n",
            "Epoch 43/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2678 - tp: 78201.0000 - fp: 13834.0000 - tn: 522138.0000 - fn: 38397.0000 - accuracy: 0.9200 - precision: 0.8497 - recall: 0.6707 - auc: 0.9393\n",
            "Epoch 00043: val_auc did not improve from 0.94477\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2678 - tp: 78201.0000 - fp: 13834.0000 - tn: 522138.0000 - fn: 38397.0000 - accuracy: 0.9200 - precision: 0.8497 - recall: 0.6707 - auc: 0.9393 - val_loss: 0.2857 - val_tp: 878.0000 - val_fp: 305.0000 - val_tn: 5151.0000 - val_fn: 258.0000 - val_accuracy: 0.9146 - val_precision: 0.7422 - val_recall: 0.7729 - val_auc: 0.9382\n",
            "Epoch 44/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2667 - tp: 78513.0000 - fp: 13762.0000 - tn: 522192.0000 - fn: 38077.0000 - accuracy: 0.9206 - precision: 0.8509 - recall: 0.6734 - auc: 0.9399\n",
            "Epoch 00044: val_auc did not improve from 0.94477\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2667 - tp: 78517.0000 - fp: 13762.0000 - tn: 522210.0000 - fn: 38081.0000 - accuracy: 0.9206 - precision: 0.8509 - recall: 0.6734 - auc: 0.9399 - val_loss: 0.2686 - val_tp: 805.0000 - val_fp: 187.0000 - val_tn: 5269.0000 - val_fn: 331.0000 - val_accuracy: 0.9214 - val_precision: 0.8115 - val_recall: 0.7086 - val_auc: 0.9385\n",
            "Epoch 45/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2654 - tp: 78821.0000 - fp: 13864.0000 - tn: 522108.0000 - fn: 37777.0000 - accuracy: 0.9209 - precision: 0.8504 - recall: 0.6760 - auc: 0.9408\n",
            "Epoch 00045: val_auc improved from 0.94477 to 0.94647, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2654 - tp: 78821.0000 - fp: 13864.0000 - tn: 522108.0000 - fn: 37777.0000 - accuracy: 0.9209 - precision: 0.8504 - recall: 0.6760 - auc: 0.9408 - val_loss: 0.2548 - val_tp: 694.0000 - val_fp: 69.0000 - val_tn: 5387.0000 - val_fn: 442.0000 - val_accuracy: 0.9225 - val_precision: 0.9096 - val_recall: 0.6109 - val_auc: 0.9465\n",
            "Epoch 46/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2643 - tp: 78502.0000 - fp: 13430.0000 - tn: 521877.0000 - fn: 37967.0000 - accuracy: 0.9211 - precision: 0.8539 - recall: 0.6740 - auc: 0.9412\n",
            "Epoch 00046: val_auc improved from 0.94647 to 0.94805, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2643 - tp: 78589.0000 - fp: 13451.0000 - tn: 522521.0000 - fn: 38009.0000 - accuracy: 0.9211 - precision: 0.8539 - recall: 0.6740 - auc: 0.9412 - val_loss: 0.2516 - val_tp: 730.0000 - val_fp: 78.0000 - val_tn: 5378.0000 - val_fn: 406.0000 - val_accuracy: 0.9266 - val_precision: 0.9035 - val_recall: 0.6426 - val_auc: 0.9481\n",
            "Epoch 47/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2635 - tp: 78940.0000 - fp: 13738.0000 - tn: 521578.0000 - fn: 37520.0000 - accuracy: 0.9214 - precision: 0.8518 - recall: 0.6778 - auc: 0.9417\n",
            "Epoch 00047: val_auc did not improve from 0.94805\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2635 - tp: 79039.0000 - fp: 13754.0000 - tn: 522218.0000 - fn: 37559.0000 - accuracy: 0.9214 - precision: 0.8518 - recall: 0.6779 - auc: 0.9417 - val_loss: 0.2659 - val_tp: 838.0000 - val_fp: 219.0000 - val_tn: 5237.0000 - val_fn: 298.0000 - val_accuracy: 0.9216 - val_precision: 0.7928 - val_recall: 0.7377 - val_auc: 0.9404\n",
            "Epoch 48/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2623 - tp: 79348.0000 - fp: 13762.0000 - tn: 522210.0000 - fn: 37250.0000 - accuracy: 0.9218 - precision: 0.8522 - recall: 0.6805 - auc: 0.9425\n",
            "Epoch 00048: val_auc improved from 0.94805 to 0.94831, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2623 - tp: 79348.0000 - fp: 13762.0000 - tn: 522210.0000 - fn: 37250.0000 - accuracy: 0.9218 - precision: 0.8522 - recall: 0.6805 - auc: 0.9425 - val_loss: 0.2489 - val_tp: 825.0000 - val_fp: 134.0000 - val_tn: 5322.0000 - val_fn: 311.0000 - val_accuracy: 0.9325 - val_precision: 0.8603 - val_recall: 0.7262 - val_auc: 0.9483\n",
            "Epoch 49/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2613 - tp: 79625.0000 - fp: 13805.0000 - tn: 521944.0000 - fn: 36914.0000 - accuracy: 0.9222 - precision: 0.8522 - recall: 0.6832 - auc: 0.9430\n",
            "Epoch 00049: val_auc improved from 0.94831 to 0.95173, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2613 - tp: 79660.0000 - fp: 13807.0000 - tn: 522165.0000 - fn: 36938.0000 - accuracy: 0.9222 - precision: 0.8523 - recall: 0.6832 - auc: 0.9430 - val_loss: 0.2453 - val_tp: 748.0000 - val_fp: 74.0000 - val_tn: 5382.0000 - val_fn: 388.0000 - val_accuracy: 0.9299 - val_precision: 0.9100 - val_recall: 0.6585 - val_auc: 0.9517\n",
            "Epoch 50/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2609 - tp: 79509.0000 - fp: 13352.0000 - tn: 522620.0000 - fn: 37089.0000 - accuracy: 0.9227 - precision: 0.8562 - recall: 0.6819 - auc: 0.9433\n",
            "Epoch 00050: val_auc did not improve from 0.95173\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2609 - tp: 79509.0000 - fp: 13352.0000 - tn: 522620.0000 - fn: 37089.0000 - accuracy: 0.9227 - precision: 0.8562 - recall: 0.6819 - auc: 0.9433 - val_loss: 0.2443 - val_tp: 795.0000 - val_fp: 119.0000 - val_tn: 5337.0000 - val_fn: 341.0000 - val_accuracy: 0.9302 - val_precision: 0.8698 - val_recall: 0.6998 - val_auc: 0.9504\n",
            "Epoch 51/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2595 - tp: 79716.0000 - fp: 13464.0000 - tn: 522070.0000 - fn: 36782.0000 - accuracy: 0.9229 - precision: 0.8555 - recall: 0.6843 - auc: 0.9443\n",
            "Epoch 00051: val_auc did not improve from 0.95173\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2595 - tp: 79788.0000 - fp: 13473.0000 - tn: 522499.0000 - fn: 36810.0000 - accuracy: 0.9229 - precision: 0.8555 - recall: 0.6843 - auc: 0.9443 - val_loss: 0.2530 - val_tp: 841.0000 - val_fp: 197.0000 - val_tn: 5259.0000 - val_fn: 295.0000 - val_accuracy: 0.9254 - val_precision: 0.8102 - val_recall: 0.7403 - val_auc: 0.9472\n",
            "Epoch 52/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2590 - tp: 80074.0000 - fp: 13678.0000 - tn: 522057.0000 - fn: 36479.0000 - accuracy: 0.9231 - precision: 0.8541 - recall: 0.6870 - auc: 0.9447\n",
            "Epoch 00052: val_auc did not improve from 0.95173\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2590 - tp: 80102.0000 - fp: 13687.0000 - tn: 522285.0000 - fn: 36496.0000 - accuracy: 0.9231 - precision: 0.8541 - recall: 0.6870 - auc: 0.9447 - val_loss: 0.2492 - val_tp: 770.0000 - val_fp: 93.0000 - val_tn: 5363.0000 - val_fn: 366.0000 - val_accuracy: 0.9304 - val_precision: 0.8922 - val_recall: 0.6778 - val_auc: 0.9484\n",
            "Epoch 53/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2578 - tp: 80214.0000 - fp: 13596.0000 - tn: 521715.0000 - fn: 36251.0000 - accuracy: 0.9235 - precision: 0.8551 - recall: 0.6887 - auc: 0.9454\n",
            "Epoch 00053: val_auc improved from 0.95173 to 0.95208, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2577 - tp: 80302.0000 - fp: 13609.0000 - tn: 522363.0000 - fn: 36296.0000 - accuracy: 0.9235 - precision: 0.8551 - recall: 0.6887 - auc: 0.9454 - val_loss: 0.2499 - val_tp: 701.0000 - val_fp: 60.0000 - val_tn: 5396.0000 - val_fn: 435.0000 - val_accuracy: 0.9249 - val_precision: 0.9212 - val_recall: 0.6171 - val_auc: 0.9521\n",
            "Epoch 54/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2575 - tp: 80100.0000 - fp: 13616.0000 - tn: 521936.0000 - fn: 36380.0000 - accuracy: 0.9233 - precision: 0.8547 - recall: 0.6877 - auc: 0.9458\n",
            "Epoch 00054: val_auc did not improve from 0.95208\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2576 - tp: 80168.0000 - fp: 13629.0000 - tn: 522343.0000 - fn: 36430.0000 - accuracy: 0.9233 - precision: 0.8547 - recall: 0.6876 - auc: 0.9458 - val_loss: 0.2470 - val_tp: 833.0000 - val_fp: 172.0000 - val_tn: 5284.0000 - val_fn: 303.0000 - val_accuracy: 0.9279 - val_precision: 0.8289 - val_recall: 0.7333 - val_auc: 0.9499\n",
            "Epoch 55/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2567 - tp: 80636.0000 - fp: 13834.0000 - tn: 522119.0000 - fn: 35955.0000 - accuracy: 0.9237 - precision: 0.8536 - recall: 0.6916 - auc: 0.9463\n",
            "Epoch 00055: val_auc improved from 0.95208 to 0.95217, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2567 - tp: 80640.0000 - fp: 13835.0000 - tn: 522137.0000 - fn: 35958.0000 - accuracy: 0.9237 - precision: 0.8536 - recall: 0.6916 - auc: 0.9463 - val_loss: 0.2411 - val_tp: 806.0000 - val_fp: 106.0000 - val_tn: 5350.0000 - val_fn: 330.0000 - val_accuracy: 0.9339 - val_precision: 0.8838 - val_recall: 0.7095 - val_auc: 0.9522\n",
            "Epoch 56/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2558 - tp: 80722.0000 - fp: 13836.0000 - tn: 521464.0000 - fn: 35754.0000 - accuracy: 0.9239 - precision: 0.8537 - recall: 0.6930 - auc: 0.9468\n",
            "Epoch 00056: val_auc did not improve from 0.95217\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2558 - tp: 80812.0000 - fp: 13864.0000 - tn: 522108.0000 - fn: 35786.0000 - accuracy: 0.9239 - precision: 0.8536 - recall: 0.6931 - auc: 0.9468 - val_loss: 0.2667 - val_tp: 801.0000 - val_fp: 206.0000 - val_tn: 5250.0000 - val_fn: 335.0000 - val_accuracy: 0.9179 - val_precision: 0.7954 - val_recall: 0.7051 - val_auc: 0.9416\n",
            "Epoch 57/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2556 - tp: 80680.0000 - fp: 13733.0000 - tn: 522239.0000 - fn: 35918.0000 - accuracy: 0.9239 - precision: 0.8545 - recall: 0.6920 - auc: 0.9471\n",
            "Epoch 00057: val_auc did not improve from 0.95217\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2556 - tp: 80680.0000 - fp: 13733.0000 - tn: 522239.0000 - fn: 35918.0000 - accuracy: 0.9239 - precision: 0.8545 - recall: 0.6920 - auc: 0.9471 - val_loss: 0.2570 - val_tp: 869.0000 - val_fp: 225.0000 - val_tn: 5231.0000 - val_fn: 267.0000 - val_accuracy: 0.9254 - val_precision: 0.7943 - val_recall: 0.7650 - val_auc: 0.9495\n",
            "Epoch 58/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2543 - tp: 80619.0000 - fp: 13794.0000 - tn: 521753.0000 - fn: 35866.0000 - accuracy: 0.9238 - precision: 0.8539 - recall: 0.6921 - auc: 0.9479\n",
            "Epoch 00058: val_auc improved from 0.95217 to 0.95450, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2544 - tp: 80692.0000 - fp: 13806.0000 - tn: 522166.0000 - fn: 35906.0000 - accuracy: 0.9238 - precision: 0.8539 - recall: 0.6921 - auc: 0.9479 - val_loss: 0.2403 - val_tp: 768.0000 - val_fp: 92.0000 - val_tn: 5364.0000 - val_fn: 368.0000 - val_accuracy: 0.9302 - val_precision: 0.8930 - val_recall: 0.6761 - val_auc: 0.9545\n",
            "Epoch 59/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2539 - tp: 80834.0000 - fp: 13661.0000 - tn: 521874.0000 - fn: 35663.0000 - accuracy: 0.9244 - precision: 0.8554 - recall: 0.6939 - auc: 0.9480\n",
            "Epoch 00059: val_auc did not improve from 0.95450\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2539 - tp: 80910.0000 - fp: 13669.0000 - tn: 522303.0000 - fn: 35688.0000 - accuracy: 0.9244 - precision: 0.8555 - recall: 0.6939 - auc: 0.9480 - val_loss: 0.2506 - val_tp: 724.0000 - val_fp: 81.0000 - val_tn: 5375.0000 - val_fn: 412.0000 - val_accuracy: 0.9252 - val_precision: 0.8994 - val_recall: 0.6373 - val_auc: 0.9488\n",
            "Epoch 60/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2533 - tp: 81377.0000 - fp: 13641.0000 - tn: 522308.0000 - fn: 35218.0000 - accuracy: 0.9251 - precision: 0.8564 - recall: 0.6979 - auc: 0.9482\n",
            "Epoch 00060: val_auc improved from 0.95450 to 0.95593, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2533 - tp: 81379.0000 - fp: 13641.0000 - tn: 522331.0000 - fn: 35219.0000 - accuracy: 0.9251 - precision: 0.8564 - recall: 0.6979 - auc: 0.9482 - val_loss: 0.2390 - val_tp: 763.0000 - val_fp: 80.0000 - val_tn: 5376.0000 - val_fn: 373.0000 - val_accuracy: 0.9313 - val_precision: 0.9051 - val_recall: 0.6717 - val_auc: 0.9559\n",
            "Epoch 61/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 81163.0000 - fp: 13551.0000 - tn: 522401.0000 - fn: 35429.0000 - accuracy: 0.9249 - precision: 0.8569 - recall: 0.6961 - auc: 0.9487\n",
            "Epoch 00061: val_auc did not improve from 0.95593\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2526 - tp: 81168.0000 - fp: 13551.0000 - tn: 522421.0000 - fn: 35430.0000 - accuracy: 0.9249 - precision: 0.8569 - recall: 0.6961 - auc: 0.9487 - val_loss: 0.2417 - val_tp: 842.0000 - val_fp: 147.0000 - val_tn: 5309.0000 - val_fn: 294.0000 - val_accuracy: 0.9331 - val_precision: 0.8514 - val_recall: 0.7412 - val_auc: 0.9541\n",
            "Epoch 62/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2520 - tp: 81267.0000 - fp: 13641.0000 - tn: 521894.0000 - fn: 35230.0000 - accuracy: 0.9250 - precision: 0.8563 - recall: 0.6976 - auc: 0.9492\n",
            "Epoch 00062: val_auc did not improve from 0.95593\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2521 - tp: 81338.0000 - fp: 13646.0000 - tn: 522326.0000 - fn: 35260.0000 - accuracy: 0.9251 - precision: 0.8563 - recall: 0.6976 - auc: 0.9492 - val_loss: 0.2395 - val_tp: 801.0000 - val_fp: 121.0000 - val_tn: 5335.0000 - val_fn: 335.0000 - val_accuracy: 0.9308 - val_precision: 0.8688 - val_recall: 0.7051 - val_auc: 0.9536\n",
            "Epoch 63/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2512 - tp: 81805.0000 - fp: 13853.0000 - tn: 522096.0000 - fn: 34790.0000 - accuracy: 0.9255 - precision: 0.8552 - recall: 0.7016 - auc: 0.9496\n",
            "Epoch 00063: val_auc did not improve from 0.95593\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2512 - tp: 81806.0000 - fp: 13855.0000 - tn: 522117.0000 - fn: 34792.0000 - accuracy: 0.9255 - precision: 0.8552 - recall: 0.7016 - auc: 0.9496 - val_loss: 0.2391 - val_tp: 831.0000 - val_fp: 140.0000 - val_tn: 5316.0000 - val_fn: 305.0000 - val_accuracy: 0.9325 - val_precision: 0.8558 - val_recall: 0.7315 - val_auc: 0.9553\n",
            "Epoch 64/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2513 - tp: 81726.0000 - fp: 13879.0000 - tn: 522074.0000 - fn: 34865.0000 - accuracy: 0.9253 - precision: 0.8548 - recall: 0.7010 - auc: 0.9495\n",
            "Epoch 00064: val_auc improved from 0.95593 to 0.95604, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2513 - tp: 81732.0000 - fp: 13879.0000 - tn: 522093.0000 - fn: 34866.0000 - accuracy: 0.9253 - precision: 0.8548 - recall: 0.7010 - auc: 0.9495 - val_loss: 0.2372 - val_tp: 764.0000 - val_fp: 84.0000 - val_tn: 5372.0000 - val_fn: 372.0000 - val_accuracy: 0.9308 - val_precision: 0.9009 - val_recall: 0.6725 - val_auc: 0.9560\n",
            "Epoch 65/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2500 - tp: 81899.0000 - fp: 13827.0000 - tn: 521694.0000 - fn: 34612.0000 - accuracy: 0.9257 - precision: 0.8556 - recall: 0.7029 - auc: 0.9505\n",
            "Epoch 00065: val_auc improved from 0.95604 to 0.95677, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2500 - tp: 81962.0000 - fp: 13842.0000 - tn: 522130.0000 - fn: 34636.0000 - accuracy: 0.9257 - precision: 0.8555 - recall: 0.7029 - auc: 0.9505 - val_loss: 0.2341 - val_tp: 826.0000 - val_fp: 122.0000 - val_tn: 5334.0000 - val_fn: 310.0000 - val_accuracy: 0.9345 - val_precision: 0.8713 - val_recall: 0.7271 - val_auc: 0.9568\n",
            "Epoch 66/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2499 - tp: 81957.0000 - fp: 13723.0000 - tn: 522229.0000 - fn: 34635.0000 - accuracy: 0.9259 - precision: 0.8566 - recall: 0.7029 - auc: 0.9504\n",
            "Epoch 00066: val_auc did not improve from 0.95677\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2499 - tp: 81959.0000 - fp: 13723.0000 - tn: 522249.0000 - fn: 34639.0000 - accuracy: 0.9259 - precision: 0.8566 - recall: 0.7029 - auc: 0.9504 - val_loss: 0.2348 - val_tp: 827.0000 - val_fp: 135.0000 - val_tn: 5321.0000 - val_fn: 309.0000 - val_accuracy: 0.9326 - val_precision: 0.8597 - val_recall: 0.7280 - val_auc: 0.9562\n",
            "Epoch 67/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2491 - tp: 81833.0000 - fp: 13609.0000 - tn: 521934.0000 - fn: 34656.0000 - accuracy: 0.9260 - precision: 0.8574 - recall: 0.7025 - auc: 0.9509\n",
            "Epoch 00067: val_auc improved from 0.95677 to 0.95933, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2491 - tp: 81903.0000 - fp: 13614.0000 - tn: 522358.0000 - fn: 34695.0000 - accuracy: 0.9260 - precision: 0.8575 - recall: 0.7024 - auc: 0.9509 - val_loss: 0.2325 - val_tp: 799.0000 - val_fp: 97.0000 - val_tn: 5359.0000 - val_fn: 337.0000 - val_accuracy: 0.9342 - val_precision: 0.8917 - val_recall: 0.7033 - val_auc: 0.9593\n",
            "Epoch 68/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2488 - tp: 82256.0000 - fp: 13885.0000 - tn: 522087.0000 - fn: 34342.0000 - accuracy: 0.9261 - precision: 0.8556 - recall: 0.7055 - auc: 0.9511\n",
            "Epoch 00068: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2488 - tp: 82256.0000 - fp: 13885.0000 - tn: 522087.0000 - fn: 34342.0000 - accuracy: 0.9261 - precision: 0.8556 - recall: 0.7055 - auc: 0.9511 - val_loss: 0.2325 - val_tp: 802.0000 - val_fp: 95.0000 - val_tn: 5361.0000 - val_fn: 334.0000 - val_accuracy: 0.9349 - val_precision: 0.8941 - val_recall: 0.7060 - val_auc: 0.9568\n",
            "Epoch 69/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2488 - tp: 82191.0000 - fp: 13897.0000 - tn: 521618.0000 - fn: 34326.0000 - accuracy: 0.9260 - precision: 0.8554 - recall: 0.7054 - auc: 0.9511\n",
            "Epoch 00069: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2488 - tp: 82250.0000 - fp: 13909.0000 - tn: 522063.0000 - fn: 34348.0000 - accuracy: 0.9261 - precision: 0.8554 - recall: 0.7054 - auc: 0.9511 - val_loss: 0.2444 - val_tp: 859.0000 - val_fp: 172.0000 - val_tn: 5284.0000 - val_fn: 277.0000 - val_accuracy: 0.9319 - val_precision: 0.8332 - val_recall: 0.7562 - val_auc: 0.9546\n",
            "Epoch 70/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2475 - tp: 82437.0000 - fp: 13536.0000 - tn: 522436.0000 - fn: 34161.0000 - accuracy: 0.9269 - precision: 0.8590 - recall: 0.7070 - auc: 0.9518\n",
            "Epoch 00070: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2475 - tp: 82437.0000 - fp: 13536.0000 - tn: 522436.0000 - fn: 34161.0000 - accuracy: 0.9269 - precision: 0.8590 - recall: 0.7070 - auc: 0.9518 - val_loss: 0.2367 - val_tp: 826.0000 - val_fp: 153.0000 - val_tn: 5303.0000 - val_fn: 310.0000 - val_accuracy: 0.9298 - val_precision: 0.8437 - val_recall: 0.7271 - val_auc: 0.9551\n",
            "Epoch 71/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2471 - tp: 82788.0000 - fp: 13782.0000 - tn: 522169.0000 - fn: 33805.0000 - accuracy: 0.9271 - precision: 0.8573 - recall: 0.7101 - auc: 0.9519\n",
            "Epoch 00071: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2471 - tp: 82791.0000 - fp: 13783.0000 - tn: 522189.0000 - fn: 33807.0000 - accuracy: 0.9271 - precision: 0.8573 - recall: 0.7101 - auc: 0.9519 - val_loss: 0.2380 - val_tp: 807.0000 - val_fp: 108.0000 - val_tn: 5348.0000 - val_fn: 329.0000 - val_accuracy: 0.9337 - val_precision: 0.8820 - val_recall: 0.7104 - val_auc: 0.9536\n",
            "Epoch 72/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2467 - tp: 82494.0000 - fp: 13825.0000 - tn: 521714.0000 - fn: 33999.0000 - accuracy: 0.9267 - precision: 0.8565 - recall: 0.7081 - auc: 0.9524\n",
            "Epoch 00072: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2467 - tp: 82570.0000 - fp: 13834.0000 - tn: 522138.0000 - fn: 34028.0000 - accuracy: 0.9267 - precision: 0.8565 - recall: 0.7082 - auc: 0.9524 - val_loss: 0.2324 - val_tp: 804.0000 - val_fp: 108.0000 - val_tn: 5348.0000 - val_fn: 332.0000 - val_accuracy: 0.9333 - val_precision: 0.8816 - val_recall: 0.7077 - val_auc: 0.9587\n",
            "Epoch 73/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2457 - tp: 82644.0000 - fp: 13433.0000 - tn: 522312.0000 - fn: 33899.0000 - accuracy: 0.9274 - precision: 0.8602 - recall: 0.7091 - auc: 0.9527\n",
            "Epoch 00073: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2457 - tp: 82685.0000 - fp: 13437.0000 - tn: 522535.0000 - fn: 33913.0000 - accuracy: 0.9274 - precision: 0.8602 - recall: 0.7091 - auc: 0.9527 - val_loss: 0.2320 - val_tp: 804.0000 - val_fp: 97.0000 - val_tn: 5359.0000 - val_fn: 332.0000 - val_accuracy: 0.9349 - val_precision: 0.8923 - val_recall: 0.7077 - val_auc: 0.9572\n",
            "Epoch 74/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2459 - tp: 82895.0000 - fp: 13708.0000 - tn: 522264.0000 - fn: 33703.0000 - accuracy: 0.9273 - precision: 0.8581 - recall: 0.7109 - auc: 0.9528\n",
            "Epoch 00074: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2459 - tp: 82895.0000 - fp: 13708.0000 - tn: 522264.0000 - fn: 33703.0000 - accuracy: 0.9273 - precision: 0.8581 - recall: 0.7109 - auc: 0.9528 - val_loss: 0.2381 - val_tp: 890.0000 - val_fp: 207.0000 - val_tn: 5249.0000 - val_fn: 246.0000 - val_accuracy: 0.9313 - val_precision: 0.8113 - val_recall: 0.7835 - val_auc: 0.9591\n",
            "Epoch 75/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2452 - tp: 82774.0000 - fp: 13590.0000 - tn: 521954.0000 - fn: 33714.0000 - accuracy: 0.9275 - precision: 0.8590 - recall: 0.7106 - auc: 0.9530\n",
            "Epoch 00075: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2452 - tp: 82855.0000 - fp: 13595.0000 - tn: 522377.0000 - fn: 33743.0000 - accuracy: 0.9275 - precision: 0.8590 - recall: 0.7106 - auc: 0.9530 - val_loss: 0.2419 - val_tp: 756.0000 - val_fp: 90.0000 - val_tn: 5366.0000 - val_fn: 380.0000 - val_accuracy: 0.9287 - val_precision: 0.8936 - val_recall: 0.6655 - val_auc: 0.9550\n",
            "Epoch 76/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2453 - tp: 83120.0000 - fp: 13546.0000 - tn: 522426.0000 - fn: 33478.0000 - accuracy: 0.9279 - precision: 0.8599 - recall: 0.7129 - auc: 0.9529\n",
            "Epoch 00076: val_auc did not improve from 0.95933\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2453 - tp: 83120.0000 - fp: 13546.0000 - tn: 522426.0000 - fn: 33478.0000 - accuracy: 0.9279 - precision: 0.8599 - recall: 0.7129 - auc: 0.9529 - val_loss: 0.2383 - val_tp: 737.0000 - val_fp: 75.0000 - val_tn: 5381.0000 - val_fn: 399.0000 - val_accuracy: 0.9281 - val_precision: 0.9076 - val_recall: 0.6488 - val_auc: 0.9570\n",
            "Epoch 77/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2449 - tp: 82917.0000 - fp: 13497.0000 - tn: 522454.0000 - fn: 33676.0000 - accuracy: 0.9277 - precision: 0.8600 - recall: 0.7112 - auc: 0.9532\n",
            "Epoch 00077: val_auc improved from 0.95933 to 0.96011, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2449 - tp: 82921.0000 - fp: 13497.0000 - tn: 522475.0000 - fn: 33677.0000 - accuracy: 0.9277 - precision: 0.8600 - recall: 0.7112 - auc: 0.9532 - val_loss: 0.2286 - val_tp: 810.0000 - val_fp: 95.0000 - val_tn: 5361.0000 - val_fn: 326.0000 - val_accuracy: 0.9361 - val_precision: 0.8950 - val_recall: 0.7130 - val_auc: 0.9601\n",
            "Epoch 78/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2438 - tp: 83185.0000 - fp: 13536.0000 - tn: 522436.0000 - fn: 33413.0000 - accuracy: 0.9281 - precision: 0.8601 - recall: 0.7134 - auc: 0.9539\n",
            "Epoch 00078: val_auc did not improve from 0.96011\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2438 - tp: 83185.0000 - fp: 13536.0000 - tn: 522436.0000 - fn: 33413.0000 - accuracy: 0.9281 - precision: 0.8601 - recall: 0.7134 - auc: 0.9539 - val_loss: 0.2401 - val_tp: 757.0000 - val_fp: 67.0000 - val_tn: 5389.0000 - val_fn: 379.0000 - val_accuracy: 0.9323 - val_precision: 0.9187 - val_recall: 0.6664 - val_auc: 0.9562\n",
            "Epoch 79/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2435 - tp: 83258.0000 - fp: 13607.0000 - tn: 521699.0000 - fn: 33212.0000 - accuracy: 0.9282 - precision: 0.8595 - recall: 0.7148 - auc: 0.9541\n",
            "Epoch 00079: val_auc improved from 0.96011 to 0.96219, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2434 - tp: 83359.0000 - fp: 13619.0000 - tn: 522353.0000 - fn: 33239.0000 - accuracy: 0.9282 - precision: 0.8596 - recall: 0.7149 - auc: 0.9541 - val_loss: 0.2291 - val_tp: 786.0000 - val_fp: 91.0000 - val_tn: 5365.0000 - val_fn: 350.0000 - val_accuracy: 0.9331 - val_precision: 0.8962 - val_recall: 0.6919 - val_auc: 0.9622\n",
            "Epoch 80/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2437 - tp: 83166.0000 - fp: 13528.0000 - tn: 522005.0000 - fn: 33333.0000 - accuracy: 0.9281 - precision: 0.8601 - recall: 0.7139 - auc: 0.9539\n",
            "Epoch 00080: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2437 - tp: 83231.0000 - fp: 13541.0000 - tn: 522431.0000 - fn: 33367.0000 - accuracy: 0.9281 - precision: 0.8601 - recall: 0.7138 - auc: 0.9539 - val_loss: 0.2294 - val_tp: 847.0000 - val_fp: 129.0000 - val_tn: 5327.0000 - val_fn: 289.0000 - val_accuracy: 0.9366 - val_precision: 0.8678 - val_recall: 0.7456 - val_auc: 0.9608\n",
            "Epoch 81/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2429 - tp: 83400.0000 - fp: 13597.0000 - tn: 522375.0000 - fn: 33198.0000 - accuracy: 0.9283 - precision: 0.8598 - recall: 0.7153 - auc: 0.9545\n",
            "Epoch 00081: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2429 - tp: 83400.0000 - fp: 13597.0000 - tn: 522375.0000 - fn: 33198.0000 - accuracy: 0.9283 - precision: 0.8598 - recall: 0.7153 - auc: 0.9545 - val_loss: 0.2417 - val_tp: 725.0000 - val_fp: 51.0000 - val_tn: 5405.0000 - val_fn: 411.0000 - val_accuracy: 0.9299 - val_precision: 0.9343 - val_recall: 0.6382 - val_auc: 0.9610\n",
            "Epoch 82/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2427 - tp: 83334.0000 - fp: 13620.0000 - tn: 521902.0000 - fn: 33176.0000 - accuracy: 0.9282 - precision: 0.8595 - recall: 0.7153 - auc: 0.9545\n",
            "Epoch 00082: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2428 - tp: 83389.0000 - fp: 13641.0000 - tn: 522331.0000 - fn: 33209.0000 - accuracy: 0.9282 - precision: 0.8594 - recall: 0.7152 - auc: 0.9545 - val_loss: 0.2334 - val_tp: 842.0000 - val_fp: 153.0000 - val_tn: 5303.0000 - val_fn: 294.0000 - val_accuracy: 0.9322 - val_precision: 0.8462 - val_recall: 0.7412 - val_auc: 0.9587\n",
            "Epoch 83/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2415 - tp: 83799.0000 - fp: 13506.0000 - tn: 522466.0000 - fn: 32799.0000 - accuracy: 0.9290 - precision: 0.8612 - recall: 0.7187 - auc: 0.9551\n",
            "Epoch 00083: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2415 - tp: 83799.0000 - fp: 13506.0000 - tn: 522466.0000 - fn: 32799.0000 - accuracy: 0.9290 - precision: 0.8612 - recall: 0.7187 - auc: 0.9551 - val_loss: 0.2310 - val_tp: 878.0000 - val_fp: 161.0000 - val_tn: 5295.0000 - val_fn: 258.0000 - val_accuracy: 0.9364 - val_precision: 0.8450 - val_recall: 0.7729 - val_auc: 0.9603\n",
            "Epoch 84/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2412 - tp: 83724.0000 - fp: 13554.0000 - tn: 521788.0000 - fn: 32710.0000 - accuracy: 0.9290 - precision: 0.8607 - recall: 0.7191 - auc: 0.9553\n",
            "Epoch 00084: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2412 - tp: 83844.0000 - fp: 13566.0000 - tn: 522406.0000 - fn: 32754.0000 - accuracy: 0.9290 - precision: 0.8607 - recall: 0.7191 - auc: 0.9553 - val_loss: 0.2398 - val_tp: 815.0000 - val_fp: 127.0000 - val_tn: 5329.0000 - val_fn: 321.0000 - val_accuracy: 0.9320 - val_precision: 0.8652 - val_recall: 0.7174 - val_auc: 0.9541\n",
            "Epoch 85/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2412 - tp: 83623.0000 - fp: 13614.0000 - tn: 522358.0000 - fn: 32975.0000 - accuracy: 0.9286 - precision: 0.8600 - recall: 0.7172 - auc: 0.9554\n",
            "Epoch 00085: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2412 - tp: 83623.0000 - fp: 13614.0000 - tn: 522358.0000 - fn: 32975.0000 - accuracy: 0.9286 - precision: 0.8600 - recall: 0.7172 - auc: 0.9554 - val_loss: 0.2361 - val_tp: 774.0000 - val_fp: 89.0000 - val_tn: 5367.0000 - val_fn: 362.0000 - val_accuracy: 0.9316 - val_precision: 0.8969 - val_recall: 0.6813 - val_auc: 0.9567\n",
            "Epoch 86/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2410 - tp: 83546.0000 - fp: 13539.0000 - tn: 521567.0000 - fn: 32868.0000 - accuracy: 0.9288 - precision: 0.8605 - recall: 0.7177 - auc: 0.9557\n",
            "Epoch 00086: val_auc did not improve from 0.96219\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2410 - tp: 83678.0000 - fp: 13560.0000 - tn: 522412.0000 - fn: 32920.0000 - accuracy: 0.9288 - precision: 0.8605 - recall: 0.7177 - auc: 0.9557 - val_loss: 0.2368 - val_tp: 887.0000 - val_fp: 187.0000 - val_tn: 5269.0000 - val_fn: 249.0000 - val_accuracy: 0.9339 - val_precision: 0.8259 - val_recall: 0.7808 - val_auc: 0.9593\n",
            "Epoch 87/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2410 - tp: 83742.0000 - fp: 13518.0000 - tn: 522224.0000 - fn: 32804.0000 - accuracy: 0.9290 - precision: 0.8610 - recall: 0.7185 - auc: 0.9556\n",
            "Epoch 00087: val_auc improved from 0.96219 to 0.96329, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2410 - tp: 83779.0000 - fp: 13525.0000 - tn: 522447.0000 - fn: 32819.0000 - accuracy: 0.9290 - precision: 0.8610 - recall: 0.7185 - auc: 0.9556 - val_loss: 0.2229 - val_tp: 842.0000 - val_fp: 125.0000 - val_tn: 5331.0000 - val_fn: 294.0000 - val_accuracy: 0.9364 - val_precision: 0.8707 - val_recall: 0.7412 - val_auc: 0.9633\n",
            "Epoch 88/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2399 - tp: 84082.0000 - fp: 13521.0000 - tn: 522430.0000 - fn: 32511.0000 - accuracy: 0.9295 - precision: 0.8615 - recall: 0.7212 - auc: 0.9561\n",
            "Epoch 00088: val_auc did not improve from 0.96329\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2399 - tp: 84085.0000 - fp: 13522.0000 - tn: 522450.0000 - fn: 32513.0000 - accuracy: 0.9295 - precision: 0.8615 - recall: 0.7212 - auc: 0.9561 - val_loss: 0.2400 - val_tp: 754.0000 - val_fp: 72.0000 - val_tn: 5384.0000 - val_fn: 382.0000 - val_accuracy: 0.9311 - val_precision: 0.9128 - val_recall: 0.6637 - val_auc: 0.9542\n",
            "Epoch 89/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2400 - tp: 84040.0000 - fp: 13412.0000 - tn: 522325.0000 - fn: 32511.0000 - accuracy: 0.9296 - precision: 0.8624 - recall: 0.7211 - auc: 0.9559\n",
            "Epoch 00089: val_auc did not improve from 0.96329\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2400 - tp: 84071.0000 - fp: 13420.0000 - tn: 522552.0000 - fn: 32527.0000 - accuracy: 0.9296 - precision: 0.8623 - recall: 0.7210 - auc: 0.9559 - val_loss: 0.2343 - val_tp: 919.0000 - val_fp: 232.0000 - val_tn: 5224.0000 - val_fn: 217.0000 - val_accuracy: 0.9319 - val_precision: 0.7984 - val_recall: 0.8090 - val_auc: 0.9619\n",
            "Epoch 90/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2401 - tp: 84163.0000 - fp: 13699.0000 - tn: 522273.0000 - fn: 32435.0000 - accuracy: 0.9293 - precision: 0.8600 - recall: 0.7218 - auc: 0.9561\n",
            "Epoch 00090: val_auc did not improve from 0.96329\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2401 - tp: 84163.0000 - fp: 13699.0000 - tn: 522273.0000 - fn: 32435.0000 - accuracy: 0.9293 - precision: 0.8600 - recall: 0.7218 - auc: 0.9561 - val_loss: 0.2345 - val_tp: 881.0000 - val_fp: 187.0000 - val_tn: 5269.0000 - val_fn: 255.0000 - val_accuracy: 0.9329 - val_precision: 0.8249 - val_recall: 0.7755 - val_auc: 0.9588\n",
            "Epoch 91/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2397 - tp: 83912.0000 - fp: 13459.0000 - tn: 521855.0000 - fn: 32550.0000 - accuracy: 0.9294 - precision: 0.8618 - recall: 0.7205 - auc: 0.9565\n",
            "Epoch 00091: val_auc did not improve from 0.96329\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2397 - tp: 84014.0000 - fp: 13475.0000 - tn: 522497.0000 - fn: 32584.0000 - accuracy: 0.9294 - precision: 0.8618 - recall: 0.7205 - auc: 0.9565 - val_loss: 0.2275 - val_tp: 804.0000 - val_fp: 106.0000 - val_tn: 5350.0000 - val_fn: 332.0000 - val_accuracy: 0.9336 - val_precision: 0.8835 - val_recall: 0.7077 - val_auc: 0.9601\n",
            "Epoch 92/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2390 - tp: 84035.0000 - fp: 13604.0000 - tn: 521734.0000 - fn: 32403.0000 - accuracy: 0.9294 - precision: 0.8607 - recall: 0.7217 - auc: 0.9566\n",
            "Epoch 00092: val_auc did not improve from 0.96329\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2390 - tp: 84144.0000 - fp: 13614.0000 - tn: 522358.0000 - fn: 32454.0000 - accuracy: 0.9294 - precision: 0.8607 - recall: 0.7217 - auc: 0.9566 - val_loss: 0.2409 - val_tp: 870.0000 - val_fp: 192.0000 - val_tn: 5264.0000 - val_fn: 266.0000 - val_accuracy: 0.9305 - val_precision: 0.8192 - val_recall: 0.7658 - val_auc: 0.9565\n",
            "Epoch 93/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2386 - tp: 84355.0000 - fp: 13552.0000 - tn: 522420.0000 - fn: 32243.0000 - accuracy: 0.9298 - precision: 0.8616 - recall: 0.7235 - auc: 0.9569\n",
            "Epoch 00093: val_auc did not improve from 0.96329\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2386 - tp: 84355.0000 - fp: 13552.0000 - tn: 522420.0000 - fn: 32243.0000 - accuracy: 0.9298 - precision: 0.8616 - recall: 0.7235 - auc: 0.9569 - val_loss: 0.2285 - val_tp: 872.0000 - val_fp: 170.0000 - val_tn: 5286.0000 - val_fn: 264.0000 - val_accuracy: 0.9342 - val_precision: 0.8369 - val_recall: 0.7676 - val_auc: 0.9608\n",
            "Epoch 94/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2382 - tp: 84403.0000 - fp: 13670.0000 - tn: 522302.0000 - fn: 32195.0000 - accuracy: 0.9297 - precision: 0.8606 - recall: 0.7239 - auc: 0.9571\n",
            "Epoch 00094: val_auc improved from 0.96329 to 0.96379, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2382 - tp: 84403.0000 - fp: 13670.0000 - tn: 522302.0000 - fn: 32195.0000 - accuracy: 0.9297 - precision: 0.8606 - recall: 0.7239 - auc: 0.9571 - val_loss: 0.2227 - val_tp: 823.0000 - val_fp: 109.0000 - val_tn: 5347.0000 - val_fn: 313.0000 - val_accuracy: 0.9360 - val_precision: 0.8830 - val_recall: 0.7245 - val_auc: 0.9638\n",
            "Epoch 95/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2387 - tp: 84316.0000 - fp: 13565.0000 - tn: 522407.0000 - fn: 32282.0000 - accuracy: 0.9297 - precision: 0.8614 - recall: 0.7231 - auc: 0.9568\n",
            "Epoch 00095: val_auc did not improve from 0.96379\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2387 - tp: 84316.0000 - fp: 13565.0000 - tn: 522407.0000 - fn: 32282.0000 - accuracy: 0.9297 - precision: 0.8614 - recall: 0.7231 - auc: 0.9568 - val_loss: 0.2444 - val_tp: 871.0000 - val_fp: 184.0000 - val_tn: 5272.0000 - val_fn: 265.0000 - val_accuracy: 0.9319 - val_precision: 0.8256 - val_recall: 0.7667 - val_auc: 0.9578\n",
            "Epoch 96/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2378 - tp: 84505.0000 - fp: 13523.0000 - tn: 522007.0000 - fn: 31997.0000 - accuracy: 0.9302 - precision: 0.8620 - recall: 0.7254 - auc: 0.9573\n",
            "Epoch 00096: val_auc did not improve from 0.96379\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2378 - tp: 84578.0000 - fp: 13541.0000 - tn: 522431.0000 - fn: 32020.0000 - accuracy: 0.9302 - precision: 0.8620 - recall: 0.7254 - auc: 0.9573 - val_loss: 0.2244 - val_tp: 876.0000 - val_fp: 146.0000 - val_tn: 5310.0000 - val_fn: 260.0000 - val_accuracy: 0.9384 - val_precision: 0.8571 - val_recall: 0.7711 - val_auc: 0.9627\n",
            "Epoch 97/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2375 - tp: 84546.0000 - fp: 13640.0000 - tn: 522314.0000 - fn: 32044.0000 - accuracy: 0.9300 - precision: 0.8611 - recall: 0.7252 - auc: 0.9578\n",
            "Epoch 00097: val_auc did not improve from 0.96379\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2375 - tp: 84551.0000 - fp: 13640.0000 - tn: 522332.0000 - fn: 32047.0000 - accuracy: 0.9300 - precision: 0.8611 - recall: 0.7251 - auc: 0.9578 - val_loss: 0.2268 - val_tp: 782.0000 - val_fp: 87.0000 - val_tn: 5369.0000 - val_fn: 354.0000 - val_accuracy: 0.9331 - val_precision: 0.8999 - val_recall: 0.6884 - val_auc: 0.9633\n",
            "Epoch 98/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2369 - tp: 84688.0000 - fp: 13801.0000 - tn: 522171.0000 - fn: 31910.0000 - accuracy: 0.9300 - precision: 0.8599 - recall: 0.7263 - auc: 0.9578\n",
            "Epoch 00098: val_auc improved from 0.96379 to 0.96459, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2369 - tp: 84688.0000 - fp: 13801.0000 - tn: 522171.0000 - fn: 31910.0000 - accuracy: 0.9300 - precision: 0.8599 - recall: 0.7263 - auc: 0.9578 - val_loss: 0.2224 - val_tp: 835.0000 - val_fp: 122.0000 - val_tn: 5334.0000 - val_fn: 301.0000 - val_accuracy: 0.9358 - val_precision: 0.8725 - val_recall: 0.7350 - val_auc: 0.9646\n",
            "Epoch 99/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2374 - tp: 84631.0000 - fp: 13625.0000 - tn: 522325.0000 - fn: 31963.0000 - accuracy: 0.9301 - precision: 0.8613 - recall: 0.7259 - auc: 0.9577\n",
            "Epoch 00099: val_auc did not improve from 0.96459\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2374 - tp: 84634.0000 - fp: 13625.0000 - tn: 522347.0000 - fn: 31964.0000 - accuracy: 0.9301 - precision: 0.8613 - recall: 0.7259 - auc: 0.9577 - val_loss: 0.2225 - val_tp: 847.0000 - val_fp: 132.0000 - val_tn: 5324.0000 - val_fn: 289.0000 - val_accuracy: 0.9361 - val_precision: 0.8652 - val_recall: 0.7456 - val_auc: 0.9642\n",
            "Epoch 100/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2365 - tp: 84977.0000 - fp: 13693.0000 - tn: 522279.0000 - fn: 31621.0000 - accuracy: 0.9306 - precision: 0.8612 - recall: 0.7288 - auc: 0.9582\n",
            "Epoch 00100: val_auc did not improve from 0.96459\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2365 - tp: 84977.0000 - fp: 13693.0000 - tn: 522279.0000 - fn: 31621.0000 - accuracy: 0.9306 - precision: 0.8612 - recall: 0.7288 - auc: 0.9582 - val_loss: 0.2227 - val_tp: 805.0000 - val_fp: 98.0000 - val_tn: 5358.0000 - val_fn: 331.0000 - val_accuracy: 0.9349 - val_precision: 0.8915 - val_recall: 0.7086 - val_auc: 0.9639\n",
            "Epoch 101/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2357 - tp: 85142.0000 - fp: 13596.0000 - tn: 522354.0000 - fn: 31452.0000 - accuracy: 0.9310 - precision: 0.8623 - recall: 0.7302 - auc: 0.9584\n",
            "Epoch 00101: val_auc did not improve from 0.96459\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2357 - tp: 85145.0000 - fp: 13597.0000 - tn: 522375.0000 - fn: 31453.0000 - accuracy: 0.9310 - precision: 0.8623 - recall: 0.7302 - auc: 0.9584 - val_loss: 0.2294 - val_tp: 861.0000 - val_fp: 162.0000 - val_tn: 5294.0000 - val_fn: 275.0000 - val_accuracy: 0.9337 - val_precision: 0.8416 - val_recall: 0.7579 - val_auc: 0.9598\n",
            "Epoch 102/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2362 - tp: 84599.0000 - fp: 13364.0000 - tn: 522185.0000 - fn: 31884.0000 - accuracy: 0.9306 - precision: 0.8636 - recall: 0.7263 - auc: 0.9581\n",
            "Epoch 00102: val_auc improved from 0.96459 to 0.96491, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2362 - tp: 84677.0000 - fp: 13374.0000 - tn: 522598.0000 - fn: 31921.0000 - accuracy: 0.9306 - precision: 0.8636 - recall: 0.7262 - auc: 0.9581 - val_loss: 0.2209 - val_tp: 815.0000 - val_fp: 100.0000 - val_tn: 5356.0000 - val_fn: 321.0000 - val_accuracy: 0.9361 - val_precision: 0.8907 - val_recall: 0.7174 - val_auc: 0.9649\n",
            "Epoch 103/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2356 - tp: 85052.0000 - fp: 13689.0000 - tn: 521835.0000 - fn: 31456.0000 - accuracy: 0.9308 - precision: 0.8614 - recall: 0.7300 - auc: 0.9587\n",
            "Epoch 00103: val_auc did not improve from 0.96491\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2356 - tp: 85113.0000 - fp: 13698.0000 - tn: 522274.0000 - fn: 31485.0000 - accuracy: 0.9308 - precision: 0.8614 - recall: 0.7300 - auc: 0.9587 - val_loss: 0.2301 - val_tp: 755.0000 - val_fp: 69.0000 - val_tn: 5387.0000 - val_fn: 381.0000 - val_accuracy: 0.9317 - val_precision: 0.9163 - val_recall: 0.6646 - val_auc: 0.9631\n",
            "Epoch 104/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2353 - tp: 84950.0000 - fp: 13634.0000 - tn: 521674.0000 - fn: 31518.0000 - accuracy: 0.9307 - precision: 0.8617 - recall: 0.7294 - auc: 0.9587\n",
            "Epoch 00104: val_auc improved from 0.96491 to 0.96510, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2353 - tp: 85045.0000 - fp: 13647.0000 - tn: 522325.0000 - fn: 31553.0000 - accuracy: 0.9307 - precision: 0.8617 - recall: 0.7294 - auc: 0.9587 - val_loss: 0.2274 - val_tp: 762.0000 - val_fp: 82.0000 - val_tn: 5374.0000 - val_fn: 374.0000 - val_accuracy: 0.9308 - val_precision: 0.9028 - val_recall: 0.6708 - val_auc: 0.9651\n",
            "Epoch 105/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2356 - tp: 84822.0000 - fp: 13482.0000 - tn: 522062.0000 - fn: 31666.0000 - accuracy: 0.9308 - precision: 0.8629 - recall: 0.7282 - auc: 0.9587\n",
            "Epoch 00105: val_auc did not improve from 0.96510\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2356 - tp: 84901.0000 - fp: 13489.0000 - tn: 522483.0000 - fn: 31697.0000 - accuracy: 0.9308 - precision: 0.8629 - recall: 0.7282 - auc: 0.9587 - val_loss: 0.2249 - val_tp: 835.0000 - val_fp: 117.0000 - val_tn: 5339.0000 - val_fn: 301.0000 - val_accuracy: 0.9366 - val_precision: 0.8771 - val_recall: 0.7350 - val_auc: 0.9637\n",
            "Epoch 106/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2348 - tp: 85107.0000 - fp: 13469.0000 - tn: 522059.0000 - fn: 31397.0000 - accuracy: 0.9312 - precision: 0.8634 - recall: 0.7305 - auc: 0.9590\n",
            "Epoch 00106: val_auc did not improve from 0.96510\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2348 - tp: 85162.0000 - fp: 13482.0000 - tn: 522490.0000 - fn: 31436.0000 - accuracy: 0.9312 - precision: 0.8633 - recall: 0.7304 - auc: 0.9590 - val_loss: 0.2297 - val_tp: 772.0000 - val_fp: 85.0000 - val_tn: 5371.0000 - val_fn: 364.0000 - val_accuracy: 0.9319 - val_precision: 0.9008 - val_recall: 0.6796 - val_auc: 0.9624\n",
            "Epoch 107/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2347 - tp: 85084.0000 - fp: 13312.0000 - tn: 522637.0000 - fn: 31511.0000 - accuracy: 0.9313 - precision: 0.8647 - recall: 0.7297 - auc: 0.9590\n",
            "Epoch 00107: val_auc did not improve from 0.96510\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2347 - tp: 85086.0000 - fp: 13313.0000 - tn: 522659.0000 - fn: 31512.0000 - accuracy: 0.9313 - precision: 0.8647 - recall: 0.7297 - auc: 0.9590 - val_loss: 0.2234 - val_tp: 848.0000 - val_fp: 126.0000 - val_tn: 5330.0000 - val_fn: 288.0000 - val_accuracy: 0.9372 - val_precision: 0.8706 - val_recall: 0.7465 - val_auc: 0.9620\n",
            "Epoch 108/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2341 - tp: 85196.0000 - fp: 13440.0000 - tn: 522091.0000 - fn: 31305.0000 - accuracy: 0.9314 - precision: 0.8637 - recall: 0.7313 - auc: 0.9593\n",
            "Epoch 00108: val_auc did not improve from 0.96510\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2342 - tp: 85268.0000 - fp: 13451.0000 - tn: 522521.0000 - fn: 31330.0000 - accuracy: 0.9314 - precision: 0.8637 - recall: 0.7313 - auc: 0.9593 - val_loss: 0.2224 - val_tp: 787.0000 - val_fp: 84.0000 - val_tn: 5372.0000 - val_fn: 349.0000 - val_accuracy: 0.9343 - val_precision: 0.9036 - val_recall: 0.6928 - val_auc: 0.9639\n",
            "Epoch 109/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2346 - tp: 85138.0000 - fp: 13722.0000 - tn: 521607.0000 - fn: 31309.0000 - accuracy: 0.9309 - precision: 0.8612 - recall: 0.7311 - auc: 0.9593\n",
            "Epoch 00109: val_auc did not improve from 0.96510\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2346 - tp: 85240.0000 - fp: 13731.0000 - tn: 522241.0000 - fn: 31358.0000 - accuracy: 0.9309 - precision: 0.8613 - recall: 0.7311 - auc: 0.9593 - val_loss: 0.2287 - val_tp: 805.0000 - val_fp: 118.0000 - val_tn: 5338.0000 - val_fn: 331.0000 - val_accuracy: 0.9319 - val_precision: 0.8722 - val_recall: 0.7086 - val_auc: 0.9610\n",
            "Epoch 110/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2339 - tp: 85118.0000 - fp: 13451.0000 - tn: 522500.0000 - fn: 31475.0000 - accuracy: 0.9312 - precision: 0.8635 - recall: 0.7300 - auc: 0.9595\n",
            "Epoch 00110: val_auc improved from 0.96510 to 0.96606, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2339 - tp: 85122.0000 - fp: 13453.0000 - tn: 522519.0000 - fn: 31476.0000 - accuracy: 0.9312 - precision: 0.8635 - recall: 0.7300 - auc: 0.9595 - val_loss: 0.2194 - val_tp: 825.0000 - val_fp: 112.0000 - val_tn: 5344.0000 - val_fn: 311.0000 - val_accuracy: 0.9358 - val_precision: 0.8805 - val_recall: 0.7262 - val_auc: 0.9661\n",
            "Epoch 111/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2341 - tp: 85218.0000 - fp: 13504.0000 - tn: 522450.0000 - fn: 31372.0000 - accuracy: 0.9312 - precision: 0.8632 - recall: 0.7309 - auc: 0.9595\n",
            "Epoch 00111: val_auc did not improve from 0.96606\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2341 - tp: 85224.0000 - fp: 13505.0000 - tn: 522467.0000 - fn: 31374.0000 - accuracy: 0.9312 - precision: 0.8632 - recall: 0.7309 - auc: 0.9595 - val_loss: 0.2240 - val_tp: 890.0000 - val_fp: 172.0000 - val_tn: 5284.0000 - val_fn: 246.0000 - val_accuracy: 0.9366 - val_precision: 0.8380 - val_recall: 0.7835 - val_auc: 0.9636\n",
            "Epoch 112/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2338 - tp: 85377.0000 - fp: 13652.0000 - tn: 521678.0000 - fn: 31069.0000 - accuracy: 0.9314 - precision: 0.8621 - recall: 0.7332 - auc: 0.9596\n",
            "Epoch 00112: val_auc did not improve from 0.96606\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2339 - tp: 85488.0000 - fp: 13682.0000 - tn: 522290.0000 - fn: 31110.0000 - accuracy: 0.9314 - precision: 0.8620 - recall: 0.7332 - auc: 0.9596 - val_loss: 0.2317 - val_tp: 903.0000 - val_fp: 213.0000 - val_tn: 5243.0000 - val_fn: 233.0000 - val_accuracy: 0.9323 - val_precision: 0.8091 - val_recall: 0.7949 - val_auc: 0.9621\n",
            "Epoch 113/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2337 - tp: 85259.0000 - fp: 13527.0000 - tn: 522223.0000 - fn: 31279.0000 - accuracy: 0.9313 - precision: 0.8631 - recall: 0.7316 - auc: 0.9597\n",
            "Epoch 00113: val_auc did not improve from 0.96606\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2337 - tp: 85298.0000 - fp: 13528.0000 - tn: 522444.0000 - fn: 31300.0000 - accuracy: 0.9313 - precision: 0.8631 - recall: 0.7316 - auc: 0.9597 - val_loss: 0.2250 - val_tp: 837.0000 - val_fp: 129.0000 - val_tn: 5327.0000 - val_fn: 299.0000 - val_accuracy: 0.9351 - val_precision: 0.8665 - val_recall: 0.7368 - val_auc: 0.9648\n",
            "Epoch 114/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2332 - tp: 85433.0000 - fp: 13449.0000 - tn: 522296.0000 - fn: 31110.0000 - accuracy: 0.9317 - precision: 0.8640 - recall: 0.7331 - auc: 0.9600\n",
            "Epoch 00114: val_auc did not improve from 0.96606\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2332 - tp: 85474.0000 - fp: 13453.0000 - tn: 522519.0000 - fn: 31124.0000 - accuracy: 0.9317 - precision: 0.8640 - recall: 0.7331 - auc: 0.9600 - val_loss: 0.2206 - val_tp: 859.0000 - val_fp: 130.0000 - val_tn: 5326.0000 - val_fn: 277.0000 - val_accuracy: 0.9383 - val_precision: 0.8686 - val_recall: 0.7562 - val_auc: 0.9652\n",
            "Epoch 115/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2332 - tp: 85494.0000 - fp: 13785.0000 - tn: 521729.0000 - fn: 31024.0000 - accuracy: 0.9313 - precision: 0.8611 - recall: 0.7337 - auc: 0.9600\n",
            "Epoch 00115: val_auc improved from 0.96606 to 0.96684, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2331 - tp: 85557.0000 - fp: 13806.0000 - tn: 522166.0000 - fn: 31041.0000 - accuracy: 0.9313 - precision: 0.8611 - recall: 0.7338 - auc: 0.9600 - val_loss: 0.2200 - val_tp: 827.0000 - val_fp: 112.0000 - val_tn: 5344.0000 - val_fn: 309.0000 - val_accuracy: 0.9361 - val_precision: 0.8807 - val_recall: 0.7280 - val_auc: 0.9668\n",
            "Epoch 116/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2327 - tp: 85511.0000 - fp: 13391.0000 - tn: 522355.0000 - fn: 31031.0000 - accuracy: 0.9319 - precision: 0.8646 - recall: 0.7337 - auc: 0.9603\n",
            "Epoch 00116: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2327 - tp: 85549.0000 - fp: 13395.0000 - tn: 522577.0000 - fn: 31049.0000 - accuracy: 0.9319 - precision: 0.8646 - recall: 0.7337 - auc: 0.9603 - val_loss: 0.2213 - val_tp: 811.0000 - val_fp: 91.0000 - val_tn: 5365.0000 - val_fn: 325.0000 - val_accuracy: 0.9369 - val_precision: 0.8991 - val_recall: 0.7139 - val_auc: 0.9648\n",
            "Epoch 117/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2322 - tp: 85681.0000 - fp: 13438.0000 - tn: 522534.0000 - fn: 30917.0000 - accuracy: 0.9320 - precision: 0.8644 - recall: 0.7348 - auc: 0.9604\n",
            "Epoch 00117: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2322 - tp: 85681.0000 - fp: 13438.0000 - tn: 522534.0000 - fn: 30917.0000 - accuracy: 0.9320 - precision: 0.8644 - recall: 0.7348 - auc: 0.9604 - val_loss: 0.2217 - val_tp: 843.0000 - val_fp: 116.0000 - val_tn: 5340.0000 - val_fn: 293.0000 - val_accuracy: 0.9380 - val_precision: 0.8790 - val_recall: 0.7421 - val_auc: 0.9651\n",
            "Epoch 118/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2329 - tp: 85498.0000 - fp: 13597.0000 - tn: 521937.0000 - fn: 31000.0000 - accuracy: 0.9316 - precision: 0.8628 - recall: 0.7339 - auc: 0.9601\n",
            "Epoch 00118: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2329 - tp: 85571.0000 - fp: 13609.0000 - tn: 522363.0000 - fn: 31027.0000 - accuracy: 0.9316 - precision: 0.8628 - recall: 0.7339 - auc: 0.9601 - val_loss: 0.2248 - val_tp: 846.0000 - val_fp: 128.0000 - val_tn: 5328.0000 - val_fn: 290.0000 - val_accuracy: 0.9366 - val_precision: 0.8686 - val_recall: 0.7447 - val_auc: 0.9632\n",
            "Epoch 119/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2329 - tp: 85621.0000 - fp: 13615.0000 - tn: 522357.0000 - fn: 30977.0000 - accuracy: 0.9317 - precision: 0.8628 - recall: 0.7343 - auc: 0.9602\n",
            "Epoch 00119: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2329 - tp: 85621.0000 - fp: 13615.0000 - tn: 522357.0000 - fn: 30977.0000 - accuracy: 0.9317 - precision: 0.8628 - recall: 0.7343 - auc: 0.9602 - val_loss: 0.2278 - val_tp: 816.0000 - val_fp: 106.0000 - val_tn: 5350.0000 - val_fn: 320.0000 - val_accuracy: 0.9354 - val_precision: 0.8850 - val_recall: 0.7183 - val_auc: 0.9615\n",
            "Epoch 120/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2325 - tp: 85564.0000 - fp: 13615.0000 - tn: 522337.0000 - fn: 31028.0000 - accuracy: 0.9316 - precision: 0.8627 - recall: 0.7339 - auc: 0.9607\n",
            "Epoch 00120: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2325 - tp: 85570.0000 - fp: 13615.0000 - tn: 522357.0000 - fn: 31028.0000 - accuracy: 0.9316 - precision: 0.8627 - recall: 0.7339 - auc: 0.9607 - val_loss: 0.2291 - val_tp: 902.0000 - val_fp: 190.0000 - val_tn: 5266.0000 - val_fn: 234.0000 - val_accuracy: 0.9357 - val_precision: 0.8260 - val_recall: 0.7940 - val_auc: 0.9644\n",
            "Epoch 121/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2323 - tp: 85842.0000 - fp: 13541.0000 - tn: 522201.0000 - fn: 30704.0000 - accuracy: 0.9322 - precision: 0.8637 - recall: 0.7366 - auc: 0.9605\n",
            "Epoch 00121: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2323 - tp: 85878.0000 - fp: 13545.0000 - tn: 522427.0000 - fn: 30720.0000 - accuracy: 0.9322 - precision: 0.8638 - recall: 0.7365 - auc: 0.9605 - val_loss: 0.2295 - val_tp: 800.0000 - val_fp: 100.0000 - val_tn: 5356.0000 - val_fn: 336.0000 - val_accuracy: 0.9339 - val_precision: 0.8889 - val_recall: 0.7042 - val_auc: 0.9611\n",
            "Epoch 122/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2320 - tp: 85776.0000 - fp: 13534.0000 - tn: 522415.0000 - fn: 30819.0000 - accuracy: 0.9320 - precision: 0.8637 - recall: 0.7357 - auc: 0.9608\n",
            "Epoch 00122: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2320 - tp: 85778.0000 - fp: 13535.0000 - tn: 522437.0000 - fn: 30820.0000 - accuracy: 0.9320 - precision: 0.8637 - recall: 0.7357 - auc: 0.9608 - val_loss: 0.2313 - val_tp: 770.0000 - val_fp: 79.0000 - val_tn: 5377.0000 - val_fn: 366.0000 - val_accuracy: 0.9325 - val_precision: 0.9069 - val_recall: 0.6778 - val_auc: 0.9628\n",
            "Epoch 123/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2315 - tp: 85630.0000 - fp: 13318.0000 - tn: 522425.0000 - fn: 30915.0000 - accuracy: 0.9322 - precision: 0.8654 - recall: 0.7347 - auc: 0.9609\n",
            "Epoch 00123: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2315 - tp: 85669.0000 - fp: 13324.0000 - tn: 522648.0000 - fn: 30929.0000 - accuracy: 0.9322 - precision: 0.8654 - recall: 0.7347 - auc: 0.9609 - val_loss: 0.2225 - val_tp: 791.0000 - val_fp: 76.0000 - val_tn: 5380.0000 - val_fn: 345.0000 - val_accuracy: 0.9361 - val_precision: 0.9123 - val_recall: 0.6963 - val_auc: 0.9662\n",
            "Epoch 124/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2317 - tp: 86008.0000 - fp: 13532.0000 - tn: 522419.0000 - fn: 30585.0000 - accuracy: 0.9324 - precision: 0.8641 - recall: 0.7377 - auc: 0.9608\n",
            "Epoch 00124: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2317 - tp: 86012.0000 - fp: 13533.0000 - tn: 522439.0000 - fn: 30586.0000 - accuracy: 0.9324 - precision: 0.8641 - recall: 0.7377 - auc: 0.9608 - val_loss: 0.2207 - val_tp: 796.0000 - val_fp: 79.0000 - val_tn: 5377.0000 - val_fn: 340.0000 - val_accuracy: 0.9364 - val_precision: 0.9097 - val_recall: 0.7007 - val_auc: 0.9666\n",
            "Epoch 125/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2316 - tp: 85781.0000 - fp: 13402.0000 - tn: 522570.0000 - fn: 30817.0000 - accuracy: 0.9322 - precision: 0.8649 - recall: 0.7357 - auc: 0.9611\n",
            "Epoch 00125: val_auc did not improve from 0.96684\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2316 - tp: 85781.0000 - fp: 13402.0000 - tn: 522570.0000 - fn: 30817.0000 - accuracy: 0.9322 - precision: 0.8649 - recall: 0.7357 - auc: 0.9611 - val_loss: 0.2228 - val_tp: 790.0000 - val_fp: 82.0000 - val_tn: 5374.0000 - val_fn: 346.0000 - val_accuracy: 0.9351 - val_precision: 0.9060 - val_recall: 0.6954 - val_auc: 0.9653\n",
            "Epoch 126/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2317 - tp: 85857.0000 - fp: 13453.0000 - tn: 522498.0000 - fn: 30736.0000 - accuracy: 0.9323 - precision: 0.8645 - recall: 0.7364 - auc: 0.9610\n",
            "Epoch 00126: val_auc improved from 0.96684 to 0.96772, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2317 - tp: 85860.0000 - fp: 13453.0000 - tn: 522519.0000 - fn: 30738.0000 - accuracy: 0.9323 - precision: 0.8645 - recall: 0.7364 - auc: 0.9610 - val_loss: 0.2165 - val_tp: 832.0000 - val_fp: 123.0000 - val_tn: 5333.0000 - val_fn: 304.0000 - val_accuracy: 0.9352 - val_precision: 0.8712 - val_recall: 0.7324 - val_auc: 0.9677\n",
            "Epoch 127/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2314 - tp: 86141.0000 - fp: 13564.0000 - tn: 522385.0000 - fn: 30454.0000 - accuracy: 0.9325 - precision: 0.8640 - recall: 0.7388 - auc: 0.9610\n",
            "Epoch 00127: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2314 - tp: 86144.0000 - fp: 13565.0000 - tn: 522407.0000 - fn: 30454.0000 - accuracy: 0.9325 - precision: 0.8640 - recall: 0.7388 - auc: 0.9610 - val_loss: 0.2188 - val_tp: 851.0000 - val_fp: 121.0000 - val_tn: 5335.0000 - val_fn: 285.0000 - val_accuracy: 0.9384 - val_precision: 0.8755 - val_recall: 0.7491 - val_auc: 0.9649\n",
            "Epoch 128/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2304 - tp: 86087.0000 - fp: 13426.0000 - tn: 522526.0000 - fn: 30505.0000 - accuracy: 0.9327 - precision: 0.8651 - recall: 0.7384 - auc: 0.9615\n",
            "Epoch 00128: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2304 - tp: 86090.0000 - fp: 13426.0000 - tn: 522546.0000 - fn: 30508.0000 - accuracy: 0.9327 - precision: 0.8651 - recall: 0.7383 - auc: 0.9615 - val_loss: 0.2195 - val_tp: 850.0000 - val_fp: 122.0000 - val_tn: 5334.0000 - val_fn: 286.0000 - val_accuracy: 0.9381 - val_precision: 0.8745 - val_recall: 0.7482 - val_auc: 0.9658\n",
            "Epoch 129/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2311 - tp: 85991.0000 - fp: 13507.0000 - tn: 521829.0000 - fn: 30449.0000 - accuracy: 0.9326 - precision: 0.8642 - recall: 0.7385 - auc: 0.9612\n",
            "Epoch 00129: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2311 - tp: 86107.0000 - fp: 13522.0000 - tn: 522450.0000 - fn: 30491.0000 - accuracy: 0.9326 - precision: 0.8643 - recall: 0.7385 - auc: 0.9612 - val_loss: 0.2153 - val_tp: 836.0000 - val_fp: 98.0000 - val_tn: 5358.0000 - val_fn: 300.0000 - val_accuracy: 0.9396 - val_precision: 0.8951 - val_recall: 0.7359 - val_auc: 0.9675\n",
            "Epoch 130/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2312 - tp: 86029.0000 - fp: 13682.0000 - tn: 521855.0000 - fn: 30466.0000 - accuracy: 0.9323 - precision: 0.8628 - recall: 0.7385 - auc: 0.9614\n",
            "Epoch 00130: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2312 - tp: 86107.0000 - fp: 13699.0000 - tn: 522273.0000 - fn: 30491.0000 - accuracy: 0.9323 - precision: 0.8627 - recall: 0.7385 - auc: 0.9614 - val_loss: 0.2239 - val_tp: 845.0000 - val_fp: 142.0000 - val_tn: 5314.0000 - val_fn: 291.0000 - val_accuracy: 0.9343 - val_precision: 0.8561 - val_recall: 0.7438 - val_auc: 0.9649\n",
            "Epoch 131/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2300 - tp: 86401.0000 - fp: 13548.0000 - tn: 522401.0000 - fn: 30194.0000 - accuracy: 0.9330 - precision: 0.8645 - recall: 0.7410 - auc: 0.9617\n",
            "Epoch 00131: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2300 - tp: 86403.0000 - fp: 13548.0000 - tn: 522424.0000 - fn: 30195.0000 - accuracy: 0.9330 - precision: 0.8645 - recall: 0.7410 - auc: 0.9617 - val_loss: 0.2192 - val_tp: 850.0000 - val_fp: 111.0000 - val_tn: 5345.0000 - val_fn: 286.0000 - val_accuracy: 0.9398 - val_precision: 0.8845 - val_recall: 0.7482 - val_auc: 0.9648\n",
            "Epoch 132/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2303 - tp: 86261.0000 - fp: 13558.0000 - tn: 522389.0000 - fn: 30336.0000 - accuracy: 0.9327 - precision: 0.8642 - recall: 0.7398 - auc: 0.9617\n",
            "Epoch 00132: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2303 - tp: 86262.0000 - fp: 13559.0000 - tn: 522413.0000 - fn: 30336.0000 - accuracy: 0.9327 - precision: 0.8642 - recall: 0.7398 - auc: 0.9617 - val_loss: 0.2234 - val_tp: 797.0000 - val_fp: 71.0000 - val_tn: 5385.0000 - val_fn: 339.0000 - val_accuracy: 0.9378 - val_precision: 0.9182 - val_recall: 0.7016 - val_auc: 0.9659\n",
            "Epoch 133/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2296 - tp: 86644.0000 - fp: 13715.0000 - tn: 522240.0000 - fn: 29945.0000 - accuracy: 0.9331 - precision: 0.8633 - recall: 0.7432 - auc: 0.9620\n",
            "Epoch 00133: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2296 - tp: 86649.0000 - fp: 13715.0000 - tn: 522257.0000 - fn: 29949.0000 - accuracy: 0.9331 - precision: 0.8633 - recall: 0.7431 - auc: 0.9620 - val_loss: 0.2242 - val_tp: 865.0000 - val_fp: 146.0000 - val_tn: 5310.0000 - val_fn: 271.0000 - val_accuracy: 0.9367 - val_precision: 0.8556 - val_recall: 0.7614 - val_auc: 0.9650\n",
            "Epoch 134/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2302 - tp: 86252.0000 - fp: 13620.0000 - tn: 522130.0000 - fn: 30286.0000 - accuracy: 0.9327 - precision: 0.8636 - recall: 0.7401 - auc: 0.9618\n",
            "Epoch 00134: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2302 - tp: 86289.0000 - fp: 13624.0000 - tn: 522348.0000 - fn: 30309.0000 - accuracy: 0.9327 - precision: 0.8636 - recall: 0.7401 - auc: 0.9618 - val_loss: 0.2169 - val_tp: 827.0000 - val_fp: 100.0000 - val_tn: 5356.0000 - val_fn: 309.0000 - val_accuracy: 0.9380 - val_precision: 0.8921 - val_recall: 0.7280 - val_auc: 0.9671\n",
            "Epoch 135/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2299 - tp: 86396.0000 - fp: 13557.0000 - tn: 522415.0000 - fn: 30202.0000 - accuracy: 0.9329 - precision: 0.8644 - recall: 0.7410 - auc: 0.9618\n",
            "Epoch 00135: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2299 - tp: 86396.0000 - fp: 13557.0000 - tn: 522415.0000 - fn: 30202.0000 - accuracy: 0.9329 - precision: 0.8644 - recall: 0.7410 - auc: 0.9618 - val_loss: 0.2215 - val_tp: 834.0000 - val_fp: 105.0000 - val_tn: 5351.0000 - val_fn: 302.0000 - val_accuracy: 0.9383 - val_precision: 0.8882 - val_recall: 0.7342 - val_auc: 0.9659\n",
            "Epoch 136/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2292 - tp: 86477.0000 - fp: 13684.0000 - tn: 522271.0000 - fn: 30112.0000 - accuracy: 0.9329 - precision: 0.8634 - recall: 0.7417 - auc: 0.9624\n",
            "Epoch 00136: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2292 - tp: 86481.0000 - fp: 13684.0000 - tn: 522288.0000 - fn: 30117.0000 - accuracy: 0.9329 - precision: 0.8634 - recall: 0.7417 - auc: 0.9624 - val_loss: 0.2308 - val_tp: 792.0000 - val_fp: 84.0000 - val_tn: 5372.0000 - val_fn: 344.0000 - val_accuracy: 0.9351 - val_precision: 0.9041 - val_recall: 0.6972 - val_auc: 0.9628\n",
            "Epoch 137/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2297 - tp: 86451.0000 - fp: 13773.0000 - tn: 522199.0000 - fn: 30147.0000 - accuracy: 0.9327 - precision: 0.8626 - recall: 0.7414 - auc: 0.9623\n",
            "Epoch 00137: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2297 - tp: 86451.0000 - fp: 13773.0000 - tn: 522199.0000 - fn: 30147.0000 - accuracy: 0.9327 - precision: 0.8626 - recall: 0.7414 - auc: 0.9623 - val_loss: 0.2233 - val_tp: 895.0000 - val_fp: 170.0000 - val_tn: 5286.0000 - val_fn: 241.0000 - val_accuracy: 0.9377 - val_precision: 0.8404 - val_recall: 0.7879 - val_auc: 0.9663\n",
            "Epoch 138/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2296 - tp: 86352.0000 - fp: 13571.0000 - tn: 522379.0000 - fn: 30242.0000 - accuracy: 0.9329 - precision: 0.8642 - recall: 0.7406 - auc: 0.9621\n",
            "Epoch 00138: val_auc did not improve from 0.96772\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2296 - tp: 86355.0000 - fp: 13571.0000 - tn: 522401.0000 - fn: 30243.0000 - accuracy: 0.9329 - precision: 0.8642 - recall: 0.7406 - auc: 0.9621 - val_loss: 0.2179 - val_tp: 874.0000 - val_fp: 134.0000 - val_tn: 5322.0000 - val_fn: 262.0000 - val_accuracy: 0.9399 - val_precision: 0.8671 - val_recall: 0.7694 - val_auc: 0.9669\n",
            "Epoch 139/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2290 - tp: 86527.0000 - fp: 13637.0000 - tn: 522096.0000 - fn: 30028.0000 - accuracy: 0.9331 - precision: 0.8639 - recall: 0.7424 - auc: 0.9625\n",
            "Epoch 00139: val_auc improved from 0.96772 to 0.96805, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2290 - tp: 86559.0000 - fp: 13641.0000 - tn: 522331.0000 - fn: 30039.0000 - accuracy: 0.9331 - precision: 0.8639 - recall: 0.7424 - auc: 0.9625 - val_loss: 0.2148 - val_tp: 861.0000 - val_fp: 120.0000 - val_tn: 5336.0000 - val_fn: 275.0000 - val_accuracy: 0.9401 - val_precision: 0.8777 - val_recall: 0.7579 - val_auc: 0.9680\n",
            "Epoch 140/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2286 - tp: 86737.0000 - fp: 13504.0000 - tn: 522446.0000 - fn: 29857.0000 - accuracy: 0.9336 - precision: 0.8653 - recall: 0.7439 - auc: 0.9626\n",
            "Epoch 00140: val_auc improved from 0.96805 to 0.96964, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2286 - tp: 86741.0000 - fp: 13504.0000 - tn: 522468.0000 - fn: 29857.0000 - accuracy: 0.9336 - precision: 0.8653 - recall: 0.7439 - auc: 0.9626 - val_loss: 0.2132 - val_tp: 881.0000 - val_fp: 124.0000 - val_tn: 5332.0000 - val_fn: 255.0000 - val_accuracy: 0.9425 - val_precision: 0.8766 - val_recall: 0.7755 - val_auc: 0.9696\n",
            "Epoch 141/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2286 - tp: 86604.0000 - fp: 13444.0000 - tn: 522504.0000 - fn: 29992.0000 - accuracy: 0.9334 - precision: 0.8656 - recall: 0.7428 - auc: 0.9626\n",
            "Epoch 00141: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2286 - tp: 86606.0000 - fp: 13446.0000 - tn: 522526.0000 - fn: 29992.0000 - accuracy: 0.9334 - precision: 0.8656 - recall: 0.7428 - auc: 0.9626 - val_loss: 0.2159 - val_tp: 887.0000 - val_fp: 136.0000 - val_tn: 5320.0000 - val_fn: 249.0000 - val_accuracy: 0.9416 - val_precision: 0.8671 - val_recall: 0.7808 - val_auc: 0.9674\n",
            "Epoch 142/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2286 - tp: 86541.0000 - fp: 13540.0000 - tn: 522411.0000 - fn: 30052.0000 - accuracy: 0.9332 - precision: 0.8647 - recall: 0.7422 - auc: 0.9628\n",
            "Epoch 00142: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2286 - tp: 86545.0000 - fp: 13540.0000 - tn: 522432.0000 - fn: 30053.0000 - accuracy: 0.9332 - precision: 0.8647 - recall: 0.7423 - auc: 0.9628 - val_loss: 0.2159 - val_tp: 873.0000 - val_fp: 127.0000 - val_tn: 5329.0000 - val_fn: 263.0000 - val_accuracy: 0.9408 - val_precision: 0.8730 - val_recall: 0.7685 - val_auc: 0.9667\n",
            "Epoch 143/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2284 - tp: 86609.0000 - fp: 13436.0000 - tn: 522536.0000 - fn: 29989.0000 - accuracy: 0.9335 - precision: 0.8657 - recall: 0.7428 - auc: 0.9627\n",
            "Epoch 00143: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2284 - tp: 86609.0000 - fp: 13436.0000 - tn: 522536.0000 - fn: 29989.0000 - accuracy: 0.9335 - precision: 0.8657 - recall: 0.7428 - auc: 0.9627 - val_loss: 0.2187 - val_tp: 863.0000 - val_fp: 140.0000 - val_tn: 5316.0000 - val_fn: 273.0000 - val_accuracy: 0.9373 - val_precision: 0.8604 - val_recall: 0.7597 - val_auc: 0.9658\n",
            "Epoch 144/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2276 - tp: 86841.0000 - fp: 13472.0000 - tn: 522476.0000 - fn: 29755.0000 - accuracy: 0.9338 - precision: 0.8657 - recall: 0.7448 - auc: 0.9630\n",
            "Epoch 00144: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2276 - tp: 86843.0000 - fp: 13472.0000 - tn: 522500.0000 - fn: 29755.0000 - accuracy: 0.9338 - precision: 0.8657 - recall: 0.7448 - auc: 0.9630 - val_loss: 0.2151 - val_tp: 843.0000 - val_fp: 103.0000 - val_tn: 5353.0000 - val_fn: 293.0000 - val_accuracy: 0.9399 - val_precision: 0.8911 - val_recall: 0.7421 - val_auc: 0.9669\n",
            "Epoch 145/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2274 - tp: 86727.0000 - fp: 13288.0000 - tn: 522237.0000 - fn: 29780.0000 - accuracy: 0.9339 - precision: 0.8671 - recall: 0.7444 - auc: 0.9632\n",
            "Epoch 00145: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2274 - tp: 86795.0000 - fp: 13294.0000 - tn: 522678.0000 - fn: 29803.0000 - accuracy: 0.9340 - precision: 0.8672 - recall: 0.7444 - auc: 0.9632 - val_loss: 0.2245 - val_tp: 764.0000 - val_fp: 56.0000 - val_tn: 5400.0000 - val_fn: 372.0000 - val_accuracy: 0.9351 - val_precision: 0.9317 - val_recall: 0.6725 - val_auc: 0.9656\n",
            "Epoch 146/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2279 - tp: 86824.0000 - fp: 13518.0000 - tn: 521816.0000 - fn: 29618.0000 - accuracy: 0.9338 - precision: 0.8653 - recall: 0.7456 - auc: 0.9631\n",
            "Epoch 00146: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2279 - tp: 86942.0000 - fp: 13532.0000 - tn: 522440.0000 - fn: 29656.0000 - accuracy: 0.9338 - precision: 0.8653 - recall: 0.7457 - auc: 0.9631 - val_loss: 0.2149 - val_tp: 873.0000 - val_fp: 139.0000 - val_tn: 5317.0000 - val_fn: 263.0000 - val_accuracy: 0.9390 - val_precision: 0.8626 - val_recall: 0.7685 - val_auc: 0.9685\n",
            "Epoch 147/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2274 - tp: 86912.0000 - fp: 13593.0000 - tn: 522359.0000 - fn: 29680.0000 - accuracy: 0.9337 - precision: 0.8648 - recall: 0.7454 - auc: 0.9632\n",
            "Epoch 00147: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2274 - tp: 86918.0000 - fp: 13594.0000 - tn: 522378.0000 - fn: 29680.0000 - accuracy: 0.9337 - precision: 0.8648 - recall: 0.7455 - auc: 0.9632 - val_loss: 0.2189 - val_tp: 845.0000 - val_fp: 121.0000 - val_tn: 5335.0000 - val_fn: 291.0000 - val_accuracy: 0.9375 - val_precision: 0.8747 - val_recall: 0.7438 - val_auc: 0.9668\n",
            "Epoch 148/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2277 - tp: 86817.0000 - fp: 13493.0000 - tn: 522479.0000 - fn: 29781.0000 - accuracy: 0.9337 - precision: 0.8655 - recall: 0.7446 - auc: 0.9631\n",
            "Epoch 00148: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2277 - tp: 86817.0000 - fp: 13493.0000 - tn: 522479.0000 - fn: 29781.0000 - accuracy: 0.9337 - precision: 0.8655 - recall: 0.7446 - auc: 0.9631 - val_loss: 0.2208 - val_tp: 798.0000 - val_fp: 90.0000 - val_tn: 5366.0000 - val_fn: 338.0000 - val_accuracy: 0.9351 - val_precision: 0.8986 - val_recall: 0.7025 - val_auc: 0.9660\n",
            "Epoch 149/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2274 - tp: 86472.0000 - fp: 13180.0000 - tn: 521929.0000 - fn: 29939.0000 - accuracy: 0.9338 - precision: 0.8677 - recall: 0.7428 - auc: 0.9634\n",
            "Epoch 00149: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2274 - tp: 86619.0000 - fp: 13194.0000 - tn: 522778.0000 - fn: 29979.0000 - accuracy: 0.9338 - precision: 0.8678 - recall: 0.7429 - auc: 0.9634 - val_loss: 0.2250 - val_tp: 781.0000 - val_fp: 88.0000 - val_tn: 5368.0000 - val_fn: 355.0000 - val_accuracy: 0.9328 - val_precision: 0.8987 - val_recall: 0.6875 - val_auc: 0.9660\n",
            "Epoch 150/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2273 - tp: 86856.0000 - fp: 13565.0000 - tn: 522168.0000 - fn: 29699.0000 - accuracy: 0.9337 - precision: 0.8649 - recall: 0.7452 - auc: 0.9635\n",
            "Epoch 00150: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2273 - tp: 86891.0000 - fp: 13573.0000 - tn: 522399.0000 - fn: 29707.0000 - accuracy: 0.9337 - precision: 0.8649 - recall: 0.7452 - auc: 0.9635 - val_loss: 0.2140 - val_tp: 859.0000 - val_fp: 95.0000 - val_tn: 5361.0000 - val_fn: 277.0000 - val_accuracy: 0.9436 - val_precision: 0.9004 - val_recall: 0.7562 - val_auc: 0.9679\n",
            "Epoch 151/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2270 - tp: 86914.0000 - fp: 13495.0000 - tn: 522455.0000 - fn: 29680.0000 - accuracy: 0.9338 - precision: 0.8656 - recall: 0.7454 - auc: 0.9636\n",
            "Epoch 00151: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2270 - tp: 86917.0000 - fp: 13495.0000 - tn: 522477.0000 - fn: 29681.0000 - accuracy: 0.9338 - precision: 0.8656 - recall: 0.7454 - auc: 0.9636 - val_loss: 0.2207 - val_tp: 878.0000 - val_fp: 163.0000 - val_tn: 5293.0000 - val_fn: 258.0000 - val_accuracy: 0.9361 - val_precision: 0.8434 - val_recall: 0.7729 - val_auc: 0.9669\n",
            "Epoch 152/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2265 - tp: 87118.0000 - fp: 13665.0000 - tn: 522285.0000 - fn: 29476.0000 - accuracy: 0.9339 - precision: 0.8644 - recall: 0.7472 - auc: 0.9639\n",
            "Epoch 00152: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2265 - tp: 87122.0000 - fp: 13665.0000 - tn: 522307.0000 - fn: 29476.0000 - accuracy: 0.9339 - precision: 0.8644 - recall: 0.7472 - auc: 0.9639 - val_loss: 0.2167 - val_tp: 856.0000 - val_fp: 119.0000 - val_tn: 5337.0000 - val_fn: 280.0000 - val_accuracy: 0.9395 - val_precision: 0.8779 - val_recall: 0.7535 - val_auc: 0.9668\n",
            "Epoch 153/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2269 - tp: 87138.0000 - fp: 13547.0000 - tn: 522425.0000 - fn: 29460.0000 - accuracy: 0.9341 - precision: 0.8655 - recall: 0.7473 - auc: 0.9637\n",
            "Epoch 00153: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2269 - tp: 87138.0000 - fp: 13547.0000 - tn: 522425.0000 - fn: 29460.0000 - accuracy: 0.9341 - precision: 0.8655 - recall: 0.7473 - auc: 0.9637 - val_loss: 0.2163 - val_tp: 846.0000 - val_fp: 114.0000 - val_tn: 5342.0000 - val_fn: 290.0000 - val_accuracy: 0.9387 - val_precision: 0.8813 - val_recall: 0.7447 - val_auc: 0.9662\n",
            "Epoch 154/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2269 - tp: 87059.0000 - fp: 13654.0000 - tn: 522296.0000 - fn: 29535.0000 - accuracy: 0.9338 - precision: 0.8644 - recall: 0.7467 - auc: 0.9636\n",
            "Epoch 00154: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2269 - tp: 87063.0000 - fp: 13654.0000 - tn: 522318.0000 - fn: 29535.0000 - accuracy: 0.9338 - precision: 0.8644 - recall: 0.7467 - auc: 0.9636 - val_loss: 0.2264 - val_tp: 792.0000 - val_fp: 82.0000 - val_tn: 5374.0000 - val_fn: 344.0000 - val_accuracy: 0.9354 - val_precision: 0.9062 - val_recall: 0.6972 - val_auc: 0.9625\n",
            "Epoch 155/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2270 - tp: 87061.0000 - fp: 13466.0000 - tn: 521869.0000 - fn: 29380.0000 - accuracy: 0.9343 - precision: 0.8660 - recall: 0.7477 - auc: 0.9637\n",
            "Epoch 00155: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2271 - tp: 87166.0000 - fp: 13481.0000 - tn: 522491.0000 - fn: 29432.0000 - accuracy: 0.9342 - precision: 0.8661 - recall: 0.7476 - auc: 0.9637 - val_loss: 0.2135 - val_tp: 863.0000 - val_fp: 102.0000 - val_tn: 5354.0000 - val_fn: 273.0000 - val_accuracy: 0.9431 - val_precision: 0.8943 - val_recall: 0.7597 - val_auc: 0.9686\n",
            "Epoch 156/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2272 - tp: 86918.0000 - fp: 13555.0000 - tn: 521981.0000 - fn: 29578.0000 - accuracy: 0.9338 - precision: 0.8651 - recall: 0.7461 - auc: 0.9636\n",
            "Epoch 00156: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2272 - tp: 86991.0000 - fp: 13566.0000 - tn: 522406.0000 - fn: 29607.0000 - accuracy: 0.9338 - precision: 0.8651 - recall: 0.7461 - auc: 0.9636 - val_loss: 0.2172 - val_tp: 869.0000 - val_fp: 122.0000 - val_tn: 5334.0000 - val_fn: 267.0000 - val_accuracy: 0.9410 - val_precision: 0.8769 - val_recall: 0.7650 - val_auc: 0.9666\n",
            "Epoch 157/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2264 - tp: 87111.0000 - fp: 13445.0000 - tn: 522084.0000 - fn: 29392.0000 - accuracy: 0.9343 - precision: 0.8663 - recall: 0.7477 - auc: 0.9638\n",
            "Epoch 00157: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2264 - tp: 87177.0000 - fp: 13462.0000 - tn: 522510.0000 - fn: 29421.0000 - accuracy: 0.9343 - precision: 0.8662 - recall: 0.7477 - auc: 0.9638 - val_loss: 0.2241 - val_tp: 798.0000 - val_fp: 83.0000 - val_tn: 5373.0000 - val_fn: 338.0000 - val_accuracy: 0.9361 - val_precision: 0.9058 - val_recall: 0.7025 - val_auc: 0.9663\n",
            "Epoch 158/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2258 - tp: 87190.0000 - fp: 13637.0000 - tn: 522312.0000 - fn: 29405.0000 - accuracy: 0.9340 - precision: 0.8647 - recall: 0.7478 - auc: 0.9642\n",
            "Epoch 00158: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2258 - tp: 87192.0000 - fp: 13637.0000 - tn: 522335.0000 - fn: 29406.0000 - accuracy: 0.9340 - precision: 0.8648 - recall: 0.7478 - auc: 0.9642 - val_loss: 0.2204 - val_tp: 819.0000 - val_fp: 84.0000 - val_tn: 5372.0000 - val_fn: 317.0000 - val_accuracy: 0.9392 - val_precision: 0.9070 - val_recall: 0.7210 - val_auc: 0.9669\n",
            "Epoch 159/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2258 - tp: 87575.0000 - fp: 13713.0000 - tn: 522236.0000 - fn: 29020.0000 - accuracy: 0.9345 - precision: 0.8646 - recall: 0.7511 - auc: 0.9642\n",
            "Epoch 00159: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2258 - tp: 87576.0000 - fp: 13713.0000 - tn: 522259.0000 - fn: 29022.0000 - accuracy: 0.9345 - precision: 0.8646 - recall: 0.7511 - auc: 0.9642 - val_loss: 0.2155 - val_tp: 842.0000 - val_fp: 108.0000 - val_tn: 5348.0000 - val_fn: 294.0000 - val_accuracy: 0.9390 - val_precision: 0.8863 - val_recall: 0.7412 - val_auc: 0.9668\n",
            "Epoch 160/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2261 - tp: 87265.0000 - fp: 13566.0000 - tn: 522162.0000 - fn: 29295.0000 - accuracy: 0.9343 - precision: 0.8655 - recall: 0.7487 - auc: 0.9641\n",
            "Epoch 00160: val_auc did not improve from 0.96964\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2261 - tp: 87298.0000 - fp: 13572.0000 - tn: 522400.0000 - fn: 29300.0000 - accuracy: 0.9343 - precision: 0.8655 - recall: 0.7487 - auc: 0.9641 - val_loss: 0.2203 - val_tp: 831.0000 - val_fp: 106.0000 - val_tn: 5350.0000 - val_fn: 305.0000 - val_accuracy: 0.9377 - val_precision: 0.8869 - val_recall: 0.7315 - val_auc: 0.9654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6d398233-521c-4b6b-d563-99fecf20d027"
      },
      "source": [
        "def plot_metrics(history):\n",
        "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch,  history.history[metric], color='b', label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color='b', linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "plot_metrics(history)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjbrzEe2ISI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "cdbd160b-97a4-4aa8-9b3f-21052f3d6cc9"
      },
      "source": [
        "# Save model weights to drive\n",
        "!cp -r best_model.h5 '/content/gdrive/My Drive/Kaggle/best_model_20200804_METRICS_2.h5'\n",
        "\n",
        "#new_model = tf.keras.models.load_model('./best_model.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model = tf.keras.models.load_model('./best_model.h5')\n",
        "#new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Kaggle/best_model_20200802_METRICS.h5', \n",
        "#                                        custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f16be94d89fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#new_model = tf.keras.models.load_model('./best_model.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Kaggle/best_model_20200802_METRICS.h5',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#                                        custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAzqpDLIS0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty some RAM space\n",
        "indices_0_new = None\n",
        "data_backup = None\n",
        "dataset_transaction = None\n",
        "X_to_train = None\n",
        "Y_to_train = None\n",
        "Y_train = None\n",
        "X_train = None"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQtd0IsFspQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n",
        "  test_size = test_size\n",
        "  y_pred = (model.predict(X_test, batch_size=128)>threshold).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        " \n",
        "  precision = precision_cal(y_pred, y_ref)\n",
        "  recall = recall_cal(y_pred, y_ref)\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre = []\n",
        "re = []\n",
        "f1 = []\n",
        "\n",
        "pre_train = []\n",
        "re_train = []\n",
        "f1_train = []\n",
        "\n",
        "threshold_value = []\n",
        "indices = np.random.randint(0, len(X_to_train), size=(len(Y_test),))\n",
        "\n",
        "for i in range(90):\n",
        "  threshold_value.append(0.1+i*0.01)\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_test, Y_test, test_size=len(Y_test), threshold=threshold_value[-1])\n",
        "  \n",
        "  pre.append(temp_pre)\n",
        "  re.append(temp_re)\n",
        "  f1.append(temp_f1)\n",
        "\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_to_train[indices], Y_to_train[indices], test_size=len(Y_to_train[indices]), threshold=threshold_value[-1])\n",
        "\n",
        "  pre_train.append(temp_pre)\n",
        "  re_train.append(temp_re)\n",
        "  f1_train.append(temp_f1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CP4zuNU6WUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(threshold_value, f1, 'b')\n",
        "plt.plot(threshold_value, pre, 'r')\n",
        "plt.plot(threshold_value, re, 'g')\n",
        "\n",
        "plt.plot(threshold_value, f1_train, '--b')\n",
        "plt.plot(threshold_value, pre_train, '--r')\n",
        "plt.plot(threshold_value, re_train, '--g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2H3PvUGb8KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "1ecf0d6b-a11f-486c-e0c5-168773607cd9"
      },
      "source": [
        "#BATCH_SIZE = 256\n",
        "baseline_results = new_model.evaluate(X_test, Y_test,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(new_model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "predictions = new_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "plot_cm(Y_test, predictions, p=0.3)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss :  0.2270195335149765\n",
            "tp :  4320.0\n",
            "fp :  500.0\n",
            "tn :  28057.0\n",
            "fn :  1816.0\n",
            "accuracy :  0.9332430362701416\n",
            "precision :  0.8962655663490295\n",
            "recall :  0.7040417194366455\n",
            "auc :  0.9600683450698853\n",
            "\n",
            "Legitimate Transactions Detected (True Negatives):  27436\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  1121\n",
            "Fraudulent Transactions Missed (False Negatives):  1225\n",
            "Fraudulent Transactions Detected (True Positives):  4911\n",
            "Total Fraudulent Transactions:  6136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUuAUMBb_9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e0822708-b430-4e6a-aa40-5584faaa8b7b"
      },
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,30])\n",
        "  plt.ylim([70,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "indices = np.random.randint(0, len(X_to_train), size=(len(Y_test),))\n",
        "\n",
        "train_prediction = new_model.predict(X_to_train[indices], batch_size=BATCH_SIZE, verbose=0)\n",
        "plot_roc(\"Train Baseline\", Y_train[indices], train_prediction, color='b')\n",
        "plot_roc(\"Test Baseline\", Y_test, predictions, color='b', linestyle='--')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f35b900a6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAEHCAYAAACA8NJyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU9fnA8c8DcghBQYGUQwURqSIQCCLIlajUu1IEkWoFtVJPtHjRX8vhVfBo8ao3iNYDFaTijWhAxBMQ5BDkVDkURAiEM5Dn98d3NrtJNptNsruzSZ736zWvnfnuzM6ThTyZ+c73EFXFGGNipZrfARhjKhdLKsaYmLKkYoyJKUsqxpiYsqRijIkpSyrGmJg6JF4fLCITgfOAzap6kld2BPAK0AJYB1ykqttERICHgHOA3cAQVV1Q0jkaNmyoLVq0yN/etWsXdevWje0PUkbJFAtYPJEkUyyQfPHMnz//F1VtFPUBqhqXBegFdAKWhJTdB4zw1kcA93rr5wDvAgJ0Bb6I5hzp6ekaKisrS5NFMsWiavFEkkyxqCZfPMA8LcXvftxuf1T1Y+DXQsUXAM95688BfUPKn/d+hs+B+iLSJF6xGWPiJ9F1Kqmquslb/wlI9dabAT+G7LfeKzPGVDBxq1MpiaqqiJS6j4CIDAWGAqSmpjJr1qz893Jycgps+ymZYgGLJ5JkigWSL55SK829UmkXXIVsaJ3KCqCJt94EWOGtPwkMCrdfpMXqVKJn8RQvmWJRTb54SJY6lWJMBwZ764OBN0LKLxOnK5CtwdskY0wFEs9Hyi8DGUBDEVkPjAbGAa+KyJXA98BF3u7v4J4ArcI9Ur48XnEZY+IrbklFVQcV89bpYfZV4Lp4xWKMSRxrUWuMiSlLKsaYmLKkYoyJKUsqxpiYsqRijIkpSyrGmJiypGKMiSlLKsaYmLKkYoyJKUsqxpiYsqRijIkpSyrGmJiypGKMiSlLKsaYmLKkYoyJKUsqxpiY8m3ga2NMctu9G/bvL/1xllSMMeTmwmuvwf/+B23awPbt8OyzUJaJEi2pGFPF5ObC1KkwbhwsXgx5ecXvu2tX6T/fkooxVcDixbBhA9x9N8ydW/L+vXtDWppbf+ih0p3LkooxlYwqzJkD11wDK1bAwYPF71unDrRrB/37Q58+0KFD0X0sqRhThezaBXfe6RJH7drwwQfw5ZeRj2nc2F2x/PnPIBL7mCypGFOB/Por3HYbzJoFe/e6W5qSdOgAzZpB/frw4IPQqFF8Y7SkYkwSUnUJ46WX4PbbYcIEV8fxzjtuvTj167v9nnwSjj8+cfGGsqRiTBLZswcyMzOKlF95ZfHHnHwynH02nHMOnHJK/GKLliUVY3yiCtnZsHAhHHUUPPcc3HVXyccdcohLIl27wvDhri4lmVhSMSaB9uyBl192FaVr10bed+RIdyvTvDkcdxwccURiYiwvX5KKiNwIXAUI8LSqPigiY7yyLd5u/6eq7/gRnzGx9Ouvrh7ktttK3rdbNzjhhDVMmHBs/AOLk4QnFRE5CZc8ugD7gfdE5C3v7fGq+kCiYzIm1lShQQN3e1McERg0CPr1c8mkaVNXPmvWD4AlldI4AfhCVXcDiMhsoJ8PcRgTc3PmQK9exb//97+71xEjICUlMTElmh9JZQlwj4gcCewBzgHmAVuB60XkMm/7ZlXd5kN8xpTKvn0wdizccUfx+1x8sXs8HI/GZslGVDXxJxW5ErgW2AUsBfYBY4FfAAXuApqo6hVhjh0KDAVITU1Nnzx5cv57OTk5pCRJ+k+mWMDiiaQ0sbgnNjXIy4Obbkrjxx/r4KoGw3vnnY859NAIPfbKGU8iZGZmzlfVzlEfoKq+LsA/gWsLlbUAlpR0bHp6uobKysrSZJFMsahaPJFEG8vrr6u6tFL8Mn266vDhqh98EP94EgWYp6X4nfbr6U9jVd0sIkfj6lO6ikgTVd3k7fIH3G2SMb4aNAguuwzOPBOeeCL8Pi1bupauv/2t2z7//MTFl4z8aqcy1atTyQWuU9XtIvKIiKThbn/WAX/xKTZTxf3ud65jXsCUKXDgQNH9Xn7Z1ZWYgnxJKqraM0zZn/yIxZiA1atdI7PCCieUb75xwwWY8KxFranSVq2C119vyvPPu0ZqkSxeDCedlJi4KjJLKqbK+fVXmDgRbr01UBK+O+9xx8EDD0Dnzm7oABMdSyqmyti7F1JTYceO4vcZM8YNYnT11VWjTUk8WFIxldbbb7s+N8cdB337QvfuRfepVQueffZzBg3qmvgAKylLKqZSGT4cJk2CbYXaYt9/f8Htb78NPgKeNWtvQmKrKmyGQlOhHTwIS7wWTZ9/DuPHF00ooR5/3DVTCyQUE3t2pWIqrF27XKe8Qw9145RE8tlnblAjE392pWIqFFWYN89VuAa6x4RLKMcf72bYCzSgt4SSOHalYiqM3bsjT8M5eDA88gjUq5e4mExRdqViklrHju7R7ltvudfRo4vus3atuxqZNMkSSjKwpGKSztSpLoGIuEGhwTVCq1u34Jgll1/u2py0aOFLmKYYdvtjksrw4e4JTmGzZxfcnjkTTj89MTGZ0rErFeOrrVth2jQYOtRt3313yceoWkJJZpZUjC9yc4ULLoCGDd3Az59+6upGatd289qEc/PNkScbN8nBbn9Mwp1zDrz7bu8CZY0bw08/wbGFBpEfNw4GDrR6k4rEkopJqA0b4N13g9vNmrl2JwsXwqmnFtx3/36oUSOx8Znys9sfE1fz57uZ9jIyYP16l0T+9z/33qZNruz99900nqFycy2hVFR2pWLi4uDBonUjLVrAe+/BBRfARx/NYsqUDG64oeA+zz4LQ4YkKkoTD5ZUTMx9/33ROpBu3dwTmw8/hD59ADKKHPfxx9CzyECjpqKx2x8TE9u2udn5wD0mPt4bTK1tW/cIuFs397h43Liix44f7/axhFI52JWKKbdFiyAtDf78ZzeO67XXugGRevd2o62FG0Htppu+4+abj6dZMxthrbIpNqmISKcojs9V1cUxjMdUIFu3unYmAc88A3feCffc4+pGwnnjDfj972HWrI00bx5+bFhTsUW6UpkNfEWkOR2hJW42QVPFrFkDrVoVLPvDH2DUqIJl9eq5K5Y//tFNzGUqv0hJ5StVPS3SwSLyUYzjMUnu/ffdbH3vvgv9+7uJtmrVcpOUT5sW3K9aNXjhBUskVVGxSaWkhBLtPqZy2LzZDYwEsHMnXHcdnHCCm80v0G8n4LvvoHXrxMdokkPUT39EpJGI3C0i/xIR+y9TReTkuCuTQEKBYDuSrKyiCSUvzxJKVVeaR8r/At4HpgEvleekInKjiCwRkaUicpNXdoSIfCAiK73XBuU5hym/WbNcnciMGcGy885zAyWJFOxRPHKkeyxsT3JMsUlFRN4XkV4hRTVxE6evA2qV9YQichJwFdAF6ACcJyLHASOAD1W1NfCht20STDU4dskppwTLU1Jg+3aXaNq3L3jMxo3uqY8xEPlK5SLgfBF5WURaASOBscBDwLXlOOcJwBequltVD+CeMvUDLgCe8/Z5DuhbjnOYUlKFsWNdBeugQa5B2qGHulHo8/LcaGz167vboYBRo9x7TZr4F7dJPpEqarOBW0XkWOAeYCNwvapuL+c5lwD3iMiRwB7gHGAekKqqm7x9fgJSiznexEGnTsGhGzdtck9u8vJce5OlSwvuW69e5KlDTdUmqhr+DXd1cg2wH3gUaAX8A3gb+I+qlnm4HBG5Ene1swtYCuwDhqhq/ZB9tqlqkXoVERkKDAVITU1Nnzx5cv57OTk5pATmbfBZMsUCkeMZP74106cHZyDv0WMLn3zSKOy+11yziosuWh/XeBItmWKB5IsnMzNzvqp2jvoAVQ27AF8CpwJ9cHUdgfLLQrfLuwD/xCWYFUATr6wJsKKkY9PT0zVUVlaWJotkikW1+Hh27w7MjFP88thjqnPmJCYePyRTLKrJFw8wT0vxOx2pTqUWsBZXMVsnJAk9D5wXddYKQ0Qae69H4+pTXgKmA4O9XQYDb5TnHCay7Gw3eHTt2nDVVeH3WbbMpZVrroEePRIbn6m4IrWovQZ327MfuDr0DVUtYZLJEk316lRygetUdbuIjANe9W6NvsdVFJs4mDHDtT1p1851ACxs7FgYYc/eTBlFqqj9FPg0HidV1SKd3FV1K2BjpMfZ5Ze7SbegYEJJSYHLLoNHH7W2JqZ8IrVTeaqkg6PZxyQPkWBCCXX77a7p/X/+YwnFlF+k25++IrI3wvsCZMY4HhMnzz13TNjyf/8b/vrXBAdjKrVISeXWKI6fE6tATHysXOlavC5ZcliR91asCI7QZkysRKpTea6490zF8Oqrbs6cK66AefOOzC//05/g+ed9DMxUajacZCV1ww2u0hVg4sRg+e7drvm9MfFiSaWS2b4dGhTTv3vwYEsoJv5KNZq+iFQTkaI35yZpPP001KlTtPzaa1cVO26sMbFUYlIRkZdE5DARqYvrDLhMRKKpxDUJkpcHY8bAN9/ArbfCnkJNE3fvhgED1tvjYpMQ0dz+nKiqO0TkEuBd3Dgn84H74xqZiYoqVK/u1hs0cAMohfYR3brVbnlMYkWTVGqISA3c+CaPqmquiITv2mwSLrRPzj/+UXC8k4MH3fgoxiRSNEnlSVynwkXAxyJyDGCjafgsNxdq1ixYFppQ9u2zhGL8UWJSUdWHgYdDir4XEWtJ67PCcxWHeuWVognHmESJpqI2VUQmiMi73vaJBIcoMD5Ztgyuv77o1cjevXCR9e82PormAnkSbhT9pt72d8BN8QrIFC8vzzWrX74cDj8cdu1yZQH79rmJvYzxUzRJpaGqvgrkAagbrLrMQ0masmvRwvXlOeEENz1GaLuTvXvtlsckh2iSyi5vQCUFEJGuQHZcozJFLFkCP/4Y3B45Mriek2NXKCZ5RPP052bcUI+tRGQu0AjoH9eoTBHt2oUvv/lmqFs3sbEYE0k0T3/mi0hvoA1uDJUVqpob98hMvunTw5f/8AMcdVRiYzGmJNE8/fkGuA3Yq6pLLKEk3vbt0KZN0XJLKCYZRVOncj5wADco9Vcicos3Cr5JgIMH3dixBw4ULC9muiZjfBfN7c/3wH3AfSLSGjf96b1A9TjHVuVlZroJ0TdvhtWrg+UrVvgXkzEliWo8Fa9p/kBvOYi7HTJxtHixmwx91qyC5WvWQMuWfkRkTHRKTCoi8gVQA3gNGKCqa+IelaF9+6JlGzZA06ZFy41JJtFcqVymqnbBnUBffVW0zHocm4qi2KQiIpeq6gvAuSJybuH3VfXfcY2sCuvSpeC2VcqaiiTSlUqgSVW9MO/Zf/M4ufTSgtuWUExFE2mKjie91ZmqOjf0PRHpHteoqqjcXHjxxeD2/v3+xWJMWUVzl/5IlGVRE5G/ishSEVkiIi+LSG0RmSQia0VkobekleccFc2OHQU7BG7cCDVq+BePMWUVqU6lG3Aq0EhEhoe8dRjlaKMiIs2AYbixb/eIyKvAxd7bt6rqlLJ+dkV2a8hQ4iefDE2a+BeLMeURqU6lJpDi7RNar7KD8ncoPAQ4VERygTrAxnJ+XoX29dfwVMhU94895l8sxpRXpDqV2cBsEZnktaqNCVXdICIPAD8Ae4AZqjpDRP4I3CMio4APgRGqui9W501WO3dCp04Fyzp39icWY2JBtJjHCyLyoKreJCJvEuZpj6r+vkwnFGkATMW1zt2Oa1Q3BZdIfsJdIT0FrFbVO8McPxQYCpCampo+efLk/PdycnJISUkpS1gxF20sZ53Vg337grk9K2uWr/EkSjLFk0yxQPLFk5mZOV9Vo/9Tp6phFyDde+0dbinuuJIWYAAwIWT7MuCxQvtkAG+V9Fnp6ekaKisrS5NFNLFs3KjqHhq75eBBf+NJpGSKJ5liUU2+eIB5Worf8Ui3P/O919mBMu8q4yhV/SbqrFXUD0BXEamDu/05HZgnIk1UdZOICG6OoSXlOEeFENrk/rvvrMWsqRyi6fszC/i9t+98YLOIzFXV4REPLIaqfiEiU4AFuCEVvsbd7rwrIo1wA0EtBK4uy+dXFP/9b3B92DBo3dq/WIyJpWj6/hyubtrTPwPPq+pob+CmMlPV0cDoQsWnleczK5KffnJjpAQ89JB/sRgTa9FccB8iIk2Ai4C34hxPlfDPfwbX7UmPqWyiSSp34ub9Wa2qX4nIscDK+IZVeanCIyHtkUMTjDGVQTQjv72Ge+wb2F4DXBjPoCqz0MrY6tWhTx//YjEmHqIZ+Lq5iEwTkc3eMlVEmiciuMqm8DizhbeNqQyiuf15FjfvT1NvedMrM6UU2s6wv82cZCqpaJJKI1V9VlUPeMsk3IRippRuvDG4PnGif3EYE0/RPFLeKiKXAi9724OArfELqXJq1coNWg3QqBHUCzf0lTGVQDRXKlfgHif/5C39gcvjGVRlk5sbTCgAc+cWv68xFV208/6UqfOgcUIHXwJrPWsqt2ie/hwrIm+KyBbv6c8bXlsVE4Xbby+4vWqVP3EYkyjR3P68BLwKNME9/XmNYP2KKcF99wXXR4xwdSvGVGbRJJU6qvrfkKc/LwC14x1YZZCXV3B77Fh/4jAmkaJ5+vOuiIwAJuMGaxoIvCMiRwCo6q9xjK9C69kzuL7SOjaYKiKapHKR9/qXQuUX45KM1a+EMXbsb/n0U7desyYcd5y/8RiTKNE8/bHpwEtp9WqYMeM3+ds//+xjMMYkmI01FmOqRa9K6tf3JxZj/GBJJcYKDwk5Z44/cRjjF0sqMfTgg0XLevRIfBzG+Cmaxm8iIpd68/EgIkeLSJf4h1bxFH6EfPCgP3EY46dorlQeA7rhOhIC7AT+E7eIKqg9e2Do0OD2eedttNHxTZUUzSPlU1S1k4h8DaCq20SkZkkHVSXp6W6mwdC2KNdfvwrXANmYqiWapJIrItXxZin0ptHIi3xI1fGXv8CCBQXL6tSBWrXsKzJVUzQX6A8D04DGInIP8AlgwzV7QidWD9hqo82YKiyaxm8vish83EyCAvRV1W/jHlkFEC6hVKsGta1nlKnCopmh8GhgN25s2vwyVf0hnoFVBJ9/XrRs/PjEx2FMMonm9udt3CRibwMfAmuAd+MZVEWxd697HTYsWHZ1pZ6s1ZiSRXP70y50W0Q6AdfGLaIK5KWX4I47oGPHYFnhUd6MqWpK3ZJCVRcAp5TnpCLyVxFZKiJLRORlEaktIi1F5AsRWSUiryT7Y+sDB1zblPr1YdcuVxaujsWYqiaaOpXhIZvVgE7AxrKeUESaAcOAE1V1j4i8ihtG4RxgvKpOFpEngCuBx8t6nnirUQNGjYLly4NlV1zhXzzGJIto2qmETiZxAFe3MjUG5z1URHKBOsAm4DTgj977zwFjSNKkMnu2e73zzmDZwIFuGlNjqrqIScVr9FZPVW+J1QlVdYOIPAD8AOwBZgDzge2qGpgIdD3QLFbnjLUpU4qWPfRQ4uMwJhmJhs7FGfqGyCGqekBEPlPVbjE7oUgD3JXOQGA7biDtKcAYVT3O2+co4F1VPSnM8UOBoQCpqanpkydPzn8vJyeHlJSUWIVarMzMjALbtWod5L33Co5xkKhYomXxFC+ZYoHkiyczM3O+qnaO+gBVDbsAC7zXx3FzKf8J6BdYijuupAUYAEwI2b7MO8cvwCFeWTfg/ZI+Kz09XUNlZWVpvO3fr+qGYgouc+cW3S8RsZSGxVO8ZIpFNfniAeZpKX7Ho6lTqY2b5vQ0XP8f8V5fjzpzFfQD0FVE6uBuf04H5gFZuNkPJwODgTfK+Plxde+9BbePOgpOPdWfWIxJRpGSSmPvyc8SgskkIPw9UxRU9QsRmQIswFX8fg08hasAniwid3tlE8p6jnjq2RM6dIBFi9z2P60XlDEFREoq1YEUCiaTgDInFQBVHQ2MLlS8Bkj6wZ969w4mFIBLL/UvFmOSUaSksklV74zwfpUzZw78JjhIPt9at0pjioiUVMJdoVRpvXoV3P7tb/2Jw5hkFqmZ/ukJi6IC+OyzgtvH2hRqxoRVbFJRm860gMJPeLp39ycOY5KdDc0chezs4HpgAKaMDF9CMSbpWVKJQmiz/MAYKm3a+BOLMcnOkkoUnnnGvYbWo6Sn+xOLMcnOkkoUPvsM3nuv4ABMNg6tMeFF00y/Stuzxw1pcOaZwbFT/vAHf2MyJpnZlUoEBw+6OXweeQR++SVYfs89/sVkTLKzK5UIxo51r7fcAu+8Eyw/7jh/4jGmIrArlQgCDd6qVYOPPgqW16jhTzzGVASWVCL44AP3evnlwbLcXH9iMaaisKRSjNzcYAL56qtg+SF2w2hMRJZUivHmm8H1b75xrzZavjEls6RSjI0boVUraNQoWGZTmhpTMksqxbj+evjwQ/dIGVzDt8MO8zcmYyoCqyEI46efoFYtOOYY+P57V2a3PsZEx5JKGI8/Dlu2QKdOwbIRI/yLx5iKxJJKGPfeC/v2FZx4/Zhj/IvHmIrE6lTC2LfPvW7Z4l7POce/WIypaCypFLJjR3A9UDF7ySX+xGJMRWRJpZC5c4Pry5a51y5JP3GIMcnDkkohr77qXps2DZbZINfGRM+SSiErVrjXevXca2qq61BojImO/boU8t//wl13wc6dbvuoo/yNx5iKJuFJRUTaiMjCkGWHiNwkImNEZENIuS/PXFq1gr//3TXTB/jLX/yIwpiKK+HtVFR1BZAGICLVgQ3ANOByYLyqPpDomAI2bICcHKhbN1h2xhl+RWNMxeR347fTgdWq+r2I/7Os9usHq1a5SdgDWrTwLZxKJTc3l/Xr17PXm+Pk8MMP59skmYw6mWIB/+KpXbs2zZs3p0Y5RyHzO6lcDLwcsn29iFwGzANuVtVtiQzmyy/da+Df88QTE3n2ym39+vXUq1ePFi1aICLs3LmTeoHacJ8lUyzgTzyqytatW1m/fj0tW7Ys12eJqsYorFKeWKQmsBFoq6o/i0gq8AugwF1AE1Ut0o1PRIYCQwFSU1PTJ0+enP9eTk4OKSkpZY4pMzOjwPbgwesYMmRdmT6rvLHEmt/xHH744bRq1YrAFenBgwepXr26b/GESqZYwL94VJXVq1eTHTolJ5CZmTlfVTuX6oP8WIALgBnFvNcCWFLSZ6Snp2uorKwsLav161Wh4PLdd2X+uHLFEg9+x7Ns2bIC2zt27PApkqKSKRZVf+Mp/O+kqgrM01L8bvv5SHkQIbc+ItIk5L0/AEsSGczUqUXLWrdOZAQmnrZu3UpaWhppaWn85je/oVmzZvnb+/fvj3jsvHnzGDZsWKnO16JFC9q1a0daWhrt2rXjjTfeKE/4RYwZM4YHHnDPNEaNGsXMmTNj+vnl4UudiojUBfoAoQ9s7xORNNztz7pC78Xds88W3F64MJFnN/F25JFHstD7Rx0zZgwpKSnccsstgKvDOHDgAIcUMwBx586d6dw5+qv/gKysLBo2bMiKFSv43e9+xwUXXFD2HyCCO++8My6fW1a+XKmo6i5VPVJVs0PK/qSq7VS1var+XlU3JTKmfv0KPkru0CGRZzd+GDJkCFdffTWZmZncdtttfPnll3Tr1o2OHTty6qmnssJrXj1r1izOO+88wCWkK664goyMDI499lgefvjhEs+zY8cOGjRokL/dt29f0tPTadu2LU899RTg6lGGDBnCSSedRNeuXRnvjV26evVqzjrrLNLT0+nZsyfLA9NkFvo5pkyZArgrpNGjR9OpUyfatWuXv/+uXbu44oor6NKlCx07doz5lVMov5/+JI2RI2HWrILz+5j4cHW1sX+6UZZnDuvXr2fmzJnUr1+fHTt2MGfOHA455BBmzpzJ//3f/zE1zH3x8uXLycrKYufOnbRp04Zrrrkm7GPYzMxMVJU1a9bwaqBTGTBx4kSOOOII9uzZw8knn8yFF17IunXr2LBhA0uWLGHnzp0cPHgQgKFDh/LEE0/QunVrvvjiC6699lo+KuE/acOGDVmwYAGPPfYYDzzwAM888wz33HMPp512GhMnTmT79u106dKFM844g7qhf0ljxJJKiMBUHKNG+RuHSZwBAwbkP2nJzs5m8ODBrFy5EhEht5hJns4991xq1apFrVq1aNy4MT///DPNmzcvsl/g9mf16tWcfvrpZGRkkJKSwsMPP8y0adMA+PHHH1m5ciVt2rRhzZo13HDDDWRmZtK3b19ycnL49NNPGTBgQP5n7gsM9hNBv379AEhPT+f1118HYMaMGUyfPj2/Hmbv3r388MMPnHDCCaX4tqJjSQU38fqbbwb7+9gE7PGlmjxtQ0L/Uo8cOZLMzEymTZvGunXryMjICHtMrVq18terV6/OgQMHIp6jVatWpKamsmzZMnbv3s3MmTP57LPPqFOnDhkZGezdu5cGDRqwaNEi3n//fSZOnMhbb73Fgw8+SP369fPrgqIViC80NlVl6tSptGnTplSfVRbWoRC44Qa47bbgdvv2/sVi/JOdnU2zZs0AmDRpUsw+d/Pmzaxdu5ZjjjmG7OxsGjRoQJ06dVi+fDmff/45AL/88gt5eXlceOGFjBw5kgULFnDYYYfRsmVLXnvtNcAlhkWLFpUphjPPPJNHHnkk0GSDr7/+OjY/XBiWVHBTcYSyoQ6qpttuu42//e1vdOzYscSrj2hkZmaSlpZGZmYm48aNIzU1lbPOOosDBw5wwgknMGLECLp27QrAhg0byMjIIC0tjauuuoqxY8cC8OKLLzJhwgQ6dOhA27Zty1zBOnLkSHJzc2nfvj1t27Zl5MiR5f75ilWaRi3JtsSi8du+fQUbvK1YUeqPCMvvxmaF+R2PNX6LnjV+q+AC/X0Cjj/enziMqSwsqYQklZNO8i8OYyqLKp9UQtsSWYM3Y8qvyieVCy4IVsyefba/sRhTGVT5pHLGGZCX59bPOsvfWIypDKp8UnnnneD6kUf6F4cxlUWVTip798KNN7r1BDQ0ND4qz9AH4DoVfvrpp2HfmzRpEo0aNSItLY22bdvSv39/du/eHdP4AwNsbdy4kf79+8f0s2OtSieVDz6AH3906zZhWOUWGPpg4cKFXH311fz1r3/N365Zs2aJx0dKKgADB68vjlYAAA2FSURBVA5k4cKFLF26lJo1a/LKK6/EMvx8TZs2ze+RnKyqdFIJ7ZuVmelfHMYf8+fPp3fv3vTq1YszzzyTTZvcaBsPP/wwJ554Iu3bt+fiiy9m3bp1PPHEE4wfP560tDTmzJlT7GceOHCAXbt25Q918Oabb3LKKafQsWNHzjjjDH7++WcAZs+enX+l1LFjR3Z6Hc/uv/9+evfuTfv27Rk9enSRz1+3bh0neW0fJk2aRL9+/TjrrLNo3bo1t4X0NZkxYwbdunWjU6dODBgwgJycnNh8adEoTUu5ZFvK26L2rruCLWlfeqlUh5bI7xashfkdT+GWmoWH7gxdnnwyuN+TT0betyxGjx6t9913n3br1k03b96sO3bs0MmTJ+vll1+uqqpNmjTRvXv3qqrqtm3b8o+5//77w37es88+qw0bNtQOHTpo48aNtUePHnrgwAFVVf311181Ly9PVVWffvppHT58uKqqnnfeefrJJ5+oqurOnTs1NzdX33//fb3qqqs0OztbDx48qOeee67Onj1bVVXr1q2rqqpr167Vtm3b5p+3ZcuWun37dt2zZ48effTR+sMPP+iWLVu0Z8+empOTo6qq48aN0zvuuCOq7yYWLWqrdC/lVauC63EalMskqX379rFkyRL69OlDXl4eqkqTJm5E0/bt23PJJZfQt29f+vbtG9XnDRw4kEcffRRV5brrruP+++9nxIgRrF+/noEDB7Jp0yb279+fP1J99+7dGT58OJdccgn9+vWjefPmzJgxgxkzZtCjRw+qVatGTk4OK1eupFevXsWe9/TTT+fwww8H4MQTT+T7779n+/btLFu2jO7duwOwf/9+unXrVp6vq1Sq9O1P4FEyQJ06/sVRFe3YsbPY64+hQ4P7DR0a6Tql7OdXVdq2bcvChQuZO3cuixcvZsaMGQC8/fbbXHfddSxYsICTTz65VJ0LRYTzzz+fjz/+GIAbbriB66+/nsWLF/Pkk0/mz3s0YsQInnnmGfbs2UP37t1Zvnw5qsrf/vY35s6dy8KFC1m1ahVXXnllxPOFG4ZBVenTp09+ndGyZcuYMGFCab+iMqvSSSVQ79aokb9xmMSrVasWW7Zs4bPPPgPcZGdLly4lLy+PH3/8kczMTO69916ys7PJycmhXr16+fUeJfnkk09o1aoVUHA4heeeey5/n9WrV9OuXTtuv/12Tj75ZJYvX86ZZ57JxIkT8+s/NmzYwObNm0v9s3Xt2pW5c+eyyrsU37VrF999912pP6esqvTtT2Cc4x07/I3DJF61atWYMmUKw4YNY9u2beTl5XHTTTdx/PHHc+mll5KdnY2qMmzYMOrXr8/5559P//79eeONN3jkkUfo2bNngc975ZVX+OSTT8jLy6N58+b547GMGTOGAQMG0KBBA0477TTWrl0LwIMPPkhWVhbVqlWjbdu2nH322dSqVYtvv/2WM844g2rVqpGSksILL7xA48aNS/WzNWrUiEmTJjFo0KD8keLuvvtujk9Ub9nSVMAk21Leitru3d1F9J/+VKrDouJ3xWhhfsdjQx9Ez4Y+qMACt8o9evgbhzGVSZVNKjt2wBdfuPWOHf2NxZjKpMomFW8wcwCaNvUvDmMqmyqbVB5/PLjuVc6bONPyPAM2cRerf58qm1Q2bnSvbmIrE2+1a9dm69atlliSlKqydetWateuXe7PqrKPlAMdCe0qJTGaN2/O+vXr2bJlC+Ams4rFf+BYSKZYwL94ateuHXZStNJKeFIRkTZAaBfOY4FRwPNeeQvcBO0Xqeq2eMQQ+sfy7rvjcQZTWI0aNfKbqIPr9dsxSWrIkykWSL54Sivhtz+qukJV01Q1DUgHdgPTgBHAh6raGvjQ246LwFMfgJAZJY0xMeB3ncrpwGpV/R64AAi0Y34OiK4nVxksXepeq1WzPj/GxJrfSeVi4GVvPVVVN3nrPwGp8TqpN2d1gQ6FxpjY8K2iVkRqAr8H/lb4PVVVEQn7mEBEhgKBfqw5IrIi5O2GwC+li6M0e5dKqWOJM4uneMkUCyRfPKUabNXPpz9nAwtU9Wdv+2cRaaKqm0SkCRC2e6aqPgU8Fe49EZmnqp3jE27pJFMsYPFEkkyxQHLGU5r9/bz9GUTw1gdgOjDYWx8MlG0mamOMr3xJKiJSF+gDvB5SPA7oIyIrgTO8bWNMBePL7Y+q7gKOLFS2Ffc0qDzC3hb5JJliAYsnkmSKBSp4PGLNpo0xseT3I2VjTCVTKZKKiJwlIitEZJWIxK0lbiniWScii0VkYWlrzmN0/okisllEloSUHSEiH4jISu+1gY+xjBGRDd73s1BEzklELN65jxKRLBFZJiJLReRGrzzh30+EWHz5fkSktoh8KSKLvHju8MpbisgX3u/XK15zkOKVZpi4ZFyA6sBqXB+imsAi4ESfY1oHNPTx/L2ATsCSkLL7gBHe+gjgXh9jGQPc4tN30wTo5K3XA74DTvTj+4kQiy/fDyBAirdeA/gC6Aq8ClzslT8BXBPpcyrDlUoXYJWqrlHV/cBkXJP/KktVPwZ+LVScsG4QUcTiG1XdpKoLvPWdwLdAM3z4fiLE4gt1AlMZ1vAWBU4DAnOtlvjdVIak0gz4MWR7PT7+w3gUmCEi870WwMkgYd0gonS9iHzj3R4l5FasMBFpAXTE/UX29fspFAv49P2ISHURWYhrfPoB7i5gu6oGJj8q8ferMiSVZNRDVTvhWg1fJyLFTzHnA3XXsX4+9nscaAWkAZuAfyU6ABFJAaYCN6lqgUlaEv39hInFt+9HVQ+qG0GgOe4u4Lel/YzKkFQ2AEeFbDf3ynyjqhu81824YR26+BmP52ev+wORukEkgqr+7P3nzQOeJsHfj4jUwP0Sv6iqgQaYvnw/4WLx+/vxYtgOZAHdgPoiEmjTVuLvV2VIKl8Brb0a6pq4ns/T/QpGROqKSL3AOvA7YEnkoxIiabpBBH55PX8ggd+PiAgwAfhWVf8d8lbCv5/iYvHr+xGRRiJS31s/FNfq/Vtccunv7Vbyd5PoGuY41Vqfg6s5Xw383edYjsU9gVoELPUjHlyfqk1ALu4e+EpcC+YPgZXATOAIH2P5L7AY+Ab3y9wkgd9ND9ytzTfAQm85x4/vJ0Isvnw/QHvga++8S4BRXvmxwJfAKuA1oFakz7EWtcaYmKoMtz/GmCRiScUYE1OWVIwxMWVJxRgTU5ZUjDExZUmlghKRgyG9WBd6zbyL2zenuPcSSUSaisgUbz0ttPetiPw+Xj3MRSRDRLJF5B1vu43XheIbEenmlR0iIjNFpE7IcS+KyK8i0r+4zzZFVdlpTyuBPeqaU1cYqrqRYCOqNKAz8I733nTi22hxjqqe563/BbgR15v8IeBC4BrgBVXdHRLvJSIyKY4xVUp2pVJJiEiKiHwoIgu8sVyK9NQWkSYi8rF3ZbNERHp65b8Tkc+8Y1/z+qIUPnaWiDwUcmwXr/wIEfmf91f/cxFp75X3DrmK+lpE6olIC+/YmsCdwEDv/YEiMkREHhWRw0XkexGp5n1OXRH5UURqiEgrEXnPu8qYIyK/9fYZ4H3uIhH5OIqvKxeo4y25XivS83FT75rySlRLRlti3vrxIMFWmNNwV52Hee81xLV+DDRuzPFeb8Zr4Ysbh6aet+/HQF2v/Ha8lpSFzjcLeNpb74U3PgrwCDDaWz8NWOitvwl099ZTvPhahBw3BHg05PPzt3HNwDO99YHAM976h0Brb/0U4CNvfTHQzFuvHyb2DOCtkO2jvZ/nM1wr0n8BGcV8z5OA/n7/e1ekxW5/Kq4Ctz9ex7R/ej2i83Dd01Nx3fgDvgImevv+T1UXikhv3MBAc11XFGriftnCeRncGCkicpj3F74H7vYBVf1IRI4UkcOAucC/ReRF4HVVXS/Rz9z2Ci6ZZOH6cj3mXT2dCrwW8jm1vNe5wCQReZWCMzSEpao/4BINInIcrpPctyLyX+/nH6mq30UbrCnIkkrlcQnQCEhX1VwRWQfUDt3BSwa9gHNxv4T/BrYBH6jqoCjOUbhPR7F9PFR1nIi8jevLMldEzgT2RvmzTMclyCOAdOAjoC5uXI8i9UiqerWInIL7ueaLSLq62RmicQ/wD2AY8AyunuWfuO/TlIHVqVQehwObvYSSCRxTeAcROQb4WVWfxv0CdQI+B7p7f7EDdRjHF3OOgd4+PYBsVc0G5uD9AopIBvCLqu4QkVaqulhV78VdIRUel2Mn7varCHWjj32Fq0R9S90wADuAtSIywDuXiEgHb72Vqn6hqqOALRQcCqNY3lXaRlVdiatfyfOWOhEPNBHZlUrl8SLwpogsBuYBy8PskwHcKiK5QA5wmapuEZEhwMsiErid+Aeu13dhe0Xka9wwg1d4ZWNwt1TfALsJDh9wk5fc8nC9td/FjckakAWMEDfK2Ngw53oF1yM2I6TsEuBxEfmHF8NkXG/w+0WkNW6M1Q+9sojE3UP9Ay9R4ua2eRH3O3FNSceb4lkvZRMVEZmFG4w54bMDlJd3BXWLBh8pl+bYSbirpSkl7Wscu/0xVcF+4KRA47doeZXMvYm+LshgVyrGmBizKxVjTExZUjHGxJQlFWNMTFlSMcbElCUVY0xMWVIxxsTU/wN7a6fWZ1b34wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43d7bce2-6a9f-403f-fa51-312ba9f6fb96"
      },
      "source": [
        "prediction = np.squeeze(predictions, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist((prediction>0.9).astype('int'), bins=[0,1,2])\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.85).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.08% 3.32%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "cca96490-6a74-445e-9688-ce64cfaa64c5"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content\")\n",
        "test_transaction = pd.read_csv('test_transaction.csv')\n",
        "test_identity = pd.read_csv('test_identity.csv', names=saved_columns, header=0)\n",
        "test_identity.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Android 6.0.1</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x720</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>LGLS676 Build/MXB48T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ie 11.0 for tablet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Trident/7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SM-G9650 Build/R16NW</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  ...  id_38  DeviceType                   DeviceInfo\n",
              "0        3663586  -45.0  280290.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "1        3663588    0.0    3579.0  ...      T      mobile         LGLS676 Build/MXB48T\n",
              "2        3663597   -5.0  185210.0  ...      F     desktop                  Trident/7.0\n",
              "3        3663601  -45.0  252944.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "4        3663602  -95.0  328680.0  ...      F      mobile         SM-G9650 Build/R16NW\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8CCDZ5bl8V6p",
        "colab": {}
      },
      "source": [
        "dataset_transaction = None\n",
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = test_identity.pop(column)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVs1d5a_wMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e9b28869-ab36-412d-cb51-a80f52a71219"
      },
      "source": [
        "test_identity.head(5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>24.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  id_03  ...  id_36  id_37  id_38  DeviceType\n",
              "0        3663586  -45.0  280290.0    NaN  ...      F      T      F      mobile\n",
              "1        3663588    0.0    3579.0    0.0  ...      F      T      T      mobile\n",
              "2        3663597   -5.0  185210.0    NaN  ...      T      T      F     desktop\n",
              "3        3663601  -45.0  252944.0    0.0  ...      F      T      F      mobile\n",
              "4        3663602  -95.0  328680.0    NaN  ...      F      T      F      mobile\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "du0_nSm48V63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd17c9f2-6cb2-4d5c-909f-caa1fc3a41e1"
      },
      "source": [
        "merged_data = pd.merge(left=test_transaction, right=test_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "TransactionID = merged_data.pop('TransactionID')\n",
        "test_transaction = None\n",
        "merged_data.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506691, 428)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoViKsx6cZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fe0e97c-d881-48a8-a3dd-6a693789b6ce"
      },
      "source": [
        "test_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "float_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = []\n",
        "for column in skip_int_columns:\n",
        "  int_columns_test.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "print(len(float_columns_test), len(int_columns_test), len(obj_columns_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzQZ6nR6wOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_normalization(X, indices, cache_min, cache_max, cache_mean):\n",
        "  X_out = copy.copy(X)\n",
        "  X_out[indices] = (X_out[indices] - cache_mean)/(cache_max - cache_min)\n",
        "  X_out[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return X_out.astype('float16')  \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM75lh_6lhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in float_columns_test:\n",
        "  # Set to float 16\n",
        "  test_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjog0oM7p4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns_test:\n",
        "  # Set to int 32\n",
        "  test_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMTT8KB74NL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1fc7806-0915-4c7c-ed39-3f96d1673482"
      },
      "source": [
        "encoded_column = 0\n",
        "for column in obj_columns_test:\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(cache[column])\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.transform(test_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(cache[column])))])\n",
        "  test_transaction = pd.concat([test_transaction, pd_encoded], axis=1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "\n",
        "\n",
        "for column in obj_columns_test:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "for column in to_remove:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_OOqFi8HrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ed465a4-667a-4eda-a8ac-1accba59f1ff"
      },
      "source": [
        "# Check if we have the same shape with the X_train\n",
        "#print(test_transaction.shape, X_train.shape)\n",
        "print(test_transaction.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506691, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9vDvpDZdpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the prediction and submit the output\n",
        "result = (new_model.predict(test_transaction)>0.9).astype('int8')\n",
        "result_pd = pd.DataFrame(result, columns=['isFraud'])\n",
        "data_to_file = pd.concat([TransactionID, result_pd], axis=1)\n",
        "data_to_file.head(5)\n",
        "data_to_file.to_csv(\"./submission.csv\", index=False)\n",
        "data_to_file.to_csv('/content/gdrive/My Drive/Kaggle/submission.csv', index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9099XTi4s2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "79db6d41-a2ce-4d56-9863-7e2e8ed571ed"
      },
      "source": [
        "!kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"New submission with model_20200804 using Keras METRICS and Relu and with threshold 0.9\""
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 4.83M/4.83M [00:02<00:00, 1.70MB/s]\n",
            "Successfully submitted to IEEE-CIS Fraud Detection"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICp4sPm6brq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3e2nvzrHir4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fe4ae314-d9e2-483a-9baa-e8b9302f14bf"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohc = OneHotEncoder()\n",
        "a = {'a': ['Null', 'A', 'B', 'C', 'D']}\n",
        "df = pd.DataFrame(a)\n",
        "df\n",
        "encoded = ohc.fit_transform(df['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsaGKlzMUlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69213f3a-42cb-4edc-cbc8-5bc6c0bb5a7e"
      },
      "source": [
        "b = {'a': ['Null', 'A', 'B', 'C', 'E']}\n",
        "df_b = pd.DataFrame(b)\n",
        "ohc_b = OneHotEncoder(handle_unknown='ignore')\n",
        "ohc_b.fit(df['a'].values.reshape(-1,1))\n",
        "encoded_b = ohc_b.transform(df_b['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded_b = pd.DataFrame(encoded_b.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvykuaRPMpZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "20c01d2a-8a3b-41b6-a7ae-b6d7a36f327f"
      },
      "source": [
        "for column in obj_columns:\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(dataset_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 5\n",
            "P_emaildomain 60\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_RMIz3NTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2e5b2844-8bdd-45ee-f5ec-83cd54c9cba7"
      },
      "source": [
        "for column in obj_columns_test:\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(test_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 4\n",
            "P_emaildomain 61\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}