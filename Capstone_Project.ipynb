{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4e24a44e-3354-4554-83f0-a710b3dce779"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab7366be-72dc-471b-d601-90a63510d196"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d8458a75-2d81-4207-e266-7d9cb16a11f5"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)\n",
        "saved_columns= np.array(dataset_identity.columns)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5caeeaab-978f-4be4-acd7-9c430a90fa26"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ba94e6cc-01f7-4a7f-9b3f-830b180eaa94"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4MWdmZ8wBEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = dataset_identity.pop(column)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS60VEEMwgHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "582002f5-7e4f-45a0-a186-2a1488865880"
      },
      "source": [
        "merged_data = pd.merge(left=dataset_transaction, right=dataset_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "dataset_transaction = None\n",
        "dataset_identity = None\n",
        "merged_data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 430)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIAS8CbdwwET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1eb06523-beee-475c-9aea-bd9d6578a88c"
      },
      "source": [
        "merged_data.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a12994b-6a1c-45d0-d4e8-800ce5f0fabf"
      },
      "source": [
        "dataset_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "dataset_identity = None\n",
        "\n",
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "cache = dict()\n",
        "print(len(float_columns), len(int_columns), len(obj_columns))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e2fefba1-c1bd-4cca-b372-0f0216f89ded"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN column for every features\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "8ffa7e2a-5efd-4a21-e0b1-7df71803638b"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 829 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  id_26_NaN_Code  id_32_NaN_Code\n",
              "0        2987000        0  ...               1               1\n",
              "1        2987001        0  ...               1               1\n",
              "2        2987002        0  ...               1               1\n",
              "3        2987003        0  ...               1               1\n",
              "4        2987004        0  ...               1               0\n",
              "\n",
              "[5 rows x 829 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ae073d23-44f5-49d8-bf40-91f3a70118f5"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "      <th>TransactionDT_NaN_Code</th>\n",
              "      <th>card1_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 831 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  TransactionDT_NaN_Code  card1_NaN_Code\n",
              "0        2987000        0  ...                       0               0\n",
              "1        2987001        0  ...                       0               0\n",
              "2        2987002        0  ...                       0               0\n",
              "3        2987003        0  ...                       0               0\n",
              "4        2987004        0  ...                       0               0\n",
              "\n",
              "[5 rows x 831 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48a89e22-4834-49b6-fe16-383a898ff5ff"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoded_column = 0\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "  cache[column] = dataset_transaction[column].values.reshape(-1,1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuvQmMmLRnM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "87cf777e-f44b-44b4-a3de-8fbe52983f62"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1011 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 1011 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b4c60a9-dcc9-43c3-85b2-320733a7d0a7"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoc4TuIx1zoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7748ab31-6eec-4551-bf5a-fe90f71ed7b0"
      },
      "source": [
        "out = ['isFraud']\n",
        "for column in dataset_transaction.columns:\n",
        "  if column.find('R_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "  if column.find('P_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "print(out)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['isFraud', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'P_emaildomain_5', 'P_emaildomain_6', 'P_emaildomain_7', 'P_emaildomain_8', 'P_emaildomain_9', 'P_emaildomain_10', 'P_emaildomain_11', 'P_emaildomain_12', 'P_emaildomain_13', 'P_emaildomain_14', 'P_emaildomain_15', 'P_emaildomain_16', 'P_emaildomain_17', 'P_emaildomain_18', 'P_emaildomain_19', 'P_emaildomain_20', 'P_emaildomain_21', 'P_emaildomain_22', 'P_emaildomain_23', 'P_emaildomain_24', 'P_emaildomain_25', 'P_emaildomain_26', 'P_emaildomain_27', 'P_emaildomain_28', 'P_emaildomain_29', 'P_emaildomain_30', 'P_emaildomain_31', 'P_emaildomain_32', 'P_emaildomain_33', 'P_emaildomain_34', 'P_emaildomain_35', 'P_emaildomain_36', 'P_emaildomain_37', 'P_emaildomain_38', 'P_emaildomain_39', 'P_emaildomain_40', 'P_emaildomain_41', 'P_emaildomain_42', 'P_emaildomain_43', 'P_emaildomain_44', 'P_emaildomain_45', 'P_emaildomain_46', 'P_emaildomain_47', 'P_emaildomain_48', 'P_emaildomain_49', 'P_emaildomain_50', 'P_emaildomain_51', 'P_emaildomain_52', 'P_emaildomain_53', 'P_emaildomain_54', 'P_emaildomain_55', 'P_emaildomain_56', 'P_emaildomain_57', 'P_emaildomain_58', 'P_emaildomain_59', 'R_emaildomain_0', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'R_emaildomain_4', 'R_emaildomain_5', 'R_emaildomain_6', 'R_emaildomain_7', 'R_emaildomain_8', 'R_emaildomain_9', 'R_emaildomain_10', 'R_emaildomain_11', 'R_emaildomain_12', 'R_emaildomain_13', 'R_emaildomain_14', 'R_emaildomain_15', 'R_emaildomain_16', 'R_emaildomain_17', 'R_emaildomain_18', 'R_emaildomain_19', 'R_emaildomain_20', 'R_emaildomain_21', 'R_emaildomain_22', 'R_emaildomain_23', 'R_emaildomain_24', 'R_emaildomain_25', 'R_emaildomain_26', 'R_emaildomain_27', 'R_emaildomain_28', 'R_emaildomain_29', 'R_emaildomain_30', 'R_emaildomain_31', 'R_emaildomain_32', 'R_emaildomain_33', 'R_emaildomain_34', 'R_emaildomain_35', 'R_emaildomain_36', 'R_emaildomain_37', 'R_emaildomain_38', 'R_emaildomain_39', 'R_emaildomain_40', 'R_emaildomain_41', 'R_emaildomain_42', 'R_emaildomain_43', 'R_emaildomain_44', 'R_emaildomain_45', 'R_emaildomain_46', 'R_emaildomain_47', 'R_emaildomain_48', 'R_emaildomain_49', 'R_emaildomain_50', 'R_emaildomain_51', 'R_emaildomain_52', 'R_emaildomain_53', 'R_emaildomain_54', 'R_emaildomain_55', 'R_emaildomain_56', 'R_emaildomain_57', 'R_emaildomain_58', 'R_emaildomain_59', 'R_emaildomain_60']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#columns_to_analyze = ['isFraud', 'DeviceType_0', 'DeviceType_1', 'DeviceType_2', 'id_15_0', 'id_15_1', 'id_15_2', 'id_15_3']#, 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2', 'card1', 'card2', 'card3']\n",
        "columns_to_analyze = out\n",
        "\n",
        "analyzing_data = dataset_transaction[columns_to_analyze]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "to_display = False\n",
        "\n",
        "if to_display:\n",
        "  mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD5CKASq2rzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the weak correlation features\n",
        "col = corr.columns\n",
        "is_fraud = np.where(col=='isFraud')[0][0]\n",
        "col = col.to_list()\n",
        "col.pop(is_fraud)\n",
        "to_remove = []\n",
        "for each_col in col:\n",
        "  if abs(corr['isFraud'][each_col]) < 0.05: # Weak correlation\n",
        "    to_remove.append(each_col)\n",
        "    a = dataset_transaction.pop(each_col)\n",
        "print(len(to_remove))\n",
        "analyzing_data = None\n",
        "\n",
        "\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "a = dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)\n",
        "X_train = X\n",
        "Y_train = Y\n",
        "\n",
        "#test_size = 20000\n",
        "#indices = np.random.randint(0, len(Y), size=(test_size,))\n",
        "#X_test = np.array(X_train)[indices]\n",
        "#Y_test = np.array(Y_train)[indices]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a09ad868-962e-4ccc-90d4-e059ea574ef6"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21ccb0e2-131b-44e3-b88d-1c157e94e35a"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 5\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_new = np.array(X_train)[indices_0_new]\n",
        "Y_new = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_new, test_size=0.05)\n",
        "\n",
        "X_to_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_train, axis=1)\n",
        "\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[2]))\n",
        "Y_test = np.squeeze(Y_test, axis=1)\n",
        "\n",
        "\n",
        "print(X_to_train.shape, X_test.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569877, 1)\n",
            "(659162, 891) (34693, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "93bca1dd-f377-49cc-ccc5-3cf4d27f33fe"
      },
      "source": [
        "X_new = None\n",
        "Y_new = None\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 17.88%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  \n",
        "  out_model.add(Dense(dense1, activation='relu',\n",
        "                      input_shape=(X_to_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(dense2, activation='relu', \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(int(dense2/2), activation='relu', \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(int(dense2/2), activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[METRICS])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "03041623-c207-4b0b-9042-e59f23238977"
      },
      "source": [
        "#my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=1e-4, l2_rate=5e-4, init_std=0.1, lr=0.00008)\n",
        "my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=5e-5, l2_rate=1e-5, init_std=0.1, lr=0.0001)\n",
        "my_model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_26 (Dense)             (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 477,825\n",
            "Trainable params: 476,545\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4a159b1-0864-4d38-8acf-a2eca013a313"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "NB_EPOCH = 2000\n",
        "PATIENCE = 20\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', patience=PATIENCE, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./best_model.h5', monitor='val_auc', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.01, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 2.2142 - tp: 46564.0000 - fp: 50406.0000 - tn: 490261.0000 - fn: 71137.0000 - accuracy: 0.8154 - precision: 0.4802 - recall: 0.3956 - auc: 0.7411\n",
            "Epoch 00001: val_auc improved from -inf to 0.84667, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 2.2137 - tp: 46617.0000 - fp: 50423.0000 - tn: 490897.0000 - fn: 71225.0000 - accuracy: 0.8155 - precision: 0.4804 - recall: 0.3956 - auc: 0.7412 - val_loss: 1.7277 - val_tp: 397.0000 - val_fp: 60.0000 - val_tn: 5359.0000 - val_fn: 776.0000 - val_accuracy: 0.8732 - val_precision: 0.8687 - val_recall: 0.3384 - val_auc: 0.8467\n",
            "Epoch 2/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 1.5058 - tp: 44900.0000 - fp: 12930.0000 - tn: 522524.0000 - fn: 71678.0000 - accuracy: 0.8702 - precision: 0.7764 - recall: 0.3851 - auc: 0.8347\n",
            "Epoch 00002: val_auc improved from 0.84667 to 0.85686, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 1.5056 - tp: 44936.0000 - fp: 12942.0000 - tn: 522959.0000 - fn: 71733.0000 - accuracy: 0.8702 - precision: 0.7764 - recall: 0.3852 - auc: 0.8347 - val_loss: 1.2519 - val_tp: 490.0000 - val_fp: 83.0000 - val_tn: 5336.0000 - val_fn: 683.0000 - val_accuracy: 0.8838 - val_precision: 0.8551 - val_recall: 0.4177 - val_auc: 0.8569\n",
            "Epoch 3/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 1.0714 - tp: 48924.0000 - fp: 12751.0000 - tn: 523150.0000 - fn: 67745.0000 - accuracy: 0.8766 - precision: 0.7933 - recall: 0.4193 - auc: 0.8505\n",
            "Epoch 00003: val_auc improved from 0.85686 to 0.86082, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 1.0714 - tp: 48924.0000 - fp: 12751.0000 - tn: 523150.0000 - fn: 67745.0000 - accuracy: 0.8766 - precision: 0.7933 - recall: 0.4193 - auc: 0.8505 - val_loss: 0.8829 - val_tp: 573.0000 - val_fp: 147.0000 - val_tn: 5272.0000 - val_fn: 600.0000 - val_accuracy: 0.8867 - val_precision: 0.7958 - val_recall: 0.4885 - val_auc: 0.8608\n",
            "Epoch 4/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.7496 - tp: 52010.0000 - fp: 13285.0000 - tn: 522377.0000 - fn: 64616.0000 - accuracy: 0.8806 - precision: 0.7965 - recall: 0.4460 - auc: 0.8592\n",
            "Epoch 00004: val_auc did not improve from 0.86082\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.7495 - tp: 52032.0000 - fp: 13298.0000 - tn: 522603.0000 - fn: 64637.0000 - accuracy: 0.8806 - precision: 0.7964 - recall: 0.4460 - auc: 0.8592 - val_loss: 0.6294 - val_tp: 539.0000 - val_fp: 122.0000 - val_tn: 5297.0000 - val_fn: 634.0000 - val_accuracy: 0.8853 - val_precision: 0.8154 - val_recall: 0.4595 - val_auc: 0.8591\n",
            "Epoch 5/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.5648 - tp: 54380.0000 - fp: 13579.0000 - tn: 522299.0000 - fn: 62286.0000 - accuracy: 0.8837 - precision: 0.8002 - recall: 0.4661 - auc: 0.8651\n",
            "Epoch 00005: val_auc improved from 0.86082 to 0.86765, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.5648 - tp: 54381.0000 - fp: 13582.0000 - tn: 522319.0000 - fn: 62288.0000 - accuracy: 0.8837 - precision: 0.8002 - recall: 0.4661 - auc: 0.8651 - val_loss: 0.5115 - val_tp: 599.0000 - val_fp: 166.0000 - val_tn: 5253.0000 - val_fn: 574.0000 - val_accuracy: 0.8877 - val_precision: 0.7830 - val_recall: 0.5107 - val_auc: 0.8676\n",
            "Epoch 6/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.4850 - tp: 56671.0000 - fp: 13892.0000 - tn: 521988.0000 - fn: 59993.0000 - accuracy: 0.8868 - precision: 0.8031 - recall: 0.4858 - auc: 0.8700\n",
            "Epoch 00006: val_auc improved from 0.86765 to 0.87781, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.4850 - tp: 56675.0000 - fp: 13892.0000 - tn: 522009.0000 - fn: 59994.0000 - accuracy: 0.8868 - precision: 0.8031 - recall: 0.4858 - auc: 0.8700 - val_loss: 0.4584 - val_tp: 687.0000 - val_fp: 235.0000 - val_tn: 5184.0000 - val_fn: 486.0000 - val_accuracy: 0.8906 - val_precision: 0.7451 - val_recall: 0.5857 - val_auc: 0.8778\n",
            "Epoch 7/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.4434 - tp: 57515.0000 - fp: 13923.0000 - tn: 521955.0000 - fn: 59151.0000 - accuracy: 0.8880 - precision: 0.8051 - recall: 0.4930 - auc: 0.8735\n",
            "Epoch 00007: val_auc did not improve from 0.87781\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.4434 - tp: 57516.0000 - fp: 13924.0000 - tn: 521977.0000 - fn: 59153.0000 - accuracy: 0.8880 - precision: 0.8051 - recall: 0.4930 - auc: 0.8735 - val_loss: 0.4386 - val_tp: 716.0000 - val_fp: 304.0000 - val_tn: 5115.0000 - val_fn: 457.0000 - val_accuracy: 0.8846 - val_precision: 0.7020 - val_recall: 0.6104 - val_auc: 0.8774\n",
            "Epoch 8/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.4165 - tp: 58787.0000 - fp: 13864.0000 - tn: 521612.0000 - fn: 57769.0000 - accuracy: 0.8901 - precision: 0.8092 - recall: 0.5044 - auc: 0.8762\n",
            "Epoch 00008: val_auc did not improve from 0.87781\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.4166 - tp: 58841.0000 - fp: 13874.0000 - tn: 522027.0000 - fn: 57828.0000 - accuracy: 0.8901 - precision: 0.8092 - recall: 0.5043 - auc: 0.8762 - val_loss: 0.4124 - val_tp: 584.0000 - val_fp: 132.0000 - val_tn: 5287.0000 - val_fn: 589.0000 - val_accuracy: 0.8906 - val_precision: 0.8156 - val_recall: 0.4979 - val_auc: 0.8750\n",
            "Epoch 9/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3960 - tp: 59747.0000 - fp: 13729.0000 - tn: 521958.0000 - fn: 56854.0000 - accuracy: 0.8918 - precision: 0.8131 - recall: 0.5124 - auc: 0.8797\n",
            "Epoch 00009: val_auc improved from 0.87781 to 0.88079, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3960 - tp: 59780.0000 - fp: 13737.0000 - tn: 522164.0000 - fn: 56889.0000 - accuracy: 0.8918 - precision: 0.8131 - recall: 0.5124 - auc: 0.8797 - val_loss: 0.3861 - val_tp: 624.0000 - val_fp: 124.0000 - val_tn: 5295.0000 - val_fn: 549.0000 - val_accuracy: 0.8979 - val_precision: 0.8342 - val_recall: 0.5320 - val_auc: 0.8808\n",
            "Epoch 10/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.3804 - tp: 60669.0000 - fp: 14078.0000 - tn: 521191.0000 - fn: 55838.0000 - accuracy: 0.8927 - precision: 0.8117 - recall: 0.5207 - auc: 0.8826\n",
            "Epoch 00010: val_auc improved from 0.88079 to 0.88818, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3805 - tp: 60747.0000 - fp: 14098.0000 - tn: 521803.0000 - fn: 55922.0000 - accuracy: 0.8927 - precision: 0.8116 - recall: 0.5207 - auc: 0.8826 - val_loss: 0.3699 - val_tp: 614.0000 - val_fp: 120.0000 - val_tn: 5299.0000 - val_fn: 559.0000 - val_accuracy: 0.8970 - val_precision: 0.8365 - val_recall: 0.5234 - val_auc: 0.8882\n",
            "Epoch 11/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3682 - tp: 61099.0000 - fp: 13523.0000 - tn: 522354.0000 - fn: 55568.0000 - accuracy: 0.8941 - precision: 0.8188 - recall: 0.5237 - auc: 0.8856\n",
            "Epoch 00011: val_auc improved from 0.88818 to 0.89582, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3682 - tp: 61100.0000 - fp: 13523.0000 - tn: 522378.0000 - fn: 55569.0000 - accuracy: 0.8941 - precision: 0.8188 - recall: 0.5237 - auc: 0.8856 - val_loss: 0.3571 - val_tp: 550.0000 - val_fp: 70.0000 - val_tn: 5349.0000 - val_fn: 623.0000 - val_accuracy: 0.8949 - val_precision: 0.8871 - val_recall: 0.4689 - val_auc: 0.8958\n",
            "Epoch 12/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3577 - tp: 62196.0000 - fp: 13695.0000 - tn: 521980.0000 - fn: 54417.0000 - accuracy: 0.8956 - precision: 0.8195 - recall: 0.5334 - auc: 0.8886\n",
            "Epoch 00012: val_auc did not improve from 0.89582\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3577 - tp: 62225.0000 - fp: 13698.0000 - tn: 522203.0000 - fn: 54444.0000 - accuracy: 0.8956 - precision: 0.8196 - recall: 0.5333 - auc: 0.8886 - val_loss: 0.3451 - val_tp: 679.0000 - val_fp: 161.0000 - val_tn: 5258.0000 - val_fn: 494.0000 - val_accuracy: 0.9006 - val_precision: 0.8083 - val_recall: 0.5789 - val_auc: 0.8947\n",
            "Epoch 13/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3486 - tp: 62949.0000 - fp: 13500.0000 - tn: 522180.0000 - fn: 53659.0000 - accuracy: 0.8970 - precision: 0.8234 - recall: 0.5398 - auc: 0.8914\n",
            "Epoch 00013: val_auc did not improve from 0.89582\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3486 - tp: 62983.0000 - fp: 13505.0000 - tn: 522396.0000 - fn: 53686.0000 - accuracy: 0.8970 - precision: 0.8234 - recall: 0.5398 - auc: 0.8914 - val_loss: 0.3531 - val_tp: 753.0000 - val_fp: 302.0000 - val_tn: 5117.0000 - val_fn: 420.0000 - val_accuracy: 0.8905 - val_precision: 0.7137 - val_recall: 0.6419 - val_auc: 0.8930\n",
            "Epoch 14/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3420 - tp: 63817.0000 - fp: 13866.0000 - tn: 522016.0000 - fn: 52845.0000 - accuracy: 0.8978 - precision: 0.8215 - recall: 0.5470 - auc: 0.8935\n",
            "Epoch 00014: val_auc improved from 0.89582 to 0.90161, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3420 - tp: 63820.0000 - fp: 13867.0000 - tn: 522034.0000 - fn: 52849.0000 - accuracy: 0.8978 - precision: 0.8215 - recall: 0.5470 - auc: 0.8935 - val_loss: 0.3353 - val_tp: 606.0000 - val_fp: 93.0000 - val_tn: 5326.0000 - val_fn: 567.0000 - val_accuracy: 0.8999 - val_precision: 0.8670 - val_recall: 0.5166 - val_auc: 0.9016\n",
            "Epoch 15/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.3357 - tp: 64445.0000 - fp: 13865.0000 - tn: 521374.0000 - fn: 52092.0000 - accuracy: 0.8988 - precision: 0.8229 - recall: 0.5530 - auc: 0.8959\n",
            "Epoch 00015: val_auc improved from 0.90161 to 0.90577, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3357 - tp: 64520.0000 - fp: 13882.0000 - tn: 522019.0000 - fn: 52149.0000 - accuracy: 0.8988 - precision: 0.8229 - recall: 0.5530 - auc: 0.8959 - val_loss: 0.3249 - val_tp: 642.0000 - val_fp: 116.0000 - val_tn: 5303.0000 - val_fn: 531.0000 - val_accuracy: 0.9019 - val_precision: 0.8470 - val_recall: 0.5473 - val_auc: 0.9058\n",
            "Epoch 16/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.3304 - tp: 65345.0000 - fp: 13935.0000 - tn: 521513.0000 - fn: 51239.0000 - accuracy: 0.9000 - precision: 0.8242 - recall: 0.5605 - auc: 0.8980\n",
            "Epoch 00016: val_auc did not improve from 0.90577\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3304 - tp: 65391.0000 - fp: 13947.0000 - tn: 521954.0000 - fn: 51278.0000 - accuracy: 0.9000 - precision: 0.8242 - recall: 0.5605 - auc: 0.8980 - val_loss: 0.3279 - val_tp: 591.0000 - val_fp: 90.0000 - val_tn: 5329.0000 - val_fn: 582.0000 - val_accuracy: 0.8981 - val_precision: 0.8678 - val_recall: 0.5038 - val_auc: 0.9020\n",
            "Epoch 17/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.3263 - tp: 65716.0000 - fp: 13795.0000 - tn: 522106.0000 - fn: 50953.0000 - accuracy: 0.9008 - precision: 0.8265 - recall: 0.5633 - auc: 0.8994\n",
            "Epoch 00017: val_auc improved from 0.90577 to 0.90731, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3263 - tp: 65716.0000 - fp: 13795.0000 - tn: 522106.0000 - fn: 50953.0000 - accuracy: 0.9008 - precision: 0.8265 - recall: 0.5633 - auc: 0.8994 - val_loss: 0.3250 - val_tp: 740.0000 - val_fp: 226.0000 - val_tn: 5193.0000 - val_fn: 433.0000 - val_accuracy: 0.9000 - val_precision: 0.7660 - val_recall: 0.6309 - val_auc: 0.9073\n",
            "Epoch 18/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3226 - tp: 66381.0000 - fp: 13919.0000 - tn: 521748.0000 - fn: 50240.0000 - accuracy: 0.9016 - precision: 0.8267 - recall: 0.5692 - auc: 0.9015\n",
            "Epoch 00018: val_auc improved from 0.90731 to 0.91103, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3226 - tp: 66405.0000 - fp: 13924.0000 - tn: 521977.0000 - fn: 50264.0000 - accuracy: 0.9016 - precision: 0.8267 - recall: 0.5692 - auc: 0.9015 - val_loss: 0.3125 - val_tp: 746.0000 - val_fp: 211.0000 - val_tn: 5208.0000 - val_fn: 427.0000 - val_accuracy: 0.9032 - val_precision: 0.7795 - val_recall: 0.6360 - val_auc: 0.9110\n",
            "Epoch 19/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3195 - tp: 66794.0000 - fp: 13931.0000 - tn: 521737.0000 - fn: 49826.0000 - accuracy: 0.9023 - precision: 0.8274 - recall: 0.5727 - auc: 0.9030\n",
            "Epoch 00019: val_auc did not improve from 0.91103\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3195 - tp: 66823.0000 - fp: 13936.0000 - tn: 521965.0000 - fn: 49846.0000 - accuracy: 0.9023 - precision: 0.8274 - recall: 0.5728 - auc: 0.9030 - val_loss: 0.3298 - val_tp: 600.0000 - val_fp: 70.0000 - val_tn: 5349.0000 - val_fn: 573.0000 - val_accuracy: 0.9025 - val_precision: 0.8955 - val_recall: 0.5115 - val_auc: 0.9055\n",
            "Epoch 20/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3169 - tp: 67480.0000 - fp: 14092.0000 - tn: 521787.0000 - fn: 49185.0000 - accuracy: 0.9030 - precision: 0.8272 - recall: 0.5784 - auc: 0.9042\n",
            "Epoch 00020: val_auc improved from 0.91103 to 0.91343, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3169 - tp: 67483.0000 - fp: 14092.0000 - tn: 521809.0000 - fn: 49186.0000 - accuracy: 0.9030 - precision: 0.8273 - recall: 0.5784 - auc: 0.9042 - val_loss: 0.3082 - val_tp: 743.0000 - val_fp: 220.0000 - val_tn: 5199.0000 - val_fn: 430.0000 - val_accuracy: 0.9014 - val_precision: 0.7715 - val_recall: 0.6334 - val_auc: 0.9134\n",
            "Epoch 21/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.3134 - tp: 68154.0000 - fp: 14425.0000 - tn: 520822.0000 - fn: 48375.0000 - accuracy: 0.9036 - precision: 0.8253 - recall: 0.5849 - auc: 0.9064\n",
            "Epoch 00021: val_auc improved from 0.91343 to 0.91492, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3135 - tp: 68233.0000 - fp: 14451.0000 - tn: 521450.0000 - fn: 48436.0000 - accuracy: 0.9036 - precision: 0.8252 - recall: 0.5848 - auc: 0.9064 - val_loss: 0.3071 - val_tp: 663.0000 - val_fp: 99.0000 - val_tn: 5320.0000 - val_fn: 510.0000 - val_accuracy: 0.9076 - val_precision: 0.8701 - val_recall: 0.5652 - val_auc: 0.9149\n",
            "Epoch 22/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.3112 - tp: 68612.0000 - fp: 14271.0000 - tn: 521197.0000 - fn: 47952.0000 - accuracy: 0.9046 - precision: 0.8278 - recall: 0.5886 - auc: 0.9076\n",
            "Epoch 00022: val_auc improved from 0.91492 to 0.91626, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3112 - tp: 68673.0000 - fp: 14280.0000 - tn: 521621.0000 - fn: 47996.0000 - accuracy: 0.9046 - precision: 0.8279 - recall: 0.5886 - auc: 0.9076 - val_loss: 0.3027 - val_tp: 632.0000 - val_fp: 93.0000 - val_tn: 5326.0000 - val_fn: 541.0000 - val_accuracy: 0.9038 - val_precision: 0.8717 - val_recall: 0.5388 - val_auc: 0.9163\n",
            "Epoch 23/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.3082 - tp: 69121.0000 - fp: 14111.0000 - tn: 521571.0000 - fn: 47485.0000 - accuracy: 0.9056 - precision: 0.8305 - recall: 0.5928 - auc: 0.9095\n",
            "Epoch 00023: val_auc improved from 0.91626 to 0.91820, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3082 - tp: 69156.0000 - fp: 14116.0000 - tn: 521785.0000 - fn: 47513.0000 - accuracy: 0.9056 - precision: 0.8305 - recall: 0.5928 - auc: 0.9095 - val_loss: 0.3117 - val_tp: 588.0000 - val_fp: 65.0000 - val_tn: 5354.0000 - val_fn: 585.0000 - val_accuracy: 0.9014 - val_precision: 0.9005 - val_recall: 0.5013 - val_auc: 0.9182\n",
            "Epoch 24/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.3058 - tp: 69587.0000 - fp: 14317.0000 - tn: 520934.0000 - fn: 46938.0000 - accuracy: 0.9060 - precision: 0.8294 - recall: 0.5972 - auc: 0.9111\n",
            "Epoch 00024: val_auc improved from 0.91820 to 0.92062, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.3058 - tp: 69671.0000 - fp: 14332.0000 - tn: 521569.0000 - fn: 46998.0000 - accuracy: 0.9060 - precision: 0.8294 - recall: 0.5972 - auc: 0.9111 - val_loss: 0.2966 - val_tp: 752.0000 - val_fp: 187.0000 - val_tn: 5232.0000 - val_fn: 421.0000 - val_accuracy: 0.9078 - val_precision: 0.8009 - val_recall: 0.6411 - val_auc: 0.9206\n",
            "Epoch 25/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3038 - tp: 70005.0000 - fp: 14104.0000 - tn: 521772.0000 - fn: 46663.0000 - accuracy: 0.9069 - precision: 0.8323 - recall: 0.6000 - auc: 0.9121\n",
            "Epoch 00025: val_auc improved from 0.92062 to 0.92276, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.3038 - tp: 70006.0000 - fp: 14105.0000 - tn: 521796.0000 - fn: 46663.0000 - accuracy: 0.9069 - precision: 0.8323 - recall: 0.6000 - auc: 0.9121 - val_loss: 0.2903 - val_tp: 715.0000 - val_fp: 120.0000 - val_tn: 5299.0000 - val_fn: 458.0000 - val_accuracy: 0.9123 - val_precision: 0.8563 - val_recall: 0.6095 - val_auc: 0.9228\n",
            "Epoch 26/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.3019 - tp: 70534.0000 - fp: 14215.0000 - tn: 521666.0000 - fn: 46129.0000 - accuracy: 0.9075 - precision: 0.8323 - recall: 0.6046 - auc: 0.9131\n",
            "Epoch 00026: val_auc did not improve from 0.92276\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.3020 - tp: 70537.0000 - fp: 14216.0000 - tn: 521685.0000 - fn: 46132.0000 - accuracy: 0.9075 - precision: 0.8323 - recall: 0.6046 - auc: 0.9131 - val_loss: 0.3124 - val_tp: 609.0000 - val_fp: 86.0000 - val_tn: 5333.0000 - val_fn: 564.0000 - val_accuracy: 0.9014 - val_precision: 0.8763 - val_recall: 0.5192 - val_auc: 0.9133\n",
            "Epoch 27/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2998 - tp: 70850.0000 - fp: 14324.0000 - tn: 521129.0000 - fn: 45729.0000 - accuracy: 0.9079 - precision: 0.8318 - recall: 0.6077 - auc: 0.9146\n",
            "Epoch 00027: val_auc did not improve from 0.92276\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2998 - tp: 70913.0000 - fp: 14337.0000 - tn: 521564.0000 - fn: 45756.0000 - accuracy: 0.9079 - precision: 0.8318 - recall: 0.6078 - auc: 0.9146 - val_loss: 0.2894 - val_tp: 726.0000 - val_fp: 137.0000 - val_tn: 5282.0000 - val_fn: 447.0000 - val_accuracy: 0.9114 - val_precision: 0.8413 - val_recall: 0.6189 - val_auc: 0.9223\n",
            "Epoch 28/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2985 - tp: 71135.0000 - fp: 14257.0000 - tn: 521644.0000 - fn: 45534.0000 - accuracy: 0.9084 - precision: 0.8330 - recall: 0.6097 - auc: 0.9151\n",
            "Epoch 00028: val_auc improved from 0.92276 to 0.92386, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2985 - tp: 71135.0000 - fp: 14257.0000 - tn: 521644.0000 - fn: 45534.0000 - accuracy: 0.9084 - precision: 0.8330 - recall: 0.6097 - auc: 0.9151 - val_loss: 0.2860 - val_tp: 724.0000 - val_fp: 127.0000 - val_tn: 5292.0000 - val_fn: 449.0000 - val_accuracy: 0.9126 - val_precision: 0.8508 - val_recall: 0.6172 - val_auc: 0.9239\n",
            "Epoch 29/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2965 - tp: 71472.0000 - fp: 14257.0000 - tn: 521416.0000 - fn: 45143.0000 - accuracy: 0.9089 - precision: 0.8337 - recall: 0.6129 - auc: 0.9165\n",
            "Epoch 00029: val_auc did not improve from 0.92386\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2965 - tp: 71505.0000 - fp: 14263.0000 - tn: 521638.0000 - fn: 45164.0000 - accuracy: 0.9089 - precision: 0.8337 - recall: 0.6129 - auc: 0.9165 - val_loss: 0.2997 - val_tp: 709.0000 - val_fp: 155.0000 - val_tn: 5264.0000 - val_fn: 464.0000 - val_accuracy: 0.9061 - val_precision: 0.8206 - val_recall: 0.6044 - val_auc: 0.9178\n",
            "Epoch 30/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2953 - tp: 71720.0000 - fp: 14483.0000 - tn: 520556.0000 - fn: 44761.0000 - accuracy: 0.9091 - precision: 0.8320 - recall: 0.6157 - auc: 0.9173\n",
            "Epoch 00030: val_auc improved from 0.92386 to 0.92545, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2953 - tp: 71846.0000 - fp: 14508.0000 - tn: 521393.0000 - fn: 44823.0000 - accuracy: 0.9091 - precision: 0.8320 - recall: 0.6158 - auc: 0.9173 - val_loss: 0.2863 - val_tp: 744.0000 - val_fp: 149.0000 - val_tn: 5270.0000 - val_fn: 429.0000 - val_accuracy: 0.9123 - val_precision: 0.8331 - val_recall: 0.6343 - val_auc: 0.9255\n",
            "Epoch 31/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2937 - tp: 72017.0000 - fp: 14356.0000 - tn: 521525.0000 - fn: 44646.0000 - accuracy: 0.9096 - precision: 0.8338 - recall: 0.6173 - auc: 0.9184\n",
            "Epoch 00031: val_auc improved from 0.92545 to 0.92745, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2937 - tp: 72019.0000 - fp: 14357.0000 - tn: 521544.0000 - fn: 44650.0000 - accuracy: 0.9096 - precision: 0.8338 - recall: 0.6173 - auc: 0.9183 - val_loss: 0.2817 - val_tp: 750.0000 - val_fp: 133.0000 - val_tn: 5286.0000 - val_fn: 423.0000 - val_accuracy: 0.9157 - val_precision: 0.8494 - val_recall: 0.6394 - val_auc: 0.9275\n",
            "Epoch 32/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2921 - tp: 72565.0000 - fp: 14393.0000 - tn: 521508.0000 - fn: 44104.0000 - accuracy: 0.9104 - precision: 0.8345 - recall: 0.6220 - auc: 0.9189\n",
            "Epoch 00032: val_auc improved from 0.92745 to 0.92834, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2921 - tp: 72565.0000 - fp: 14393.0000 - tn: 521508.0000 - fn: 44104.0000 - accuracy: 0.9104 - precision: 0.8345 - recall: 0.6220 - auc: 0.9189 - val_loss: 0.2805 - val_tp: 755.0000 - val_fp: 135.0000 - val_tn: 5284.0000 - val_fn: 418.0000 - val_accuracy: 0.9161 - val_precision: 0.8483 - val_recall: 0.6436 - val_auc: 0.9283\n",
            "Epoch 33/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2910 - tp: 72633.0000 - fp: 14343.0000 - tn: 521558.0000 - fn: 44036.0000 - accuracy: 0.9105 - precision: 0.8351 - recall: 0.6226 - auc: 0.9198\n",
            "Epoch 00033: val_auc did not improve from 0.92834\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2910 - tp: 72633.0000 - fp: 14343.0000 - tn: 521558.0000 - fn: 44036.0000 - accuracy: 0.9105 - precision: 0.8351 - recall: 0.6226 - auc: 0.9198 - val_loss: 0.2842 - val_tp: 706.0000 - val_fp: 104.0000 - val_tn: 5315.0000 - val_fn: 467.0000 - val_accuracy: 0.9134 - val_precision: 0.8716 - val_recall: 0.6019 - val_auc: 0.9244\n",
            "Epoch 34/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2901 - tp: 72775.0000 - fp: 14261.0000 - tn: 521410.0000 - fn: 43842.0000 - accuracy: 0.9109 - precision: 0.8361 - recall: 0.6241 - auc: 0.9203\n",
            "Epoch 00034: val_auc did not improve from 0.92834\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2901 - tp: 72811.0000 - fp: 14266.0000 - tn: 521635.0000 - fn: 43858.0000 - accuracy: 0.9109 - precision: 0.8362 - recall: 0.6241 - auc: 0.9203 - val_loss: 0.2880 - val_tp: 663.0000 - val_fp: 87.0000 - val_tn: 5332.0000 - val_fn: 510.0000 - val_accuracy: 0.9094 - val_precision: 0.8840 - val_recall: 0.5652 - val_auc: 0.9265\n",
            "Epoch 35/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2893 - tp: 72855.0000 - fp: 14294.0000 - tn: 520942.0000 - fn: 43685.0000 - accuracy: 0.9110 - precision: 0.8360 - recall: 0.6252 - auc: 0.9209\n",
            "Epoch 00035: val_auc improved from 0.92834 to 0.93169, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2893 - tp: 72925.0000 - fp: 14314.0000 - tn: 521587.0000 - fn: 43744.0000 - accuracy: 0.9110 - precision: 0.8359 - recall: 0.6251 - auc: 0.9209 - val_loss: 0.2753 - val_tp: 753.0000 - val_fp: 120.0000 - val_tn: 5299.0000 - val_fn: 420.0000 - val_accuracy: 0.9181 - val_precision: 0.8625 - val_recall: 0.6419 - val_auc: 0.9317\n",
            "Epoch 36/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2879 - tp: 73294.0000 - fp: 14334.0000 - tn: 521118.0000 - fn: 43286.0000 - accuracy: 0.9116 - precision: 0.8364 - recall: 0.6287 - auc: 0.9216\n",
            "Epoch 00036: val_auc improved from 0.93169 to 0.93207, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2879 - tp: 73350.0000 - fp: 14349.0000 - tn: 521552.0000 - fn: 43319.0000 - accuracy: 0.9116 - precision: 0.8364 - recall: 0.6287 - auc: 0.9216 - val_loss: 0.2742 - val_tp: 731.0000 - val_fp: 101.0000 - val_tn: 5318.0000 - val_fn: 442.0000 - val_accuracy: 0.9176 - val_precision: 0.8786 - val_recall: 0.6232 - val_auc: 0.9321\n",
            "Epoch 37/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2865 - tp: 73531.0000 - fp: 14310.0000 - tn: 521591.0000 - fn: 43138.0000 - accuracy: 0.9120 - precision: 0.8371 - recall: 0.6303 - auc: 0.9227\n",
            "Epoch 00037: val_auc did not improve from 0.93207\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2865 - tp: 73531.0000 - fp: 14310.0000 - tn: 521591.0000 - fn: 43138.0000 - accuracy: 0.9120 - precision: 0.8371 - recall: 0.6303 - auc: 0.9227 - val_loss: 0.2749 - val_tp: 756.0000 - val_fp: 135.0000 - val_tn: 5284.0000 - val_fn: 417.0000 - val_accuracy: 0.9163 - val_precision: 0.8485 - val_recall: 0.6445 - val_auc: 0.9308\n",
            "Epoch 38/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2860 - tp: 73638.0000 - fp: 14434.0000 - tn: 521041.0000 - fn: 42919.0000 - accuracy: 0.9120 - precision: 0.8361 - recall: 0.6318 - auc: 0.9230\n",
            "Epoch 00038: val_auc did not improve from 0.93207\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2860 - tp: 73704.0000 - fp: 14444.0000 - tn: 521457.0000 - fn: 42965.0000 - accuracy: 0.9120 - precision: 0.8361 - recall: 0.6317 - auc: 0.9230 - val_loss: 0.2733 - val_tp: 812.0000 - val_fp: 185.0000 - val_tn: 5234.0000 - val_fn: 361.0000 - val_accuracy: 0.9172 - val_precision: 0.8144 - val_recall: 0.6922 - val_auc: 0.9312\n",
            "Epoch 39/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2851 - tp: 73818.0000 - fp: 14295.0000 - tn: 521606.0000 - fn: 42851.0000 - accuracy: 0.9124 - precision: 0.8378 - recall: 0.6327 - auc: 0.9233\n",
            "Epoch 00039: val_auc did not improve from 0.93207\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2851 - tp: 73818.0000 - fp: 14295.0000 - tn: 521606.0000 - fn: 42851.0000 - accuracy: 0.9124 - precision: 0.8378 - recall: 0.6327 - auc: 0.9233 - val_loss: 0.2838 - val_tp: 717.0000 - val_fp: 106.0000 - val_tn: 5313.0000 - val_fn: 456.0000 - val_accuracy: 0.9147 - val_precision: 0.8712 - val_recall: 0.6113 - val_auc: 0.9282\n",
            "Epoch 40/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2835 - tp: 74100.0000 - fp: 14282.0000 - tn: 521598.0000 - fn: 42564.0000 - accuracy: 0.9129 - precision: 0.8384 - recall: 0.6352 - auc: 0.9245\n",
            "Epoch 00040: val_auc improved from 0.93207 to 0.93379, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2835 - tp: 74103.0000 - fp: 14282.0000 - tn: 521619.0000 - fn: 42566.0000 - accuracy: 0.9129 - precision: 0.8384 - recall: 0.6352 - auc: 0.9245 - val_loss: 0.2706 - val_tp: 835.0000 - val_fp: 193.0000 - val_tn: 5226.0000 - val_fn: 338.0000 - val_accuracy: 0.9194 - val_precision: 0.8123 - val_recall: 0.7118 - val_auc: 0.9338\n",
            "Epoch 41/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2833 - tp: 74068.0000 - fp: 14341.0000 - tn: 520927.0000 - fn: 42440.0000 - accuracy: 0.9129 - precision: 0.8378 - recall: 0.6357 - auc: 0.9244\n",
            "Epoch 00041: val_auc improved from 0.93379 to 0.93500, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2833 - tp: 74174.0000 - fp: 14355.0000 - tn: 521546.0000 - fn: 42495.0000 - accuracy: 0.9129 - precision: 0.8378 - recall: 0.6358 - auc: 0.9245 - val_loss: 0.2684 - val_tp: 821.0000 - val_fp: 169.0000 - val_tn: 5250.0000 - val_fn: 352.0000 - val_accuracy: 0.9210 - val_precision: 0.8293 - val_recall: 0.6999 - val_auc: 0.9350\n",
            "Epoch 42/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2826 - tp: 74290.0000 - fp: 14232.0000 - tn: 521230.0000 - fn: 42280.0000 - accuracy: 0.9133 - precision: 0.8392 - recall: 0.6373 - auc: 0.9249\n",
            "Epoch 00042: val_auc did not improve from 0.93500\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2827 - tp: 74348.0000 - fp: 14241.0000 - tn: 521660.0000 - fn: 42321.0000 - accuracy: 0.9133 - precision: 0.8392 - recall: 0.6373 - auc: 0.9249 - val_loss: 0.2990 - val_tp: 579.0000 - val_fp: 43.0000 - val_tn: 5376.0000 - val_fn: 594.0000 - val_accuracy: 0.9034 - val_precision: 0.9309 - val_recall: 0.4936 - val_auc: 0.9317\n",
            "Epoch 43/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2817 - tp: 74427.0000 - fp: 14317.0000 - tn: 521584.0000 - fn: 42242.0000 - accuracy: 0.9133 - precision: 0.8387 - recall: 0.6379 - auc: 0.9256\n",
            "Epoch 00043: val_auc improved from 0.93500 to 0.93659, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2817 - tp: 74427.0000 - fp: 14317.0000 - tn: 521584.0000 - fn: 42242.0000 - accuracy: 0.9133 - precision: 0.8387 - recall: 0.6379 - auc: 0.9256 - val_loss: 0.2642 - val_tp: 791.0000 - val_fp: 135.0000 - val_tn: 5284.0000 - val_fn: 382.0000 - val_accuracy: 0.9216 - val_precision: 0.8542 - val_recall: 0.6743 - val_auc: 0.9366\n",
            "Epoch 44/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2811 - tp: 74786.0000 - fp: 14404.0000 - tn: 521275.0000 - fn: 41823.0000 - accuracy: 0.9138 - precision: 0.8385 - recall: 0.6413 - auc: 0.9256\n",
            "Epoch 00044: val_auc improved from 0.93659 to 0.93701, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2811 - tp: 74818.0000 - fp: 14409.0000 - tn: 521492.0000 - fn: 41851.0000 - accuracy: 0.9138 - precision: 0.8385 - recall: 0.6413 - auc: 0.9256 - val_loss: 0.2691 - val_tp: 716.0000 - val_fp: 91.0000 - val_tn: 5328.0000 - val_fn: 457.0000 - val_accuracy: 0.9169 - val_precision: 0.8872 - val_recall: 0.6104 - val_auc: 0.9370\n",
            "Epoch 45/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2810 - tp: 74492.0000 - fp: 14253.0000 - tn: 521003.0000 - fn: 42028.0000 - accuracy: 0.9136 - precision: 0.8394 - recall: 0.6393 - auc: 0.9256\n",
            "Epoch 00045: val_auc did not improve from 0.93701\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2810 - tp: 74592.0000 - fp: 14274.0000 - tn: 521627.0000 - fn: 42077.0000 - accuracy: 0.9136 - precision: 0.8394 - recall: 0.6393 - auc: 0.9257 - val_loss: 0.2797 - val_tp: 884.0000 - val_fp: 288.0000 - val_tn: 5131.0000 - val_fn: 289.0000 - val_accuracy: 0.9125 - val_precision: 0.7543 - val_recall: 0.7536 - val_auc: 0.9346\n",
            "Epoch 46/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2798 - tp: 74814.0000 - fp: 14258.0000 - tn: 521202.0000 - fn: 41758.0000 - accuracy: 0.9141 - precision: 0.8399 - recall: 0.6418 - auc: 0.9265\n",
            "Epoch 00046: val_auc did not improve from 0.93701\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2798 - tp: 74876.0000 - fp: 14264.0000 - tn: 521637.0000 - fn: 41793.0000 - accuracy: 0.9141 - precision: 0.8400 - recall: 0.6418 - auc: 0.9265 - val_loss: 0.2735 - val_tp: 744.0000 - val_fp: 105.0000 - val_tn: 5314.0000 - val_fn: 429.0000 - val_accuracy: 0.9190 - val_precision: 0.8763 - val_recall: 0.6343 - val_auc: 0.9312\n",
            "Epoch 47/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2793 - tp: 74734.0000 - fp: 14231.0000 - tn: 521433.0000 - fn: 41890.0000 - accuracy: 0.9140 - precision: 0.8400 - recall: 0.6408 - auc: 0.9267\n",
            "Epoch 00047: val_auc improved from 0.93701 to 0.94073, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2794 - tp: 74760.0000 - fp: 14238.0000 - tn: 521663.0000 - fn: 41909.0000 - accuracy: 0.9140 - precision: 0.8400 - recall: 0.6408 - auc: 0.9267 - val_loss: 0.2624 - val_tp: 753.0000 - val_fp: 103.0000 - val_tn: 5316.0000 - val_fn: 420.0000 - val_accuracy: 0.9207 - val_precision: 0.8797 - val_recall: 0.6419 - val_auc: 0.9407\n",
            "Epoch 48/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2784 - tp: 75119.0000 - fp: 14266.0000 - tn: 521611.0000 - fn: 41548.0000 - accuracy: 0.9145 - precision: 0.8404 - recall: 0.6439 - auc: 0.9274\n",
            "Epoch 00048: val_auc did not improve from 0.94073\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2784 - tp: 75121.0000 - fp: 14266.0000 - tn: 521635.0000 - fn: 41548.0000 - accuracy: 0.9145 - precision: 0.8404 - recall: 0.6439 - auc: 0.9274 - val_loss: 0.2593 - val_tp: 807.0000 - val_fp: 139.0000 - val_tn: 5280.0000 - val_fn: 366.0000 - val_accuracy: 0.9234 - val_precision: 0.8531 - val_recall: 0.6880 - val_auc: 0.9394\n",
            "Epoch 49/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2780 - tp: 75147.0000 - fp: 14371.0000 - tn: 521294.0000 - fn: 41476.0000 - accuracy: 0.9144 - precision: 0.8395 - recall: 0.6444 - auc: 0.9277\n",
            "Epoch 00049: val_auc did not improve from 0.94073\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2780 - tp: 75178.0000 - fp: 14375.0000 - tn: 521526.0000 - fn: 41491.0000 - accuracy: 0.9144 - precision: 0.8395 - recall: 0.6444 - auc: 0.9277 - val_loss: 0.2795 - val_tp: 730.0000 - val_fp: 105.0000 - val_tn: 5314.0000 - val_fn: 443.0000 - val_accuracy: 0.9169 - val_precision: 0.8743 - val_recall: 0.6223 - val_auc: 0.9295\n",
            "Epoch 50/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2770 - tp: 75267.0000 - fp: 14137.0000 - tn: 521118.0000 - fn: 41254.0000 - accuracy: 0.9150 - precision: 0.8419 - recall: 0.6460 - auc: 0.9281\n",
            "Epoch 00050: val_auc improved from 0.94073 to 0.94125, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2770 - tp: 75372.0000 - fp: 14153.0000 - tn: 521748.0000 - fn: 41297.0000 - accuracy: 0.9150 - precision: 0.8419 - recall: 0.6460 - auc: 0.9281 - val_loss: 0.2605 - val_tp: 796.0000 - val_fp: 136.0000 - val_tn: 5283.0000 - val_fn: 377.0000 - val_accuracy: 0.9222 - val_precision: 0.8541 - val_recall: 0.6786 - val_auc: 0.9412\n",
            "Epoch 51/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2765 - tp: 75507.0000 - fp: 14236.0000 - tn: 521643.0000 - fn: 41158.0000 - accuracy: 0.9151 - precision: 0.8414 - recall: 0.6472 - auc: 0.9286\n",
            "Epoch 00051: val_auc did not improve from 0.94125\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2765 - tp: 75509.0000 - fp: 14237.0000 - tn: 521664.0000 - fn: 41160.0000 - accuracy: 0.9151 - precision: 0.8414 - recall: 0.6472 - auc: 0.9286 - val_loss: 0.2654 - val_tp: 772.0000 - val_fp: 129.0000 - val_tn: 5290.0000 - val_fn: 401.0000 - val_accuracy: 0.9196 - val_precision: 0.8568 - val_recall: 0.6581 - val_auc: 0.9375\n",
            "Epoch 52/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2759 - tp: 75589.0000 - fp: 14195.0000 - tn: 521046.0000 - fn: 40946.0000 - accuracy: 0.9154 - precision: 0.8419 - recall: 0.6486 - auc: 0.9290\n",
            "Epoch 00052: val_auc did not improve from 0.94125\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2759 - tp: 75676.0000 - fp: 14209.0000 - tn: 521692.0000 - fn: 40993.0000 - accuracy: 0.9154 - precision: 0.8419 - recall: 0.6486 - auc: 0.9290 - val_loss: 0.2597 - val_tp: 744.0000 - val_fp: 95.0000 - val_tn: 5324.0000 - val_fn: 429.0000 - val_accuracy: 0.9205 - val_precision: 0.8868 - val_recall: 0.6343 - val_auc: 0.9411\n",
            "Epoch 53/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2749 - tp: 75575.0000 - fp: 14214.0000 - tn: 521248.0000 - fn: 40995.0000 - accuracy: 0.9153 - precision: 0.8417 - recall: 0.6483 - auc: 0.9298\n",
            "Epoch 00053: val_auc did not improve from 0.94125\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2749 - tp: 75645.0000 - fp: 14225.0000 - tn: 521676.0000 - fn: 41024.0000 - accuracy: 0.9153 - precision: 0.8417 - recall: 0.6484 - auc: 0.9298 - val_loss: 0.2602 - val_tp: 734.0000 - val_fp: 86.0000 - val_tn: 5333.0000 - val_fn: 439.0000 - val_accuracy: 0.9204 - val_precision: 0.8951 - val_recall: 0.6257 - val_auc: 0.9408\n",
            "Epoch 54/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2742 - tp: 75902.0000 - fp: 14288.0000 - tn: 521169.0000 - fn: 40673.0000 - accuracy: 0.9157 - precision: 0.8416 - recall: 0.6511 - auc: 0.9300\n",
            "Epoch 00054: val_auc did not improve from 0.94125\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2742 - tp: 75967.0000 - fp: 14299.0000 - tn: 521602.0000 - fn: 40702.0000 - accuracy: 0.9157 - precision: 0.8416 - recall: 0.6511 - auc: 0.9300 - val_loss: 0.2734 - val_tp: 869.0000 - val_fp: 277.0000 - val_tn: 5142.0000 - val_fn: 304.0000 - val_accuracy: 0.9119 - val_precision: 0.7583 - val_recall: 0.7408 - val_auc: 0.9371\n",
            "Epoch 55/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2742 - tp: 75937.0000 - fp: 14249.0000 - tn: 521000.0000 - fn: 40590.0000 - accuracy: 0.9159 - precision: 0.8420 - recall: 0.6517 - auc: 0.9301\n",
            "Epoch 00055: val_auc did not improve from 0.94125\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2742 - tp: 76038.0000 - fp: 14268.0000 - tn: 521633.0000 - fn: 40631.0000 - accuracy: 0.9159 - precision: 0.8420 - recall: 0.6517 - auc: 0.9301 - val_loss: 0.2669 - val_tp: 708.0000 - val_fp: 80.0000 - val_tn: 5339.0000 - val_fn: 465.0000 - val_accuracy: 0.9173 - val_precision: 0.8985 - val_recall: 0.6036 - val_auc: 0.9411\n",
            "Epoch 56/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2735 - tp: 75915.0000 - fp: 14173.0000 - tn: 521728.0000 - fn: 40754.0000 - accuracy: 0.9158 - precision: 0.8427 - recall: 0.6507 - auc: 0.9303\n",
            "Epoch 00056: val_auc improved from 0.94125 to 0.94190, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2735 - tp: 75915.0000 - fp: 14173.0000 - tn: 521728.0000 - fn: 40754.0000 - accuracy: 0.9158 - precision: 0.8427 - recall: 0.6507 - auc: 0.9303 - val_loss: 0.2596 - val_tp: 829.0000 - val_fp: 177.0000 - val_tn: 5242.0000 - val_fn: 344.0000 - val_accuracy: 0.9210 - val_precision: 0.8241 - val_recall: 0.7067 - val_auc: 0.9419\n",
            "Epoch 57/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2729 - tp: 76153.0000 - fp: 14293.0000 - tn: 521382.0000 - fn: 40460.0000 - accuracy: 0.9161 - precision: 0.8420 - recall: 0.6530 - auc: 0.9307\n",
            "Epoch 00057: val_auc did not improve from 0.94190\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2729 - tp: 76190.0000 - fp: 14302.0000 - tn: 521599.0000 - fn: 40479.0000 - accuracy: 0.9161 - precision: 0.8420 - recall: 0.6530 - auc: 0.9307 - val_loss: 0.2612 - val_tp: 854.0000 - val_fp: 217.0000 - val_tn: 5202.0000 - val_fn: 319.0000 - val_accuracy: 0.9187 - val_precision: 0.7974 - val_recall: 0.7280 - val_auc: 0.9409\n",
            "Epoch 58/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2724 - tp: 76345.0000 - fp: 14162.0000 - tn: 521502.0000 - fn: 40279.0000 - accuracy: 0.9165 - precision: 0.8435 - recall: 0.6546 - auc: 0.9311\n",
            "Epoch 00058: val_auc improved from 0.94190 to 0.94511, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2724 - tp: 76374.0000 - fp: 14168.0000 - tn: 521733.0000 - fn: 40295.0000 - accuracy: 0.9165 - precision: 0.8435 - recall: 0.6546 - auc: 0.9311 - val_loss: 0.2540 - val_tp: 765.0000 - val_fp: 109.0000 - val_tn: 5310.0000 - val_fn: 408.0000 - val_accuracy: 0.9216 - val_precision: 0.8753 - val_recall: 0.6522 - val_auc: 0.9451\n",
            "Epoch 59/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2719 - tp: 76345.0000 - fp: 13978.0000 - tn: 521499.0000 - fn: 40210.0000 - accuracy: 0.9169 - precision: 0.8452 - recall: 0.6550 - auc: 0.9314\n",
            "Epoch 00059: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2719 - tp: 76416.0000 - fp: 13987.0000 - tn: 521914.0000 - fn: 40253.0000 - accuracy: 0.9169 - precision: 0.8453 - recall: 0.6550 - auc: 0.9314 - val_loss: 0.2549 - val_tp: 809.0000 - val_fp: 131.0000 - val_tn: 5288.0000 - val_fn: 364.0000 - val_accuracy: 0.9249 - val_precision: 0.8606 - val_recall: 0.6897 - val_auc: 0.9432\n",
            "Epoch 60/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2712 - tp: 76669.0000 - fp: 14179.0000 - tn: 521501.0000 - fn: 39939.0000 - accuracy: 0.9170 - precision: 0.8439 - recall: 0.6575 - auc: 0.9321\n",
            "Epoch 00060: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2712 - tp: 76712.0000 - fp: 14183.0000 - tn: 521718.0000 - fn: 39957.0000 - accuracy: 0.9170 - precision: 0.8440 - recall: 0.6575 - auc: 0.9321 - val_loss: 0.2632 - val_tp: 872.0000 - val_fp: 231.0000 - val_tn: 5188.0000 - val_fn: 301.0000 - val_accuracy: 0.9193 - val_precision: 0.7906 - val_recall: 0.7434 - val_auc: 0.9417\n",
            "Epoch 61/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2712 - tp: 76507.0000 - fp: 14072.0000 - tn: 521829.0000 - fn: 40162.0000 - accuracy: 0.9169 - precision: 0.8446 - recall: 0.6558 - auc: 0.9321\n",
            "Epoch 00061: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2712 - tp: 76507.0000 - fp: 14072.0000 - tn: 521829.0000 - fn: 40162.0000 - accuracy: 0.9169 - precision: 0.8446 - recall: 0.6558 - auc: 0.9321 - val_loss: 0.2567 - val_tp: 726.0000 - val_fp: 80.0000 - val_tn: 5339.0000 - val_fn: 447.0000 - val_accuracy: 0.9201 - val_precision: 0.9007 - val_recall: 0.6189 - val_auc: 0.9449\n",
            "Epoch 62/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2704 - tp: 76774.0000 - fp: 14130.0000 - tn: 521771.0000 - fn: 39895.0000 - accuracy: 0.9172 - precision: 0.8446 - recall: 0.6580 - auc: 0.9322\n",
            "Epoch 00062: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2704 - tp: 76774.0000 - fp: 14130.0000 - tn: 521771.0000 - fn: 39895.0000 - accuracy: 0.9172 - precision: 0.8446 - recall: 0.6580 - auc: 0.9322 - val_loss: 0.2543 - val_tp: 792.0000 - val_fp: 126.0000 - val_tn: 5293.0000 - val_fn: 381.0000 - val_accuracy: 0.9231 - val_precision: 0.8627 - val_recall: 0.6752 - val_auc: 0.9439\n",
            "Epoch 63/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2705 - tp: 76543.0000 - fp: 14045.0000 - tn: 521833.0000 - fn: 40123.0000 - accuracy: 0.9170 - precision: 0.8450 - recall: 0.6561 - auc: 0.9321\n",
            "Epoch 00063: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2705 - tp: 76546.0000 - fp: 14046.0000 - tn: 521855.0000 - fn: 40123.0000 - accuracy: 0.9170 - precision: 0.8450 - recall: 0.6561 - auc: 0.9321 - val_loss: 0.2548 - val_tp: 762.0000 - val_fp: 105.0000 - val_tn: 5314.0000 - val_fn: 411.0000 - val_accuracy: 0.9217 - val_precision: 0.8789 - val_recall: 0.6496 - val_auc: 0.9441\n",
            "Epoch 64/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2692 - tp: 76774.0000 - fp: 14037.0000 - tn: 521209.0000 - fn: 39756.0000 - accuracy: 0.9175 - precision: 0.8454 - recall: 0.6588 - auc: 0.9330\n",
            "Epoch 00064: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2692 - tp: 76862.0000 - fp: 14053.0000 - tn: 521848.0000 - fn: 39807.0000 - accuracy: 0.9175 - precision: 0.8454 - recall: 0.6588 - auc: 0.9330 - val_loss: 0.2510 - val_tp: 780.0000 - val_fp: 102.0000 - val_tn: 5317.0000 - val_fn: 393.0000 - val_accuracy: 0.9249 - val_precision: 0.8844 - val_recall: 0.6650 - val_auc: 0.9448\n",
            "Epoch 65/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2696 - tp: 76733.0000 - fp: 14273.0000 - tn: 520791.0000 - fn: 39723.0000 - accuracy: 0.9171 - precision: 0.8432 - recall: 0.6589 - auc: 0.9330\n",
            "Epoch 00065: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2696 - tp: 76861.0000 - fp: 14282.0000 - tn: 521619.0000 - fn: 39808.0000 - accuracy: 0.9171 - precision: 0.8433 - recall: 0.6588 - auc: 0.9331 - val_loss: 0.2651 - val_tp: 774.0000 - val_fp: 127.0000 - val_tn: 5292.0000 - val_fn: 399.0000 - val_accuracy: 0.9202 - val_precision: 0.8590 - val_recall: 0.6598 - val_auc: 0.9368\n",
            "Epoch 66/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2690 - tp: 76841.0000 - fp: 13763.0000 - tn: 522114.0000 - fn: 39826.0000 - accuracy: 0.9179 - precision: 0.8481 - recall: 0.6586 - auc: 0.9331\n",
            "Epoch 00066: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2690 - tp: 76842.0000 - fp: 13763.0000 - tn: 522138.0000 - fn: 39827.0000 - accuracy: 0.9179 - precision: 0.8481 - recall: 0.6586 - auc: 0.9331 - val_loss: 0.2574 - val_tp: 809.0000 - val_fp: 148.0000 - val_tn: 5271.0000 - val_fn: 364.0000 - val_accuracy: 0.9223 - val_precision: 0.8454 - val_recall: 0.6897 - val_auc: 0.9412\n",
            "Epoch 67/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2683 - tp: 77278.0000 - fp: 14242.0000 - tn: 521659.0000 - fn: 39391.0000 - accuracy: 0.9178 - precision: 0.8444 - recall: 0.6624 - auc: 0.9337\n",
            "Epoch 00067: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2683 - tp: 77278.0000 - fp: 14242.0000 - tn: 521659.0000 - fn: 39391.0000 - accuracy: 0.9178 - precision: 0.8444 - recall: 0.6624 - auc: 0.9337 - val_loss: 0.2623 - val_tp: 727.0000 - val_fp: 109.0000 - val_tn: 5310.0000 - val_fn: 446.0000 - val_accuracy: 0.9158 - val_precision: 0.8696 - val_recall: 0.6198 - val_auc: 0.9395\n",
            "Epoch 68/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2684 - tp: 76915.0000 - fp: 13869.0000 - tn: 521382.0000 - fn: 39610.0000 - accuracy: 0.9179 - precision: 0.8472 - recall: 0.6601 - auc: 0.9336\n",
            "Epoch 00068: val_auc did not improve from 0.94511\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2684 - tp: 77007.0000 - fp: 13890.0000 - tn: 522011.0000 - fn: 39662.0000 - accuracy: 0.9179 - precision: 0.8472 - recall: 0.6600 - auc: 0.9336 - val_loss: 0.2565 - val_tp: 727.0000 - val_fp: 77.0000 - val_tn: 5342.0000 - val_fn: 446.0000 - val_accuracy: 0.9207 - val_precision: 0.9042 - val_recall: 0.6198 - val_auc: 0.9449\n",
            "Epoch 69/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2678 - tp: 77435.0000 - fp: 14194.0000 - tn: 521476.0000 - fn: 39183.0000 - accuracy: 0.9182 - precision: 0.8451 - recall: 0.6640 - auc: 0.9341\n",
            "Epoch 00069: val_auc improved from 0.94511 to 0.94766, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2678 - tp: 77467.0000 - fp: 14201.0000 - tn: 521700.0000 - fn: 39202.0000 - accuracy: 0.9182 - precision: 0.8451 - recall: 0.6640 - auc: 0.9341 - val_loss: 0.2478 - val_tp: 767.0000 - val_fp: 88.0000 - val_tn: 5331.0000 - val_fn: 406.0000 - val_accuracy: 0.9251 - val_precision: 0.8971 - val_recall: 0.6539 - val_auc: 0.9477\n",
            "Epoch 70/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2680 - tp: 77138.0000 - fp: 13811.0000 - tn: 522068.0000 - fn: 39527.0000 - accuracy: 0.9183 - precision: 0.8481 - recall: 0.6612 - auc: 0.9336\n",
            "Epoch 00070: val_auc improved from 0.94766 to 0.94849, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2680 - tp: 77142.0000 - fp: 13812.0000 - tn: 522089.0000 - fn: 39527.0000 - accuracy: 0.9183 - precision: 0.8481 - recall: 0.6612 - auc: 0.9336 - val_loss: 0.2498 - val_tp: 741.0000 - val_fp: 90.0000 - val_tn: 5329.0000 - val_fn: 432.0000 - val_accuracy: 0.9208 - val_precision: 0.8917 - val_recall: 0.6317 - val_auc: 0.9485\n",
            "Epoch 71/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2673 - tp: 77229.0000 - fp: 14006.0000 - tn: 521245.0000 - fn: 39296.0000 - accuracy: 0.9182 - precision: 0.8465 - recall: 0.6628 - auc: 0.9344\n",
            "Epoch 00071: val_auc did not improve from 0.94849\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2673 - tp: 77322.0000 - fp: 14024.0000 - tn: 521877.0000 - fn: 39347.0000 - accuracy: 0.9182 - precision: 0.8465 - recall: 0.6627 - auc: 0.9344 - val_loss: 0.2500 - val_tp: 774.0000 - val_fp: 108.0000 - val_tn: 5311.0000 - val_fn: 399.0000 - val_accuracy: 0.9231 - val_precision: 0.8776 - val_recall: 0.6598 - val_auc: 0.9464\n",
            "Epoch 72/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2672 - tp: 77251.0000 - fp: 13886.0000 - tn: 521778.0000 - fn: 39373.0000 - accuracy: 0.9184 - precision: 0.8476 - recall: 0.6624 - auc: 0.9344\n",
            "Epoch 00072: val_auc did not improve from 0.94849\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2672 - tp: 77285.0000 - fp: 13893.0000 - tn: 522008.0000 - fn: 39384.0000 - accuracy: 0.9184 - precision: 0.8476 - recall: 0.6624 - auc: 0.9345 - val_loss: 0.2505 - val_tp: 801.0000 - val_fp: 132.0000 - val_tn: 5287.0000 - val_fn: 372.0000 - val_accuracy: 0.9235 - val_precision: 0.8585 - val_recall: 0.6829 - val_auc: 0.9457\n",
            "Epoch 73/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2666 - tp: 77431.0000 - fp: 13885.0000 - tn: 521565.0000 - fn: 39151.0000 - accuracy: 0.9187 - precision: 0.8479 - recall: 0.6642 - auc: 0.9346\n",
            "Epoch 00073: val_auc did not improve from 0.94849\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2666 - tp: 77487.0000 - fp: 13899.0000 - tn: 522002.0000 - fn: 39182.0000 - accuracy: 0.9187 - precision: 0.8479 - recall: 0.6642 - auc: 0.9346 - val_loss: 0.2533 - val_tp: 812.0000 - val_fp: 135.0000 - val_tn: 5284.0000 - val_fn: 361.0000 - val_accuracy: 0.9248 - val_precision: 0.8574 - val_recall: 0.6922 - val_auc: 0.9436\n",
            "Epoch 74/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2664 - tp: 77487.0000 - fp: 13966.0000 - tn: 521280.0000 - fn: 39043.0000 - accuracy: 0.9187 - precision: 0.8473 - recall: 0.6650 - auc: 0.9348\n",
            "Epoch 00074: val_auc did not improve from 0.94849\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2664 - tp: 77567.0000 - fp: 13983.0000 - tn: 521918.0000 - fn: 39102.0000 - accuracy: 0.9187 - precision: 0.8473 - recall: 0.6648 - auc: 0.9347 - val_loss: 0.2549 - val_tp: 784.0000 - val_fp: 124.0000 - val_tn: 5295.0000 - val_fn: 389.0000 - val_accuracy: 0.9222 - val_precision: 0.8634 - val_recall: 0.6684 - val_auc: 0.9431\n",
            "Epoch 75/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2662 - tp: 77612.0000 - fp: 14017.0000 - tn: 521219.0000 - fn: 38928.0000 - accuracy: 0.9188 - precision: 0.8470 - recall: 0.6660 - auc: 0.9350\n",
            "Epoch 00075: val_auc improved from 0.94849 to 0.95163, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2662 - tp: 77695.0000 - fp: 14038.0000 - tn: 521863.0000 - fn: 38974.0000 - accuracy: 0.9188 - precision: 0.8470 - recall: 0.6659 - auc: 0.9350 - val_loss: 0.2434 - val_tp: 790.0000 - val_fp: 98.0000 - val_tn: 5321.0000 - val_fn: 383.0000 - val_accuracy: 0.9270 - val_precision: 0.8896 - val_recall: 0.6735 - val_auc: 0.9516\n",
            "Epoch 76/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2657 - tp: 77818.0000 - fp: 14158.0000 - tn: 521743.0000 - fn: 38851.0000 - accuracy: 0.9188 - precision: 0.8461 - recall: 0.6670 - auc: 0.9355\n",
            "Epoch 00076: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2657 - tp: 77818.0000 - fp: 14158.0000 - tn: 521743.0000 - fn: 38851.0000 - accuracy: 0.9188 - precision: 0.8461 - recall: 0.6670 - auc: 0.9355 - val_loss: 0.2466 - val_tp: 768.0000 - val_fp: 96.0000 - val_tn: 5323.0000 - val_fn: 405.0000 - val_accuracy: 0.9240 - val_precision: 0.8889 - val_recall: 0.6547 - val_auc: 0.9483\n",
            "Epoch 77/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2653 - tp: 77623.0000 - fp: 13724.0000 - tn: 521935.0000 - fn: 39006.0000 - accuracy: 0.9192 - precision: 0.8498 - recall: 0.6656 - auc: 0.9357\n",
            "Epoch 00077: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2654 - tp: 77645.0000 - fp: 13733.0000 - tn: 522168.0000 - fn: 39024.0000 - accuracy: 0.9192 - precision: 0.8497 - recall: 0.6655 - auc: 0.9357 - val_loss: 0.2512 - val_tp: 740.0000 - val_fp: 88.0000 - val_tn: 5331.0000 - val_fn: 433.0000 - val_accuracy: 0.9210 - val_precision: 0.8937 - val_recall: 0.6309 - val_auc: 0.9471\n",
            "Epoch 78/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2658 - tp: 77639.0000 - fp: 13924.0000 - tn: 521977.0000 - fn: 39030.0000 - accuracy: 0.9189 - precision: 0.8479 - recall: 0.6655 - auc: 0.9352\n",
            "Epoch 00078: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2658 - tp: 77639.0000 - fp: 13924.0000 - tn: 521977.0000 - fn: 39030.0000 - accuracy: 0.9189 - precision: 0.8479 - recall: 0.6655 - auc: 0.9352 - val_loss: 0.2536 - val_tp: 874.0000 - val_fp: 229.0000 - val_tn: 5190.0000 - val_fn: 299.0000 - val_accuracy: 0.9199 - val_precision: 0.7924 - val_recall: 0.7451 - val_auc: 0.9465\n",
            "Epoch 79/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2655 - tp: 77577.0000 - fp: 13842.0000 - tn: 521417.0000 - fn: 38940.0000 - accuracy: 0.9190 - precision: 0.8486 - recall: 0.6658 - auc: 0.9356\n",
            "Epoch 00079: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2655 - tp: 77682.0000 - fp: 13859.0000 - tn: 522042.0000 - fn: 38987.0000 - accuracy: 0.9190 - precision: 0.8486 - recall: 0.6658 - auc: 0.9356 - val_loss: 0.2591 - val_tp: 851.0000 - val_fp: 203.0000 - val_tn: 5216.0000 - val_fn: 322.0000 - val_accuracy: 0.9204 - val_precision: 0.8074 - val_recall: 0.7255 - val_auc: 0.9430\n",
            "Epoch 80/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2645 - tp: 77827.0000 - fp: 13939.0000 - tn: 521537.0000 - fn: 38729.0000 - accuracy: 0.9192 - precision: 0.8481 - recall: 0.6677 - auc: 0.9362\n",
            "Epoch 00080: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2646 - tp: 77901.0000 - fp: 13951.0000 - tn: 521950.0000 - fn: 38768.0000 - accuracy: 0.9192 - precision: 0.8481 - recall: 0.6677 - auc: 0.9362 - val_loss: 0.2474 - val_tp: 783.0000 - val_fp: 104.0000 - val_tn: 5315.0000 - val_fn: 390.0000 - val_accuracy: 0.9251 - val_precision: 0.8828 - val_recall: 0.6675 - val_auc: 0.9485\n",
            "Epoch 81/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2641 - tp: 77811.0000 - fp: 13986.0000 - tn: 521246.0000 - fn: 38733.0000 - accuracy: 0.9191 - precision: 0.8476 - recall: 0.6677 - auc: 0.9365\n",
            "Epoch 00081: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2641 - tp: 77892.0000 - fp: 14000.0000 - tn: 521901.0000 - fn: 38777.0000 - accuracy: 0.9191 - precision: 0.8476 - recall: 0.6676 - auc: 0.9365 - val_loss: 0.2648 - val_tp: 700.0000 - val_fp: 62.0000 - val_tn: 5357.0000 - val_fn: 473.0000 - val_accuracy: 0.9188 - val_precision: 0.9186 - val_recall: 0.5968 - val_auc: 0.9418\n",
            "Epoch 82/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2643 - tp: 77766.0000 - fp: 13832.0000 - tn: 521416.0000 - fn: 38762.0000 - accuracy: 0.9193 - precision: 0.8490 - recall: 0.6674 - auc: 0.9364\n",
            "Epoch 00082: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2643 - tp: 77866.0000 - fp: 13850.0000 - tn: 522051.0000 - fn: 38803.0000 - accuracy: 0.9193 - precision: 0.8490 - recall: 0.6674 - auc: 0.9364 - val_loss: 0.2466 - val_tp: 876.0000 - val_fp: 184.0000 - val_tn: 5235.0000 - val_fn: 297.0000 - val_accuracy: 0.9270 - val_precision: 0.8264 - val_recall: 0.7468 - val_auc: 0.9483\n",
            "Epoch 83/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2643 - tp: 78107.0000 - fp: 14090.0000 - tn: 521374.0000 - fn: 38461.0000 - accuracy: 0.9194 - precision: 0.8472 - recall: 0.6701 - auc: 0.9365\n",
            "Epoch 00083: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2643 - tp: 78172.0000 - fp: 14101.0000 - tn: 521800.0000 - fn: 38497.0000 - accuracy: 0.9194 - precision: 0.8472 - recall: 0.6700 - auc: 0.9365 - val_loss: 0.2456 - val_tp: 784.0000 - val_fp: 107.0000 - val_tn: 5312.0000 - val_fn: 389.0000 - val_accuracy: 0.9248 - val_precision: 0.8799 - val_recall: 0.6684 - val_auc: 0.9485\n",
            "Epoch 84/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2637 - tp: 78049.0000 - fp: 13915.0000 - tn: 521334.0000 - fn: 38478.0000 - accuracy: 0.9196 - precision: 0.8487 - recall: 0.6698 - auc: 0.9366\n",
            "Epoch 00084: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2638 - tp: 78136.0000 - fp: 13929.0000 - tn: 521972.0000 - fn: 38533.0000 - accuracy: 0.9196 - precision: 0.8487 - recall: 0.6697 - auc: 0.9366 - val_loss: 0.2468 - val_tp: 869.0000 - val_fp: 186.0000 - val_tn: 5233.0000 - val_fn: 304.0000 - val_accuracy: 0.9257 - val_precision: 0.8237 - val_recall: 0.7408 - val_auc: 0.9483\n",
            "Epoch 85/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2635 - tp: 78261.0000 - fp: 14006.0000 - tn: 521677.0000 - fn: 38344.0000 - accuracy: 0.9197 - precision: 0.8482 - recall: 0.6712 - auc: 0.9367\n",
            "Epoch 00085: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2635 - tp: 78303.0000 - fp: 14009.0000 - tn: 521892.0000 - fn: 38366.0000 - accuracy: 0.9197 - precision: 0.8482 - recall: 0.6712 - auc: 0.9367 - val_loss: 0.2503 - val_tp: 862.0000 - val_fp: 178.0000 - val_tn: 5241.0000 - val_fn: 311.0000 - val_accuracy: 0.9258 - val_precision: 0.8288 - val_recall: 0.7349 - val_auc: 0.9476\n",
            "Epoch 86/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2638 - tp: 78052.0000 - fp: 13841.0000 - tn: 522037.0000 - fn: 38614.0000 - accuracy: 0.9196 - precision: 0.8494 - recall: 0.6690 - auc: 0.9367\n",
            "Epoch 00086: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2638 - tp: 78054.0000 - fp: 13841.0000 - tn: 522060.0000 - fn: 38615.0000 - accuracy: 0.9196 - precision: 0.8494 - recall: 0.6690 - auc: 0.9367 - val_loss: 0.2412 - val_tp: 813.0000 - val_fp: 126.0000 - val_tn: 5293.0000 - val_fn: 360.0000 - val_accuracy: 0.9263 - val_precision: 0.8658 - val_recall: 0.6931 - val_auc: 0.9515\n",
            "Epoch 87/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2634 - tp: 78397.0000 - fp: 13983.0000 - tn: 521899.0000 - fn: 38265.0000 - accuracy: 0.9199 - precision: 0.8486 - recall: 0.6720 - auc: 0.9367\n",
            "Epoch 00087: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2634 - tp: 78402.0000 - fp: 13983.0000 - tn: 521918.0000 - fn: 38267.0000 - accuracy: 0.9199 - precision: 0.8486 - recall: 0.6720 - auc: 0.9367 - val_loss: 0.2427 - val_tp: 816.0000 - val_fp: 121.0000 - val_tn: 5298.0000 - val_fn: 357.0000 - val_accuracy: 0.9275 - val_precision: 0.8709 - val_recall: 0.6957 - val_auc: 0.9496\n",
            "Epoch 88/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2627 - tp: 78292.0000 - fp: 13758.0000 - tn: 522122.0000 - fn: 38372.0000 - accuracy: 0.9201 - precision: 0.8505 - recall: 0.6711 - auc: 0.9373\n",
            "Epoch 00088: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2627 - tp: 78296.0000 - fp: 13760.0000 - tn: 522141.0000 - fn: 38373.0000 - accuracy: 0.9201 - precision: 0.8505 - recall: 0.6711 - auc: 0.9373 - val_loss: 0.2486 - val_tp: 843.0000 - val_fp: 156.0000 - val_tn: 5263.0000 - val_fn: 330.0000 - val_accuracy: 0.9263 - val_precision: 0.8438 - val_recall: 0.7187 - val_auc: 0.9490\n",
            "Epoch 89/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2624 - tp: 78577.0000 - fp: 13704.0000 - tn: 522197.0000 - fn: 38092.0000 - accuracy: 0.9206 - precision: 0.8515 - recall: 0.6735 - auc: 0.9373\n",
            "Epoch 00089: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2624 - tp: 78577.0000 - fp: 13704.0000 - tn: 522197.0000 - fn: 38092.0000 - accuracy: 0.9206 - precision: 0.8515 - recall: 0.6735 - auc: 0.9373 - val_loss: 0.2456 - val_tp: 764.0000 - val_fp: 87.0000 - val_tn: 5332.0000 - val_fn: 409.0000 - val_accuracy: 0.9248 - val_precision: 0.8978 - val_recall: 0.6513 - val_auc: 0.9504\n",
            "Epoch 90/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2628 - tp: 78202.0000 - fp: 13677.0000 - tn: 521579.0000 - fn: 38318.0000 - accuracy: 0.9202 - precision: 0.8511 - recall: 0.6711 - auc: 0.9371\n",
            "Epoch 00090: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2628 - tp: 78302.0000 - fp: 13689.0000 - tn: 522212.0000 - fn: 38367.0000 - accuracy: 0.9202 - precision: 0.8512 - recall: 0.6711 - auc: 0.9371 - val_loss: 0.2505 - val_tp: 755.0000 - val_fp: 82.0000 - val_tn: 5337.0000 - val_fn: 418.0000 - val_accuracy: 0.9242 - val_precision: 0.9020 - val_recall: 0.6436 - val_auc: 0.9496\n",
            "Epoch 91/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2623 - tp: 78500.0000 - fp: 13755.0000 - tn: 522122.0000 - fn: 38167.0000 - accuracy: 0.9204 - precision: 0.8509 - recall: 0.6729 - auc: 0.9375\n",
            "Epoch 00091: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2623 - tp: 78501.0000 - fp: 13755.0000 - tn: 522146.0000 - fn: 38168.0000 - accuracy: 0.9204 - precision: 0.8509 - recall: 0.6729 - auc: 0.9375 - val_loss: 0.2474 - val_tp: 850.0000 - val_fp: 163.0000 - val_tn: 5256.0000 - val_fn: 323.0000 - val_accuracy: 0.9263 - val_precision: 0.8391 - val_recall: 0.7246 - val_auc: 0.9483\n",
            "Epoch 92/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2622 - tp: 78380.0000 - fp: 13850.0000 - tn: 521604.0000 - fn: 38198.0000 - accuracy: 0.9202 - precision: 0.8498 - recall: 0.6723 - auc: 0.9380\n",
            "Epoch 00092: val_auc did not improve from 0.95163\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2622 - tp: 78441.0000 - fp: 13857.0000 - tn: 522044.0000 - fn: 38228.0000 - accuracy: 0.9202 - precision: 0.8499 - recall: 0.6723 - auc: 0.9380 - val_loss: 0.2467 - val_tp: 797.0000 - val_fp: 123.0000 - val_tn: 5296.0000 - val_fn: 376.0000 - val_accuracy: 0.9243 - val_precision: 0.8663 - val_recall: 0.6795 - val_auc: 0.9506\n",
            "Epoch 93/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2617 - tp: 78724.0000 - fp: 13940.0000 - tn: 521307.0000 - fn: 37805.0000 - accuracy: 0.9206 - precision: 0.8496 - recall: 0.6756 - auc: 0.9379\n",
            "Epoch 00093: val_auc improved from 0.95163 to 0.95210, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2617 - tp: 78819.0000 - fp: 13953.0000 - tn: 521948.0000 - fn: 37850.0000 - accuracy: 0.9206 - precision: 0.8496 - recall: 0.6756 - auc: 0.9379 - val_loss: 0.2421 - val_tp: 802.0000 - val_fp: 101.0000 - val_tn: 5318.0000 - val_fn: 371.0000 - val_accuracy: 0.9284 - val_precision: 0.8882 - val_recall: 0.6837 - val_auc: 0.9521\n",
            "Epoch 94/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2618 - tp: 78538.0000 - fp: 13965.0000 - tn: 521915.0000 - fn: 38126.0000 - accuracy: 0.9202 - precision: 0.8490 - recall: 0.6732 - auc: 0.9382\n",
            "Epoch 00094: val_auc improved from 0.95210 to 0.95257, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2618 - tp: 78540.0000 - fp: 13965.0000 - tn: 521936.0000 - fn: 38129.0000 - accuracy: 0.9202 - precision: 0.8490 - recall: 0.6732 - auc: 0.9382 - val_loss: 0.2400 - val_tp: 816.0000 - val_fp: 136.0000 - val_tn: 5283.0000 - val_fn: 357.0000 - val_accuracy: 0.9252 - val_precision: 0.8571 - val_recall: 0.6957 - val_auc: 0.9526\n",
            "Epoch 95/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2607 - tp: 78633.0000 - fp: 13862.0000 - tn: 521596.0000 - fn: 37941.0000 - accuracy: 0.9206 - precision: 0.8501 - recall: 0.6745 - auc: 0.9390\n",
            "Epoch 00095: val_auc did not improve from 0.95257\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2607 - tp: 78705.0000 - fp: 13873.0000 - tn: 522028.0000 - fn: 37964.0000 - accuracy: 0.9206 - precision: 0.8501 - recall: 0.6746 - auc: 0.9390 - val_loss: 0.2602 - val_tp: 745.0000 - val_fp: 84.0000 - val_tn: 5335.0000 - val_fn: 428.0000 - val_accuracy: 0.9223 - val_precision: 0.8987 - val_recall: 0.6351 - val_auc: 0.9450\n",
            "Epoch 96/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2617 - tp: 78415.0000 - fp: 14000.0000 - tn: 521470.0000 - fn: 38147.0000 - accuracy: 0.9200 - precision: 0.8485 - recall: 0.6727 - auc: 0.9383\n",
            "Epoch 00096: val_auc did not improve from 0.95257\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2617 - tp: 78489.0000 - fp: 14014.0000 - tn: 521887.0000 - fn: 38180.0000 - accuracy: 0.9200 - precision: 0.8485 - recall: 0.6727 - auc: 0.9383 - val_loss: 0.2430 - val_tp: 822.0000 - val_fp: 122.0000 - val_tn: 5297.0000 - val_fn: 351.0000 - val_accuracy: 0.9282 - val_precision: 0.8708 - val_recall: 0.7008 - val_auc: 0.9494\n",
            "Epoch 97/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2608 - tp: 78822.0000 - fp: 13798.0000 - tn: 521883.0000 - fn: 37785.0000 - accuracy: 0.9209 - precision: 0.8510 - recall: 0.6760 - auc: 0.9390\n",
            "Epoch 00097: val_auc did not improve from 0.95257\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2608 - tp: 78859.0000 - fp: 13806.0000 - tn: 522095.0000 - fn: 37810.0000 - accuracy: 0.9209 - precision: 0.8510 - recall: 0.6759 - auc: 0.9390 - val_loss: 0.2554 - val_tp: 745.0000 - val_fp: 90.0000 - val_tn: 5329.0000 - val_fn: 428.0000 - val_accuracy: 0.9214 - val_precision: 0.8922 - val_recall: 0.6351 - val_auc: 0.9470\n",
            "Epoch 98/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2616 - tp: 78792.0000 - fp: 14002.0000 - tn: 521877.0000 - fn: 37873.0000 - accuracy: 0.9205 - precision: 0.8491 - recall: 0.6754 - auc: 0.9383\n",
            "Epoch 00098: val_auc did not improve from 0.95257\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2616 - tp: 78795.0000 - fp: 14003.0000 - tn: 521898.0000 - fn: 37874.0000 - accuracy: 0.9205 - precision: 0.8491 - recall: 0.6754 - auc: 0.9383 - val_loss: 0.2452 - val_tp: 837.0000 - val_fp: 139.0000 - val_tn: 5280.0000 - val_fn: 336.0000 - val_accuracy: 0.9279 - val_precision: 0.8576 - val_recall: 0.7136 - val_auc: 0.9493\n",
            "Epoch 99/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2602 - tp: 78824.0000 - fp: 13907.0000 - tn: 521757.0000 - fn: 37800.0000 - accuracy: 0.9207 - precision: 0.8500 - recall: 0.6759 - auc: 0.9392\n",
            "Epoch 00099: val_auc did not improve from 0.95257\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2602 - tp: 78850.0000 - fp: 13912.0000 - tn: 521989.0000 - fn: 37819.0000 - accuracy: 0.9207 - precision: 0.8500 - recall: 0.6758 - auc: 0.9392 - val_loss: 0.2429 - val_tp: 761.0000 - val_fp: 96.0000 - val_tn: 5323.0000 - val_fn: 412.0000 - val_accuracy: 0.9229 - val_precision: 0.8880 - val_recall: 0.6488 - val_auc: 0.9518\n",
            "Epoch 100/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2599 - tp: 78759.0000 - fp: 13705.0000 - tn: 521335.0000 - fn: 37721.0000 - accuracy: 0.9211 - precision: 0.8518 - recall: 0.6762 - auc: 0.9395\n",
            "Epoch 00100: val_auc did not improve from 0.95257\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2599 - tp: 78876.0000 - fp: 13719.0000 - tn: 522182.0000 - fn: 37793.0000 - accuracy: 0.9211 - precision: 0.8518 - recall: 0.6761 - auc: 0.9395 - val_loss: 0.2436 - val_tp: 846.0000 - val_fp: 155.0000 - val_tn: 5264.0000 - val_fn: 327.0000 - val_accuracy: 0.9269 - val_precision: 0.8452 - val_recall: 0.7212 - val_auc: 0.9510\n",
            "Epoch 101/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2605 - tp: 78839.0000 - fp: 13789.0000 - tn: 522090.0000 - fn: 37826.0000 - accuracy: 0.9209 - precision: 0.8511 - recall: 0.6758 - auc: 0.9390\n",
            "Epoch 00101: val_auc improved from 0.95257 to 0.95352, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2605 - tp: 78843.0000 - fp: 13790.0000 - tn: 522111.0000 - fn: 37826.0000 - accuracy: 0.9209 - precision: 0.8511 - recall: 0.6758 - auc: 0.9390 - val_loss: 0.2393 - val_tp: 806.0000 - val_fp: 104.0000 - val_tn: 5315.0000 - val_fn: 367.0000 - val_accuracy: 0.9285 - val_precision: 0.8857 - val_recall: 0.6871 - val_auc: 0.9535\n",
            "Epoch 102/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2609 - tp: 78555.0000 - fp: 13729.0000 - tn: 522172.0000 - fn: 38114.0000 - accuracy: 0.9206 - precision: 0.8512 - recall: 0.6733 - auc: 0.9387\n",
            "Epoch 00102: val_auc did not improve from 0.95352\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2609 - tp: 78555.0000 - fp: 13729.0000 - tn: 522172.0000 - fn: 38114.0000 - accuracy: 0.9206 - precision: 0.8512 - recall: 0.6733 - auc: 0.9387 - val_loss: 0.2556 - val_tp: 736.0000 - val_fp: 90.0000 - val_tn: 5329.0000 - val_fn: 437.0000 - val_accuracy: 0.9201 - val_precision: 0.8910 - val_recall: 0.6275 - val_auc: 0.9487\n",
            "Epoch 103/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2598 - tp: 78995.0000 - fp: 13785.0000 - tn: 522116.0000 - fn: 37674.0000 - accuracy: 0.9211 - precision: 0.8514 - recall: 0.6771 - auc: 0.9394\n",
            "Epoch 00103: val_auc improved from 0.95352 to 0.95480, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2598 - tp: 78995.0000 - fp: 13785.0000 - tn: 522116.0000 - fn: 37674.0000 - accuracy: 0.9211 - precision: 0.8514 - recall: 0.6771 - auc: 0.9394 - val_loss: 0.2366 - val_tp: 806.0000 - val_fp: 105.0000 - val_tn: 5314.0000 - val_fn: 367.0000 - val_accuracy: 0.9284 - val_precision: 0.8847 - val_recall: 0.6871 - val_auc: 0.9548\n",
            "Epoch 104/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2597 - tp: 79052.0000 - fp: 13702.0000 - tn: 521975.0000 - fn: 37559.0000 - accuracy: 0.9214 - precision: 0.8523 - recall: 0.6779 - auc: 0.9393\n",
            "Epoch 00104: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2597 - tp: 79091.0000 - fp: 13706.0000 - tn: 522195.0000 - fn: 37578.0000 - accuracy: 0.9214 - precision: 0.8523 - recall: 0.6779 - auc: 0.9393 - val_loss: 0.2453 - val_tp: 759.0000 - val_fp: 100.0000 - val_tn: 5319.0000 - val_fn: 414.0000 - val_accuracy: 0.9220 - val_precision: 0.8836 - val_recall: 0.6471 - val_auc: 0.9509\n",
            "Epoch 105/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2600 - tp: 79030.0000 - fp: 13722.0000 - tn: 522179.0000 - fn: 37639.0000 - accuracy: 0.9213 - precision: 0.8521 - recall: 0.6774 - auc: 0.9392\n",
            "Epoch 00105: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2600 - tp: 79030.0000 - fp: 13722.0000 - tn: 522179.0000 - fn: 37639.0000 - accuracy: 0.9213 - precision: 0.8521 - recall: 0.6774 - auc: 0.9392 - val_loss: 0.2995 - val_tp: 691.0000 - val_fp: 90.0000 - val_tn: 5329.0000 - val_fn: 482.0000 - val_accuracy: 0.9132 - val_precision: 0.8848 - val_recall: 0.5891 - val_auc: 0.9307\n",
            "Epoch 106/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2600 - tp: 78837.0000 - fp: 14012.0000 - tn: 521226.0000 - fn: 37701.0000 - accuracy: 0.9207 - precision: 0.8491 - recall: 0.6765 - auc: 0.9395\n",
            "Epoch 00106: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2600 - tp: 78931.0000 - fp: 14028.0000 - tn: 521873.0000 - fn: 37738.0000 - accuracy: 0.9207 - precision: 0.8491 - recall: 0.6765 - auc: 0.9395 - val_loss: 0.2377 - val_tp: 866.0000 - val_fp: 153.0000 - val_tn: 5266.0000 - val_fn: 307.0000 - val_accuracy: 0.9302 - val_precision: 0.8499 - val_recall: 0.7383 - val_auc: 0.9532\n",
            "Epoch 107/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2598 - tp: 78878.0000 - fp: 13819.0000 - tn: 521856.0000 - fn: 37735.0000 - accuracy: 0.9210 - precision: 0.8509 - recall: 0.6764 - auc: 0.9395\n",
            "Epoch 00107: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2598 - tp: 78909.0000 - fp: 13822.0000 - tn: 522079.0000 - fn: 37760.0000 - accuracy: 0.9210 - precision: 0.8509 - recall: 0.6763 - auc: 0.9395 - val_loss: 0.2422 - val_tp: 884.0000 - val_fp: 190.0000 - val_tn: 5229.0000 - val_fn: 289.0000 - val_accuracy: 0.9273 - val_precision: 0.8231 - val_recall: 0.7536 - val_auc: 0.9523\n",
            "Epoch 108/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2596 - tp: 78823.0000 - fp: 13784.0000 - tn: 521677.0000 - fn: 37748.0000 - accuracy: 0.9210 - precision: 0.8512 - recall: 0.6762 - auc: 0.9397\n",
            "Epoch 00108: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2596 - tp: 78883.0000 - fp: 13788.0000 - tn: 522113.0000 - fn: 37786.0000 - accuracy: 0.9210 - precision: 0.8512 - recall: 0.6761 - auc: 0.9397 - val_loss: 0.2511 - val_tp: 904.0000 - val_fp: 236.0000 - val_tn: 5183.0000 - val_fn: 269.0000 - val_accuracy: 0.9234 - val_precision: 0.7930 - val_recall: 0.7707 - val_auc: 0.9488\n",
            "Epoch 109/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2594 - tp: 78901.0000 - fp: 13693.0000 - tn: 522187.0000 - fn: 37763.0000 - accuracy: 0.9211 - precision: 0.8521 - recall: 0.6763 - auc: 0.9399\n",
            "Epoch 00109: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2593 - tp: 78906.0000 - fp: 13693.0000 - tn: 522208.0000 - fn: 37763.0000 - accuracy: 0.9211 - precision: 0.8521 - recall: 0.6763 - auc: 0.9399 - val_loss: 0.2389 - val_tp: 803.0000 - val_fp: 106.0000 - val_tn: 5313.0000 - val_fn: 370.0000 - val_accuracy: 0.9278 - val_precision: 0.8834 - val_recall: 0.6846 - val_auc: 0.9536\n",
            "Epoch 110/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2590 - tp: 79074.0000 - fp: 13633.0000 - tn: 522038.0000 - fn: 37543.0000 - accuracy: 0.9215 - precision: 0.8529 - recall: 0.6781 - auc: 0.9400\n",
            "Epoch 00110: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2590 - tp: 79108.0000 - fp: 13642.0000 - tn: 522259.0000 - fn: 37561.0000 - accuracy: 0.9215 - precision: 0.8529 - recall: 0.6781 - auc: 0.9400 - val_loss: 0.2533 - val_tp: 753.0000 - val_fp: 91.0000 - val_tn: 5328.0000 - val_fn: 420.0000 - val_accuracy: 0.9225 - val_precision: 0.8922 - val_recall: 0.6419 - val_auc: 0.9485\n",
            "Epoch 111/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2593 - tp: 79033.0000 - fp: 13867.0000 - tn: 522012.0000 - fn: 37632.0000 - accuracy: 0.9211 - precision: 0.8507 - recall: 0.6774 - auc: 0.9398\n",
            "Epoch 00111: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2593 - tp: 79037.0000 - fp: 13868.0000 - tn: 522033.0000 - fn: 37632.0000 - accuracy: 0.9211 - precision: 0.8507 - recall: 0.6774 - auc: 0.9398 - val_loss: 0.2384 - val_tp: 870.0000 - val_fp: 160.0000 - val_tn: 5259.0000 - val_fn: 303.0000 - val_accuracy: 0.9298 - val_precision: 0.8447 - val_recall: 0.7417 - val_auc: 0.9528\n",
            "Epoch 112/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2586 - tp: 79111.0000 - fp: 13772.0000 - tn: 522109.0000 - fn: 37552.0000 - accuracy: 0.9213 - precision: 0.8517 - recall: 0.6781 - auc: 0.9405\n",
            "Epoch 00112: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2586 - tp: 79115.0000 - fp: 13772.0000 - tn: 522129.0000 - fn: 37554.0000 - accuracy: 0.9213 - precision: 0.8517 - recall: 0.6781 - auc: 0.9405 - val_loss: 0.2406 - val_tp: 808.0000 - val_fp: 110.0000 - val_tn: 5309.0000 - val_fn: 365.0000 - val_accuracy: 0.9279 - val_precision: 0.8802 - val_recall: 0.6888 - val_auc: 0.9516\n",
            "Epoch 113/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2585 - tp: 79088.0000 - fp: 13756.0000 - tn: 521488.0000 - fn: 37444.0000 - accuracy: 0.9214 - precision: 0.8518 - recall: 0.6787 - auc: 0.9405\n",
            "Epoch 00113: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2585 - tp: 79164.0000 - fp: 13777.0000 - tn: 522124.0000 - fn: 37505.0000 - accuracy: 0.9214 - precision: 0.8518 - recall: 0.6785 - auc: 0.9404 - val_loss: 0.2451 - val_tp: 774.0000 - val_fp: 91.0000 - val_tn: 5328.0000 - val_fn: 399.0000 - val_accuracy: 0.9257 - val_precision: 0.8948 - val_recall: 0.6598 - val_auc: 0.9510\n",
            "Epoch 114/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2584 - tp: 79343.0000 - fp: 13755.0000 - tn: 521693.0000 - fn: 37241.0000 - accuracy: 0.9218 - precision: 0.8523 - recall: 0.6806 - auc: 0.9403\n",
            "Epoch 00114: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2584 - tp: 79400.0000 - fp: 13769.0000 - tn: 522132.0000 - fn: 37269.0000 - accuracy: 0.9218 - precision: 0.8522 - recall: 0.6806 - auc: 0.9403 - val_loss: 0.2410 - val_tp: 860.0000 - val_fp: 146.0000 - val_tn: 5273.0000 - val_fn: 313.0000 - val_accuracy: 0.9304 - val_precision: 0.8549 - val_recall: 0.7332 - val_auc: 0.9509\n",
            "Epoch 115/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2590 - tp: 79179.0000 - fp: 13766.0000 - tn: 522135.0000 - fn: 37490.0000 - accuracy: 0.9215 - precision: 0.8519 - recall: 0.6787 - auc: 0.9401\n",
            "Epoch 00115: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2590 - tp: 79179.0000 - fp: 13766.0000 - tn: 522135.0000 - fn: 37490.0000 - accuracy: 0.9215 - precision: 0.8519 - recall: 0.6787 - auc: 0.9401 - val_loss: 0.2548 - val_tp: 916.0000 - val_fp: 268.0000 - val_tn: 5151.0000 - val_fn: 257.0000 - val_accuracy: 0.9204 - val_precision: 0.7736 - val_recall: 0.7809 - val_auc: 0.9492\n",
            "Epoch 116/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2576 - tp: 79142.0000 - fp: 13627.0000 - tn: 522274.0000 - fn: 37527.0000 - accuracy: 0.9216 - precision: 0.8531 - recall: 0.6783 - auc: 0.9411\n",
            "Epoch 00116: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2576 - tp: 79142.0000 - fp: 13627.0000 - tn: 522274.0000 - fn: 37527.0000 - accuracy: 0.9216 - precision: 0.8531 - recall: 0.6783 - auc: 0.9411 - val_loss: 0.2364 - val_tp: 833.0000 - val_fp: 129.0000 - val_tn: 5290.0000 - val_fn: 340.0000 - val_accuracy: 0.9289 - val_precision: 0.8659 - val_recall: 0.7101 - val_auc: 0.9540\n",
            "Epoch 117/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2585 - tp: 79218.0000 - fp: 13892.0000 - tn: 521557.0000 - fn: 37365.0000 - accuracy: 0.9214 - precision: 0.8508 - recall: 0.6795 - auc: 0.9405\n",
            "Epoch 00117: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2585 - tp: 79279.0000 - fp: 13902.0000 - tn: 521999.0000 - fn: 37390.0000 - accuracy: 0.9214 - precision: 0.8508 - recall: 0.6795 - auc: 0.9405 - val_loss: 0.2381 - val_tp: 844.0000 - val_fp: 121.0000 - val_tn: 5298.0000 - val_fn: 329.0000 - val_accuracy: 0.9317 - val_precision: 0.8746 - val_recall: 0.7195 - val_auc: 0.9521\n",
            "Epoch 118/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2579 - tp: 79237.0000 - fp: 13821.0000 - tn: 522055.0000 - fn: 37431.0000 - accuracy: 0.9215 - precision: 0.8515 - recall: 0.6792 - auc: 0.9409\n",
            "Epoch 00118: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2579 - tp: 79237.0000 - fp: 13821.0000 - tn: 522080.0000 - fn: 37432.0000 - accuracy: 0.9215 - precision: 0.8515 - recall: 0.6792 - auc: 0.9409 - val_loss: 0.2385 - val_tp: 791.0000 - val_fp: 91.0000 - val_tn: 5328.0000 - val_fn: 382.0000 - val_accuracy: 0.9282 - val_precision: 0.8968 - val_recall: 0.6743 - val_auc: 0.9543\n",
            "Epoch 119/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2582 - tp: 79115.0000 - fp: 13719.0000 - tn: 521734.0000 - fn: 37464.0000 - accuracy: 0.9215 - precision: 0.8522 - recall: 0.6786 - auc: 0.9406\n",
            "Epoch 00119: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2582 - tp: 79176.0000 - fp: 13734.0000 - tn: 522167.0000 - fn: 37493.0000 - accuracy: 0.9215 - precision: 0.8522 - recall: 0.6786 - auc: 0.9406 - val_loss: 0.2432 - val_tp: 832.0000 - val_fp: 143.0000 - val_tn: 5276.0000 - val_fn: 341.0000 - val_accuracy: 0.9266 - val_precision: 0.8533 - val_recall: 0.7093 - val_auc: 0.9499\n",
            "Epoch 120/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2581 - tp: 79015.0000 - fp: 13717.0000 - tn: 521742.0000 - fn: 37558.0000 - accuracy: 0.9214 - precision: 0.8521 - recall: 0.6778 - auc: 0.9406\n",
            "Epoch 00120: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2582 - tp: 79077.0000 - fp: 13727.0000 - tn: 522174.0000 - fn: 37592.0000 - accuracy: 0.9214 - precision: 0.8521 - recall: 0.6778 - auc: 0.9406 - val_loss: 0.2385 - val_tp: 842.0000 - val_fp: 144.0000 - val_tn: 5275.0000 - val_fn: 331.0000 - val_accuracy: 0.9279 - val_precision: 0.8540 - val_recall: 0.7178 - val_auc: 0.9532\n",
            "Epoch 121/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2580 - tp: 79432.0000 - fp: 13907.0000 - tn: 521994.0000 - fn: 37237.0000 - accuracy: 0.9216 - precision: 0.8510 - recall: 0.6808 - auc: 0.9410\n",
            "Epoch 00121: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2580 - tp: 79432.0000 - fp: 13907.0000 - tn: 521994.0000 - fn: 37237.0000 - accuracy: 0.9216 - precision: 0.8510 - recall: 0.6808 - auc: 0.9410 - val_loss: 0.2459 - val_tp: 746.0000 - val_fp: 76.0000 - val_tn: 5343.0000 - val_fn: 427.0000 - val_accuracy: 0.9237 - val_precision: 0.9075 - val_recall: 0.6360 - val_auc: 0.9522\n",
            "Epoch 122/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2573 - tp: 79279.0000 - fp: 13627.0000 - tn: 521603.0000 - fn: 37267.0000 - accuracy: 0.9219 - precision: 0.8533 - recall: 0.6802 - auc: 0.9413\n",
            "Epoch 00122: val_auc did not improve from 0.95480\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2572 - tp: 79365.0000 - fp: 13640.0000 - tn: 522261.0000 - fn: 37304.0000 - accuracy: 0.9219 - precision: 0.8533 - recall: 0.6803 - auc: 0.9413 - val_loss: 0.2403 - val_tp: 801.0000 - val_fp: 109.0000 - val_tn: 5310.0000 - val_fn: 372.0000 - val_accuracy: 0.9270 - val_precision: 0.8802 - val_recall: 0.6829 - val_auc: 0.9528\n",
            "Epoch 123/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2574 - tp: 79587.0000 - fp: 13739.0000 - tn: 522140.0000 - fn: 37078.0000 - accuracy: 0.9221 - precision: 0.8528 - recall: 0.6822 - auc: 0.9413\n",
            "Epoch 00123: val_auc improved from 0.95480 to 0.95533, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2574 - tp: 79589.0000 - fp: 13739.0000 - tn: 522162.0000 - fn: 37080.0000 - accuracy: 0.9221 - precision: 0.8528 - recall: 0.6822 - auc: 0.9413 - val_loss: 0.2385 - val_tp: 803.0000 - val_fp: 98.0000 - val_tn: 5321.0000 - val_fn: 370.0000 - val_accuracy: 0.9290 - val_precision: 0.8912 - val_recall: 0.6846 - val_auc: 0.9553\n",
            "Epoch 124/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2574 - tp: 79426.0000 - fp: 13787.0000 - tn: 521877.0000 - fn: 37198.0000 - accuracy: 0.9218 - precision: 0.8521 - recall: 0.6810 - auc: 0.9414\n",
            "Epoch 00124: val_auc did not improve from 0.95533\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2574 - tp: 79452.0000 - fp: 13792.0000 - tn: 522109.0000 - fn: 37217.0000 - accuracy: 0.9218 - precision: 0.8521 - recall: 0.6810 - auc: 0.9414 - val_loss: 0.2478 - val_tp: 818.0000 - val_fp: 139.0000 - val_tn: 5280.0000 - val_fn: 355.0000 - val_accuracy: 0.9251 - val_precision: 0.8548 - val_recall: 0.6974 - val_auc: 0.9478\n",
            "Epoch 125/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2570 - tp: 79612.0000 - fp: 13842.0000 - tn: 521201.0000 - fn: 36865.0000 - accuracy: 0.9222 - precision: 0.8519 - recall: 0.6835 - auc: 0.9415\n",
            "Epoch 00125: val_auc did not improve from 0.95533\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2570 - tp: 79744.0000 - fp: 13868.0000 - tn: 522033.0000 - fn: 36925.0000 - accuracy: 0.9222 - precision: 0.8519 - recall: 0.6835 - auc: 0.9415 - val_loss: 0.2376 - val_tp: 836.0000 - val_fp: 114.0000 - val_tn: 5305.0000 - val_fn: 337.0000 - val_accuracy: 0.9316 - val_precision: 0.8800 - val_recall: 0.7127 - val_auc: 0.9526\n",
            "Epoch 126/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2578 - tp: 79509.0000 - fp: 13870.0000 - tn: 522012.0000 - fn: 37153.0000 - accuracy: 0.9218 - precision: 0.8515 - recall: 0.6815 - auc: 0.9411\n",
            "Epoch 00126: val_auc improved from 0.95533 to 0.95545, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2578 - tp: 79513.0000 - fp: 13871.0000 - tn: 522030.0000 - fn: 37156.0000 - accuracy: 0.9218 - precision: 0.8515 - recall: 0.6815 - auc: 0.9411 - val_loss: 0.2374 - val_tp: 780.0000 - val_fp: 91.0000 - val_tn: 5328.0000 - val_fn: 393.0000 - val_accuracy: 0.9266 - val_precision: 0.8955 - val_recall: 0.6650 - val_auc: 0.9555\n",
            "Epoch 127/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2567 - tp: 79518.0000 - fp: 13744.0000 - tn: 522134.0000 - fn: 37148.0000 - accuracy: 0.9220 - precision: 0.8526 - recall: 0.6816 - auc: 0.9419\n",
            "Epoch 00127: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2567 - tp: 79519.0000 - fp: 13744.0000 - tn: 522157.0000 - fn: 37150.0000 - accuracy: 0.9220 - precision: 0.8526 - recall: 0.6816 - auc: 0.9419 - val_loss: 0.2444 - val_tp: 850.0000 - val_fp: 142.0000 - val_tn: 5277.0000 - val_fn: 323.0000 - val_accuracy: 0.9295 - val_precision: 0.8569 - val_recall: 0.7246 - val_auc: 0.9524\n",
            "Epoch 128/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2563 - tp: 79686.0000 - fp: 13513.0000 - tn: 522388.0000 - fn: 36983.0000 - accuracy: 0.9226 - precision: 0.8550 - recall: 0.6830 - auc: 0.9421\n",
            "Epoch 00128: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2563 - tp: 79686.0000 - fp: 13513.0000 - tn: 522388.0000 - fn: 36983.0000 - accuracy: 0.9226 - precision: 0.8550 - recall: 0.6830 - auc: 0.9421 - val_loss: 0.2397 - val_tp: 812.0000 - val_fp: 108.0000 - val_tn: 5311.0000 - val_fn: 361.0000 - val_accuracy: 0.9289 - val_precision: 0.8826 - val_recall: 0.6922 - val_auc: 0.9527\n",
            "Epoch 129/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2564 - tp: 79804.0000 - fp: 13658.0000 - tn: 522243.0000 - fn: 36865.0000 - accuracy: 0.9226 - precision: 0.8539 - recall: 0.6840 - auc: 0.9419\n",
            "Epoch 00129: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2564 - tp: 79804.0000 - fp: 13658.0000 - tn: 522243.0000 - fn: 36865.0000 - accuracy: 0.9226 - precision: 0.8539 - recall: 0.6840 - auc: 0.9419 - val_loss: 0.2415 - val_tp: 784.0000 - val_fp: 104.0000 - val_tn: 5315.0000 - val_fn: 389.0000 - val_accuracy: 0.9252 - val_precision: 0.8829 - val_recall: 0.6684 - val_auc: 0.9527\n",
            "Epoch 130/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2562 - tp: 79581.0000 - fp: 13649.0000 - tn: 522252.0000 - fn: 37088.0000 - accuracy: 0.9223 - precision: 0.8536 - recall: 0.6821 - auc: 0.9423\n",
            "Epoch 00130: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2562 - tp: 79581.0000 - fp: 13649.0000 - tn: 522252.0000 - fn: 37088.0000 - accuracy: 0.9223 - precision: 0.8536 - recall: 0.6821 - auc: 0.9423 - val_loss: 0.2489 - val_tp: 706.0000 - val_fp: 53.0000 - val_tn: 5366.0000 - val_fn: 467.0000 - val_accuracy: 0.9211 - val_precision: 0.9302 - val_recall: 0.6019 - val_auc: 0.9548\n",
            "Epoch 131/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2564 - tp: 79572.0000 - fp: 13557.0000 - tn: 522107.0000 - fn: 37052.0000 - accuracy: 0.9224 - precision: 0.8544 - recall: 0.6823 - auc: 0.9420\n",
            "Epoch 00131: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2563 - tp: 79604.0000 - fp: 13559.0000 - tn: 522342.0000 - fn: 37065.0000 - accuracy: 0.9224 - precision: 0.8545 - recall: 0.6823 - auc: 0.9420 - val_loss: 0.2408 - val_tp: 786.0000 - val_fp: 113.0000 - val_tn: 5306.0000 - val_fn: 387.0000 - val_accuracy: 0.9242 - val_precision: 0.8743 - val_recall: 0.6701 - val_auc: 0.9524\n",
            "Epoch 132/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2560 - tp: 79585.0000 - fp: 13516.0000 - tn: 522152.0000 - fn: 37035.0000 - accuracy: 0.9225 - precision: 0.8548 - recall: 0.6824 - auc: 0.9423\n",
            "Epoch 00132: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2560 - tp: 79617.0000 - fp: 13521.0000 - tn: 522380.0000 - fn: 37052.0000 - accuracy: 0.9225 - precision: 0.8548 - recall: 0.6824 - auc: 0.9423 - val_loss: 0.2442 - val_tp: 799.0000 - val_fp: 103.0000 - val_tn: 5316.0000 - val_fn: 374.0000 - val_accuracy: 0.9276 - val_precision: 0.8858 - val_recall: 0.6812 - val_auc: 0.9516\n",
            "Epoch 133/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2568 - tp: 79682.0000 - fp: 13508.0000 - tn: 521946.0000 - fn: 36896.0000 - accuracy: 0.9227 - precision: 0.8550 - recall: 0.6835 - auc: 0.9417\n",
            "Epoch 00133: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2568 - tp: 79739.0000 - fp: 13519.0000 - tn: 522382.0000 - fn: 36930.0000 - accuracy: 0.9227 - precision: 0.8550 - recall: 0.6835 - auc: 0.9417 - val_loss: 0.2402 - val_tp: 830.0000 - val_fp: 144.0000 - val_tn: 5275.0000 - val_fn: 343.0000 - val_accuracy: 0.9261 - val_precision: 0.8522 - val_recall: 0.7076 - val_auc: 0.9535\n",
            "Epoch 134/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2557 - tp: 79834.0000 - fp: 13516.0000 - tn: 522147.0000 - fn: 36791.0000 - accuracy: 0.9229 - precision: 0.8552 - recall: 0.6845 - auc: 0.9423\n",
            "Epoch 00134: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2557 - tp: 79860.0000 - fp: 13518.0000 - tn: 522383.0000 - fn: 36809.0000 - accuracy: 0.9229 - precision: 0.8552 - recall: 0.6845 - auc: 0.9423 - val_loss: 0.2380 - val_tp: 809.0000 - val_fp: 116.0000 - val_tn: 5303.0000 - val_fn: 364.0000 - val_accuracy: 0.9272 - val_precision: 0.8746 - val_recall: 0.6897 - val_auc: 0.9538\n",
            "Epoch 135/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2556 - tp: 79987.0000 - fp: 13620.0000 - tn: 522043.0000 - fn: 36638.0000 - accuracy: 0.9230 - precision: 0.8545 - recall: 0.6858 - auc: 0.9425\n",
            "Epoch 00135: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2556 - tp: 80018.0000 - fp: 13626.0000 - tn: 522275.0000 - fn: 36651.0000 - accuracy: 0.9230 - precision: 0.8545 - recall: 0.6859 - auc: 0.9425 - val_loss: 0.2394 - val_tp: 800.0000 - val_fp: 99.0000 - val_tn: 5320.0000 - val_fn: 373.0000 - val_accuracy: 0.9284 - val_precision: 0.8899 - val_recall: 0.6820 - val_auc: 0.9528\n",
            "Epoch 136/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2561 - tp: 79840.0000 - fp: 13649.0000 - tn: 522231.0000 - fn: 36824.0000 - accuracy: 0.9227 - precision: 0.8540 - recall: 0.6844 - auc: 0.9422\n",
            "Epoch 00136: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2561 - tp: 79844.0000 - fp: 13650.0000 - tn: 522251.0000 - fn: 36825.0000 - accuracy: 0.9227 - precision: 0.8540 - recall: 0.6844 - auc: 0.9422 - val_loss: 0.2355 - val_tp: 861.0000 - val_fp: 158.0000 - val_tn: 5261.0000 - val_fn: 312.0000 - val_accuracy: 0.9287 - val_precision: 0.8449 - val_recall: 0.7340 - val_auc: 0.9544\n",
            "Epoch 137/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2558 - tp: 79613.0000 - fp: 13688.0000 - tn: 521580.0000 - fn: 36895.0000 - accuracy: 0.9224 - precision: 0.8533 - recall: 0.6833 - auc: 0.9422\n",
            "Epoch 00137: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2559 - tp: 79713.0000 - fp: 13705.0000 - tn: 522196.0000 - fn: 36956.0000 - accuracy: 0.9224 - precision: 0.8533 - recall: 0.6832 - auc: 0.9422 - val_loss: 0.2340 - val_tp: 883.0000 - val_fp: 156.0000 - val_tn: 5263.0000 - val_fn: 290.0000 - val_accuracy: 0.9323 - val_precision: 0.8499 - val_recall: 0.7528 - val_auc: 0.9554\n",
            "Epoch 138/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2552 - tp: 79887.0000 - fp: 13512.0000 - tn: 521963.0000 - fn: 36670.0000 - accuracy: 0.9230 - precision: 0.8553 - recall: 0.6854 - auc: 0.9428\n",
            "Epoch 00138: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2553 - tp: 79956.0000 - fp: 13524.0000 - tn: 522377.0000 - fn: 36713.0000 - accuracy: 0.9230 - precision: 0.8553 - recall: 0.6853 - auc: 0.9428 - val_loss: 0.2443 - val_tp: 864.0000 - val_fp: 175.0000 - val_tn: 5244.0000 - val_fn: 309.0000 - val_accuracy: 0.9266 - val_precision: 0.8316 - val_recall: 0.7366 - val_auc: 0.9511\n",
            "Epoch 139/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2555 - tp: 79931.0000 - fp: 13730.0000 - tn: 522171.0000 - fn: 36738.0000 - accuracy: 0.9227 - precision: 0.8534 - recall: 0.6851 - auc: 0.9427\n",
            "Epoch 00139: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2555 - tp: 79931.0000 - fp: 13730.0000 - tn: 522171.0000 - fn: 36738.0000 - accuracy: 0.9227 - precision: 0.8534 - recall: 0.6851 - auc: 0.9427 - val_loss: 0.2370 - val_tp: 831.0000 - val_fp: 133.0000 - val_tn: 5286.0000 - val_fn: 342.0000 - val_accuracy: 0.9279 - val_precision: 0.8620 - val_recall: 0.7084 - val_auc: 0.9541\n",
            "Epoch 140/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2553 - tp: 80036.0000 - fp: 13688.0000 - tn: 521767.0000 - fn: 36541.0000 - accuracy: 0.9230 - precision: 0.8540 - recall: 0.6866 - auc: 0.9427\n",
            "Epoch 00140: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2553 - tp: 80093.0000 - fp: 13698.0000 - tn: 522203.0000 - fn: 36576.0000 - accuracy: 0.9230 - precision: 0.8540 - recall: 0.6865 - auc: 0.9427 - val_loss: 0.2352 - val_tp: 844.0000 - val_fp: 127.0000 - val_tn: 5292.0000 - val_fn: 329.0000 - val_accuracy: 0.9308 - val_precision: 0.8692 - val_recall: 0.7195 - val_auc: 0.9554\n",
            "Epoch 141/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2556 - tp: 79904.0000 - fp: 13690.0000 - tn: 522185.0000 - fn: 36765.0000 - accuracy: 0.9227 - precision: 0.8537 - recall: 0.6849 - auc: 0.9425\n",
            "Epoch 00141: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2556 - tp: 79904.0000 - fp: 13694.0000 - tn: 522207.0000 - fn: 36765.0000 - accuracy: 0.9227 - precision: 0.8537 - recall: 0.6849 - auc: 0.9425 - val_loss: 0.2434 - val_tp: 853.0000 - val_fp: 157.0000 - val_tn: 5262.0000 - val_fn: 320.0000 - val_accuracy: 0.9276 - val_precision: 0.8446 - val_recall: 0.7272 - val_auc: 0.9514\n",
            "Epoch 142/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2552 - tp: 80228.0000 - fp: 13668.0000 - tn: 522233.0000 - fn: 36441.0000 - accuracy: 0.9232 - precision: 0.8544 - recall: 0.6877 - auc: 0.9427\n",
            "Epoch 00142: val_auc did not improve from 0.95545\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2552 - tp: 80228.0000 - fp: 13668.0000 - tn: 522233.0000 - fn: 36441.0000 - accuracy: 0.9232 - precision: 0.8544 - recall: 0.6877 - auc: 0.9427 - val_loss: 0.2414 - val_tp: 845.0000 - val_fp: 154.0000 - val_tn: 5265.0000 - val_fn: 328.0000 - val_accuracy: 0.9269 - val_precision: 0.8458 - val_recall: 0.7204 - val_auc: 0.9526\n",
            "Epoch 143/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2548 - tp: 80141.0000 - fp: 13649.0000 - tn: 522232.0000 - fn: 36522.0000 - accuracy: 0.9231 - precision: 0.8545 - recall: 0.6869 - auc: 0.9431\n",
            "Epoch 00143: val_auc improved from 0.95545 to 0.95566, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2548 - tp: 80144.0000 - fp: 13649.0000 - tn: 522252.0000 - fn: 36525.0000 - accuracy: 0.9231 - precision: 0.8545 - recall: 0.6869 - auc: 0.9431 - val_loss: 0.2334 - val_tp: 847.0000 - val_fp: 138.0000 - val_tn: 5281.0000 - val_fn: 326.0000 - val_accuracy: 0.9296 - val_precision: 0.8599 - val_recall: 0.7221 - val_auc: 0.9557\n",
            "Epoch 144/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2553 - tp: 79978.0000 - fp: 13662.0000 - tn: 522239.0000 - fn: 36691.0000 - accuracy: 0.9228 - precision: 0.8541 - recall: 0.6855 - auc: 0.9429\n",
            "Epoch 00144: val_auc did not improve from 0.95566\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2553 - tp: 79978.0000 - fp: 13662.0000 - tn: 522239.0000 - fn: 36691.0000 - accuracy: 0.9228 - precision: 0.8541 - recall: 0.6855 - auc: 0.9429 - val_loss: 0.2424 - val_tp: 817.0000 - val_fp: 141.0000 - val_tn: 5278.0000 - val_fn: 356.0000 - val_accuracy: 0.9246 - val_precision: 0.8528 - val_recall: 0.6965 - val_auc: 0.9531\n",
            "Epoch 145/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2554 - tp: 79904.0000 - fp: 13456.0000 - tn: 522419.0000 - fn: 36765.0000 - accuracy: 0.9230 - precision: 0.8559 - recall: 0.6849 - auc: 0.9427\n",
            "Epoch 00145: val_auc improved from 0.95566 to 0.95620, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2554 - tp: 79904.0000 - fp: 13457.0000 - tn: 522444.0000 - fn: 36765.0000 - accuracy: 0.9230 - precision: 0.8559 - recall: 0.6849 - auc: 0.9427 - val_loss: 0.2353 - val_tp: 820.0000 - val_fp: 112.0000 - val_tn: 5307.0000 - val_fn: 353.0000 - val_accuracy: 0.9295 - val_precision: 0.8798 - val_recall: 0.6991 - val_auc: 0.9562\n",
            "Epoch 146/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2553 - tp: 79843.0000 - fp: 13631.0000 - tn: 521824.0000 - fn: 36734.0000 - accuracy: 0.9228 - precision: 0.8542 - recall: 0.6849 - auc: 0.9427\n",
            "Epoch 00146: val_auc improved from 0.95620 to 0.95642, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2554 - tp: 79900.0000 - fp: 13642.0000 - tn: 522259.0000 - fn: 36769.0000 - accuracy: 0.9228 - precision: 0.8542 - recall: 0.6848 - auc: 0.9427 - val_loss: 0.2332 - val_tp: 789.0000 - val_fp: 87.0000 - val_tn: 5332.0000 - val_fn: 384.0000 - val_accuracy: 0.9285 - val_precision: 0.9007 - val_recall: 0.6726 - val_auc: 0.9564\n",
            "Epoch 147/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2549 - tp: 80169.0000 - fp: 13680.0000 - tn: 521990.0000 - fn: 36449.0000 - accuracy: 0.9231 - precision: 0.8542 - recall: 0.6874 - auc: 0.9428\n",
            "Epoch 00147: val_auc improved from 0.95642 to 0.95762, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2550 - tp: 80200.0000 - fp: 13688.0000 - tn: 522213.0000 - fn: 36469.0000 - accuracy: 0.9231 - precision: 0.8542 - recall: 0.6874 - auc: 0.9428 - val_loss: 0.2380 - val_tp: 797.0000 - val_fp: 83.0000 - val_tn: 5336.0000 - val_fn: 376.0000 - val_accuracy: 0.9304 - val_precision: 0.9057 - val_recall: 0.6795 - val_auc: 0.9576\n",
            "Epoch 148/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2548 - tp: 80179.0000 - fp: 13660.0000 - tn: 522018.0000 - fn: 36431.0000 - accuracy: 0.9232 - precision: 0.8544 - recall: 0.6876 - auc: 0.9429\n",
            "Epoch 00148: val_auc did not improve from 0.95762\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2548 - tp: 80222.0000 - fp: 13662.0000 - tn: 522239.0000 - fn: 36447.0000 - accuracy: 0.9232 - precision: 0.8545 - recall: 0.6876 - auc: 0.9429 - val_loss: 0.2382 - val_tp: 848.0000 - val_fp: 144.0000 - val_tn: 5275.0000 - val_fn: 325.0000 - val_accuracy: 0.9289 - val_precision: 0.8548 - val_recall: 0.7229 - val_auc: 0.9551\n",
            "Epoch 149/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2550 - tp: 79945.0000 - fp: 13613.0000 - tn: 522288.0000 - fn: 36724.0000 - accuracy: 0.9229 - precision: 0.8545 - recall: 0.6852 - auc: 0.9428\n",
            "Epoch 00149: val_auc did not improve from 0.95762\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2550 - tp: 79945.0000 - fp: 13613.0000 - tn: 522288.0000 - fn: 36724.0000 - accuracy: 0.9229 - precision: 0.8545 - recall: 0.6852 - auc: 0.9428 - val_loss: 0.2308 - val_tp: 833.0000 - val_fp: 109.0000 - val_tn: 5310.0000 - val_fn: 340.0000 - val_accuracy: 0.9319 - val_precision: 0.8843 - val_recall: 0.7101 - val_auc: 0.9570\n",
            "Epoch 150/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2541 - tp: 80336.0000 - fp: 13595.0000 - tn: 522285.0000 - fn: 36328.0000 - accuracy: 0.9235 - precision: 0.8553 - recall: 0.6886 - auc: 0.9434\n",
            "Epoch 00150: val_auc improved from 0.95762 to 0.95792, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2541 - tp: 80339.0000 - fp: 13595.0000 - tn: 522306.0000 - fn: 36330.0000 - accuracy: 0.9235 - precision: 0.8553 - recall: 0.6886 - auc: 0.9434 - val_loss: 0.2340 - val_tp: 811.0000 - val_fp: 104.0000 - val_tn: 5315.0000 - val_fn: 362.0000 - val_accuracy: 0.9293 - val_precision: 0.8863 - val_recall: 0.6914 - val_auc: 0.9579\n",
            "Epoch 151/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2547 - tp: 80208.0000 - fp: 13584.0000 - tn: 522075.0000 - fn: 36421.0000 - accuracy: 0.9233 - precision: 0.8552 - recall: 0.6877 - auc: 0.9430\n",
            "Epoch 00151: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2547 - tp: 80234.0000 - fp: 13589.0000 - tn: 522312.0000 - fn: 36435.0000 - accuracy: 0.9233 - precision: 0.8552 - recall: 0.6877 - auc: 0.9431 - val_loss: 0.2467 - val_tp: 805.0000 - val_fp: 148.0000 - val_tn: 5271.0000 - val_fn: 368.0000 - val_accuracy: 0.9217 - val_precision: 0.8447 - val_recall: 0.6863 - val_auc: 0.9516\n",
            "Epoch 152/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2544 - tp: 80210.0000 - fp: 13724.0000 - tn: 521325.0000 - fn: 36261.0000 - accuracy: 0.9233 - precision: 0.8539 - recall: 0.6887 - auc: 0.9434\n",
            "Epoch 00152: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2544 - tp: 80346.0000 - fp: 13746.0000 - tn: 522155.0000 - fn: 36323.0000 - accuracy: 0.9233 - precision: 0.8539 - recall: 0.6887 - auc: 0.9434 - val_loss: 0.2486 - val_tp: 761.0000 - val_fp: 70.0000 - val_tn: 5349.0000 - val_fn: 412.0000 - val_accuracy: 0.9269 - val_precision: 0.9158 - val_recall: 0.6488 - val_auc: 0.9554\n",
            "Epoch 153/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2538 - tp: 80370.0000 - fp: 13531.0000 - tn: 522349.0000 - fn: 36294.0000 - accuracy: 0.9236 - precision: 0.8559 - recall: 0.6889 - auc: 0.9437\n",
            "Epoch 00153: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2538 - tp: 80374.0000 - fp: 13532.0000 - tn: 522369.0000 - fn: 36295.0000 - accuracy: 0.9236 - precision: 0.8559 - recall: 0.6889 - auc: 0.9437 - val_loss: 0.2408 - val_tp: 807.0000 - val_fp: 111.0000 - val_tn: 5308.0000 - val_fn: 366.0000 - val_accuracy: 0.9276 - val_precision: 0.8791 - val_recall: 0.6880 - val_auc: 0.9539\n",
            "Epoch 154/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2538 - tp: 80459.0000 - fp: 13627.0000 - tn: 522254.0000 - fn: 36204.0000 - accuracy: 0.9236 - precision: 0.8552 - recall: 0.6897 - auc: 0.9436\n",
            "Epoch 00154: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2538 - tp: 80464.0000 - fp: 13628.0000 - tn: 522273.0000 - fn: 36205.0000 - accuracy: 0.9236 - precision: 0.8552 - recall: 0.6897 - auc: 0.9436 - val_loss: 0.2368 - val_tp: 857.0000 - val_fp: 149.0000 - val_tn: 5270.0000 - val_fn: 316.0000 - val_accuracy: 0.9295 - val_precision: 0.8519 - val_recall: 0.7306 - val_auc: 0.9549\n",
            "Epoch 155/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2546 - tp: 80279.0000 - fp: 13554.0000 - tn: 522347.0000 - fn: 36390.0000 - accuracy: 0.9235 - precision: 0.8556 - recall: 0.6881 - auc: 0.9431\n",
            "Epoch 00155: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2546 - tp: 80279.0000 - fp: 13554.0000 - tn: 522347.0000 - fn: 36390.0000 - accuracy: 0.9235 - precision: 0.8556 - recall: 0.6881 - auc: 0.9431 - val_loss: 0.2419 - val_tp: 845.0000 - val_fp: 163.0000 - val_tn: 5256.0000 - val_fn: 328.0000 - val_accuracy: 0.9255 - val_precision: 0.8383 - val_recall: 0.7204 - val_auc: 0.9510\n",
            "Epoch 156/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2543 - tp: 80255.0000 - fp: 13512.0000 - tn: 522366.0000 - fn: 36411.0000 - accuracy: 0.9235 - precision: 0.8559 - recall: 0.6879 - auc: 0.9433\n",
            "Epoch 00156: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2543 - tp: 80257.0000 - fp: 13513.0000 - tn: 522388.0000 - fn: 36412.0000 - accuracy: 0.9235 - precision: 0.8559 - recall: 0.6879 - auc: 0.9433 - val_loss: 0.2346 - val_tp: 821.0000 - val_fp: 108.0000 - val_tn: 5311.0000 - val_fn: 352.0000 - val_accuracy: 0.9302 - val_precision: 0.8837 - val_recall: 0.6999 - val_auc: 0.9549\n",
            "Epoch 157/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2539 - tp: 80332.0000 - fp: 13659.0000 - tn: 522008.0000 - fn: 36289.0000 - accuracy: 0.9234 - precision: 0.8547 - recall: 0.6888 - auc: 0.9436\n",
            "Epoch 00157: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2539 - tp: 80366.0000 - fp: 13670.0000 - tn: 522231.0000 - fn: 36303.0000 - accuracy: 0.9234 - precision: 0.8546 - recall: 0.6888 - auc: 0.9436 - val_loss: 0.2347 - val_tp: 806.0000 - val_fp: 92.0000 - val_tn: 5327.0000 - val_fn: 367.0000 - val_accuracy: 0.9304 - val_precision: 0.8976 - val_recall: 0.6871 - val_auc: 0.9564\n",
            "Epoch 158/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2541 - tp: 80312.0000 - fp: 13515.0000 - tn: 521521.0000 - fn: 36172.0000 - accuracy: 0.9237 - precision: 0.8560 - recall: 0.6895 - auc: 0.9436\n",
            "Epoch 00158: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2541 - tp: 80444.0000 - fp: 13547.0000 - tn: 522354.0000 - fn: 36225.0000 - accuracy: 0.9237 - precision: 0.8559 - recall: 0.6895 - auc: 0.9436 - val_loss: 0.2405 - val_tp: 822.0000 - val_fp: 124.0000 - val_tn: 5295.0000 - val_fn: 351.0000 - val_accuracy: 0.9279 - val_precision: 0.8689 - val_recall: 0.7008 - val_auc: 0.9520\n",
            "Epoch 159/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2538 - tp: 80433.0000 - fp: 13492.0000 - tn: 522172.0000 - fn: 36191.0000 - accuracy: 0.9238 - precision: 0.8564 - recall: 0.6897 - auc: 0.9436\n",
            "Epoch 00159: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2538 - tp: 80465.0000 - fp: 13501.0000 - tn: 522400.0000 - fn: 36204.0000 - accuracy: 0.9238 - precision: 0.8563 - recall: 0.6897 - auc: 0.9436 - val_loss: 0.2341 - val_tp: 843.0000 - val_fp: 130.0000 - val_tn: 5289.0000 - val_fn: 330.0000 - val_accuracy: 0.9302 - val_precision: 0.8664 - val_recall: 0.7187 - val_auc: 0.9560\n",
            "Epoch 160/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2537 - tp: 80262.0000 - fp: 13537.0000 - tn: 521935.0000 - fn: 36298.0000 - accuracy: 0.9236 - precision: 0.8557 - recall: 0.6886 - auc: 0.9439\n",
            "Epoch 00160: val_auc did not improve from 0.95792\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2537 - tp: 80331.0000 - fp: 13546.0000 - tn: 522355.0000 - fn: 36338.0000 - accuracy: 0.9236 - precision: 0.8557 - recall: 0.6885 - auc: 0.9438 - val_loss: 0.2385 - val_tp: 891.0000 - val_fp: 182.0000 - val_tn: 5237.0000 - val_fn: 282.0000 - val_accuracy: 0.9296 - val_precision: 0.8304 - val_recall: 0.7596 - val_auc: 0.9547\n",
            "Epoch 161/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2540 - tp: 80304.0000 - fp: 13700.0000 - tn: 521546.0000 - fn: 36226.0000 - accuracy: 0.9234 - precision: 0.8543 - recall: 0.6891 - auc: 0.9436\n",
            "Epoch 00161: val_auc improved from 0.95792 to 0.95797, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2540 - tp: 80395.0000 - fp: 13710.0000 - tn: 522191.0000 - fn: 36274.0000 - accuracy: 0.9234 - precision: 0.8543 - recall: 0.6891 - auc: 0.9436 - val_loss: 0.2348 - val_tp: 777.0000 - val_fp: 74.0000 - val_tn: 5345.0000 - val_fn: 396.0000 - val_accuracy: 0.9287 - val_precision: 0.9130 - val_recall: 0.6624 - val_auc: 0.9580\n",
            "Epoch 162/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2533 - tp: 80317.0000 - fp: 13369.0000 - tn: 522307.0000 - fn: 36295.0000 - accuracy: 0.9239 - precision: 0.8573 - recall: 0.6888 - auc: 0.9440\n",
            "Epoch 00162: val_auc improved from 0.95797 to 0.95826, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2533 - tp: 80359.0000 - fp: 13376.0000 - tn: 522525.0000 - fn: 36310.0000 - accuracy: 0.9239 - precision: 0.8573 - recall: 0.6888 - auc: 0.9440 - val_loss: 0.2321 - val_tp: 812.0000 - val_fp: 113.0000 - val_tn: 5306.0000 - val_fn: 361.0000 - val_accuracy: 0.9281 - val_precision: 0.8778 - val_recall: 0.6922 - val_auc: 0.9583\n",
            "Epoch 163/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2533 - tp: 80486.0000 - fp: 13572.0000 - tn: 521685.0000 - fn: 36033.0000 - accuracy: 0.9239 - precision: 0.8557 - recall: 0.6908 - auc: 0.9440\n",
            "Epoch 00163: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2534 - tp: 80588.0000 - fp: 13589.0000 - tn: 522312.0000 - fn: 36081.0000 - accuracy: 0.9239 - precision: 0.8557 - recall: 0.6907 - auc: 0.9440 - val_loss: 0.2330 - val_tp: 830.0000 - val_fp: 110.0000 - val_tn: 5309.0000 - val_fn: 343.0000 - val_accuracy: 0.9313 - val_precision: 0.8830 - val_recall: 0.7076 - val_auc: 0.9570\n",
            "Epoch 164/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2540 - tp: 80391.0000 - fp: 13529.0000 - tn: 522353.0000 - fn: 36271.0000 - accuracy: 0.9237 - precision: 0.8560 - recall: 0.6891 - auc: 0.9438\n",
            "Epoch 00164: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2540 - tp: 80395.0000 - fp: 13530.0000 - tn: 522371.0000 - fn: 36274.0000 - accuracy: 0.9237 - precision: 0.8559 - recall: 0.6891 - auc: 0.9438 - val_loss: 0.2322 - val_tp: 831.0000 - val_fp: 109.0000 - val_tn: 5310.0000 - val_fn: 342.0000 - val_accuracy: 0.9316 - val_precision: 0.8840 - val_recall: 0.7084 - val_auc: 0.9576\n",
            "Epoch 165/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2539 - tp: 80288.0000 - fp: 13417.0000 - tn: 522270.0000 - fn: 36313.0000 - accuracy: 0.9238 - precision: 0.8568 - recall: 0.6886 - auc: 0.9435\n",
            "Epoch 00165: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2539 - tp: 80331.0000 - fp: 13424.0000 - tn: 522477.0000 - fn: 36338.0000 - accuracy: 0.9237 - precision: 0.8568 - recall: 0.6885 - auc: 0.9435 - val_loss: 0.2399 - val_tp: 785.0000 - val_fp: 81.0000 - val_tn: 5338.0000 - val_fn: 388.0000 - val_accuracy: 0.9289 - val_precision: 0.9065 - val_recall: 0.6692 - val_auc: 0.9568\n",
            "Epoch 166/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2537 - tp: 80259.0000 - fp: 13276.0000 - tn: 522604.0000 - fn: 36405.0000 - accuracy: 0.9239 - precision: 0.8581 - recall: 0.6880 - auc: 0.9439\n",
            "Epoch 00166: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2537 - tp: 80262.0000 - fp: 13278.0000 - tn: 522623.0000 - fn: 36407.0000 - accuracy: 0.9239 - precision: 0.8581 - recall: 0.6879 - auc: 0.9438 - val_loss: 0.2325 - val_tp: 850.0000 - val_fp: 138.0000 - val_tn: 5281.0000 - val_fn: 323.0000 - val_accuracy: 0.9301 - val_precision: 0.8603 - val_recall: 0.7246 - val_auc: 0.9566\n",
            "Epoch 167/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2535 - tp: 80495.0000 - fp: 13612.0000 - tn: 522289.0000 - fn: 36174.0000 - accuracy: 0.9237 - precision: 0.8554 - recall: 0.6899 - auc: 0.9441\n",
            "Epoch 00167: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2535 - tp: 80495.0000 - fp: 13612.0000 - tn: 522289.0000 - fn: 36174.0000 - accuracy: 0.9237 - precision: 0.8554 - recall: 0.6899 - auc: 0.9441 - val_loss: 0.2346 - val_tp: 824.0000 - val_fp: 106.0000 - val_tn: 5313.0000 - val_fn: 349.0000 - val_accuracy: 0.9310 - val_precision: 0.8860 - val_recall: 0.7025 - val_auc: 0.9564\n",
            "Epoch 168/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2534 - tp: 80512.0000 - fp: 13536.0000 - tn: 522135.0000 - fn: 36105.0000 - accuracy: 0.9239 - precision: 0.8561 - recall: 0.6904 - auc: 0.9440\n",
            "Epoch 00168: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2534 - tp: 80550.0000 - fp: 13541.0000 - tn: 522360.0000 - fn: 36119.0000 - accuracy: 0.9239 - precision: 0.8561 - recall: 0.6904 - auc: 0.9440 - val_loss: 0.2450 - val_tp: 870.0000 - val_fp: 199.0000 - val_tn: 5220.0000 - val_fn: 303.0000 - val_accuracy: 0.9238 - val_precision: 0.8138 - val_recall: 0.7417 - val_auc: 0.9531\n",
            "Epoch 169/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2532 - tp: 80480.0000 - fp: 13509.0000 - tn: 521949.0000 - fn: 36094.0000 - accuracy: 0.9239 - precision: 0.8563 - recall: 0.6904 - auc: 0.9443\n",
            "Epoch 00169: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2532 - tp: 80541.0000 - fp: 13518.0000 - tn: 522383.0000 - fn: 36128.0000 - accuracy: 0.9239 - precision: 0.8563 - recall: 0.6903 - auc: 0.9443 - val_loss: 0.2338 - val_tp: 846.0000 - val_fp: 120.0000 - val_tn: 5299.0000 - val_fn: 327.0000 - val_accuracy: 0.9322 - val_precision: 0.8758 - val_recall: 0.7212 - val_auc: 0.9550\n",
            "Epoch 170/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2526 - tp: 80825.0000 - fp: 13540.0000 - tn: 522361.0000 - fn: 35844.0000 - accuracy: 0.9243 - precision: 0.8565 - recall: 0.6928 - auc: 0.9444\n",
            "Epoch 00170: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2526 - tp: 80825.0000 - fp: 13540.0000 - tn: 522361.0000 - fn: 35844.0000 - accuracy: 0.9243 - precision: 0.8565 - recall: 0.6928 - auc: 0.9444 - val_loss: 0.2371 - val_tp: 870.0000 - val_fp: 158.0000 - val_tn: 5261.0000 - val_fn: 303.0000 - val_accuracy: 0.9301 - val_precision: 0.8463 - val_recall: 0.7417 - val_auc: 0.9540\n",
            "Epoch 171/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2533 - tp: 80460.0000 - fp: 13373.0000 - tn: 522528.0000 - fn: 36209.0000 - accuracy: 0.9240 - precision: 0.8575 - recall: 0.6896 - auc: 0.9442\n",
            "Epoch 00171: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2533 - tp: 80460.0000 - fp: 13373.0000 - tn: 522528.0000 - fn: 36209.0000 - accuracy: 0.9240 - precision: 0.8575 - recall: 0.6896 - auc: 0.9442 - val_loss: 0.2450 - val_tp: 827.0000 - val_fp: 146.0000 - val_tn: 5273.0000 - val_fn: 346.0000 - val_accuracy: 0.9254 - val_precision: 0.8499 - val_recall: 0.7050 - val_auc: 0.9515\n",
            "Epoch 172/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2528 - tp: 80679.0000 - fp: 13603.0000 - tn: 521859.0000 - fn: 35891.0000 - accuracy: 0.9241 - precision: 0.8557 - recall: 0.6921 - auc: 0.9443\n",
            "Epoch 00172: val_auc did not improve from 0.95826\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2529 - tp: 80748.0000 - fp: 13613.0000 - tn: 522288.0000 - fn: 35921.0000 - accuracy: 0.9241 - precision: 0.8557 - recall: 0.6921 - auc: 0.9443 - val_loss: 0.2306 - val_tp: 845.0000 - val_fp: 128.0000 - val_tn: 5291.0000 - val_fn: 328.0000 - val_accuracy: 0.9308 - val_precision: 0.8684 - val_recall: 0.7204 - val_auc: 0.9572\n",
            "Epoch 173/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 80697.0000 - fp: 13555.0000 - tn: 521908.0000 - fn: 35872.0000 - accuracy: 0.9242 - precision: 0.8562 - recall: 0.6923 - auc: 0.9445\n",
            "Epoch 00173: val_auc improved from 0.95826 to 0.95850, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2526 - tp: 80764.0000 - fp: 13561.0000 - tn: 522340.0000 - fn: 35905.0000 - accuracy: 0.9242 - precision: 0.8562 - recall: 0.6922 - auc: 0.9445 - val_loss: 0.2290 - val_tp: 846.0000 - val_fp: 120.0000 - val_tn: 5299.0000 - val_fn: 327.0000 - val_accuracy: 0.9322 - val_precision: 0.8758 - val_recall: 0.7212 - val_auc: 0.9585\n",
            "Epoch 174/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 80906.0000 - fp: 13771.0000 - tn: 522108.0000 - fn: 35759.0000 - accuracy: 0.9241 - precision: 0.8545 - recall: 0.6935 - auc: 0.9446\n",
            "Epoch 00174: val_auc did not improve from 0.95850\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2526 - tp: 80908.0000 - fp: 13771.0000 - tn: 522130.0000 - fn: 35761.0000 - accuracy: 0.9241 - precision: 0.8546 - recall: 0.6935 - auc: 0.9446 - val_loss: 0.2328 - val_tp: 833.0000 - val_fp: 120.0000 - val_tn: 5299.0000 - val_fn: 340.0000 - val_accuracy: 0.9302 - val_precision: 0.8741 - val_recall: 0.7101 - val_auc: 0.9568\n",
            "Epoch 175/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2537 - tp: 80446.0000 - fp: 13666.0000 - tn: 521793.0000 - fn: 36127.0000 - accuracy: 0.9236 - precision: 0.8548 - recall: 0.6901 - auc: 0.9440\n",
            "Epoch 00175: val_auc did not improve from 0.95850\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2537 - tp: 80518.0000 - fp: 13672.0000 - tn: 522229.0000 - fn: 36151.0000 - accuracy: 0.9237 - precision: 0.8548 - recall: 0.6901 - auc: 0.9440 - val_loss: 0.2338 - val_tp: 824.0000 - val_fp: 109.0000 - val_tn: 5310.0000 - val_fn: 349.0000 - val_accuracy: 0.9305 - val_precision: 0.8832 - val_recall: 0.7025 - val_auc: 0.9562\n",
            "Epoch 176/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2531 - tp: 80736.0000 - fp: 13546.0000 - tn: 522330.0000 - fn: 35932.0000 - accuracy: 0.9242 - precision: 0.8563 - recall: 0.6920 - auc: 0.9442\n",
            "Epoch 00176: val_auc did not improve from 0.95850\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2531 - tp: 80737.0000 - fp: 13547.0000 - tn: 522354.0000 - fn: 35932.0000 - accuracy: 0.9242 - precision: 0.8563 - recall: 0.6920 - auc: 0.9442 - val_loss: 0.2287 - val_tp: 855.0000 - val_fp: 129.0000 - val_tn: 5290.0000 - val_fn: 318.0000 - val_accuracy: 0.9322 - val_precision: 0.8689 - val_recall: 0.7289 - val_auc: 0.9580\n",
            "Epoch 177/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2531 - tp: 80418.0000 - fp: 13433.0000 - tn: 522447.0000 - fn: 36246.0000 - accuracy: 0.9239 - precision: 0.8569 - recall: 0.6893 - auc: 0.9442\n",
            "Epoch 00177: val_auc did not improve from 0.95850\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2531 - tp: 80419.0000 - fp: 13434.0000 - tn: 522467.0000 - fn: 36250.0000 - accuracy: 0.9239 - precision: 0.8569 - recall: 0.6893 - auc: 0.9442 - val_loss: 0.2372 - val_tp: 768.0000 - val_fp: 75.0000 - val_tn: 5344.0000 - val_fn: 405.0000 - val_accuracy: 0.9272 - val_precision: 0.9110 - val_recall: 0.6547 - val_auc: 0.9576\n",
            "Epoch 178/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2533 - tp: 80404.0000 - fp: 13347.0000 - tn: 522118.0000 - fn: 36163.0000 - accuracy: 0.9241 - precision: 0.8576 - recall: 0.6898 - auc: 0.9441\n",
            "Epoch 00178: val_auc improved from 0.95850 to 0.95976, saving model to ./best_model.h5\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2533 - tp: 80470.0000 - fp: 13356.0000 - tn: 522545.0000 - fn: 36199.0000 - accuracy: 0.9241 - precision: 0.8577 - recall: 0.6897 - auc: 0.9441 - val_loss: 0.2305 - val_tp: 809.0000 - val_fp: 85.0000 - val_tn: 5334.0000 - val_fn: 364.0000 - val_accuracy: 0.9319 - val_precision: 0.9049 - val_recall: 0.6897 - val_auc: 0.9598\n",
            "Epoch 179/2000\n",
            "2546/2550 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 80573.0000 - fp: 13378.0000 - tn: 521869.0000 - fn: 35956.0000 - accuracy: 0.9243 - precision: 0.8576 - recall: 0.6914 - auc: 0.9446\n",
            "Epoch 00179: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2526 - tp: 80664.0000 - fp: 13393.0000 - tn: 522508.0000 - fn: 36005.0000 - accuracy: 0.9243 - precision: 0.8576 - recall: 0.6914 - auc: 0.9446 - val_loss: 0.2308 - val_tp: 853.0000 - val_fp: 121.0000 - val_tn: 5298.0000 - val_fn: 320.0000 - val_accuracy: 0.9331 - val_precision: 0.8758 - val_recall: 0.7272 - val_auc: 0.9573\n",
            "Epoch 180/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2528 - tp: 80702.0000 - fp: 13314.0000 - tn: 522587.0000 - fn: 35967.0000 - accuracy: 0.9245 - precision: 0.8584 - recall: 0.6917 - auc: 0.9444\n",
            "Epoch 00180: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2528 - tp: 80702.0000 - fp: 13314.0000 - tn: 522587.0000 - fn: 35967.0000 - accuracy: 0.9245 - precision: 0.8584 - recall: 0.6917 - auc: 0.9444 - val_loss: 0.2328 - val_tp: 844.0000 - val_fp: 119.0000 - val_tn: 5300.0000 - val_fn: 329.0000 - val_accuracy: 0.9320 - val_precision: 0.8764 - val_recall: 0.7195 - val_auc: 0.9553\n",
            "Epoch 181/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2525 - tp: 80486.0000 - fp: 13481.0000 - tn: 521979.0000 - fn: 36086.0000 - accuracy: 0.9240 - precision: 0.8565 - recall: 0.6904 - auc: 0.9448\n",
            "Epoch 00181: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2525 - tp: 80557.0000 - fp: 13494.0000 - tn: 522407.0000 - fn: 36112.0000 - accuracy: 0.9240 - precision: 0.8565 - recall: 0.6905 - auc: 0.9448 - val_loss: 0.2423 - val_tp: 773.0000 - val_fp: 85.0000 - val_tn: 5334.0000 - val_fn: 400.0000 - val_accuracy: 0.9264 - val_precision: 0.9009 - val_recall: 0.6590 - val_auc: 0.9559\n",
            "Epoch 182/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 80517.0000 - fp: 13356.0000 - tn: 522522.0000 - fn: 36149.0000 - accuracy: 0.9241 - precision: 0.8577 - recall: 0.6901 - auc: 0.9446\n",
            "Epoch 00182: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2526 - tp: 80519.0000 - fp: 13359.0000 - tn: 522542.0000 - fn: 36150.0000 - accuracy: 0.9241 - precision: 0.8577 - recall: 0.6901 - auc: 0.9446 - val_loss: 0.2389 - val_tp: 853.0000 - val_fp: 156.0000 - val_tn: 5263.0000 - val_fn: 320.0000 - val_accuracy: 0.9278 - val_precision: 0.8454 - val_recall: 0.7272 - val_auc: 0.9534\n",
            "Epoch 183/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2531 - tp: 80228.0000 - fp: 13406.0000 - tn: 522269.0000 - fn: 36385.0000 - accuracy: 0.9237 - precision: 0.8568 - recall: 0.6880 - auc: 0.9444\n",
            "Epoch 00183: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2531 - tp: 80266.0000 - fp: 13411.0000 - tn: 522490.0000 - fn: 36403.0000 - accuracy: 0.9237 - precision: 0.8568 - recall: 0.6880 - auc: 0.9444 - val_loss: 0.2392 - val_tp: 853.0000 - val_fp: 143.0000 - val_tn: 5276.0000 - val_fn: 320.0000 - val_accuracy: 0.9298 - val_precision: 0.8564 - val_recall: 0.7272 - val_auc: 0.9527\n",
            "Epoch 184/2000\n",
            "2549/2550 [============================>.] - ETA: 0s - loss: 0.2527 - tp: 80682.0000 - fp: 13470.0000 - tn: 522413.0000 - fn: 35979.0000 - accuracy: 0.9242 - precision: 0.8569 - recall: 0.6916 - auc: 0.9447\n",
            "Epoch 00184: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2527 - tp: 80685.0000 - fp: 13471.0000 - tn: 522430.0000 - fn: 35984.0000 - accuracy: 0.9242 - precision: 0.8569 - recall: 0.6916 - auc: 0.9447 - val_loss: 0.2368 - val_tp: 846.0000 - val_fp: 140.0000 - val_tn: 5279.0000 - val_fn: 327.0000 - val_accuracy: 0.9292 - val_precision: 0.8580 - val_recall: 0.7212 - val_auc: 0.9544\n",
            "Epoch 185/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 80697.0000 - fp: 13440.0000 - tn: 522238.0000 - fn: 35913.0000 - accuracy: 0.9243 - precision: 0.8572 - recall: 0.6920 - auc: 0.9447\n",
            "Epoch 00185: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2526 - tp: 80731.0000 - fp: 13443.0000 - tn: 522458.0000 - fn: 35938.0000 - accuracy: 0.9243 - precision: 0.8573 - recall: 0.6920 - auc: 0.9446 - val_loss: 0.2381 - val_tp: 781.0000 - val_fp: 87.0000 - val_tn: 5332.0000 - val_fn: 392.0000 - val_accuracy: 0.9273 - val_precision: 0.8998 - val_recall: 0.6658 - val_auc: 0.9565\n",
            "Epoch 186/2000\n",
            "2545/2550 [============================>.] - ETA: 0s - loss: 0.2521 - tp: 80749.0000 - fp: 13554.0000 - tn: 521473.0000 - fn: 35744.0000 - accuracy: 0.9243 - precision: 0.8563 - recall: 0.6932 - auc: 0.9448\n",
            "Epoch 00186: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2522 - tp: 80865.0000 - fp: 13577.0000 - tn: 522324.0000 - fn: 35804.0000 - accuracy: 0.9243 - precision: 0.8562 - recall: 0.6931 - auc: 0.9448 - val_loss: 0.2439 - val_tp: 757.0000 - val_fp: 75.0000 - val_tn: 5344.0000 - val_fn: 416.0000 - val_accuracy: 0.9255 - val_precision: 0.9099 - val_recall: 0.6454 - val_auc: 0.9562\n",
            "Epoch 187/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2527 - tp: 80595.0000 - fp: 13408.0000 - tn: 522268.0000 - fn: 36017.0000 - accuracy: 0.9242 - precision: 0.8574 - recall: 0.6911 - auc: 0.9445\n",
            "Epoch 00187: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2527 - tp: 80628.0000 - fp: 13412.0000 - tn: 522489.0000 - fn: 36041.0000 - accuracy: 0.9242 - precision: 0.8574 - recall: 0.6911 - auc: 0.9445 - val_loss: 0.2458 - val_tp: 758.0000 - val_fp: 70.0000 - val_tn: 5349.0000 - val_fn: 415.0000 - val_accuracy: 0.9264 - val_precision: 0.9155 - val_recall: 0.6462 - val_auc: 0.9515\n",
            "Epoch 188/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2523 - tp: 80789.0000 - fp: 13501.0000 - tn: 522173.0000 - fn: 35825.0000 - accuracy: 0.9244 - precision: 0.8568 - recall: 0.6928 - auc: 0.9447\n",
            "Epoch 00188: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2524 - tp: 80829.0000 - fp: 13508.0000 - tn: 522393.0000 - fn: 35840.0000 - accuracy: 0.9244 - precision: 0.8568 - recall: 0.6928 - auc: 0.9447 - val_loss: 0.2424 - val_tp: 895.0000 - val_fp: 202.0000 - val_tn: 5217.0000 - val_fn: 278.0000 - val_accuracy: 0.9272 - val_precision: 0.8159 - val_recall: 0.7630 - val_auc: 0.9534\n",
            "Epoch 189/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2523 - tp: 80824.0000 - fp: 13671.0000 - tn: 521786.0000 - fn: 35751.0000 - accuracy: 0.9242 - precision: 0.8553 - recall: 0.6933 - auc: 0.9448\n",
            "Epoch 00189: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2523 - tp: 80893.0000 - fp: 13690.0000 - tn: 522211.0000 - fn: 35776.0000 - accuracy: 0.9242 - precision: 0.8553 - recall: 0.6934 - auc: 0.9448 - val_loss: 0.2364 - val_tp: 894.0000 - val_fp: 198.0000 - val_tn: 5221.0000 - val_fn: 279.0000 - val_accuracy: 0.9276 - val_precision: 0.8187 - val_recall: 0.7621 - val_auc: 0.9561\n",
            "Epoch 190/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2525 - tp: 80598.0000 - fp: 13454.0000 - tn: 522447.0000 - fn: 36071.0000 - accuracy: 0.9241 - precision: 0.8570 - recall: 0.6908 - auc: 0.9446\n",
            "Epoch 00190: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2525 - tp: 80598.0000 - fp: 13454.0000 - tn: 522447.0000 - fn: 36071.0000 - accuracy: 0.9241 - precision: 0.8570 - recall: 0.6908 - auc: 0.9446 - val_loss: 0.2490 - val_tp: 896.0000 - val_fp: 247.0000 - val_tn: 5172.0000 - val_fn: 277.0000 - val_accuracy: 0.9205 - val_precision: 0.7839 - val_recall: 0.7639 - val_auc: 0.9532\n",
            "Epoch 191/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2524 - tp: 80612.0000 - fp: 13365.0000 - tn: 522084.0000 - fn: 35971.0000 - accuracy: 0.9243 - precision: 0.8578 - recall: 0.6915 - auc: 0.9446\n",
            "Epoch 00191: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2524 - tp: 80666.0000 - fp: 13374.0000 - tn: 522527.0000 - fn: 36003.0000 - accuracy: 0.9243 - precision: 0.8578 - recall: 0.6914 - auc: 0.9446 - val_loss: 0.2357 - val_tp: 802.0000 - val_fp: 108.0000 - val_tn: 5311.0000 - val_fn: 371.0000 - val_accuracy: 0.9273 - val_precision: 0.8813 - val_recall: 0.6837 - val_auc: 0.9570\n",
            "Epoch 192/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2522 - tp: 80622.0000 - fp: 13366.0000 - tn: 522535.0000 - fn: 36047.0000 - accuracy: 0.9243 - precision: 0.8578 - recall: 0.6910 - auc: 0.9447\n",
            "Epoch 00192: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 10ms/step - loss: 0.2522 - tp: 80622.0000 - fp: 13366.0000 - tn: 522535.0000 - fn: 36047.0000 - accuracy: 0.9243 - precision: 0.8578 - recall: 0.6910 - auc: 0.9447 - val_loss: 0.2316 - val_tp: 830.0000 - val_fp: 108.0000 - val_tn: 5311.0000 - val_fn: 343.0000 - val_accuracy: 0.9316 - val_precision: 0.8849 - val_recall: 0.7076 - val_auc: 0.9581\n",
            "Epoch 193/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2517 - tp: 81005.0000 - fp: 13599.0000 - tn: 522302.0000 - fn: 35664.0000 - accuracy: 0.9245 - precision: 0.8563 - recall: 0.6943 - auc: 0.9453\n",
            "Epoch 00193: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2517 - tp: 81005.0000 - fp: 13599.0000 - tn: 522302.0000 - fn: 35664.0000 - accuracy: 0.9245 - precision: 0.8563 - recall: 0.6943 - auc: 0.9453 - val_loss: 0.2337 - val_tp: 812.0000 - val_fp: 98.0000 - val_tn: 5321.0000 - val_fn: 361.0000 - val_accuracy: 0.9304 - val_precision: 0.8923 - val_recall: 0.6922 - val_auc: 0.9576\n",
            "Epoch 194/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2520 - tp: 80749.0000 - fp: 13338.0000 - tn: 522134.0000 - fn: 35811.0000 - accuracy: 0.9246 - precision: 0.8582 - recall: 0.6928 - auc: 0.9450\n",
            "Epoch 00194: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2520 - tp: 80825.0000 - fp: 13347.0000 - tn: 522554.0000 - fn: 35844.0000 - accuracy: 0.9246 - precision: 0.8583 - recall: 0.6928 - auc: 0.9450 - val_loss: 0.2320 - val_tp: 798.0000 - val_fp: 92.0000 - val_tn: 5327.0000 - val_fn: 375.0000 - val_accuracy: 0.9292 - val_precision: 0.8966 - val_recall: 0.6803 - val_auc: 0.9590\n",
            "Epoch 195/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2517 - tp: 80796.0000 - fp: 13368.0000 - tn: 522307.0000 - fn: 35817.0000 - accuracy: 0.9246 - precision: 0.8580 - recall: 0.6929 - auc: 0.9452\n",
            "Epoch 00195: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2518 - tp: 80831.0000 - fp: 13376.0000 - tn: 522525.0000 - fn: 35838.0000 - accuracy: 0.9246 - precision: 0.8580 - recall: 0.6928 - auc: 0.9452 - val_loss: 0.2353 - val_tp: 798.0000 - val_fp: 87.0000 - val_tn: 5332.0000 - val_fn: 375.0000 - val_accuracy: 0.9299 - val_precision: 0.9017 - val_recall: 0.6803 - val_auc: 0.9554\n",
            "Epoch 196/2000\n",
            "2550/2550 [==============================] - ETA: 0s - loss: 0.2516 - tp: 80899.0000 - fp: 13419.0000 - tn: 522482.0000 - fn: 35770.0000 - accuracy: 0.9246 - precision: 0.8577 - recall: 0.6934 - auc: 0.9451\n",
            "Epoch 00196: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 28s 11ms/step - loss: 0.2516 - tp: 80899.0000 - fp: 13419.0000 - tn: 522482.0000 - fn: 35770.0000 - accuracy: 0.9246 - precision: 0.8577 - recall: 0.6934 - auc: 0.9451 - val_loss: 0.2348 - val_tp: 829.0000 - val_fp: 116.0000 - val_tn: 5303.0000 - val_fn: 344.0000 - val_accuracy: 0.9302 - val_precision: 0.8772 - val_recall: 0.7067 - val_auc: 0.9548\n",
            "Epoch 197/2000\n",
            "2547/2550 [============================>.] - ETA: 0s - loss: 0.2511 - tp: 80875.0000 - fp: 13398.0000 - tn: 522066.0000 - fn: 35693.0000 - accuracy: 0.9247 - precision: 0.8579 - recall: 0.6938 - auc: 0.9455\n",
            "Epoch 00197: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 27s 11ms/step - loss: 0.2512 - tp: 80942.0000 - fp: 13412.0000 - tn: 522489.0000 - fn: 35727.0000 - accuracy: 0.9247 - precision: 0.8579 - recall: 0.6938 - auc: 0.9454 - val_loss: 0.2373 - val_tp: 861.0000 - val_fp: 141.0000 - val_tn: 5278.0000 - val_fn: 312.0000 - val_accuracy: 0.9313 - val_precision: 0.8593 - val_recall: 0.7340 - val_auc: 0.9550\n",
            "Epoch 198/2000\n",
            "2548/2550 [============================>.] - ETA: 0s - loss: 0.2510 - tp: 80984.0000 - fp: 13539.0000 - tn: 522126.0000 - fn: 35639.0000 - accuracy: 0.9246 - precision: 0.8568 - recall: 0.6944 - auc: 0.9457\n",
            "Epoch 00198: val_auc did not improve from 0.95976\n",
            "2550/2550 [==============================] - 26s 10ms/step - loss: 0.2510 - tp: 81016.0000 - fp: 13545.0000 - tn: 522356.0000 - fn: 35653.0000 - accuracy: 0.9246 - precision: 0.8568 - recall: 0.6944 - auc: 0.9457 - val_loss: 0.2306 - val_tp: 836.0000 - val_fp: 97.0000 - val_tn: 5322.0000 - val_fn: 337.0000 - val_accuracy: 0.9342 - val_precision: 0.8960 - val_recall: 0.7127 - val_auc: 0.9577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "663f4dc0-2594-4fcd-b88c-9a365bad1c0c"
      },
      "source": [
        "def plot_metrics(history):\n",
        "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch,  history.history[metric], color='b', label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color='b', linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "plot_metrics(history)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjbrzEe2ISI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "f6382e3f-574b-40cc-f972-4eaee85c01f2"
      },
      "source": [
        "# Save model weights to drive\n",
        "!cp -r best_model.h5 '/content/gdrive/My Drive/Kaggle/best_model_20200804_METRICS_2.h5'\n",
        "\n",
        "#new_model = tf.keras.models.load_model('./best_model.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model = tf.keras.models.load_model('./best_model.h5')\n",
        "#new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Kaggle/best_model_20200802_METRICS.h5', \n",
        "#                                        custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_26 (Dense)             (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 477,825\n",
            "Trainable params: 476,545\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAzqpDLIS0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty some RAM space\n",
        "indices_0_new = None\n",
        "data_backup = None\n",
        "dataset_transaction = None\n",
        "X_to_train = None\n",
        "Y_to_train = None\n",
        "Y_train = None\n",
        "X_train = None"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQtd0IsFspQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n",
        "  test_size = test_size\n",
        "  y_pred = (model.predict(X_test, batch_size=128)>threshold).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        " \n",
        "  precision = precision_cal(y_pred, y_ref)\n",
        "  recall = recall_cal(y_pred, y_ref)\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre = []\n",
        "re = []\n",
        "f1 = []\n",
        "\n",
        "pre_train = []\n",
        "re_train = []\n",
        "f1_train = []\n",
        "\n",
        "threshold_value = []\n",
        "indices = np.random.randint(0, len(X_to_train), size=(len(Y_test),))\n",
        "\n",
        "for i in range(90):\n",
        "  threshold_value.append(0.1+i*0.01)\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_test, Y_test, test_size=len(Y_test), threshold=threshold_value[-1])\n",
        "  \n",
        "  pre.append(temp_pre)\n",
        "  re.append(temp_re)\n",
        "  f1.append(temp_f1)\n",
        "\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_to_train[indices], Y_to_train[indices], test_size=len(Y_to_train[indices]), threshold=threshold_value[-1])\n",
        "\n",
        "  pre_train.append(temp_pre)\n",
        "  re_train.append(temp_re)\n",
        "  f1_train.append(temp_f1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CP4zuNU6WUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "387b504f-a2b0-49f7-b2b5-2c1677d51833"
      },
      "source": [
        "plt.plot(threshold_value, f1, 'b')\n",
        "plt.plot(threshold_value, pre, 'r')\n",
        "plt.plot(threshold_value, re, 'g')\n",
        "\n",
        "plt.plot(threshold_value, f1_train, '--b')\n",
        "plt.plot(threshold_value, pre_train, '--r')\n",
        "plt.plot(threshold_value, re_train, '--g')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f35b93ae240>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2H3PvUGb8KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "1ecf0d6b-a11f-486c-e0c5-168773607cd9"
      },
      "source": [
        "#BATCH_SIZE = 256\n",
        "baseline_results = new_model.evaluate(X_test, Y_test,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(new_model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "predictions = new_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "plot_cm(Y_test, predictions, p=0.3)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss :  0.2270195335149765\n",
            "tp :  4320.0\n",
            "fp :  500.0\n",
            "tn :  28057.0\n",
            "fn :  1816.0\n",
            "accuracy :  0.9332430362701416\n",
            "precision :  0.8962655663490295\n",
            "recall :  0.7040417194366455\n",
            "auc :  0.9600683450698853\n",
            "\n",
            "Legitimate Transactions Detected (True Negatives):  27436\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  1121\n",
            "Fraudulent Transactions Missed (False Negatives):  1225\n",
            "Fraudulent Transactions Detected (True Positives):  4911\n",
            "Total Fraudulent Transactions:  6136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUuAUMBb_9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e0822708-b430-4e6a-aa40-5584faaa8b7b"
      },
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,30])\n",
        "  plt.ylim([70,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "indices = np.random.randint(0, len(X_to_train), size=(len(Y_test),))\n",
        "\n",
        "train_prediction = new_model.predict(X_to_train[indices], batch_size=BATCH_SIZE, verbose=0)\n",
        "plot_roc(\"Train Baseline\", Y_train[indices], train_prediction, color='b')\n",
        "plot_roc(\"Test Baseline\", Y_test, predictions, color='b', linestyle='--')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f35b900a6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43d7bce2-6a9f-403f-fa51-312ba9f6fb96"
      },
      "source": [
        "prediction = np.squeeze(predictions, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist((prediction>0.9).astype('int'), bins=[0,1,2])\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.85).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.08% 3.32%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXMUlEQVR4nO3de4xV5b3G8e9zwEu8VEGoJUAdTMkhkLSRTtSqab00ilDFppdgbIuWhtqi0bRpiyWpja0p/lO16eXEICfYGNGirdTLsVQwTWtAB0UQKTqiVggKCqLElBb7O3+sd+hiMpe9O3utGXyfT7Iza73vu/b67XcWz96z1t4bRQRmZpaH/xrsAszMrD4OfTOzjDj0zcwy4tA3M8uIQ9/MLCPDB7uAvowaNSra2toGuwwzs0PK2rVr34iI0T31DenQb2tro6OjY7DLMDM7pEh6pbc+n94xM8uIQ9/MLCMOfTOzjAzpc/oD1Tb/wcEuwd7HXl44Y7BLMGuaX+mbmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHvplZRhz6ZmYZaTj0JQ2T9LSkB9L6BElrJHVKulvS4an9iLTemfrbSvdxXWrfLOmCVj8YMzPrWzOv9K8BNpXWbwJujoiPALuBOal9DrA7td+cxiFpMjALmAJMA34padjAyjczs2Y0FPqSxgEzgEVpXcC5wLI0ZAlwSVqemdZJ/eel8TOBpRGxLyJeAjqBU1vxIMzMrDGNvtK/Bfgu8K+0fgLwVkTsT+tbgbFpeSzwKkDq35PGH2jvYZsDJM2V1CGpY+fOnU08FDMz60+/oS/pM8COiFhbQz1ExG0R0R4R7aNHj65jl2Zm2RjewJgzgYslTQeOBD4A3AocL2l4ejU/DtiWxm8DxgNbJQ0HjgPeLLV3KW9jZmY16PeVfkRcFxHjIqKN4kLsyoi4DFgFfD4Nmw3cn5aXp3VS/8qIiNQ+K727ZwIwEXiiZY/EzMz61cgr/d58D1gq6cfA08Dtqf124NeSOoFdFE8URMRGSfcAzwH7gXkR8d4A9m9mZk1qKvQj4jHgsbS8hR7efRMRfwe+0Mv2NwI3NlukmZm1hj+Ra2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZaTf0Jc0XtIqSc9J2ijpmtQ+UtIKSS+knyNSuyT9TFKnpPWSppbua3Ya/4Kk2dU9LDMz60kjr/T3A9+OiMnA6cA8SZOB+cCjETEReDStA1wITEy3ucCvoHiSAK4HTgNOBa7veqIwM7N69Bv6EbE9Ip5Ky+8Am4CxwExgSRq2BLgkLc8E7ojCauB4SWOAC4AVEbErInYDK4BpLX00ZmbWp6bO6UtqA04B1gAnRsT21PUacGJaHgu8Wtpsa2rrrb37PuZK6pDUsXPnzmbKMzOzfjQc+pKOAe4Fro2It8t9ERFAtKKgiLgtItojon306NGtuEszM0saCn1Jh1EE/p0RcV9qfj2dtiH93JHatwHjS5uPS229tZuZWU0aefeOgNuBTRHx01LXcqDrHTizgftL7V9J7+I5HdiTTgM9ApwvaUS6gHt+ajMzs5oMb2DMmcCXgQ2S1qW27wMLgXskzQFeAb6Y+h4CpgOdwLvAFQARsUvSj4An07gbImJXSx6FmZk1pN/Qj4g/A+ql+7wexgcwr5f7WgwsbqZAMzNrHX8i18wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy8jwuncoaRpwKzAMWBQRC+uuwawV2uY/ONgl2PvYywtnVHK/tb7SlzQM+AVwITAZuFTS5DprMDPLWd2nd04FOiNiS0T8A1gKzKy5BjOzbNV9emcs8GppfStwWnmApLnA3LS6V9LmAexvFPDGALaviutqjutqjutqzpCsSzcNqK6Teuuo/Zx+fyLiNuC2VtyXpI6IaG/FfbWS62qO62qO62pObnXVfXpnGzC+tD4utZmZWQ3qDv0ngYmSJkg6HJgFLK+5BjOzbNV6eici9ku6CniE4i2biyNiY4W7bMlpogq4rua4rua4ruZkVZcioor7NTOzIcifyDUzy4hD38wsI4dk6EuaJmmzpE5J83voP0LS3al/jaS2Ut91qX2zpAtqrutbkp6TtF7So5JOKvW9J2ldurX04nYDdV0uaWdp/18r9c2W9EK6za65rptLNT0v6a1SX5XztVjSDknP9tIvST9Lda+XNLXUV+V89VfXZameDZIel/SxUt/LqX2dpI6a6zpb0p7S7+sHpb4+j4GK6/pOqaZn0zE1MvVVOV/jJa1KWbBR0jU9jKnuGIuIQ+pGcQH4ReBk4HDgGWBytzHfBP4nLc8C7k7Lk9P4I4AJ6X6G1VjXOcBRafkbXXWl9b2DOF+XAz/vYduRwJb0c0RaHlFXXd3GX01x4b/S+Ur3/UlgKvBsL/3TgYcBAacDa6qerwbrOqNrfxRfdbKm1PcyMGqQ5uts4IGBHgOtrqvb2IuAlTXN1xhgalo+Fni+h3+TlR1jh+Ir/Ua+ymEmsCQtLwPOk6TUvjQi9kXES0Bnur9a6oqIVRHxblpdTfE5haoN5KsvLgBWRMSuiNgNrACmDVJdlwJ3tWjffYqIPwG7+hgyE7gjCquB4yWNodr56reuiHg87RfqO74ama/eVPq1LE3WVefxtT0inkrL7wCbKL6toKyyY+xQDP2evsqh+4QdGBMR+4E9wAkNbltlXWVzKJ7JuxwpqUPSakmXtKimZur6XPozcpmkrg/QDYn5SqfBJgArS81VzVcjequ9yvlqVvfjK4A/SFqr4qtO6vYJSc9IeljSlNQ2JOZL0lEUwXlvqbmW+VJx6vkUYE23rsqOsSH3NQw5kPQloB34VKn5pIjYJulkYKWkDRHxYk0l/R64KyL2Sfo6xV9J59a070bMApZFxHultsGcryFN0jkUoX9WqfmsNF8fBFZI+mt6JVyHpyh+X3slTQd+B0ysad+NuAj4S0SU/yqofL4kHUPxRHNtRLzdyvvuy6H4Sr+Rr3I4MEbScOA44M0Gt62yLiR9GlgAXBwR+7raI2Jb+rkFeIzi2b+WuiLizVIti4CPN7ptlXWVzKLbn94Vzlcjeqt90L9mRNJHKX6HMyPiza720nztAH5L605r9isi3o6IvWn5IeAwSaMYAvOV9HV8VTJfkg6jCPw7I+K+HoZUd4xVcaGiyhvFXydbKP7c77r4M6XbmHkcfCH3nrQ8hYMv5G6hdRdyG6nrFIoLVxO7tY8AjkjLo4AXaNEFrQbrGlNa/iywOv590eilVN+ItDyyrrrSuEkUF9VUx3yV9tFG7xcmZ3DwRbYnqp6vBuv6MMV1qjO6tR8NHFtafhyYVmNdH+r6/VGE59/S3DV0DFRVV+o/juK8/9F1zVd67HcAt/QxprJjrGWTW+eN4sr28xQBuiC13UDx6hngSOA36R/AE8DJpW0XpO02AxfWXNcfgdeBdem2PLWfAWxIB/0GYE7Ndf0E2Jj2vwqYVNr2q2keO4Er6qwrrf8QWNhtu6rn6y5gO/BPinOmc4ArgStTvyj+M6AX0/7ba5qv/upaBOwuHV8dqf3kNFfPpN/zgprruqp0fK2m9KTU0zFQV11pzOUUb+4ob1f1fJ1Fcc1gfel3Nb2uY8xfw2BmlpGGzulLOj69q+OvkjZJ+oSkkZJWpA8IrJA0Io0dlA+umJlZ/xq9kHsr8H8RMQn4GMX7SucDj0bERODRtA7Fh0Impttc4FcA6ZNu11P8T1mnAtd3PVGYmVk9+g19ScdRfLLtdoCI+EdEvMXBH4BaAnS9V3pQPrhiZmb9a+R9+hOAncD/pu/yWAtcA5wYEdvTmNeAE9PygD5UoNL/kXv00Ud/fNKkSQ0/GDMzg7Vr174REaN76msk9IdTfH/F1RGxRtKt/PtUDgAREZJackU4Sv9Hbnt7e3R0tPS7jszM3vckvdJbXyPn9LcCWyOi62PCyyieBF5Pp21IP3ek/iH7wRUzs9z1G/oR8RrwqqT/Tk3nAc9R/N+2Xe/AmQ3cn5aXA19J7+I5HdiTTgM9ApwvaUS6gHt+ajMzs5o0+t07VwN3qvjPzLcAV1A8YdwjaQ7wCvDFNPYhig8adALvprFExC5JP6L4z9EBboiDv+vCzMwqNqQ/nDXQc/pt8x9sYTVmB3t54YzBLsGsR5LWRkR7T32H4heumZnZf8ihb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlpOPQlDZP0tKQH0voESWskdUq6W9Lhqf2ItN6Z+ttK93Fdat8s6YJWPxgzM+tbM6/0rwE2ldZvAm6OiI8Au4E5qX0OsDu135zGIWkyMAuYAkwDfilp2MDKNzOzZjQU+pLGATOARWldwLnAsjRkCXBJWp6Z1kn956XxM4GlEbEvIl4COoFTW/EgzMysMY2+0r8F+C7wr7R+AvBWROxP61uBsWl5LPAqQOrfk8YfaO9hmwMkzZXUIalj586dTTwUMzPrT7+hL+kzwI6IWFtDPUTEbRHRHhHto0ePrmOXZmbZGN7AmDOBiyVNB44EPgDcChwvaXh6NT8O2JbGbwPGA1slDQeOA94stXcpb2NmZjXo95V+RFwXEeMioo3iQuzKiLgMWAV8Pg2bDdyflpendVL/yoiI1D4rvbtnAjAReKJlj8TMzPrVyCv93nwPWCrpx8DTwO2p/Xbg15I6gV0UTxRExEZJ9wDPAfuBeRHx3gD2b2ZmTWoq9CPiMeCxtLyFHt59ExF/B77Qy/Y3Ajc2W6SZmbWGP5FrZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llxKFvZpYRh76ZWUYc+mZmGXHom5llpN/QlzRe0ipJz0naKOma1D5S0gpJL6SfI1K7JP1MUqek9ZKmlu5rdhr/gqTZ1T0sMzPrSSOv9PcD346IycDpwDxJk4H5wKMRMRF4NK0DXAhMTLe5wK+geJIArgdOA04Fru96ojAzs3r0G/oRsT0inkrL7wCbgLHATGBJGrYEuCQtzwTuiMJq4HhJY4ALgBURsSsidgMrgGktfTRmZtanps7pS2oDTgHWACdGxPbU9RpwYloeC7xa2mxrauutvfs+5krqkNSxc+fOZsozM7N+NBz6ko4B7gWujYi3y30REUC0oqCIuC0i2iOiffTo0a24SzMzSxoKfUmHUQT+nRFxX2p+PZ22If3ckdq3AeNLm49Lbb21m5lZTRp5946A24FNEfHTUtdyoOsdOLOB+0vtX0nv4jkd2JNOAz0CnC9pRLqAe35qMzOzmgxvYMyZwJeBDZLWpbbvAwuBeyTNAV4Bvpj6HgKmA53Au8AVABGxS9KPgCfTuBsiYldLHoWZmTWk39CPiD8D6qX7vB7GBzCvl/taDCxupkAzM2sdfyLXzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLyPC6dyhpGnArMAxYFBEL667BrBXa5j842CXY+9jLC2dUcr+1vtKXNAz4BXAhMBm4VNLkOmswM8tZ3ad3TgU6I2JLRPwDWArMrLkGM7Ns1X16Zyzwaml9K3BaeYCkucDctLpX0uYB7G8U8MYAtq+K62qO62qO62rOkKxLNw2orpN666j9nH5/IuI24LZW3Jekjohob8V9tZLrao7rao7rak5uddV9emcbML60Pi61mZlZDeoO/SeBiZImSDocmAUsr7kGM7Ns1Xp6JyL2S7oKeITiLZuLI2JjhbtsyWmiCriu5riu5riu5mRVlyKiivs1M7MhyJ/INTPLiEPfzCwjh2ToS5omabOkTknze+g/QtLdqX+NpLZS33WpfbOkC2qu61uSnpO0XtKjkk4q9b0naV26tfTidgN1XS5pZ2n/Xyv1zZb0QrrNrrmum0s1PS/prVJflfO1WNIOSc/20i9JP0t1r5c0tdRX5Xz1V9dlqZ4Nkh6X9LFS38upfZ2kjprrOlvSntLv6welvj6PgYrr+k6ppmfTMTUy9VU5X+MlrUpZsFHSNT2Mqe4Yi4hD6kZxAfhF4GTgcOAZYHK3Md8E/ictzwLuTsuT0/gjgAnpfobVWNc5wFFp+RtddaX1vYM4X5cDP+9h25HAlvRzRFoeUVdd3cZfTXHhv9L5Svf9SWAq8Gwv/dOBhwEBpwNrqp6vBus6o2t/FF91sqbU9zIwapDm62zggYEeA62uq9vYi4CVNc3XGGBqWj4WeL6Hf5OVHWOH4iv9Rr7KYSawJC0vA86TpNS+NCL2RcRLQGe6v1rqiohVEfFuWl1N8TmFqg3kqy8uAFZExK6I2A2sAKYNUl2XAne1aN99iog/Abv6GDITuCMKq4HjJY2h2vnqt66IeDztF+o7vhqZr95U+rUsTdZV5/G1PSKeSsvvAJsovq2grLJj7FAM/Z6+yqH7hB0YExH7gT3ACQ1uW2VdZXMonsm7HCmpQ9JqSZe0qKZm6vpc+jNymaSuD9ANiflKp8EmACtLzVXNVyN6q73K+WpW9+MrgD9IWqviq07q9glJz0h6WNKU1DYk5kvSURTBeW+puZb5UnHq+RRgTbeuyo6xIfc1DDmQ9CWgHfhUqfmkiNgm6WRgpaQNEfFiTSX9HrgrIvZJ+jrFX0nn1rTvRswClkXEe6W2wZyvIU3SORShf1ap+aw0Xx8EVkj6a3olXIenKH5feyVNB34HTKxp3424CPhLRJT/Kqh8viQdQ/FEc21EvN3K++7LofhKv5GvcjgwRtJw4DjgzQa3rbIuJH0aWABcHBH7utojYlv6uQV4jOLZv5a6IuLNUi2LgI83um2VdZXMotuf3hXOVyN6q33Qv2ZE0kcpfoczI+LNrvbSfO0AfkvrTmv2KyLejoi9afkh4DBJoxgC85X0dXxVMl+SDqMI/Dsj4r4ehlR3jFVxoaLKG8VfJ1so/tzvuvgzpduYeRx8IfeetDyFgy/kbqF1F3IbqesUigtXE7u1jwCOSMujgBdo0QWtBusaU1r+LLA6/n3R6KVU34i0PLKuutK4SRQX1VTHfJX20UbvFyZncPBFtieqnq8G6/owxXWqM7q1Hw0cW1p+HJhWY10f6vr9UYTn39LcNXQMVFVX6j+O4rz/0XXNV3rsdwC39DGmsmOsZZNb543iyvbzFAG6ILXdQPHqGeBI4DfpH8ATwMmlbRek7TYDF9Zc1x+B14F16bY8tZ8BbEgH/QZgTs11/QTYmPa/CphU2varaR47gSvqrCut/xBY2G27qufrLmA78E+Kc6ZzgCuBK1O/KP4zoBfT/ttrmq/+6loE7C4dXx2p/eQ0V8+k3/OCmuu6qnR8rab0pNTTMVBXXWnM5RRv7ihvV/V8nUVxzWB96Xc1va5jzF/DYGaWkUPxnL6Zmf2HHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZeT/AZd9tefKBS1NAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "cca96490-6a74-445e-9688-ce64cfaa64c5"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content\")\n",
        "test_transaction = pd.read_csv('test_transaction.csv')\n",
        "test_identity = pd.read_csv('test_identity.csv', names=saved_columns, header=0)\n",
        "test_identity.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Android 6.0.1</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x720</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>LGLS676 Build/MXB48T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ie 11.0 for tablet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Trident/7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SM-G9650 Build/R16NW</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  ...  id_38  DeviceType                   DeviceInfo\n",
              "0        3663586  -45.0  280290.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "1        3663588    0.0    3579.0  ...      T      mobile         LGLS676 Build/MXB48T\n",
              "2        3663597   -5.0  185210.0  ...      F     desktop                  Trident/7.0\n",
              "3        3663601  -45.0  252944.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "4        3663602  -95.0  328680.0  ...      F      mobile         SM-G9650 Build/R16NW\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8CCDZ5bl8V6p",
        "colab": {}
      },
      "source": [
        "dataset_transaction = None\n",
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = test_identity.pop(column)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVs1d5a_wMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e9b28869-ab36-412d-cb51-a80f52a71219"
      },
      "source": [
        "test_identity.head(5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>24.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  id_03  ...  id_36  id_37  id_38  DeviceType\n",
              "0        3663586  -45.0  280290.0    NaN  ...      F      T      F      mobile\n",
              "1        3663588    0.0    3579.0    0.0  ...      F      T      T      mobile\n",
              "2        3663597   -5.0  185210.0    NaN  ...      T      T      F     desktop\n",
              "3        3663601  -45.0  252944.0    0.0  ...      F      T      F      mobile\n",
              "4        3663602  -95.0  328680.0    NaN  ...      F      T      F      mobile\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "du0_nSm48V63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd17c9f2-6cb2-4d5c-909f-caa1fc3a41e1"
      },
      "source": [
        "merged_data = pd.merge(left=test_transaction, right=test_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "TransactionID = merged_data.pop('TransactionID')\n",
        "test_transaction = None\n",
        "merged_data.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506691, 428)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoViKsx6cZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fe0e97c-d881-48a8-a3dd-6a693789b6ce"
      },
      "source": [
        "test_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "float_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = []\n",
        "for column in skip_int_columns:\n",
        "  int_columns_test.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "print(len(float_columns_test), len(int_columns_test), len(obj_columns_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzQZ6nR6wOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_normalization(X, indices, cache_min, cache_max, cache_mean):\n",
        "  X_out = copy.copy(X)\n",
        "  X_out[indices] = (X_out[indices] - cache_mean)/(cache_max - cache_min)\n",
        "  X_out[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return X_out.astype('float16')  \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM75lh_6lhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in float_columns_test:\n",
        "  # Set to float 16\n",
        "  test_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjog0oM7p4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns_test:\n",
        "  # Set to int 32\n",
        "  test_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMTT8KB74NL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1fc7806-0915-4c7c-ed39-3f96d1673482"
      },
      "source": [
        "encoded_column = 0\n",
        "for column in obj_columns_test:\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(cache[column])\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.transform(test_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(cache[column])))])\n",
        "  test_transaction = pd.concat([test_transaction, pd_encoded], axis=1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "\n",
        "\n",
        "for column in obj_columns_test:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "for column in to_remove:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_OOqFi8HrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ed465a4-667a-4eda-a8ac-1accba59f1ff"
      },
      "source": [
        "# Check if we have the same shape with the X_train\n",
        "#print(test_transaction.shape, X_train.shape)\n",
        "print(test_transaction.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506691, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9vDvpDZdpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the prediction and submit the output\n",
        "result = (new_model.predict(test_transaction)>0.9).astype('int8')\n",
        "result_pd = pd.DataFrame(result, columns=['isFraud'])\n",
        "data_to_file = pd.concat([TransactionID, result_pd], axis=1)\n",
        "data_to_file.head(5)\n",
        "data_to_file.to_csv(\"./submission.csv\", index=False)\n",
        "data_to_file.to_csv('/content/gdrive/My Drive/Kaggle/submission.csv', index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9099XTi4s2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "79db6d41-a2ce-4d56-9863-7e2e8ed571ed"
      },
      "source": [
        "!kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"New submission with model_20200804 using Keras METRICS and Relu and with threshold 0.9\""
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 4.83M/4.83M [00:02<00:00, 1.70MB/s]\n",
            "Successfully submitted to IEEE-CIS Fraud Detection"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICp4sPm6brq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3e2nvzrHir4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fe4ae314-d9e2-483a-9baa-e8b9302f14bf"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohc = OneHotEncoder()\n",
        "a = {'a': ['Null', 'A', 'B', 'C', 'D']}\n",
        "df = pd.DataFrame(a)\n",
        "df\n",
        "encoded = ohc.fit_transform(df['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsaGKlzMUlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69213f3a-42cb-4edc-cbc8-5bc6c0bb5a7e"
      },
      "source": [
        "b = {'a': ['Null', 'A', 'B', 'C', 'E']}\n",
        "df_b = pd.DataFrame(b)\n",
        "ohc_b = OneHotEncoder(handle_unknown='ignore')\n",
        "ohc_b.fit(df['a'].values.reshape(-1,1))\n",
        "encoded_b = ohc_b.transform(df_b['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded_b = pd.DataFrame(encoded_b.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvykuaRPMpZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "20c01d2a-8a3b-41b6-a7ae-b6d7a36f327f"
      },
      "source": [
        "for column in obj_columns:\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(dataset_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 5\n",
            "P_emaildomain 60\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_RMIz3NTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2e5b2844-8bdd-45ee-f5ec-83cd54c9cba7"
      },
      "source": [
        "for column in obj_columns_test:\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(test_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 4\n",
            "P_emaildomain 61\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}