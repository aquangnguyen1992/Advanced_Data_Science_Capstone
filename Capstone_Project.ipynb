{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "63f66180-933d-4af6-f2fb-43ecf023718d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ec07d1a-70fc-4e2f-b510-a1c2384ecfa2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "4658f6b8-4064-4d30-d8f0-32b09e5b9c57"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)\n",
        "saved_columns= np.array(dataset_identity.columns)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ba7fbfc5-e6f1-4551-82f2-f00169b9b91c"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "26cdd230-a450-471b-a859-761300d22349"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4MWdmZ8wBEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = dataset_identity.pop(column)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS60VEEMwgHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56e65c5f-6013-4faf-83ad-582cdda9cd0e"
      },
      "source": [
        "merged_data = pd.merge(left=dataset_transaction, right=dataset_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "dataset_transaction = None\n",
        "dataset_identity = None\n",
        "merged_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 430)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIAS8CbdwwET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "74069b08-8d33-4ddf-9471-a6dd8f8813bf"
      },
      "source": [
        "merged_data.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8efe06b-66f7-45d1-e52c-df12f02dea7c"
      },
      "source": [
        "dataset_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "dataset_identity = None\n",
        "\n",
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "cache = dict()\n",
        "print(len(float_columns), len(int_columns), len(obj_columns))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "b7ae436a-8ff7-41a1-d201-274640191cec"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN column for every features\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "97261f01-dc36-47a2-baab-6b941478038f"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 829 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  id_26_NaN_Code  id_32_NaN_Code\n",
              "0        2987000        0  ...               1               1\n",
              "1        2987001        0  ...               1               1\n",
              "2        2987002        0  ...               1               1\n",
              "3        2987003        0  ...               1               1\n",
              "4        2987004        0  ...               1               0\n",
              "\n",
              "[5 rows x 829 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ae073d23-44f5-49d8-bf40-91f3a70118f5"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "      <th>TransactionDT_NaN_Code</th>\n",
              "      <th>card1_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 831 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  TransactionDT_NaN_Code  card1_NaN_Code\n",
              "0        2987000        0  ...                       0               0\n",
              "1        2987001        0  ...                       0               0\n",
              "2        2987002        0  ...                       0               0\n",
              "3        2987003        0  ...                       0               0\n",
              "4        2987004        0  ...                       0               0\n",
              "\n",
              "[5 rows x 831 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e2369d2-a499-4684-d420-1282d4f15a93"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoded_column = 0\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "  cache[column] = dataset_transaction[column].values.reshape(-1,1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuvQmMmLRnM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "fcd1b917-c277-4174-947f-8bd52d4f5f4b"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1011 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 1011 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0737106e-4651-4079-a18d-bf0e1431765e"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoc4TuIx1zoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "34ca2290-bfbc-4e1f-f9dd-673aac7bdaf9"
      },
      "source": [
        "out = ['isFraud']\n",
        "for column in dataset_transaction.columns:\n",
        "  if column.find('R_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "  if column.find('P_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "print(out)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['isFraud', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'P_emaildomain_5', 'P_emaildomain_6', 'P_emaildomain_7', 'P_emaildomain_8', 'P_emaildomain_9', 'P_emaildomain_10', 'P_emaildomain_11', 'P_emaildomain_12', 'P_emaildomain_13', 'P_emaildomain_14', 'P_emaildomain_15', 'P_emaildomain_16', 'P_emaildomain_17', 'P_emaildomain_18', 'P_emaildomain_19', 'P_emaildomain_20', 'P_emaildomain_21', 'P_emaildomain_22', 'P_emaildomain_23', 'P_emaildomain_24', 'P_emaildomain_25', 'P_emaildomain_26', 'P_emaildomain_27', 'P_emaildomain_28', 'P_emaildomain_29', 'P_emaildomain_30', 'P_emaildomain_31', 'P_emaildomain_32', 'P_emaildomain_33', 'P_emaildomain_34', 'P_emaildomain_35', 'P_emaildomain_36', 'P_emaildomain_37', 'P_emaildomain_38', 'P_emaildomain_39', 'P_emaildomain_40', 'P_emaildomain_41', 'P_emaildomain_42', 'P_emaildomain_43', 'P_emaildomain_44', 'P_emaildomain_45', 'P_emaildomain_46', 'P_emaildomain_47', 'P_emaildomain_48', 'P_emaildomain_49', 'P_emaildomain_50', 'P_emaildomain_51', 'P_emaildomain_52', 'P_emaildomain_53', 'P_emaildomain_54', 'P_emaildomain_55', 'P_emaildomain_56', 'P_emaildomain_57', 'P_emaildomain_58', 'P_emaildomain_59', 'R_emaildomain_0', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'R_emaildomain_4', 'R_emaildomain_5', 'R_emaildomain_6', 'R_emaildomain_7', 'R_emaildomain_8', 'R_emaildomain_9', 'R_emaildomain_10', 'R_emaildomain_11', 'R_emaildomain_12', 'R_emaildomain_13', 'R_emaildomain_14', 'R_emaildomain_15', 'R_emaildomain_16', 'R_emaildomain_17', 'R_emaildomain_18', 'R_emaildomain_19', 'R_emaildomain_20', 'R_emaildomain_21', 'R_emaildomain_22', 'R_emaildomain_23', 'R_emaildomain_24', 'R_emaildomain_25', 'R_emaildomain_26', 'R_emaildomain_27', 'R_emaildomain_28', 'R_emaildomain_29', 'R_emaildomain_30', 'R_emaildomain_31', 'R_emaildomain_32', 'R_emaildomain_33', 'R_emaildomain_34', 'R_emaildomain_35', 'R_emaildomain_36', 'R_emaildomain_37', 'R_emaildomain_38', 'R_emaildomain_39', 'R_emaildomain_40', 'R_emaildomain_41', 'R_emaildomain_42', 'R_emaildomain_43', 'R_emaildomain_44', 'R_emaildomain_45', 'R_emaildomain_46', 'R_emaildomain_47', 'R_emaildomain_48', 'R_emaildomain_49', 'R_emaildomain_50', 'R_emaildomain_51', 'R_emaildomain_52', 'R_emaildomain_53', 'R_emaildomain_54', 'R_emaildomain_55', 'R_emaildomain_56', 'R_emaildomain_57', 'R_emaildomain_58', 'R_emaildomain_59', 'R_emaildomain_60']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#columns_to_analyze = ['isFraud', 'DeviceType_0', 'DeviceType_1', 'DeviceType_2', 'id_15_0', 'id_15_1', 'id_15_2', 'id_15_3']#, 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2', 'card1', 'card2', 'card3']\n",
        "columns_to_analyze = out\n",
        "\n",
        "analyzing_data = dataset_transaction[columns_to_analyze]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "to_display = False\n",
        "\n",
        "if to_display:\n",
        "  mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD5CKASq2rzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "472bc10b-f567-40c3-e6ff-8c6e5b86bcb1"
      },
      "source": [
        "# Remove the weak correlation features\n",
        "col = corr.columns\n",
        "is_fraud = np.where(col=='isFraud')[0][0]\n",
        "col = col.to_list()\n",
        "col.pop(is_fraud)\n",
        "to_remove = []\n",
        "for each_col in col:\n",
        "  if abs(corr['isFraud'][each_col]) < 0.05: # Weak correlation\n",
        "    to_remove.append(each_col)\n",
        "    a = dataset_transaction.pop(each_col)\n",
        "print(len(to_remove))\n",
        "analyzing_data = None\n",
        "\n",
        "\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 893 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 893 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "c0d307af-58c4-4cea-96fe-f3b3ee8d5f25"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "a = dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 892 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  ...  DeviceType_1  DeviceType_2\n",
              "0        0      -0.463379  ...             0             0\n",
              "1        0      -0.463379  ...             0             0\n",
              "2        0      -0.463379  ...             0             0\n",
              "3        0      -0.463379  ...             0             0\n",
              "4        0      -0.463379  ...             0             1\n",
              "\n",
              "[5 rows x 892 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27de42bb-ffe8-409a-b37e-12ce0925e026"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)\n",
        "X_train = X\n",
        "Y_train = Y\n",
        "\n",
        "test_size = 20000\n",
        "indices = np.random.randint(0, len(Y), size=(test_size,))\n",
        "X_test = np.array(X_train)[indices]\n",
        "Y_test = np.array(Y_train)[indices]\n",
        "\n",
        "print(X_train.shape, Y_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(590540, 891) (590540,) (20000, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e4dc99dd-a450-45f0-e182-99ba564def05"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bdd874e7-d806-4821-c86b-bf9fdcccd180"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 5\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_to_train = np.array(X_train)[indices_0_new]\n",
        "Y_to_train = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "\n",
        "X_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_to_train, axis=1)\n",
        "print(X_to_train.shape, Y_to_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569877, 1)\n",
            "(693855, 1)\n",
            "(693855, 891) (693855,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "912d603b-f1b3-4875-c042-3975cd38dd7a"
      },
      "source": [
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 17.87%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  out_model.add(Dense(dense1, activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                      input_shape=(X_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(dense2, activation=tf.keras.layers.LeakyReLU(alpha=0.01), \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[METRICS])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "592baf51-d6ae-4601-ff6f-bd746e913a4b"
      },
      "source": [
        "my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=1e-4, l2_rate=5e-4, init_std=0.1, lr=0.00005)\n",
        "my_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 428,033\n",
            "Trainable params: 427,009\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1aa367f-3a3f-417c-de3d-d1bfe8f52b4a"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "NB_EPOCH = 2000\n",
        "PATIENCE = 20\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', patience=PATIENCE, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./best_model.h5', monitor='val_auc', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.2, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 2.2830 - tp: 45774.0000 - fp: 57227.0000 - tn: 397581.0000 - fn: 53146.0000 - accuracy: 0.8007 - precision: 0.4444 - recall: 0.4627 - auc: 0.7410\n",
            "Epoch 00001: val_auc improved from -inf to 0.84577, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 21s 10ms/step - loss: 2.2820 - tp: 45862.0000 - fp: 57268.0000 - tn: 398655.0000 - fn: 53299.0000 - accuracy: 0.8008 - precision: 0.4447 - recall: 0.4625 - auc: 0.7411 - val_loss: 1.8490 - val_tp: 10020.0000 - val_fp: 2336.0000 - val_tn: 111618.0000 - val_fn: 14797.0000 - val_accuracy: 0.8765 - val_precision: 0.8109 - val_recall: 0.4038 - val_auc: 0.8458\n",
            "Epoch 2/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 1.7215 - tp: 39909.0000 - fp: 14208.0000 - tn: 441440.0000 - fn: 59195.0000 - accuracy: 0.8677 - precision: 0.7375 - recall: 0.4027 - auc: 0.8191\n",
            "Epoch 00002: val_auc improved from 0.84577 to 0.85923, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 1.7214 - tp: 39925.0000 - fp: 14219.0000 - tn: 441704.0000 - fn: 59236.0000 - accuracy: 0.8677 - precision: 0.7374 - recall: 0.4026 - auc: 0.8191 - val_loss: 1.5495 - val_tp: 9343.0000 - val_fp: 1364.0000 - val_tn: 112590.0000 - val_fn: 15474.0000 - val_accuracy: 0.8787 - val_precision: 0.8726 - val_recall: 0.3765 - val_auc: 0.8592\n",
            "Epoch 3/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 1.4624 - tp: 41082.0000 - fp: 11782.0000 - tn: 443031.0000 - fn: 57833.0000 - accuracy: 0.8743 - precision: 0.7771 - recall: 0.4153 - auc: 0.8424\n",
            "Epoch 00003: val_auc improved from 0.85923 to 0.86798, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 1.4621 - tp: 41194.0000 - fp: 11803.0000 - tn: 444120.0000 - fn: 57967.0000 - accuracy: 0.8743 - precision: 0.7773 - recall: 0.4154 - auc: 0.8425 - val_loss: 1.3327 - val_tp: 9967.0000 - val_fp: 1435.0000 - val_tn: 112519.0000 - val_fn: 14850.0000 - val_accuracy: 0.8826 - val_precision: 0.8741 - val_recall: 0.4016 - val_auc: 0.8680\n",
            "Epoch 4/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 1.2567 - tp: 43492.0000 - fp: 11475.0000 - tn: 444389.0000 - fn: 55652.0000 - accuracy: 0.8791 - precision: 0.7912 - recall: 0.4387 - auc: 0.8561\n",
            "Epoch 00004: val_auc improved from 0.86798 to 0.87303, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 1.2567 - tp: 43498.0000 - fp: 11476.0000 - tn: 444447.0000 - fn: 55663.0000 - accuracy: 0.8790 - precision: 0.7912 - recall: 0.4387 - auc: 0.8561 - val_loss: 1.1541 - val_tp: 9950.0000 - val_fp: 1290.0000 - val_tn: 112664.0000 - val_fn: 14867.0000 - val_accuracy: 0.8836 - val_precision: 0.8852 - val_recall: 0.4009 - val_auc: 0.8730\n",
            "Epoch 5/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 1.0868 - tp: 45226.0000 - fp: 11273.0000 - tn: 444176.0000 - fn: 53821.0000 - accuracy: 0.8826 - precision: 0.8005 - recall: 0.4566 - auc: 0.8646\n",
            "Epoch 00005: val_auc improved from 0.87303 to 0.87571, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 1.0867 - tp: 45279.0000 - fp: 11288.0000 - tn: 444635.0000 - fn: 53882.0000 - accuracy: 0.8826 - precision: 0.8004 - recall: 0.4566 - auc: 0.8646 - val_loss: 1.0031 - val_tp: 11071.0000 - val_fp: 1913.0000 - val_tn: 112041.0000 - val_fn: 13746.0000 - val_accuracy: 0.8872 - val_precision: 0.8527 - val_recall: 0.4461 - val_auc: 0.8757\n",
            "Epoch 6/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.9461 - tp: 47223.0000 - fp: 11220.0000 - tn: 444703.0000 - fn: 51938.0000 - accuracy: 0.8862 - precision: 0.8080 - recall: 0.4762 - auc: 0.8712\n",
            "Epoch 00006: val_auc improved from 0.87571 to 0.88489, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.9461 - tp: 47223.0000 - fp: 11220.0000 - tn: 444703.0000 - fn: 51938.0000 - accuracy: 0.8862 - precision: 0.8080 - recall: 0.4762 - auc: 0.8712 - val_loss: 0.8741 - val_tp: 11286.0000 - val_fp: 1644.0000 - val_tn: 112310.0000 - val_fn: 13531.0000 - val_accuracy: 0.8906 - val_precision: 0.8729 - val_recall: 0.4548 - val_auc: 0.8849\n",
            "Epoch 7/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.8324 - tp: 48493.0000 - fp: 11427.0000 - tn: 443800.0000 - fn: 50520.0000 - accuracy: 0.8882 - precision: 0.8093 - recall: 0.4898 - auc: 0.8762\n",
            "Epoch 00007: val_auc improved from 0.88489 to 0.88932, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.8323 - tp: 48565.0000 - fp: 11443.0000 - tn: 444480.0000 - fn: 50596.0000 - accuracy: 0.8882 - precision: 0.8093 - recall: 0.4898 - auc: 0.8762 - val_loss: 0.7731 - val_tp: 12263.0000 - val_fp: 2207.0000 - val_tn: 111747.0000 - val_fn: 12554.0000 - val_accuracy: 0.8936 - val_precision: 0.8475 - val_recall: 0.4941 - val_auc: 0.8893\n",
            "Epoch 8/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.7425 - tp: 49680.0000 - fp: 11506.0000 - tn: 443924.0000 - fn: 49386.0000 - accuracy: 0.8902 - precision: 0.8120 - recall: 0.5015 - auc: 0.8803\n",
            "Epoch 00008: val_auc improved from 0.88932 to 0.89037, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.7425 - tp: 49726.0000 - fp: 11516.0000 - tn: 444407.0000 - fn: 49435.0000 - accuracy: 0.8902 - precision: 0.8120 - recall: 0.5015 - auc: 0.8803 - val_loss: 0.6986 - val_tp: 12308.0000 - val_fp: 2176.0000 - val_tn: 111778.0000 - val_fn: 12509.0000 - val_accuracy: 0.8942 - val_precision: 0.8498 - val_recall: 0.4960 - val_auc: 0.8904\n",
            "Epoch 9/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.6756 - tp: 50801.0000 - fp: 11740.0000 - tn: 443278.0000 - fn: 48165.0000 - accuracy: 0.8919 - precision: 0.8123 - recall: 0.5133 - auc: 0.8838\n",
            "Epoch 00009: val_auc improved from 0.89037 to 0.89180, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.6755 - tp: 50913.0000 - fp: 11762.0000 - tn: 444161.0000 - fn: 48248.0000 - accuracy: 0.8919 - precision: 0.8123 - recall: 0.5134 - auc: 0.8839 - val_loss: 0.6436 - val_tp: 12308.0000 - val_fp: 2051.0000 - val_tn: 111903.0000 - val_fn: 12509.0000 - val_accuracy: 0.8951 - val_precision: 0.8572 - val_recall: 0.4960 - val_auc: 0.8918\n",
            "Epoch 10/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.6281 - tp: 51734.0000 - fp: 11936.0000 - tn: 443296.0000 - fn: 47274.0000 - accuracy: 0.8932 - precision: 0.8125 - recall: 0.5225 - auc: 0.8869\n",
            "Epoch 00010: val_auc improved from 0.89180 to 0.89628, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 21s 10ms/step - loss: 0.6280 - tp: 51822.0000 - fp: 11952.0000 - tn: 443971.0000 - fn: 47339.0000 - accuracy: 0.8932 - precision: 0.8126 - recall: 0.5226 - auc: 0.8869 - val_loss: 0.6004 - val_tp: 12912.0000 - val_fp: 2376.0000 - val_tn: 111578.0000 - val_fn: 11905.0000 - val_accuracy: 0.8971 - val_precision: 0.8446 - val_recall: 0.5203 - val_auc: 0.8963\n",
            "Epoch 11/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.5933 - tp: 52525.0000 - fp: 11919.0000 - tn: 443133.0000 - fn: 46407.0000 - accuracy: 0.8947 - precision: 0.8150 - recall: 0.5309 - auc: 0.8895\n",
            "Epoch 00011: val_auc improved from 0.89628 to 0.89795, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.5932 - tp: 52663.0000 - fp: 11941.0000 - tn: 443982.0000 - fn: 46498.0000 - accuracy: 0.8947 - precision: 0.8152 - recall: 0.5311 - auc: 0.8895 - val_loss: 0.5722 - val_tp: 12847.0000 - val_fp: 2164.0000 - val_tn: 111790.0000 - val_fn: 11970.0000 - val_accuracy: 0.8981 - val_precision: 0.8558 - val_recall: 0.5177 - val_auc: 0.8980\n",
            "Epoch 12/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.5676 - tp: 53105.0000 - fp: 11926.0000 - tn: 443109.0000 - fn: 45844.0000 - accuracy: 0.8957 - precision: 0.8166 - recall: 0.5367 - auc: 0.8918\n",
            "Epoch 00012: val_auc improved from 0.89795 to 0.90106, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.5677 - tp: 53210.0000 - fp: 11951.0000 - tn: 443972.0000 - fn: 45951.0000 - accuracy: 0.8957 - precision: 0.8166 - recall: 0.5366 - auc: 0.8917 - val_loss: 0.5476 - val_tp: 13288.0000 - val_fp: 2361.0000 - val_tn: 111593.0000 - val_fn: 11529.0000 - val_accuracy: 0.8999 - val_precision: 0.8491 - val_recall: 0.5354 - val_auc: 0.9011\n",
            "Epoch 13/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.5468 - tp: 54006.0000 - fp: 12161.0000 - tn: 443281.0000 - fn: 45048.0000 - accuracy: 0.8968 - precision: 0.8162 - recall: 0.5452 - auc: 0.8941\n",
            "Epoch 00013: val_auc improved from 0.90106 to 0.90156, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.5468 - tp: 54064.0000 - fp: 12175.0000 - tn: 443748.0000 - fn: 45097.0000 - accuracy: 0.8968 - precision: 0.8162 - recall: 0.5452 - auc: 0.8940 - val_loss: 0.5309 - val_tp: 14742.0000 - val_fp: 3831.0000 - val_tn: 110123.0000 - val_fn: 10075.0000 - val_accuracy: 0.8998 - val_precision: 0.7937 - val_recall: 0.5940 - val_auc: 0.9016\n",
            "Epoch 14/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.5290 - tp: 54639.0000 - fp: 12212.0000 - tn: 443442.0000 - fn: 44459.0000 - accuracy: 0.8978 - precision: 0.8173 - recall: 0.5514 - auc: 0.8967\n",
            "Epoch 00014: val_auc improved from 0.90156 to 0.90440, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.5291 - tp: 54667.0000 - fp: 12220.0000 - tn: 443703.0000 - fn: 44494.0000 - accuracy: 0.8978 - precision: 0.8173 - recall: 0.5513 - auc: 0.8967 - val_loss: 0.5299 - val_tp: 10818.0000 - val_fp: 973.0000 - val_tn: 112981.0000 - val_fn: 13999.0000 - val_accuracy: 0.8921 - val_precision: 0.9175 - val_recall: 0.4359 - val_auc: 0.9044\n",
            "Epoch 15/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.5140 - tp: 55157.0000 - fp: 12158.0000 - tn: 443489.0000 - fn: 43948.0000 - accuracy: 0.8989 - precision: 0.8194 - recall: 0.5566 - auc: 0.8985\n",
            "Epoch 00015: val_auc improved from 0.90440 to 0.90973, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.5140 - tp: 55186.0000 - fp: 12163.0000 - tn: 443760.0000 - fn: 43975.0000 - accuracy: 0.8989 - precision: 0.8194 - recall: 0.5565 - auc: 0.8985 - val_loss: 0.4979 - val_tp: 13108.0000 - val_fp: 1750.0000 - val_tn: 112204.0000 - val_fn: 11709.0000 - val_accuracy: 0.9030 - val_precision: 0.8822 - val_recall: 0.5282 - val_auc: 0.9097\n",
            "Epoch 16/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.5004 - tp: 55842.0000 - fp: 12075.0000 - tn: 443848.0000 - fn: 43319.0000 - accuracy: 0.9002 - precision: 0.8222 - recall: 0.5631 - auc: 0.9003\n",
            "Epoch 00016: val_auc improved from 0.90973 to 0.90984, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.5004 - tp: 55842.0000 - fp: 12075.0000 - tn: 443848.0000 - fn: 43319.0000 - accuracy: 0.9002 - precision: 0.8222 - recall: 0.5631 - auc: 0.9003 - val_loss: 0.4873 - val_tp: 14971.0000 - val_fp: 3601.0000 - val_tn: 110353.0000 - val_fn: 9846.0000 - val_accuracy: 0.9031 - val_precision: 0.8061 - val_recall: 0.6033 - val_auc: 0.9098\n",
            "Epoch 17/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.4886 - tp: 56174.0000 - fp: 12307.0000 - tn: 442940.0000 - fn: 42819.0000 - accuracy: 0.9005 - precision: 0.8203 - recall: 0.5675 - auc: 0.9024\n",
            "Epoch 00017: val_auc improved from 0.90984 to 0.91014, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4886 - tp: 56270.0000 - fp: 12322.0000 - tn: 443601.0000 - fn: 42891.0000 - accuracy: 0.9005 - precision: 0.8204 - recall: 0.5675 - auc: 0.9024 - val_loss: 0.4756 - val_tp: 14183.0000 - val_fp: 2585.0000 - val_tn: 111369.0000 - val_fn: 10634.0000 - val_accuracy: 0.9047 - val_precision: 0.8458 - val_recall: 0.5715 - val_auc: 0.9101\n",
            "Epoch 18/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.4781 - tp: 56691.0000 - fp: 12120.0000 - tn: 443527.0000 - fn: 42414.0000 - accuracy: 0.9017 - precision: 0.8239 - recall: 0.5720 - auc: 0.9041\n",
            "Epoch 00018: val_auc improved from 0.91014 to 0.91153, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4781 - tp: 56721.0000 - fp: 12126.0000 - tn: 443797.0000 - fn: 42440.0000 - accuracy: 0.9017 - precision: 0.8239 - recall: 0.5720 - auc: 0.9041 - val_loss: 0.4672 - val_tp: 14670.0000 - val_fp: 3237.0000 - val_tn: 110717.0000 - val_fn: 10147.0000 - val_accuracy: 0.9036 - val_precision: 0.8192 - val_recall: 0.5911 - val_auc: 0.9115\n",
            "Epoch 19/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.4690 - tp: 57175.0000 - fp: 12245.0000 - tn: 443202.0000 - fn: 41874.0000 - accuracy: 0.9024 - precision: 0.8236 - recall: 0.5772 - auc: 0.9057\n",
            "Epoch 00019: val_auc improved from 0.91153 to 0.91624, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4690 - tp: 57234.0000 - fp: 12257.0000 - tn: 443666.0000 - fn: 41927.0000 - accuracy: 0.9024 - precision: 0.8236 - recall: 0.5772 - auc: 0.9058 - val_loss: 0.4536 - val_tp: 14461.0000 - val_fp: 2492.0000 - val_tn: 111462.0000 - val_fn: 10356.0000 - val_accuracy: 0.9074 - val_precision: 0.8530 - val_recall: 0.5827 - val_auc: 0.9162\n",
            "Epoch 20/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.4615 - tp: 57512.0000 - fp: 12210.0000 - tn: 443214.0000 - fn: 41560.0000 - accuracy: 0.9030 - precision: 0.8249 - recall: 0.5805 - auc: 0.9069\n",
            "Epoch 00020: val_auc did not improve from 0.91624\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4614 - tp: 57570.0000 - fp: 12231.0000 - tn: 443692.0000 - fn: 41591.0000 - accuracy: 0.9030 - precision: 0.8248 - recall: 0.5806 - auc: 0.9069 - val_loss: 0.4518 - val_tp: 13568.0000 - val_fp: 1902.0000 - val_tn: 112052.0000 - val_fn: 11249.0000 - val_accuracy: 0.9052 - val_precision: 0.8771 - val_recall: 0.5467 - val_auc: 0.9155\n",
            "Epoch 21/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.4533 - tp: 58064.0000 - fp: 12276.0000 - tn: 443172.0000 - fn: 40984.0000 - accuracy: 0.9039 - precision: 0.8255 - recall: 0.5862 - auc: 0.9089\n",
            "Epoch 00021: val_auc did not improve from 0.91624\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4533 - tp: 58135.0000 - fp: 12290.0000 - tn: 443633.0000 - fn: 41026.0000 - accuracy: 0.9039 - precision: 0.8255 - recall: 0.5863 - auc: 0.9089 - val_loss: 0.4491 - val_tp: 13888.0000 - val_fp: 2520.0000 - val_tn: 111434.0000 - val_fn: 10929.0000 - val_accuracy: 0.9031 - val_precision: 0.8464 - val_recall: 0.5596 - val_auc: 0.9154\n",
            "Epoch 22/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.4461 - tp: 58255.0000 - fp: 12087.0000 - tn: 443143.0000 - fn: 40755.0000 - accuracy: 0.9047 - precision: 0.8282 - recall: 0.5884 - auc: 0.9107\n",
            "Epoch 00022: val_auc improved from 0.91624 to 0.91913, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4461 - tp: 58341.0000 - fp: 12108.0000 - tn: 443815.0000 - fn: 40820.0000 - accuracy: 0.9046 - precision: 0.8281 - recall: 0.5883 - auc: 0.9107 - val_loss: 0.4340 - val_tp: 14300.0000 - val_fp: 2176.0000 - val_tn: 111778.0000 - val_fn: 10517.0000 - val_accuracy: 0.9085 - val_precision: 0.8679 - val_recall: 0.5762 - val_auc: 0.9191\n",
            "Epoch 23/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.4391 - tp: 59049.0000 - fp: 12290.0000 - tn: 443366.0000 - fn: 40047.0000 - accuracy: 0.9057 - precision: 0.8277 - recall: 0.5959 - auc: 0.9122\n",
            "Epoch 00023: val_auc improved from 0.91913 to 0.92066, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.4392 - tp: 59080.0000 - fp: 12294.0000 - tn: 443629.0000 - fn: 40081.0000 - accuracy: 0.9056 - precision: 0.8278 - recall: 0.5958 - auc: 0.9122 - val_loss: 0.4282 - val_tp: 14818.0000 - val_fp: 2726.0000 - val_tn: 111228.0000 - val_fn: 9999.0000 - val_accuracy: 0.9083 - val_precision: 0.8446 - val_recall: 0.5971 - val_auc: 0.9207\n",
            "Epoch 24/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.4335 - tp: 59161.0000 - fp: 12254.0000 - tn: 443602.0000 - fn: 39991.0000 - accuracy: 0.9059 - precision: 0.8284 - recall: 0.5967 - auc: 0.9131\n",
            "Epoch 00024: val_auc improved from 0.92066 to 0.92443, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4335 - tp: 59169.0000 - fp: 12258.0000 - tn: 443665.0000 - fn: 39992.0000 - accuracy: 0.9059 - precision: 0.8284 - recall: 0.5967 - auc: 0.9131 - val_loss: 0.4205 - val_tp: 14249.0000 - val_fp: 1929.0000 - val_tn: 112025.0000 - val_fn: 10568.0000 - val_accuracy: 0.9099 - val_precision: 0.8808 - val_recall: 0.5742 - val_auc: 0.9244\n",
            "Epoch 25/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.4272 - tp: 59518.0000 - fp: 12187.0000 - tn: 443017.0000 - fn: 39518.0000 - accuracy: 0.9067 - precision: 0.8300 - recall: 0.6010 - auc: 0.9149\n",
            "Epoch 00025: val_auc did not improve from 0.92443\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4272 - tp: 59596.0000 - fp: 12213.0000 - tn: 443710.0000 - fn: 39565.0000 - accuracy: 0.9067 - precision: 0.8299 - recall: 0.6010 - auc: 0.9148 - val_loss: 0.4157 - val_tp: 15824.0000 - val_fp: 3415.0000 - val_tn: 110539.0000 - val_fn: 8993.0000 - val_accuracy: 0.9106 - val_precision: 0.8225 - val_recall: 0.6376 - val_auc: 0.9223\n",
            "Epoch 26/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.4210 - tp: 59999.0000 - fp: 12287.0000 - tn: 442723.0000 - fn: 38975.0000 - accuracy: 0.9075 - precision: 0.8300 - recall: 0.6062 - auc: 0.9166\n",
            "Epoch 00026: val_auc did not improve from 0.92443\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4209 - tp: 60116.0000 - fp: 12320.0000 - tn: 443603.0000 - fn: 39045.0000 - accuracy: 0.9075 - precision: 0.8299 - recall: 0.6062 - auc: 0.9166 - val_loss: 0.4119 - val_tp: 15271.0000 - val_fp: 2886.0000 - val_tn: 111068.0000 - val_fn: 9546.0000 - val_accuracy: 0.9104 - val_precision: 0.8411 - val_recall: 0.6153 - val_auc: 0.9228\n",
            "Epoch 27/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.4165 - tp: 60095.0000 - fp: 12167.0000 - tn: 443756.0000 - fn: 39066.0000 - accuracy: 0.9077 - precision: 0.8316 - recall: 0.6060 - auc: 0.9169\n",
            "Epoch 00027: val_auc improved from 0.92443 to 0.92498, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4165 - tp: 60095.0000 - fp: 12167.0000 - tn: 443756.0000 - fn: 39066.0000 - accuracy: 0.9077 - precision: 0.8316 - recall: 0.6060 - auc: 0.9169 - val_loss: 0.4049 - val_tp: 15357.0000 - val_fp: 2853.0000 - val_tn: 111101.0000 - val_fn: 9460.0000 - val_accuracy: 0.9113 - val_precision: 0.8433 - val_recall: 0.6188 - val_auc: 0.9250\n",
            "Epoch 28/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.4106 - tp: 60546.0000 - fp: 12358.0000 - tn: 442630.0000 - fn: 38450.0000 - accuracy: 0.9083 - precision: 0.8305 - recall: 0.6116 - auc: 0.9191\n",
            "Epoch 00028: val_auc improved from 0.92498 to 0.92592, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4106 - tp: 60649.0000 - fp: 12380.0000 - tn: 443543.0000 - fn: 38512.0000 - accuracy: 0.9083 - precision: 0.8305 - recall: 0.6116 - auc: 0.9191 - val_loss: 0.4013 - val_tp: 16082.0000 - val_fp: 3476.0000 - val_tn: 110478.0000 - val_fn: 8735.0000 - val_accuracy: 0.9120 - val_precision: 0.8223 - val_recall: 0.6480 - val_auc: 0.9259\n",
            "Epoch 29/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.4055 - tp: 60984.0000 - fp: 12247.0000 - tn: 442964.0000 - fn: 38045.0000 - accuracy: 0.9093 - precision: 0.8328 - recall: 0.6158 - auc: 0.9202\n",
            "Epoch 00029: val_auc improved from 0.92592 to 0.92840, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4055 - tp: 61058.0000 - fp: 12265.0000 - tn: 443658.0000 - fn: 38103.0000 - accuracy: 0.9093 - precision: 0.8327 - recall: 0.6157 - auc: 0.9202 - val_loss: 0.3935 - val_tp: 15137.0000 - val_fp: 2274.0000 - val_tn: 111680.0000 - val_fn: 9680.0000 - val_accuracy: 0.9139 - val_precision: 0.8694 - val_recall: 0.6099 - val_auc: 0.9284\n",
            "Epoch 30/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.4006 - tp: 61232.0000 - fp: 12185.0000 - tn: 443052.0000 - fn: 37771.0000 - accuracy: 0.9099 - precision: 0.8340 - recall: 0.6185 - auc: 0.9215\n",
            "Epoch 00030: val_auc improved from 0.92840 to 0.93038, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.4006 - tp: 61330.0000 - fp: 12210.0000 - tn: 443713.0000 - fn: 37831.0000 - accuracy: 0.9098 - precision: 0.8340 - recall: 0.6185 - auc: 0.9215 - val_loss: 0.3877 - val_tp: 15731.0000 - val_fp: 2759.0000 - val_tn: 111195.0000 - val_fn: 9086.0000 - val_accuracy: 0.9146 - val_precision: 0.8508 - val_recall: 0.6339 - val_auc: 0.9304\n",
            "Epoch 31/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3964 - tp: 61540.0000 - fp: 12285.0000 - tn: 443378.0000 - fn: 37549.0000 - accuracy: 0.9102 - precision: 0.8336 - recall: 0.6211 - auc: 0.9226\n",
            "Epoch 00031: val_auc improved from 0.93038 to 0.93094, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3964 - tp: 61585.0000 - fp: 12293.0000 - tn: 443630.0000 - fn: 37576.0000 - accuracy: 0.9102 - precision: 0.8336 - recall: 0.6211 - auc: 0.9226 - val_loss: 0.3835 - val_tp: 16349.0000 - val_fp: 3231.0000 - val_tn: 110723.0000 - val_fn: 8468.0000 - val_accuracy: 0.9157 - val_precision: 0.8350 - val_recall: 0.6588 - val_auc: 0.9309\n",
            "Epoch 32/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3926 - tp: 61897.0000 - fp: 12223.0000 - tn: 442998.0000 - fn: 37122.0000 - accuracy: 0.9110 - precision: 0.8351 - recall: 0.6251 - auc: 0.9234\n",
            "Epoch 00032: val_auc improved from 0.93094 to 0.93118, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3925 - tp: 61989.0000 - fp: 12239.0000 - tn: 443684.0000 - fn: 37172.0000 - accuracy: 0.9110 - precision: 0.8351 - recall: 0.6251 - auc: 0.9234 - val_loss: 0.3837 - val_tp: 16388.0000 - val_fp: 3452.0000 - val_tn: 110502.0000 - val_fn: 8429.0000 - val_accuracy: 0.9144 - val_precision: 0.8260 - val_recall: 0.6604 - val_auc: 0.9312\n",
            "Epoch 33/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3886 - tp: 62046.0000 - fp: 12132.0000 - tn: 443530.0000 - fn: 37044.0000 - accuracy: 0.9114 - precision: 0.8364 - recall: 0.6262 - auc: 0.9243\n",
            "Epoch 00033: val_auc improved from 0.93118 to 0.93251, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3886 - tp: 62080.0000 - fp: 12139.0000 - tn: 443784.0000 - fn: 37081.0000 - accuracy: 0.9113 - precision: 0.8364 - recall: 0.6261 - auc: 0.9243 - val_loss: 0.3911 - val_tp: 18208.0000 - val_fp: 6114.0000 - val_tn: 107840.0000 - val_fn: 6609.0000 - val_accuracy: 0.9083 - val_precision: 0.7486 - val_recall: 0.7337 - val_auc: 0.9325\n",
            "Epoch 34/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3848 - tp: 62237.0000 - fp: 12086.0000 - tn: 443143.0000 - fn: 36774.0000 - accuracy: 0.9118 - precision: 0.8374 - recall: 0.6286 - auc: 0.9258\n",
            "Epoch 00034: val_auc improved from 0.93251 to 0.93415, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3848 - tp: 62328.0000 - fp: 12107.0000 - tn: 443816.0000 - fn: 36833.0000 - accuracy: 0.9118 - precision: 0.8373 - recall: 0.6286 - auc: 0.9258 - val_loss: 0.3750 - val_tp: 15551.0000 - val_fp: 2621.0000 - val_tn: 111333.0000 - val_fn: 9266.0000 - val_accuracy: 0.9143 - val_precision: 0.8558 - val_recall: 0.6266 - val_auc: 0.9342\n",
            "Epoch 35/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.3814 - tp: 62566.0000 - fp: 12297.0000 - tn: 443626.0000 - fn: 36595.0000 - accuracy: 0.9119 - precision: 0.8357 - recall: 0.6310 - auc: 0.9267\n",
            "Epoch 00035: val_auc improved from 0.93415 to 0.93440, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3814 - tp: 62566.0000 - fp: 12297.0000 - tn: 443626.0000 - fn: 36595.0000 - accuracy: 0.9119 - precision: 0.8357 - recall: 0.6310 - auc: 0.9267 - val_loss: 0.3756 - val_tp: 14720.0000 - val_fp: 1912.0000 - val_tn: 112042.0000 - val_fn: 10097.0000 - val_accuracy: 0.9135 - val_precision: 0.8850 - val_recall: 0.5931 - val_auc: 0.9344\n",
            "Epoch 36/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3779 - tp: 62979.0000 - fp: 12034.0000 - tn: 443619.0000 - fn: 36120.0000 - accuracy: 0.9132 - precision: 0.8396 - recall: 0.6355 - auc: 0.9275\n",
            "Epoch 00036: val_auc improved from 0.93440 to 0.93646, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3778 - tp: 63024.0000 - fp: 12038.0000 - tn: 443885.0000 - fn: 36137.0000 - accuracy: 0.9132 - precision: 0.8396 - recall: 0.6356 - auc: 0.9275 - val_loss: 0.3679 - val_tp: 15405.0000 - val_fp: 2198.0000 - val_tn: 111756.0000 - val_fn: 9412.0000 - val_accuracy: 0.9163 - val_precision: 0.8751 - val_recall: 0.6207 - val_auc: 0.9365\n",
            "Epoch 37/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.3741 - tp: 63310.0000 - fp: 12052.0000 - tn: 443871.0000 - fn: 35851.0000 - accuracy: 0.9137 - precision: 0.8401 - recall: 0.6385 - auc: 0.9290\n",
            "Epoch 00037: val_auc improved from 0.93646 to 0.93807, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3741 - tp: 63310.0000 - fp: 12052.0000 - tn: 443871.0000 - fn: 35851.0000 - accuracy: 0.9137 - precision: 0.8401 - recall: 0.6385 - auc: 0.9290 - val_loss: 0.3619 - val_tp: 16007.0000 - val_fp: 2645.0000 - val_tn: 111309.0000 - val_fn: 8810.0000 - val_accuracy: 0.9175 - val_precision: 0.8582 - val_recall: 0.6450 - val_auc: 0.9381\n",
            "Epoch 38/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3711 - tp: 63572.0000 - fp: 12179.0000 - tn: 443466.0000 - fn: 35535.0000 - accuracy: 0.9140 - precision: 0.8392 - recall: 0.6414 - auc: 0.9299\n",
            "Epoch 00038: val_auc improved from 0.93807 to 0.93905, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3711 - tp: 63608.0000 - fp: 12185.0000 - tn: 443738.0000 - fn: 35553.0000 - accuracy: 0.9140 - precision: 0.8392 - recall: 0.6415 - auc: 0.9299 - val_loss: 0.3580 - val_tp: 16281.0000 - val_fp: 2846.0000 - val_tn: 111108.0000 - val_fn: 8536.0000 - val_accuracy: 0.9180 - val_precision: 0.8512 - val_recall: 0.6560 - val_auc: 0.9391\n",
            "Epoch 39/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.3681 - tp: 63667.0000 - fp: 12005.0000 - tn: 442790.0000 - fn: 35266.0000 - accuracy: 0.9146 - precision: 0.8414 - recall: 0.6435 - auc: 0.9307\n",
            "Epoch 00039: val_auc improved from 0.93905 to 0.94059, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3681 - tp: 63810.0000 - fp: 12029.0000 - tn: 443894.0000 - fn: 35351.0000 - accuracy: 0.9146 - precision: 0.8414 - recall: 0.6435 - auc: 0.9307 - val_loss: 0.3565 - val_tp: 15391.0000 - val_fp: 1963.0000 - val_tn: 111991.0000 - val_fn: 9426.0000 - val_accuracy: 0.9179 - val_precision: 0.8869 - val_recall: 0.6202 - val_auc: 0.9406\n",
            "Epoch 40/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3658 - tp: 63905.0000 - fp: 12096.0000 - tn: 443764.0000 - fn: 35243.0000 - accuracy: 0.9147 - precision: 0.8408 - recall: 0.6445 - auc: 0.9315\n",
            "Epoch 00040: val_auc improved from 0.94059 to 0.94311, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3658 - tp: 63914.0000 - fp: 12098.0000 - tn: 443825.0000 - fn: 35247.0000 - accuracy: 0.9147 - precision: 0.8408 - recall: 0.6445 - auc: 0.9315 - val_loss: 0.3492 - val_tp: 16207.0000 - val_fp: 2312.0000 - val_tn: 111642.0000 - val_fn: 8610.0000 - val_accuracy: 0.9213 - val_precision: 0.8752 - val_recall: 0.6531 - val_auc: 0.9431\n",
            "Epoch 41/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.3625 - tp: 64290.0000 - fp: 12126.0000 - tn: 443317.0000 - fn: 34763.0000 - accuracy: 0.9154 - precision: 0.8413 - recall: 0.6490 - auc: 0.9326\n",
            "Epoch 00041: val_auc improved from 0.94311 to 0.94341, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 21s 9ms/step - loss: 0.3625 - tp: 64362.0000 - fp: 12134.0000 - tn: 443789.0000 - fn: 34799.0000 - accuracy: 0.9154 - precision: 0.8414 - recall: 0.6491 - auc: 0.9326 - val_loss: 0.3475 - val_tp: 16187.0000 - val_fp: 2352.0000 - val_tn: 111602.0000 - val_fn: 8630.0000 - val_accuracy: 0.9209 - val_precision: 0.8731 - val_recall: 0.6523 - val_auc: 0.9434\n",
            "Epoch 42/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.3601 - tp: 64360.0000 - fp: 12039.0000 - tn: 443884.0000 - fn: 34801.0000 - accuracy: 0.9156 - precision: 0.8424 - recall: 0.6490 - auc: 0.9332\n",
            "Epoch 00042: val_auc did not improve from 0.94341\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3601 - tp: 64360.0000 - fp: 12039.0000 - tn: 443884.0000 - fn: 34801.0000 - accuracy: 0.9156 - precision: 0.8424 - recall: 0.6490 - auc: 0.9332 - val_loss: 0.3475 - val_tp: 16452.0000 - val_fp: 2642.0000 - val_tn: 111312.0000 - val_fn: 8365.0000 - val_accuracy: 0.9207 - val_precision: 0.8616 - val_recall: 0.6629 - val_auc: 0.9424\n",
            "Epoch 43/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3572 - tp: 64665.0000 - fp: 11996.0000 - tn: 443245.0000 - fn: 34334.0000 - accuracy: 0.9164 - precision: 0.8435 - recall: 0.6532 - auc: 0.9343\n",
            "Epoch 00043: val_auc did not improve from 0.94341\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3572 - tp: 64765.0000 - fp: 12006.0000 - tn: 443917.0000 - fn: 34396.0000 - accuracy: 0.9164 - precision: 0.8436 - recall: 0.6531 - auc: 0.9343 - val_loss: 0.3515 - val_tp: 15479.0000 - val_fp: 2020.0000 - val_tn: 111934.0000 - val_fn: 9338.0000 - val_accuracy: 0.9182 - val_precision: 0.8846 - val_recall: 0.6237 - val_auc: 0.9414\n",
            "Epoch 44/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3552 - tp: 64913.0000 - fp: 12045.0000 - tn: 443180.0000 - fn: 34102.0000 - accuracy: 0.9167 - precision: 0.8435 - recall: 0.6556 - auc: 0.9347\n",
            "Epoch 00044: val_auc did not improve from 0.94341\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3552 - tp: 65003.0000 - fp: 12063.0000 - tn: 443860.0000 - fn: 34158.0000 - accuracy: 0.9167 - precision: 0.8435 - recall: 0.6555 - auc: 0.9347 - val_loss: 0.3462 - val_tp: 17887.0000 - val_fp: 3953.0000 - val_tn: 110001.0000 - val_fn: 6930.0000 - val_accuracy: 0.9216 - val_precision: 0.8190 - val_recall: 0.7208 - val_auc: 0.9432\n",
            "Epoch 45/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3526 - tp: 65194.0000 - fp: 12075.0000 - tn: 443572.0000 - fn: 33911.0000 - accuracy: 0.9171 - precision: 0.8437 - recall: 0.6578 - auc: 0.9358\n",
            "Epoch 00045: val_auc improved from 0.94341 to 0.94512, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3526 - tp: 65231.0000 - fp: 12081.0000 - tn: 443842.0000 - fn: 33930.0000 - accuracy: 0.9171 - precision: 0.8437 - recall: 0.6578 - auc: 0.9358 - val_loss: 0.3401 - val_tp: 16249.0000 - val_fp: 2292.0000 - val_tn: 111662.0000 - val_fn: 8568.0000 - val_accuracy: 0.9217 - val_precision: 0.8764 - val_recall: 0.6548 - val_auc: 0.9451\n",
            "Epoch 46/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3499 - tp: 65306.0000 - fp: 11931.0000 - tn: 443315.0000 - fn: 33688.0000 - accuracy: 0.9177 - precision: 0.8455 - recall: 0.6597 - auc: 0.9365\n",
            "Epoch 00046: val_auc did not improve from 0.94512\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3499 - tp: 65419.0000 - fp: 11949.0000 - tn: 443974.0000 - fn: 33742.0000 - accuracy: 0.9177 - precision: 0.8456 - recall: 0.6597 - auc: 0.9365 - val_loss: 0.3423 - val_tp: 16418.0000 - val_fp: 2578.0000 - val_tn: 111376.0000 - val_fn: 8399.0000 - val_accuracy: 0.9209 - val_precision: 0.8643 - val_recall: 0.6616 - val_auc: 0.9415\n",
            "Epoch 47/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.3475 - tp: 65748.0000 - fp: 11995.0000 - tn: 443444.0000 - fn: 33309.0000 - accuracy: 0.9183 - precision: 0.8457 - recall: 0.6637 - auc: 0.9374\n",
            "Epoch 00047: val_auc improved from 0.94512 to 0.94613, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3475 - tp: 65818.0000 - fp: 12010.0000 - tn: 443913.0000 - fn: 33343.0000 - accuracy: 0.9183 - precision: 0.8457 - recall: 0.6637 - auc: 0.9375 - val_loss: 0.3370 - val_tp: 16219.0000 - val_fp: 2271.0000 - val_tn: 111683.0000 - val_fn: 8598.0000 - val_accuracy: 0.9217 - val_precision: 0.8772 - val_recall: 0.6535 - val_auc: 0.9461\n",
            "Epoch 48/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.3459 - tp: 65819.0000 - fp: 12092.0000 - tn: 443357.0000 - fn: 33228.0000 - accuracy: 0.9183 - precision: 0.8448 - recall: 0.6645 - auc: 0.9379\n",
            "Epoch 00048: val_auc did not improve from 0.94613\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3459 - tp: 65886.0000 - fp: 12100.0000 - tn: 443823.0000 - fn: 33275.0000 - accuracy: 0.9183 - precision: 0.8448 - recall: 0.6644 - auc: 0.9379 - val_loss: 0.3373 - val_tp: 16631.0000 - val_fp: 2584.0000 - val_tn: 111370.0000 - val_fn: 8186.0000 - val_accuracy: 0.9224 - val_precision: 0.8655 - val_recall: 0.6701 - val_auc: 0.9446\n",
            "Epoch 49/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3435 - tp: 66138.0000 - fp: 11901.0000 - tn: 443748.0000 - fn: 32965.0000 - accuracy: 0.9191 - precision: 0.8475 - recall: 0.6674 - auc: 0.9388\n",
            "Epoch 00049: val_auc improved from 0.94613 to 0.94693, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3435 - tp: 66184.0000 - fp: 11907.0000 - tn: 444016.0000 - fn: 32977.0000 - accuracy: 0.9191 - precision: 0.8475 - recall: 0.6674 - auc: 0.9388 - val_loss: 0.3322 - val_tp: 17783.0000 - val_fp: 3811.0000 - val_tn: 110143.0000 - val_fn: 7034.0000 - val_accuracy: 0.9218 - val_precision: 0.8235 - val_recall: 0.7166 - val_auc: 0.9469\n",
            "Epoch 50/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.3416 - tp: 66134.0000 - fp: 11814.0000 - tn: 443639.0000 - fn: 32909.0000 - accuracy: 0.9193 - precision: 0.8484 - recall: 0.6677 - auc: 0.9392\n",
            "Epoch 00050: val_auc improved from 0.94693 to 0.94763, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3416 - tp: 66212.0000 - fp: 11830.0000 - tn: 444093.0000 - fn: 32949.0000 - accuracy: 0.9193 - precision: 0.8484 - recall: 0.6677 - auc: 0.9392 - val_loss: 0.3311 - val_tp: 18318.0000 - val_fp: 4296.0000 - val_tn: 109658.0000 - val_fn: 6499.0000 - val_accuracy: 0.9222 - val_precision: 0.8100 - val_recall: 0.7381 - val_auc: 0.9476\n",
            "Epoch 51/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3399 - tp: 66408.0000 - fp: 11943.0000 - tn: 443922.0000 - fn: 32735.0000 - accuracy: 0.9195 - precision: 0.8476 - recall: 0.6698 - auc: 0.9399\n",
            "Epoch 00051: val_auc improved from 0.94763 to 0.94873, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3399 - tp: 66419.0000 - fp: 11944.0000 - tn: 443979.0000 - fn: 32742.0000 - accuracy: 0.9195 - precision: 0.8476 - recall: 0.6698 - auc: 0.9399 - val_loss: 0.3271 - val_tp: 18008.0000 - val_fp: 3667.0000 - val_tn: 110287.0000 - val_fn: 6809.0000 - val_accuracy: 0.9245 - val_precision: 0.8308 - val_recall: 0.7256 - val_auc: 0.9487\n",
            "Epoch 52/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.3375 - tp: 66394.0000 - fp: 11907.0000 - tn: 443112.0000 - fn: 32571.0000 - accuracy: 0.9197 - precision: 0.8479 - recall: 0.6709 - auc: 0.9410\n",
            "Epoch 00052: val_auc improved from 0.94873 to 0.94967, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3375 - tp: 66526.0000 - fp: 11942.0000 - tn: 443981.0000 - fn: 32635.0000 - accuracy: 0.9197 - precision: 0.8478 - recall: 0.6709 - auc: 0.9410 - val_loss: 0.3240 - val_tp: 16864.0000 - val_fp: 2480.0000 - val_tn: 111474.0000 - val_fn: 7953.0000 - val_accuracy: 0.9248 - val_precision: 0.8718 - val_recall: 0.6795 - val_auc: 0.9497\n",
            "Epoch 53/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.3359 - tp: 66649.0000 - fp: 11977.0000 - tn: 443032.0000 - fn: 32326.0000 - accuracy: 0.9200 - precision: 0.8477 - recall: 0.6734 - auc: 0.9413\n",
            "Epoch 00053: val_auc did not improve from 0.94967\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3358 - tp: 66769.0000 - fp: 11997.0000 - tn: 443926.0000 - fn: 32392.0000 - accuracy: 0.9200 - precision: 0.8477 - recall: 0.6733 - auc: 0.9413 - val_loss: 0.3262 - val_tp: 15888.0000 - val_fp: 1719.0000 - val_tn: 112235.0000 - val_fn: 8929.0000 - val_accuracy: 0.9233 - val_precision: 0.9024 - val_recall: 0.6402 - val_auc: 0.9493\n",
            "Epoch 54/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3337 - tp: 66911.0000 - fp: 11819.0000 - tn: 443398.0000 - fn: 32112.0000 - accuracy: 0.9207 - precision: 0.8499 - recall: 0.6757 - auc: 0.9419\n",
            "Epoch 00054: val_auc improved from 0.94967 to 0.95273, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3337 - tp: 67004.0000 - fp: 11839.0000 - tn: 444084.0000 - fn: 32157.0000 - accuracy: 0.9207 - precision: 0.8498 - recall: 0.6757 - auc: 0.9419 - val_loss: 0.3184 - val_tp: 16680.0000 - val_fp: 2121.0000 - val_tn: 111833.0000 - val_fn: 8137.0000 - val_accuracy: 0.9261 - val_precision: 0.8872 - val_recall: 0.6721 - val_auc: 0.9527\n",
            "Epoch 55/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3321 - tp: 67095.0000 - fp: 11931.0000 - tn: 443931.0000 - fn: 32051.0000 - accuracy: 0.9208 - precision: 0.8490 - recall: 0.6767 - auc: 0.9424\n",
            "Epoch 00055: val_auc did not improve from 0.95273\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3321 - tp: 67105.0000 - fp: 11935.0000 - tn: 443988.0000 - fn: 32056.0000 - accuracy: 0.9207 - precision: 0.8490 - recall: 0.6767 - auc: 0.9424 - val_loss: 0.3171 - val_tp: 17120.0000 - val_fp: 2448.0000 - val_tn: 111506.0000 - val_fn: 7697.0000 - val_accuracy: 0.9269 - val_precision: 0.8749 - val_recall: 0.6898 - val_auc: 0.9524\n",
            "Epoch 56/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3302 - tp: 67218.0000 - fp: 11922.0000 - tn: 443937.0000 - fn: 31931.0000 - accuracy: 0.9210 - precision: 0.8494 - recall: 0.6779 - auc: 0.9432\n",
            "Epoch 00056: val_auc did not improve from 0.95273\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3302 - tp: 67227.0000 - fp: 11925.0000 - tn: 443998.0000 - fn: 31934.0000 - accuracy: 0.9210 - precision: 0.8493 - recall: 0.6780 - auc: 0.9432 - val_loss: 0.3191 - val_tp: 16575.0000 - val_fp: 2059.0000 - val_tn: 111895.0000 - val_fn: 8242.0000 - val_accuracy: 0.9258 - val_precision: 0.8895 - val_recall: 0.6679 - val_auc: 0.9509\n",
            "Epoch 57/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.3291 - tp: 67280.0000 - fp: 12058.0000 - tn: 442746.0000 - fn: 31644.0000 - accuracy: 0.9211 - precision: 0.8480 - recall: 0.6801 - auc: 0.9433\n",
            "Epoch 00057: val_auc improved from 0.95273 to 0.95361, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3291 - tp: 67436.0000 - fp: 12080.0000 - tn: 443843.0000 - fn: 31725.0000 - accuracy: 0.9211 - precision: 0.8481 - recall: 0.6801 - auc: 0.9433 - val_loss: 0.3133 - val_tp: 16867.0000 - val_fp: 2107.0000 - val_tn: 111847.0000 - val_fn: 7950.0000 - val_accuracy: 0.9275 - val_precision: 0.8890 - val_recall: 0.6797 - val_auc: 0.9536\n",
            "Epoch 58/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3264 - tp: 67627.0000 - fp: 11792.0000 - tn: 443439.0000 - fn: 31382.0000 - accuracy: 0.9221 - precision: 0.8515 - recall: 0.6830 - auc: 0.9445\n",
            "Epoch 00058: val_auc did not improve from 0.95361\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3264 - tp: 67727.0000 - fp: 11806.0000 - tn: 444117.0000 - fn: 31434.0000 - accuracy: 0.9221 - precision: 0.8516 - recall: 0.6830 - auc: 0.9445 - val_loss: 0.3312 - val_tp: 14727.0000 - val_fp: 1197.0000 - val_tn: 112757.0000 - val_fn: 10090.0000 - val_accuracy: 0.9187 - val_precision: 0.9248 - val_recall: 0.5934 - val_auc: 0.9523\n",
            "Epoch 59/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.3255 - tp: 67764.0000 - fp: 11965.0000 - tn: 443958.0000 - fn: 31397.0000 - accuracy: 0.9219 - precision: 0.8499 - recall: 0.6834 - auc: 0.9446\n",
            "Epoch 00059: val_auc improved from 0.95361 to 0.95452, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3255 - tp: 67764.0000 - fp: 11965.0000 - tn: 443958.0000 - fn: 31397.0000 - accuracy: 0.9219 - precision: 0.8499 - recall: 0.6834 - auc: 0.9446 - val_loss: 0.3146 - val_tp: 15869.0000 - val_fp: 1500.0000 - val_tn: 112454.0000 - val_fn: 8948.0000 - val_accuracy: 0.9247 - val_precision: 0.9136 - val_recall: 0.6394 - val_auc: 0.9545\n",
            "Epoch 60/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3236 - tp: 67889.0000 - fp: 11844.0000 - tn: 443810.0000 - fn: 31209.0000 - accuracy: 0.9224 - precision: 0.8515 - recall: 0.6851 - auc: 0.9453\n",
            "Epoch 00060: val_auc improved from 0.95452 to 0.95487, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3236 - tp: 67929.0000 - fp: 11848.0000 - tn: 444075.0000 - fn: 31232.0000 - accuracy: 0.9224 - precision: 0.8515 - recall: 0.6850 - auc: 0.9453 - val_loss: 0.3120 - val_tp: 16346.0000 - val_fp: 1771.0000 - val_tn: 112183.0000 - val_fn: 8471.0000 - val_accuracy: 0.9262 - val_precision: 0.9022 - val_recall: 0.6587 - val_auc: 0.9549\n",
            "Epoch 61/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3230 - tp: 67968.0000 - fp: 11829.0000 - tn: 444034.0000 - fn: 31177.0000 - accuracy: 0.9225 - precision: 0.8518 - recall: 0.6855 - auc: 0.9450\n",
            "Epoch 00061: val_auc improved from 0.95487 to 0.95594, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3230 - tp: 67978.0000 - fp: 11831.0000 - tn: 444092.0000 - fn: 31183.0000 - accuracy: 0.9225 - precision: 0.8518 - recall: 0.6855 - auc: 0.9450 - val_loss: 0.3063 - val_tp: 17296.0000 - val_fp: 2423.0000 - val_tn: 111531.0000 - val_fn: 7521.0000 - val_accuracy: 0.9283 - val_precision: 0.8771 - val_recall: 0.6969 - val_auc: 0.9559\n",
            "Epoch 62/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3213 - tp: 68158.0000 - fp: 11706.0000 - tn: 444155.0000 - fn: 30989.0000 - accuracy: 0.9231 - precision: 0.8534 - recall: 0.6874 - auc: 0.9456\n",
            "Epoch 00062: val_auc did not improve from 0.95594\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3213 - tp: 68167.0000 - fp: 11709.0000 - tn: 444214.0000 - fn: 30994.0000 - accuracy: 0.9231 - precision: 0.8534 - recall: 0.6874 - auc: 0.9456 - val_loss: 0.3087 - val_tp: 16665.0000 - val_fp: 1921.0000 - val_tn: 112033.0000 - val_fn: 8152.0000 - val_accuracy: 0.9274 - val_precision: 0.8966 - val_recall: 0.6715 - val_auc: 0.9546\n",
            "Epoch 63/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3192 - tp: 68370.0000 - fp: 11797.0000 - tn: 444063.0000 - fn: 30778.0000 - accuracy: 0.9233 - precision: 0.8528 - recall: 0.6896 - auc: 0.9466\n",
            "Epoch 00063: val_auc did not improve from 0.95594\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3192 - tp: 68379.0000 - fp: 11800.0000 - tn: 444123.0000 - fn: 30782.0000 - accuracy: 0.9233 - precision: 0.8528 - recall: 0.6896 - auc: 0.9466 - val_loss: 0.3076 - val_tp: 16826.0000 - val_fp: 2011.0000 - val_tn: 111943.0000 - val_fn: 7991.0000 - val_accuracy: 0.9279 - val_precision: 0.8932 - val_recall: 0.6780 - val_auc: 0.9558\n",
            "Epoch 64/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.3184 - tp: 68178.0000 - fp: 11696.0000 - tn: 443099.0000 - fn: 30755.0000 - accuracy: 0.9233 - precision: 0.8536 - recall: 0.6891 - auc: 0.9466\n",
            "Epoch 00064: val_auc did not improve from 0.95594\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3184 - tp: 68332.0000 - fp: 11738.0000 - tn: 444185.0000 - fn: 30829.0000 - accuracy: 0.9233 - precision: 0.8534 - recall: 0.6891 - auc: 0.9466 - val_loss: 0.3046 - val_tp: 17555.0000 - val_fp: 2659.0000 - val_tn: 111295.0000 - val_fn: 7262.0000 - val_accuracy: 0.9285 - val_precision: 0.8685 - val_recall: 0.7074 - val_auc: 0.9547\n",
            "Epoch 65/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.3170 - tp: 68359.0000 - fp: 11690.0000 - tn: 444170.0000 - fn: 30789.0000 - accuracy: 0.9235 - precision: 0.8540 - recall: 0.6895 - auc: 0.9471\n",
            "Epoch 00065: val_auc did not improve from 0.95594\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3170 - tp: 68367.0000 - fp: 11691.0000 - tn: 444232.0000 - fn: 30794.0000 - accuracy: 0.9235 - precision: 0.8540 - recall: 0.6895 - auc: 0.9471 - val_loss: 0.3074 - val_tp: 16207.0000 - val_fp: 1591.0000 - val_tn: 112363.0000 - val_fn: 8610.0000 - val_accuracy: 0.9265 - val_precision: 0.9106 - val_recall: 0.6531 - val_auc: 0.9552\n",
            "Epoch 66/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.3153 - tp: 68628.0000 - fp: 11745.0000 - tn: 444178.0000 - fn: 30533.0000 - accuracy: 0.9238 - precision: 0.8539 - recall: 0.6921 - auc: 0.9478\n",
            "Epoch 00066: val_auc did not improve from 0.95594\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3153 - tp: 68628.0000 - fp: 11745.0000 - tn: 444178.0000 - fn: 30533.0000 - accuracy: 0.9238 - precision: 0.8539 - recall: 0.6921 - auc: 0.9478 - val_loss: 0.3067 - val_tp: 17541.0000 - val_fp: 2868.0000 - val_tn: 111086.0000 - val_fn: 7276.0000 - val_accuracy: 0.9269 - val_precision: 0.8595 - val_recall: 0.7068 - val_auc: 0.9526\n",
            "Epoch 67/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3141 - tp: 68870.0000 - fp: 11774.0000 - tn: 443873.0000 - fn: 30235.0000 - accuracy: 0.9243 - precision: 0.8540 - recall: 0.6949 - auc: 0.9482\n",
            "Epoch 00067: val_auc did not improve from 0.95594\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3141 - tp: 68916.0000 - fp: 11781.0000 - tn: 444142.0000 - fn: 30245.0000 - accuracy: 0.9243 - precision: 0.8540 - recall: 0.6950 - auc: 0.9482 - val_loss: 0.3073 - val_tp: 16693.0000 - val_fp: 2312.0000 - val_tn: 111642.0000 - val_fn: 8124.0000 - val_accuracy: 0.9248 - val_precision: 0.8783 - val_recall: 0.6726 - val_auc: 0.9543\n",
            "Epoch 68/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3126 - tp: 69067.0000 - fp: 11799.0000 - tn: 443856.0000 - fn: 30030.0000 - accuracy: 0.9246 - precision: 0.8541 - recall: 0.6970 - auc: 0.9485\n",
            "Epoch 00068: val_auc improved from 0.95594 to 0.95663, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3126 - tp: 69109.0000 - fp: 11804.0000 - tn: 444119.0000 - fn: 30052.0000 - accuracy: 0.9246 - precision: 0.8541 - recall: 0.6969 - auc: 0.9485 - val_loss: 0.2997 - val_tp: 18217.0000 - val_fp: 3165.0000 - val_tn: 110789.0000 - val_fn: 6600.0000 - val_accuracy: 0.9296 - val_precision: 0.8520 - val_recall: 0.7341 - val_auc: 0.9566\n",
            "Epoch 69/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.3116 - tp: 68809.0000 - fp: 11656.0000 - tn: 443365.0000 - fn: 30154.0000 - accuracy: 0.9245 - precision: 0.8551 - recall: 0.6953 - auc: 0.9489\n",
            "Epoch 00069: val_auc improved from 0.95663 to 0.95774, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3115 - tp: 68961.0000 - fp: 11678.0000 - tn: 444245.0000 - fn: 30200.0000 - accuracy: 0.9246 - precision: 0.8552 - recall: 0.6954 - auc: 0.9489 - val_loss: 0.2959 - val_tp: 17701.0000 - val_fp: 2578.0000 - val_tn: 111376.0000 - val_fn: 7116.0000 - val_accuracy: 0.9301 - val_precision: 0.8729 - val_recall: 0.7133 - val_auc: 0.9577\n",
            "Epoch 70/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3103 - tp: 69036.0000 - fp: 11567.0000 - tn: 444084.0000 - fn: 30065.0000 - accuracy: 0.9250 - precision: 0.8565 - recall: 0.6966 - auc: 0.9490\n",
            "Epoch 00070: val_auc improved from 0.95774 to 0.95807, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3103 - tp: 69075.0000 - fp: 11577.0000 - tn: 444346.0000 - fn: 30086.0000 - accuracy: 0.9249 - precision: 0.8565 - recall: 0.6966 - auc: 0.9490 - val_loss: 0.2960 - val_tp: 17589.0000 - val_fp: 2487.0000 - val_tn: 111467.0000 - val_fn: 7228.0000 - val_accuracy: 0.9300 - val_precision: 0.8761 - val_recall: 0.7087 - val_auc: 0.9581\n",
            "Epoch 71/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3093 - tp: 69243.0000 - fp: 11736.0000 - tn: 443920.0000 - fn: 29853.0000 - accuracy: 0.9250 - precision: 0.8551 - recall: 0.6987 - auc: 0.9497\n",
            "Epoch 00071: val_auc did not improve from 0.95807\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3093 - tp: 69290.0000 - fp: 11741.0000 - tn: 444182.0000 - fn: 29871.0000 - accuracy: 0.9250 - precision: 0.8551 - recall: 0.6988 - auc: 0.9497 - val_loss: 0.2999 - val_tp: 18993.0000 - val_fp: 4000.0000 - val_tn: 109954.0000 - val_fn: 5824.0000 - val_accuracy: 0.9292 - val_precision: 0.8260 - val_recall: 0.7653 - val_auc: 0.9571\n",
            "Epoch 72/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.3075 - tp: 69477.0000 - fp: 11715.0000 - tn: 443723.0000 - fn: 29581.0000 - accuracy: 0.9255 - precision: 0.8557 - recall: 0.7014 - auc: 0.9504\n",
            "Epoch 00072: val_auc improved from 0.95807 to 0.95810, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 21s 10ms/step - loss: 0.3074 - tp: 69552.0000 - fp: 11723.0000 - tn: 444200.0000 - fn: 29609.0000 - accuracy: 0.9255 - precision: 0.8558 - recall: 0.7014 - auc: 0.9504 - val_loss: 0.2938 - val_tp: 17988.0000 - val_fp: 2729.0000 - val_tn: 111225.0000 - val_fn: 6829.0000 - val_accuracy: 0.9311 - val_precision: 0.8683 - val_recall: 0.7248 - val_auc: 0.9581\n",
            "Epoch 73/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3066 - tp: 69573.0000 - fp: 11582.0000 - tn: 444057.0000 - fn: 29540.0000 - accuracy: 0.9259 - precision: 0.8573 - recall: 0.7020 - auc: 0.9504\n",
            "Epoch 00073: val_auc improved from 0.95810 to 0.95944, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3066 - tp: 69609.0000 - fp: 11596.0000 - tn: 444327.0000 - fn: 29552.0000 - accuracy: 0.9259 - precision: 0.8572 - recall: 0.7020 - auc: 0.9504 - val_loss: 0.2923 - val_tp: 18123.0000 - val_fp: 2954.0000 - val_tn: 111000.0000 - val_fn: 6694.0000 - val_accuracy: 0.9305 - val_precision: 0.8598 - val_recall: 0.7303 - val_auc: 0.9594\n",
            "Epoch 74/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3055 - tp: 69528.0000 - fp: 11556.0000 - tn: 443676.0000 - fn: 29480.0000 - accuracy: 0.9260 - precision: 0.8575 - recall: 0.7022 - auc: 0.9509\n",
            "Epoch 00074: val_auc improved from 0.95944 to 0.96074, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3055 - tp: 69638.0000 - fp: 11573.0000 - tn: 444350.0000 - fn: 29523.0000 - accuracy: 0.9260 - precision: 0.8575 - recall: 0.7023 - auc: 0.9509 - val_loss: 0.2888 - val_tp: 17668.0000 - val_fp: 2268.0000 - val_tn: 111686.0000 - val_fn: 7149.0000 - val_accuracy: 0.9321 - val_precision: 0.8862 - val_recall: 0.7119 - val_auc: 0.9607\n",
            "Epoch 75/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3045 - tp: 69455.0000 - fp: 11455.0000 - tn: 443769.0000 - fn: 29561.0000 - accuracy: 0.9260 - precision: 0.8584 - recall: 0.7015 - auc: 0.9511\n",
            "Epoch 00075: val_auc did not improve from 0.96074\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3044 - tp: 69552.0000 - fp: 11466.0000 - tn: 444457.0000 - fn: 29609.0000 - accuracy: 0.9260 - precision: 0.8585 - recall: 0.7014 - auc: 0.9512 - val_loss: 0.2884 - val_tp: 17590.0000 - val_fp: 2173.0000 - val_tn: 111781.0000 - val_fn: 7227.0000 - val_accuracy: 0.9323 - val_precision: 0.8900 - val_recall: 0.7088 - val_auc: 0.9602\n",
            "Epoch 76/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.3041 - tp: 69500.0000 - fp: 11532.0000 - tn: 443271.0000 - fn: 29425.0000 - accuracy: 0.9260 - precision: 0.8577 - recall: 0.7026 - auc: 0.9509\n",
            "Epoch 00076: val_auc did not improve from 0.96074\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3042 - tp: 69658.0000 - fp: 11559.0000 - tn: 444364.0000 - fn: 29503.0000 - accuracy: 0.9260 - precision: 0.8577 - recall: 0.7025 - auc: 0.9509 - val_loss: 0.2909 - val_tp: 17305.0000 - val_fp: 1980.0000 - val_tn: 111974.0000 - val_fn: 7512.0000 - val_accuracy: 0.9316 - val_precision: 0.8973 - val_recall: 0.6973 - val_auc: 0.9606\n",
            "Epoch 77/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.3032 - tp: 69667.0000 - fp: 11544.0000 - tn: 443906.0000 - fn: 29379.0000 - accuracy: 0.9262 - precision: 0.8579 - recall: 0.7034 - auc: 0.9514\n",
            "Epoch 00077: val_auc did not improve from 0.96074\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3032 - tp: 69749.0000 - fp: 11555.0000 - tn: 444368.0000 - fn: 29412.0000 - accuracy: 0.9262 - precision: 0.8579 - recall: 0.7034 - auc: 0.9514 - val_loss: 0.2942 - val_tp: 19274.0000 - val_fp: 4241.0000 - val_tn: 109713.0000 - val_fn: 5543.0000 - val_accuracy: 0.9295 - val_precision: 0.8196 - val_recall: 0.7766 - val_auc: 0.9589\n",
            "Epoch 78/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.3012 - tp: 69961.0000 - fp: 11453.0000 - tn: 444194.0000 - fn: 29144.0000 - accuracy: 0.9268 - precision: 0.8593 - recall: 0.7059 - auc: 0.9521\n",
            "Epoch 00078: val_auc did not improve from 0.96074\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3012 - tp: 69999.0000 - fp: 11461.0000 - tn: 444462.0000 - fn: 29162.0000 - accuracy: 0.9268 - precision: 0.8593 - recall: 0.7059 - auc: 0.9521 - val_loss: 0.2882 - val_tp: 17417.0000 - val_fp: 2101.0000 - val_tn: 111853.0000 - val_fn: 7400.0000 - val_accuracy: 0.9315 - val_precision: 0.8924 - val_recall: 0.7018 - val_auc: 0.9604\n",
            "Epoch 79/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.3010 - tp: 69793.0000 - fp: 11523.0000 - tn: 443710.0000 - fn: 29214.0000 - accuracy: 0.9265 - precision: 0.8583 - recall: 0.7049 - auc: 0.9521\n",
            "Epoch 00079: val_auc did not improve from 0.96074\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.3010 - tp: 69902.0000 - fp: 11534.0000 - tn: 444389.0000 - fn: 29259.0000 - accuracy: 0.9265 - precision: 0.8584 - recall: 0.7049 - auc: 0.9521 - val_loss: 0.2882 - val_tp: 17045.0000 - val_fp: 1805.0000 - val_tn: 112149.0000 - val_fn: 7772.0000 - val_accuracy: 0.9310 - val_precision: 0.9042 - val_recall: 0.6868 - val_auc: 0.9607\n",
            "Epoch 80/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2999 - tp: 70260.0000 - fp: 11667.0000 - tn: 444256.0000 - fn: 28901.0000 - accuracy: 0.9269 - precision: 0.8576 - recall: 0.7085 - auc: 0.9524\n",
            "Epoch 00080: val_auc did not improve from 0.96074\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2999 - tp: 70260.0000 - fp: 11667.0000 - tn: 444256.0000 - fn: 28901.0000 - accuracy: 0.9269 - precision: 0.8576 - recall: 0.7085 - auc: 0.9524 - val_loss: 0.2928 - val_tp: 17438.0000 - val_fp: 2498.0000 - val_tn: 111456.0000 - val_fn: 7379.0000 - val_accuracy: 0.9288 - val_precision: 0.8747 - val_recall: 0.7027 - val_auc: 0.9572\n",
            "Epoch 81/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2987 - tp: 70344.0000 - fp: 11486.0000 - tn: 444374.0000 - fn: 28804.0000 - accuracy: 0.9274 - precision: 0.8596 - recall: 0.7095 - auc: 0.9527\n",
            "Epoch 00081: val_auc improved from 0.96074 to 0.96187, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2987 - tp: 70352.0000 - fp: 11487.0000 - tn: 444436.0000 - fn: 28809.0000 - accuracy: 0.9274 - precision: 0.8596 - recall: 0.7095 - auc: 0.9527 - val_loss: 0.2854 - val_tp: 17298.0000 - val_fp: 1846.0000 - val_tn: 112108.0000 - val_fn: 7519.0000 - val_accuracy: 0.9325 - val_precision: 0.9036 - val_recall: 0.6970 - val_auc: 0.9619\n",
            "Epoch 82/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2976 - tp: 70152.0000 - fp: 11503.0000 - tn: 443742.0000 - fn: 28843.0000 - accuracy: 0.9272 - precision: 0.8591 - recall: 0.7086 - auc: 0.9530\n",
            "Epoch 00082: val_auc did not improve from 0.96187\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2976 - tp: 70274.0000 - fp: 11518.0000 - tn: 444405.0000 - fn: 28887.0000 - accuracy: 0.9272 - precision: 0.8592 - recall: 0.7087 - auc: 0.9530 - val_loss: 0.2844 - val_tp: 17654.0000 - val_fp: 2236.0000 - val_tn: 111718.0000 - val_fn: 7163.0000 - val_accuracy: 0.9323 - val_precision: 0.8876 - val_recall: 0.7114 - val_auc: 0.9612\n",
            "Epoch 83/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2968 - tp: 70496.0000 - fp: 11472.0000 - tn: 444451.0000 - fn: 28665.0000 - accuracy: 0.9277 - precision: 0.8600 - recall: 0.7109 - auc: 0.9534\n",
            "Epoch 00083: val_auc did not improve from 0.96187\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2968 - tp: 70496.0000 - fp: 11472.0000 - tn: 444451.0000 - fn: 28665.0000 - accuracy: 0.9277 - precision: 0.8600 - recall: 0.7109 - auc: 0.9534 - val_loss: 0.2870 - val_tp: 17152.0000 - val_fp: 2004.0000 - val_tn: 111950.0000 - val_fn: 7665.0000 - val_accuracy: 0.9303 - val_precision: 0.8954 - val_recall: 0.6911 - val_auc: 0.9606\n",
            "Epoch 84/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2962 - tp: 70372.0000 - fp: 11541.0000 - tn: 444319.0000 - fn: 28776.0000 - accuracy: 0.9274 - precision: 0.8591 - recall: 0.7098 - auc: 0.9535\n",
            "Epoch 00084: val_auc improved from 0.96187 to 0.96402, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2962 - tp: 70379.0000 - fp: 11543.0000 - tn: 444380.0000 - fn: 28782.0000 - accuracy: 0.9274 - precision: 0.8591 - recall: 0.7097 - auc: 0.9535 - val_loss: 0.2774 - val_tp: 17980.0000 - val_fp: 2203.0000 - val_tn: 111751.0000 - val_fn: 6837.0000 - val_accuracy: 0.9349 - val_precision: 0.8908 - val_recall: 0.7245 - val_auc: 0.9640\n",
            "Epoch 85/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2947 - tp: 70533.0000 - fp: 11514.0000 - tn: 443520.0000 - fn: 28417.0000 - accuracy: 0.9279 - precision: 0.8597 - recall: 0.7128 - auc: 0.9541\n",
            "Epoch 00085: val_auc did not improve from 0.96402\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2948 - tp: 70673.0000 - fp: 11531.0000 - tn: 444392.0000 - fn: 28488.0000 - accuracy: 0.9279 - precision: 0.8597 - recall: 0.7127 - auc: 0.9540 - val_loss: 0.3036 - val_tp: 15359.0000 - val_fp: 1078.0000 - val_tn: 112876.0000 - val_fn: 9458.0000 - val_accuracy: 0.9241 - val_precision: 0.9344 - val_recall: 0.6189 - val_auc: 0.9600\n",
            "Epoch 86/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2938 - tp: 70482.0000 - fp: 11461.0000 - tn: 443549.0000 - fn: 28492.0000 - accuracy: 0.9279 - precision: 0.8601 - recall: 0.7121 - auc: 0.9543\n",
            "Epoch 00086: val_auc did not improve from 0.96402\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2938 - tp: 70613.0000 - fp: 11486.0000 - tn: 444437.0000 - fn: 28548.0000 - accuracy: 0.9279 - precision: 0.8601 - recall: 0.7121 - auc: 0.9543 - val_loss: 0.2769 - val_tp: 18426.0000 - val_fp: 2671.0000 - val_tn: 111283.0000 - val_fn: 6391.0000 - val_accuracy: 0.9347 - val_precision: 0.8734 - val_recall: 0.7425 - val_auc: 0.9633\n",
            "Epoch 87/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2930 - tp: 70937.0000 - fp: 11597.0000 - tn: 444326.0000 - fn: 28224.0000 - accuracy: 0.9283 - precision: 0.8595 - recall: 0.7154 - auc: 0.9545\n",
            "Epoch 00087: val_auc did not improve from 0.96402\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2930 - tp: 70937.0000 - fp: 11597.0000 - tn: 444326.0000 - fn: 28224.0000 - accuracy: 0.9283 - precision: 0.8595 - recall: 0.7154 - auc: 0.9545 - val_loss: 0.2787 - val_tp: 18208.0000 - val_fp: 2621.0000 - val_tn: 111333.0000 - val_fn: 6609.0000 - val_accuracy: 0.9335 - val_precision: 0.8742 - val_recall: 0.7337 - val_auc: 0.9621\n",
            "Epoch 88/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2929 - tp: 70783.0000 - fp: 11598.0000 - tn: 444325.0000 - fn: 28378.0000 - accuracy: 0.9280 - precision: 0.8592 - recall: 0.7138 - auc: 0.9544\n",
            "Epoch 00088: val_auc did not improve from 0.96402\n",
            "2169/2169 [==============================] - 21s 9ms/step - loss: 0.2929 - tp: 70783.0000 - fp: 11598.0000 - tn: 444325.0000 - fn: 28378.0000 - accuracy: 0.9280 - precision: 0.8592 - recall: 0.7138 - auc: 0.9544 - val_loss: 0.2807 - val_tp: 18009.0000 - val_fp: 2525.0000 - val_tn: 111429.0000 - val_fn: 6808.0000 - val_accuracy: 0.9327 - val_precision: 0.8770 - val_recall: 0.7257 - val_auc: 0.9612\n",
            "Epoch 89/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2915 - tp: 70968.0000 - fp: 11427.0000 - tn: 444431.0000 - fn: 28182.0000 - accuracy: 0.9286 - precision: 0.8613 - recall: 0.7158 - auc: 0.9549\n",
            "Epoch 00089: val_auc improved from 0.96402 to 0.96559, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2915 - tp: 70976.0000 - fp: 11430.0000 - tn: 444493.0000 - fn: 28185.0000 - accuracy: 0.9286 - precision: 0.8613 - recall: 0.7158 - auc: 0.9549 - val_loss: 0.2717 - val_tp: 18010.0000 - val_fp: 2023.0000 - val_tn: 111931.0000 - val_fn: 6807.0000 - val_accuracy: 0.9364 - val_precision: 0.8990 - val_recall: 0.7257 - val_auc: 0.9656\n",
            "Epoch 90/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2908 - tp: 70894.0000 - fp: 11473.0000 - tn: 444182.0000 - fn: 28203.0000 - accuracy: 0.9285 - precision: 0.8607 - recall: 0.7154 - auc: 0.9551\n",
            "Epoch 00090: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2909 - tp: 70935.0000 - fp: 11485.0000 - tn: 444438.0000 - fn: 28226.0000 - accuracy: 0.9285 - precision: 0.8607 - recall: 0.7154 - auc: 0.9551 - val_loss: 0.2803 - val_tp: 16583.0000 - val_fp: 1327.0000 - val_tn: 112627.0000 - val_fn: 8234.0000 - val_accuracy: 0.9311 - val_precision: 0.9259 - val_recall: 0.6682 - val_auc: 0.9646\n",
            "Epoch 91/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2894 - tp: 71171.0000 - fp: 11362.0000 - tn: 444561.0000 - fn: 27990.0000 - accuracy: 0.9291 - precision: 0.8623 - recall: 0.7177 - auc: 0.9557\n",
            "Epoch 00091: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2894 - tp: 71171.0000 - fp: 11362.0000 - tn: 444561.0000 - fn: 27990.0000 - accuracy: 0.9291 - precision: 0.8623 - recall: 0.7177 - auc: 0.9557 - val_loss: 0.2868 - val_tp: 17093.0000 - val_fp: 1952.0000 - val_tn: 112002.0000 - val_fn: 7724.0000 - val_accuracy: 0.9303 - val_precision: 0.8975 - val_recall: 0.6888 - val_auc: 0.9607\n",
            "Epoch 92/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2889 - tp: 71422.0000 - fp: 11419.0000 - tn: 444504.0000 - fn: 27739.0000 - accuracy: 0.9295 - precision: 0.8622 - recall: 0.7203 - auc: 0.9555\n",
            "Epoch 00092: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2889 - tp: 71422.0000 - fp: 11419.0000 - tn: 444504.0000 - fn: 27739.0000 - accuracy: 0.9295 - precision: 0.8622 - recall: 0.7203 - auc: 0.9555 - val_loss: 0.2737 - val_tp: 18328.0000 - val_fp: 2595.0000 - val_tn: 111359.0000 - val_fn: 6489.0000 - val_accuracy: 0.9345 - val_precision: 0.8760 - val_recall: 0.7385 - val_auc: 0.9645\n",
            "Epoch 93/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2881 - tp: 71207.0000 - fp: 11344.0000 - tn: 444094.0000 - fn: 27851.0000 - accuracy: 0.9293 - precision: 0.8626 - recall: 0.7188 - auc: 0.9557\n",
            "Epoch 00093: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2881 - tp: 71284.0000 - fp: 11357.0000 - tn: 444566.0000 - fn: 27877.0000 - accuracy: 0.9293 - precision: 0.8626 - recall: 0.7189 - auc: 0.9558 - val_loss: 0.2706 - val_tp: 18813.0000 - val_fp: 2929.0000 - val_tn: 111025.0000 - val_fn: 6004.0000 - val_accuracy: 0.9356 - val_precision: 0.8653 - val_recall: 0.7581 - val_auc: 0.9653\n",
            "Epoch 94/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2877 - tp: 71155.0000 - fp: 11365.0000 - tn: 444294.0000 - fn: 27938.0000 - accuracy: 0.9292 - precision: 0.8623 - recall: 0.7181 - auc: 0.9558\n",
            "Epoch 00094: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2877 - tp: 71210.0000 - fp: 11373.0000 - tn: 444550.0000 - fn: 27951.0000 - accuracy: 0.9292 - precision: 0.8623 - recall: 0.7181 - auc: 0.9558 - val_loss: 0.2769 - val_tp: 16894.0000 - val_fp: 1491.0000 - val_tn: 112463.0000 - val_fn: 7923.0000 - val_accuracy: 0.9322 - val_precision: 0.9189 - val_recall: 0.6807 - val_auc: 0.9648\n",
            "Epoch 95/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2867 - tp: 71388.0000 - fp: 11424.0000 - tn: 444436.0000 - fn: 27760.0000 - accuracy: 0.9294 - precision: 0.8620 - recall: 0.7200 - auc: 0.9562\n",
            "Epoch 00095: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2867 - tp: 71395.0000 - fp: 11428.0000 - tn: 444495.0000 - fn: 27766.0000 - accuracy: 0.9294 - precision: 0.8620 - recall: 0.7200 - auc: 0.9562 - val_loss: 0.2743 - val_tp: 19314.0000 - val_fp: 3638.0000 - val_tn: 110316.0000 - val_fn: 5503.0000 - val_accuracy: 0.9341 - val_precision: 0.8415 - val_recall: 0.7783 - val_auc: 0.9636\n",
            "Epoch 96/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2861 - tp: 71352.0000 - fp: 11404.0000 - tn: 443625.0000 - fn: 27603.0000 - accuracy: 0.9296 - precision: 0.8622 - recall: 0.7211 - auc: 0.9565\n",
            "Epoch 00096: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2861 - tp: 71485.0000 - fp: 11424.0000 - tn: 444499.0000 - fn: 27676.0000 - accuracy: 0.9296 - precision: 0.8622 - recall: 0.7209 - auc: 0.9565 - val_loss: 0.2744 - val_tp: 18739.0000 - val_fp: 2945.0000 - val_tn: 111009.0000 - val_fn: 6078.0000 - val_accuracy: 0.9350 - val_precision: 0.8642 - val_recall: 0.7551 - val_auc: 0.9641\n",
            "Epoch 97/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2852 - tp: 71427.0000 - fp: 11258.0000 - tn: 443769.0000 - fn: 27530.0000 - accuracy: 0.9300 - precision: 0.8638 - recall: 0.7218 - auc: 0.9567\n",
            "Epoch 00097: val_auc did not improve from 0.96559\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2852 - tp: 71563.0000 - fp: 11280.0000 - tn: 444643.0000 - fn: 27598.0000 - accuracy: 0.9300 - precision: 0.8638 - recall: 0.7217 - auc: 0.9567 - val_loss: 0.2731 - val_tp: 17970.0000 - val_fp: 2325.0000 - val_tn: 111629.0000 - val_fn: 6847.0000 - val_accuracy: 0.9339 - val_precision: 0.8854 - val_recall: 0.7241 - val_auc: 0.9635\n",
            "Epoch 98/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2838 - tp: 71717.0000 - fp: 11378.0000 - tn: 444272.0000 - fn: 27385.0000 - accuracy: 0.9301 - precision: 0.8631 - recall: 0.7237 - auc: 0.9572\n",
            "Epoch 00098: val_auc improved from 0.96559 to 0.96635, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2838 - tp: 71759.0000 - fp: 11383.0000 - tn: 444540.0000 - fn: 27402.0000 - accuracy: 0.9301 - precision: 0.8631 - recall: 0.7237 - auc: 0.9572 - val_loss: 0.2668 - val_tp: 18597.0000 - val_fp: 2613.0000 - val_tn: 111341.0000 - val_fn: 6220.0000 - val_accuracy: 0.9363 - val_precision: 0.8768 - val_recall: 0.7494 - val_auc: 0.9664\n",
            "Epoch 99/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2837 - tp: 71648.0000 - fp: 11229.0000 - tn: 444420.0000 - fn: 27455.0000 - accuracy: 0.9303 - precision: 0.8645 - recall: 0.7230 - auc: 0.9569\n",
            "Epoch 00099: val_auc did not improve from 0.96635\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2837 - tp: 71690.0000 - fp: 11236.0000 - tn: 444687.0000 - fn: 27471.0000 - accuracy: 0.9303 - precision: 0.8645 - recall: 0.7230 - auc: 0.9569 - val_loss: 0.2712 - val_tp: 17949.0000 - val_fp: 2247.0000 - val_tn: 111707.0000 - val_fn: 6868.0000 - val_accuracy: 0.9343 - val_precision: 0.8887 - val_recall: 0.7233 - val_auc: 0.9651\n",
            "Epoch 100/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2833 - tp: 71802.0000 - fp: 11499.0000 - tn: 444424.0000 - fn: 27359.0000 - accuracy: 0.9300 - precision: 0.8620 - recall: 0.7241 - auc: 0.9572\n",
            "Epoch 00100: val_auc did not improve from 0.96635\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2833 - tp: 71802.0000 - fp: 11499.0000 - tn: 444424.0000 - fn: 27359.0000 - accuracy: 0.9300 - precision: 0.8620 - recall: 0.7241 - auc: 0.9572 - val_loss: 0.2666 - val_tp: 18547.0000 - val_fp: 2550.0000 - val_tn: 111404.0000 - val_fn: 6270.0000 - val_accuracy: 0.9364 - val_precision: 0.8791 - val_recall: 0.7474 - val_auc: 0.9659\n",
            "Epoch 101/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2820 - tp: 71706.0000 - fp: 11362.0000 - tn: 444291.0000 - fn: 27393.0000 - accuracy: 0.9301 - precision: 0.8632 - recall: 0.7236 - auc: 0.9577\n",
            "Epoch 00101: val_auc improved from 0.96635 to 0.96656, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2820 - tp: 71745.0000 - fp: 11368.0000 - tn: 444555.0000 - fn: 27416.0000 - accuracy: 0.9301 - precision: 0.8632 - recall: 0.7235 - auc: 0.9577 - val_loss: 0.2657 - val_tp: 18176.0000 - val_fp: 2278.0000 - val_tn: 111676.0000 - val_fn: 6641.0000 - val_accuracy: 0.9357 - val_precision: 0.8886 - val_recall: 0.7324 - val_auc: 0.9666\n",
            "Epoch 102/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2816 - tp: 71697.0000 - fp: 11327.0000 - tn: 444325.0000 - fn: 27403.0000 - accuracy: 0.9302 - precision: 0.8636 - recall: 0.7235 - auc: 0.9577\n",
            "Epoch 00102: val_auc did not improve from 0.96656\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2816 - tp: 71741.0000 - fp: 11332.0000 - tn: 444591.0000 - fn: 27420.0000 - accuracy: 0.9302 - precision: 0.8636 - recall: 0.7235 - auc: 0.9577 - val_loss: 0.2741 - val_tp: 20237.0000 - val_fp: 4769.0000 - val_tn: 109185.0000 - val_fn: 4580.0000 - val_accuracy: 0.9326 - val_precision: 0.8093 - val_recall: 0.8154 - val_auc: 0.9657\n",
            "Epoch 103/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2811 - tp: 71819.0000 - fp: 11208.0000 - tn: 444022.0000 - fn: 27191.0000 - accuracy: 0.9307 - precision: 0.8650 - recall: 0.7254 - auc: 0.9577\n",
            "Epoch 00103: val_auc did not improve from 0.96656\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2811 - tp: 71932.0000 - fp: 11227.0000 - tn: 444696.0000 - fn: 27229.0000 - accuracy: 0.9307 - precision: 0.8650 - recall: 0.7254 - auc: 0.9577 - val_loss: 0.2698 - val_tp: 18207.0000 - val_fp: 2501.0000 - val_tn: 111453.0000 - val_fn: 6610.0000 - val_accuracy: 0.9343 - val_precision: 0.8792 - val_recall: 0.7337 - val_auc: 0.9654\n",
            "Epoch 104/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2798 - tp: 72048.0000 - fp: 11292.0000 - tn: 444631.0000 - fn: 27113.0000 - accuracy: 0.9308 - precision: 0.8645 - recall: 0.7266 - auc: 0.9582\n",
            "Epoch 00104: val_auc did not improve from 0.96656\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2798 - tp: 72048.0000 - fp: 11292.0000 - tn: 444631.0000 - fn: 27113.0000 - accuracy: 0.9308 - precision: 0.8645 - recall: 0.7266 - auc: 0.9582 - val_loss: 0.2658 - val_tp: 17777.0000 - val_fp: 1963.0000 - val_tn: 111991.0000 - val_fn: 7040.0000 - val_accuracy: 0.9351 - val_precision: 0.9006 - val_recall: 0.7163 - val_auc: 0.9665\n",
            "Epoch 105/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2796 - tp: 72171.0000 - fp: 11308.0000 - tn: 444615.0000 - fn: 26990.0000 - accuracy: 0.9310 - precision: 0.8645 - recall: 0.7278 - auc: 0.9583\n",
            "Epoch 00105: val_auc did not improve from 0.96656\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2796 - tp: 72171.0000 - fp: 11308.0000 - tn: 444615.0000 - fn: 26990.0000 - accuracy: 0.9310 - precision: 0.8645 - recall: 0.7278 - auc: 0.9583 - val_loss: 0.2666 - val_tp: 18642.0000 - val_fp: 2644.0000 - val_tn: 111310.0000 - val_fn: 6175.0000 - val_accuracy: 0.9364 - val_precision: 0.8758 - val_recall: 0.7512 - val_auc: 0.9655\n",
            "Epoch 106/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2788 - tp: 72095.0000 - fp: 11323.0000 - tn: 444538.0000 - fn: 27052.0000 - accuracy: 0.9309 - precision: 0.8643 - recall: 0.7272 - auc: 0.9585\n",
            "Epoch 00106: val_auc improved from 0.96656 to 0.96747, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2788 - tp: 72104.0000 - fp: 11324.0000 - tn: 444599.0000 - fn: 27057.0000 - accuracy: 0.9309 - precision: 0.8643 - recall: 0.7271 - auc: 0.9585 - val_loss: 0.2659 - val_tp: 17453.0000 - val_fp: 1558.0000 - val_tn: 112396.0000 - val_fn: 7364.0000 - val_accuracy: 0.9357 - val_precision: 0.9180 - val_recall: 0.7033 - val_auc: 0.9675\n",
            "Epoch 107/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2781 - tp: 72055.0000 - fp: 11315.0000 - tn: 443916.0000 - fn: 26954.0000 - accuracy: 0.9310 - precision: 0.8643 - recall: 0.7278 - auc: 0.9586\n",
            "Epoch 00107: val_auc did not improve from 0.96747\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2781 - tp: 72159.0000 - fp: 11333.0000 - tn: 444590.0000 - fn: 27002.0000 - accuracy: 0.9309 - precision: 0.8643 - recall: 0.7277 - auc: 0.9586 - val_loss: 0.2659 - val_tp: 18904.0000 - val_fp: 2890.0000 - val_tn: 111064.0000 - val_fn: 5913.0000 - val_accuracy: 0.9366 - val_precision: 0.8674 - val_recall: 0.7617 - val_auc: 0.9662\n",
            "Epoch 108/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2771 - tp: 72175.0000 - fp: 11195.0000 - tn: 444028.0000 - fn: 26842.0000 - accuracy: 0.9314 - precision: 0.8657 - recall: 0.7289 - auc: 0.9590\n",
            "Epoch 00108: val_auc did not improve from 0.96747\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2771 - tp: 72273.0000 - fp: 11211.0000 - tn: 444712.0000 - fn: 26888.0000 - accuracy: 0.9314 - precision: 0.8657 - recall: 0.7288 - auc: 0.9589 - val_loss: 0.2675 - val_tp: 17420.0000 - val_fp: 1740.0000 - val_tn: 112214.0000 - val_fn: 7397.0000 - val_accuracy: 0.9342 - val_precision: 0.9092 - val_recall: 0.7019 - val_auc: 0.9666\n",
            "Epoch 109/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2770 - tp: 72314.0000 - fp: 11258.0000 - tn: 444197.0000 - fn: 26727.0000 - accuracy: 0.9315 - precision: 0.8653 - recall: 0.7301 - auc: 0.9589\n",
            "Epoch 00109: val_auc did not improve from 0.96747\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2770 - tp: 72402.0000 - fp: 11270.0000 - tn: 444653.0000 - fn: 26759.0000 - accuracy: 0.9315 - precision: 0.8653 - recall: 0.7301 - auc: 0.9589 - val_loss: 0.2646 - val_tp: 19050.0000 - val_fp: 3177.0000 - val_tn: 110777.0000 - val_fn: 5767.0000 - val_accuracy: 0.9355 - val_precision: 0.8571 - val_recall: 0.7676 - val_auc: 0.9658\n",
            "Epoch 110/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2763 - tp: 72424.0000 - fp: 11170.0000 - tn: 444753.0000 - fn: 26737.0000 - accuracy: 0.9317 - precision: 0.8664 - recall: 0.7304 - auc: 0.9592\n",
            "Epoch 00110: val_auc did not improve from 0.96747\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2763 - tp: 72424.0000 - fp: 11170.0000 - tn: 444753.0000 - fn: 26737.0000 - accuracy: 0.9317 - precision: 0.8664 - recall: 0.7304 - auc: 0.9592 - val_loss: 0.2620 - val_tp: 19155.0000 - val_fp: 3127.0000 - val_tn: 110827.0000 - val_fn: 5662.0000 - val_accuracy: 0.9367 - val_precision: 0.8597 - val_recall: 0.7718 - val_auc: 0.9665\n",
            "Epoch 111/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2759 - tp: 72136.0000 - fp: 11185.0000 - tn: 443645.0000 - fn: 26762.0000 - accuracy: 0.9315 - precision: 0.8658 - recall: 0.7294 - auc: 0.9592\n",
            "Epoch 00111: val_auc did not improve from 0.96747\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2759 - tp: 72328.0000 - fp: 11197.0000 - tn: 444726.0000 - fn: 26833.0000 - accuracy: 0.9315 - precision: 0.8659 - recall: 0.7294 - auc: 0.9592 - val_loss: 0.2643 - val_tp: 17726.0000 - val_fp: 1717.0000 - val_tn: 112237.0000 - val_fn: 7091.0000 - val_accuracy: 0.9365 - val_precision: 0.9117 - val_recall: 0.7143 - val_auc: 0.9669\n",
            "Epoch 112/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2743 - tp: 72675.0000 - fp: 11301.0000 - tn: 444355.0000 - fn: 26421.0000 - accuracy: 0.9320 - precision: 0.8654 - recall: 0.7334 - auc: 0.9599\n",
            "Epoch 00112: val_auc did not improve from 0.96747\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2743 - tp: 72720.0000 - fp: 11304.0000 - tn: 444619.0000 - fn: 26441.0000 - accuracy: 0.9320 - precision: 0.8655 - recall: 0.7334 - auc: 0.9599 - val_loss: 0.2598 - val_tp: 18314.0000 - val_fp: 2136.0000 - val_tn: 111818.0000 - val_fn: 6503.0000 - val_accuracy: 0.9377 - val_precision: 0.8956 - val_recall: 0.7380 - val_auc: 0.9675\n",
            "Epoch 113/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2737 - tp: 72722.0000 - fp: 11252.0000 - tn: 444186.0000 - fn: 26336.0000 - accuracy: 0.9322 - precision: 0.8660 - recall: 0.7341 - auc: 0.9600\n",
            "Epoch 00113: val_auc improved from 0.96747 to 0.96769, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2737 - tp: 72795.0000 - fp: 11265.0000 - tn: 444658.0000 - fn: 26366.0000 - accuracy: 0.9322 - precision: 0.8660 - recall: 0.7341 - auc: 0.9600 - val_loss: 0.2601 - val_tp: 17958.0000 - val_fp: 1902.0000 - val_tn: 112052.0000 - val_fn: 6859.0000 - val_accuracy: 0.9369 - val_precision: 0.9042 - val_recall: 0.7236 - val_auc: 0.9677\n",
            "Epoch 114/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2743 - tp: 72539.0000 - fp: 11028.0000 - tn: 444895.0000 - fn: 26622.0000 - accuracy: 0.9322 - precision: 0.8680 - recall: 0.7315 - auc: 0.9595\n",
            "Epoch 00114: val_auc did not improve from 0.96769\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2743 - tp: 72539.0000 - fp: 11028.0000 - tn: 444895.0000 - fn: 26622.0000 - accuracy: 0.9322 - precision: 0.8680 - recall: 0.7315 - auc: 0.9595 - val_loss: 0.2666 - val_tp: 19843.0000 - val_fp: 3803.0000 - val_tn: 110151.0000 - val_fn: 4974.0000 - val_accuracy: 0.9368 - val_precision: 0.8392 - val_recall: 0.7996 - val_auc: 0.9669\n",
            "Epoch 115/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2735 - tp: 72499.0000 - fp: 11209.0000 - tn: 444229.0000 - fn: 26559.0000 - accuracy: 0.9319 - precision: 0.8661 - recall: 0.7319 - auc: 0.9598\n",
            "Epoch 00115: val_auc improved from 0.96769 to 0.96942, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2736 - tp: 72562.0000 - fp: 11221.0000 - tn: 444702.0000 - fn: 26599.0000 - accuracy: 0.9319 - precision: 0.8661 - recall: 0.7318 - auc: 0.9597 - val_loss: 0.2546 - val_tp: 18655.0000 - val_fp: 2180.0000 - val_tn: 111774.0000 - val_fn: 6162.0000 - val_accuracy: 0.9399 - val_precision: 0.8954 - val_recall: 0.7517 - val_auc: 0.9694\n",
            "Epoch 116/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2729 - tp: 72664.0000 - fp: 11186.0000 - tn: 444674.0000 - fn: 26484.0000 - accuracy: 0.9321 - precision: 0.8666 - recall: 0.7329 - auc: 0.9599\n",
            "Epoch 00116: val_auc did not improve from 0.96942\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2729 - tp: 72675.0000 - fp: 11190.0000 - tn: 444733.0000 - fn: 26486.0000 - accuracy: 0.9321 - precision: 0.8666 - recall: 0.7329 - auc: 0.9599 - val_loss: 0.2650 - val_tp: 17449.0000 - val_fp: 1604.0000 - val_tn: 112350.0000 - val_fn: 7368.0000 - val_accuracy: 0.9353 - val_precision: 0.9158 - val_recall: 0.7031 - val_auc: 0.9665\n",
            "Epoch 117/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2727 - tp: 72669.0000 - fp: 11216.0000 - tn: 444643.0000 - fn: 26480.0000 - accuracy: 0.9321 - precision: 0.8663 - recall: 0.7329 - auc: 0.9600\n",
            "Epoch 00117: val_auc did not improve from 0.96942\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2727 - tp: 72677.0000 - fp: 11216.0000 - tn: 444707.0000 - fn: 26484.0000 - accuracy: 0.9321 - precision: 0.8663 - recall: 0.7329 - auc: 0.9600 - val_loss: 0.2649 - val_tp: 16873.0000 - val_fp: 1313.0000 - val_tn: 112641.0000 - val_fn: 7944.0000 - val_accuracy: 0.9333 - val_precision: 0.9278 - val_recall: 0.6799 - val_auc: 0.9683\n",
            "Epoch 118/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2715 - tp: 72796.0000 - fp: 11173.0000 - tn: 444688.0000 - fn: 26351.0000 - accuracy: 0.9324 - precision: 0.8669 - recall: 0.7342 - auc: 0.9604\n",
            "Epoch 00118: val_auc did not improve from 0.96942\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2716 - tp: 72805.0000 - fp: 11176.0000 - tn: 444747.0000 - fn: 26356.0000 - accuracy: 0.9324 - precision: 0.8669 - recall: 0.7342 - auc: 0.9604 - val_loss: 0.2673 - val_tp: 17633.0000 - val_fp: 1834.0000 - val_tn: 112120.0000 - val_fn: 7184.0000 - val_accuracy: 0.9350 - val_precision: 0.9058 - val_recall: 0.7105 - val_auc: 0.9657\n",
            "Epoch 119/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2711 - tp: 72851.0000 - fp: 11247.0000 - tn: 444184.0000 - fn: 26214.0000 - accuracy: 0.9324 - precision: 0.8663 - recall: 0.7354 - auc: 0.9603\n",
            "Epoch 00119: val_auc did not improve from 0.96942\n",
            "2169/2169 [==============================] - 21s 9ms/step - loss: 0.2711 - tp: 72922.0000 - fp: 11262.0000 - tn: 444661.0000 - fn: 26239.0000 - accuracy: 0.9324 - precision: 0.8662 - recall: 0.7354 - auc: 0.9603 - val_loss: 0.2627 - val_tp: 17122.0000 - val_fp: 1529.0000 - val_tn: 112425.0000 - val_fn: 7695.0000 - val_accuracy: 0.9335 - val_precision: 0.9180 - val_recall: 0.6899 - val_auc: 0.9685\n",
            "Epoch 120/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2708 - tp: 72637.0000 - fp: 11186.0000 - tn: 443615.0000 - fn: 26290.0000 - accuracy: 0.9323 - precision: 0.8666 - recall: 0.7342 - auc: 0.9605\n",
            "Epoch 00120: val_auc did not improve from 0.96942\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2708 - tp: 72800.0000 - fp: 11213.0000 - tn: 444710.0000 - fn: 26361.0000 - accuracy: 0.9323 - precision: 0.8665 - recall: 0.7342 - auc: 0.9605 - val_loss: 0.2560 - val_tp: 18285.0000 - val_fp: 2089.0000 - val_tn: 111865.0000 - val_fn: 6532.0000 - val_accuracy: 0.9379 - val_precision: 0.8975 - val_recall: 0.7368 - val_auc: 0.9688\n",
            "Epoch 121/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2694 - tp: 73042.0000 - fp: 11259.0000 - tn: 443759.0000 - fn: 25924.0000 - accuracy: 0.9329 - precision: 0.8664 - recall: 0.7381 - auc: 0.9612\n",
            "Epoch 00121: val_auc improved from 0.96942 to 0.97000, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2694 - tp: 73189.0000 - fp: 11284.0000 - tn: 444639.0000 - fn: 25972.0000 - accuracy: 0.9329 - precision: 0.8664 - recall: 0.7381 - auc: 0.9612 - val_loss: 0.2540 - val_tp: 17882.0000 - val_fp: 1492.0000 - val_tn: 112462.0000 - val_fn: 6935.0000 - val_accuracy: 0.9393 - val_precision: 0.9230 - val_recall: 0.7206 - val_auc: 0.9700\n",
            "Epoch 122/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2697 - tp: 72826.0000 - fp: 11178.0000 - tn: 443629.0000 - fn: 26095.0000 - accuracy: 0.9327 - precision: 0.8669 - recall: 0.7362 - auc: 0.9609\n",
            "Epoch 00122: val_auc did not improve from 0.97000\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2697 - tp: 72996.0000 - fp: 11201.0000 - tn: 444722.0000 - fn: 26165.0000 - accuracy: 0.9327 - precision: 0.8670 - recall: 0.7361 - auc: 0.9609 - val_loss: 0.2552 - val_tp: 18063.0000 - val_fp: 1939.0000 - val_tn: 112015.0000 - val_fn: 6754.0000 - val_accuracy: 0.9374 - val_precision: 0.9031 - val_recall: 0.7278 - val_auc: 0.9690\n",
            "Epoch 123/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2695 - tp: 72938.0000 - fp: 11155.0000 - tn: 444768.0000 - fn: 26223.0000 - accuracy: 0.9327 - precision: 0.8673 - recall: 0.7356 - auc: 0.9607\n",
            "Epoch 00123: val_auc did not improve from 0.97000\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2695 - tp: 72938.0000 - fp: 11155.0000 - tn: 444768.0000 - fn: 26223.0000 - accuracy: 0.9327 - precision: 0.8673 - recall: 0.7356 - auc: 0.9607 - val_loss: 0.2645 - val_tp: 20219.0000 - val_fp: 4699.0000 - val_tn: 109255.0000 - val_fn: 4598.0000 - val_accuracy: 0.9330 - val_precision: 0.8114 - val_recall: 0.8147 - val_auc: 0.9670\n",
            "Epoch 124/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2685 - tp: 72929.0000 - fp: 11118.0000 - tn: 443710.0000 - fn: 25971.0000 - accuracy: 0.9330 - precision: 0.8677 - recall: 0.7374 - auc: 0.9612\n",
            "Epoch 00124: val_auc did not improve from 0.97000\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2686 - tp: 73110.0000 - fp: 11149.0000 - tn: 444774.0000 - fn: 26051.0000 - accuracy: 0.9330 - precision: 0.8677 - recall: 0.7373 - auc: 0.9612 - val_loss: 0.2563 - val_tp: 18654.0000 - val_fp: 2492.0000 - val_tn: 111462.0000 - val_fn: 6163.0000 - val_accuracy: 0.9376 - val_precision: 0.8822 - val_recall: 0.7517 - val_auc: 0.9675\n",
            "Epoch 125/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2678 - tp: 73160.0000 - fp: 11134.0000 - tn: 444297.0000 - fn: 25905.0000 - accuracy: 0.9332 - precision: 0.8679 - recall: 0.7385 - auc: 0.9613\n",
            "Epoch 00125: val_auc did not improve from 0.97000\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2677 - tp: 73228.0000 - fp: 11142.0000 - tn: 444781.0000 - fn: 25933.0000 - accuracy: 0.9332 - precision: 0.8679 - recall: 0.7385 - auc: 0.9613 - val_loss: 0.2615 - val_tp: 19383.0000 - val_fp: 3312.0000 - val_tn: 110642.0000 - val_fn: 5434.0000 - val_accuracy: 0.9370 - val_precision: 0.8541 - val_recall: 0.7810 - val_auc: 0.9659\n",
            "Epoch 126/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2680 - tp: 72988.0000 - fp: 11097.0000 - tn: 444552.0000 - fn: 26115.0000 - accuracy: 0.9329 - precision: 0.8680 - recall: 0.7365 - auc: 0.9610\n",
            "Epoch 00126: val_auc did not improve from 0.97000\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2680 - tp: 73034.0000 - fp: 11102.0000 - tn: 444821.0000 - fn: 26127.0000 - accuracy: 0.9329 - precision: 0.8680 - recall: 0.7365 - auc: 0.9611 - val_loss: 0.2546 - val_tp: 18283.0000 - val_fp: 2137.0000 - val_tn: 111817.0000 - val_fn: 6534.0000 - val_accuracy: 0.9375 - val_precision: 0.8953 - val_recall: 0.7367 - val_auc: 0.9682\n",
            "Epoch 127/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2667 - tp: 73190.0000 - fp: 11019.0000 - tn: 444843.0000 - fn: 25956.0000 - accuracy: 0.9334 - precision: 0.8691 - recall: 0.7382 - auc: 0.9616\n",
            "Epoch 00127: val_auc did not improve from 0.97000\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2667 - tp: 73202.0000 - fp: 11019.0000 - tn: 444904.0000 - fn: 25959.0000 - accuracy: 0.9334 - precision: 0.8692 - recall: 0.7382 - auc: 0.9616 - val_loss: 0.2511 - val_tp: 19048.0000 - val_fp: 2689.0000 - val_tn: 111265.0000 - val_fn: 5769.0000 - val_accuracy: 0.9391 - val_precision: 0.8763 - val_recall: 0.7675 - val_auc: 0.9695\n",
            "Epoch 128/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2664 - tp: 73183.0000 - fp: 11054.0000 - tn: 444869.0000 - fn: 25978.0000 - accuracy: 0.9333 - precision: 0.8688 - recall: 0.7380 - auc: 0.9617\n",
            "Epoch 00128: val_auc improved from 0.97000 to 0.97017, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2664 - tp: 73183.0000 - fp: 11054.0000 - tn: 444869.0000 - fn: 25978.0000 - accuracy: 0.9333 - precision: 0.8688 - recall: 0.7380 - auc: 0.9617 - val_loss: 0.2559 - val_tp: 17396.0000 - val_fp: 1393.0000 - val_tn: 112561.0000 - val_fn: 7421.0000 - val_accuracy: 0.9365 - val_precision: 0.9259 - val_recall: 0.7010 - val_auc: 0.9702\n",
            "Epoch 129/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2657 - tp: 73270.0000 - fp: 11045.0000 - tn: 444605.0000 - fn: 25832.0000 - accuracy: 0.9335 - precision: 0.8690 - recall: 0.7393 - auc: 0.9618\n",
            "Epoch 00129: val_auc improved from 0.97017 to 0.97039, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2657 - tp: 73306.0000 - fp: 11051.0000 - tn: 444872.0000 - fn: 25855.0000 - accuracy: 0.9335 - precision: 0.8690 - recall: 0.7393 - auc: 0.9618 - val_loss: 0.2488 - val_tp: 18953.0000 - val_fp: 2484.0000 - val_tn: 111470.0000 - val_fn: 5864.0000 - val_accuracy: 0.9398 - val_precision: 0.8841 - val_recall: 0.7637 - val_auc: 0.9704\n",
            "Epoch 130/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2655 - tp: 73349.0000 - fp: 10931.0000 - tn: 444928.0000 - fn: 25800.0000 - accuracy: 0.9338 - precision: 0.8703 - recall: 0.7398 - auc: 0.9618\n",
            "Epoch 00130: val_auc improved from 0.97039 to 0.97132, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2655 - tp: 73357.0000 - fp: 10931.0000 - tn: 444992.0000 - fn: 25804.0000 - accuracy: 0.9338 - precision: 0.8703 - recall: 0.7398 - auc: 0.9618 - val_loss: 0.2454 - val_tp: 19445.0000 - val_fp: 2865.0000 - val_tn: 111089.0000 - val_fn: 5372.0000 - val_accuracy: 0.9406 - val_precision: 0.8716 - val_recall: 0.7835 - val_auc: 0.9713\n",
            "Epoch 131/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2654 - tp: 73413.0000 - fp: 11062.0000 - tn: 444861.0000 - fn: 25748.0000 - accuracy: 0.9337 - precision: 0.8691 - recall: 0.7403 - auc: 0.9617\n",
            "Epoch 00131: val_auc did not improve from 0.97132\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2654 - tp: 73413.0000 - fp: 11062.0000 - tn: 444861.0000 - fn: 25748.0000 - accuracy: 0.9337 - precision: 0.8691 - recall: 0.7403 - auc: 0.9617 - val_loss: 0.2467 - val_tp: 19062.0000 - val_fp: 2387.0000 - val_tn: 111567.0000 - val_fn: 5755.0000 - val_accuracy: 0.9413 - val_precision: 0.8887 - val_recall: 0.7681 - val_auc: 0.9706\n",
            "Epoch 132/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2644 - tp: 73335.0000 - fp: 11099.0000 - tn: 444755.0000 - fn: 25819.0000 - accuracy: 0.9335 - precision: 0.8685 - recall: 0.7396 - auc: 0.9621\n",
            "Epoch 00132: val_auc did not improve from 0.97132\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2644 - tp: 73340.0000 - fp: 11100.0000 - tn: 444823.0000 - fn: 25821.0000 - accuracy: 0.9335 - precision: 0.8685 - recall: 0.7396 - auc: 0.9621 - val_loss: 0.2504 - val_tp: 19124.0000 - val_fp: 2819.0000 - val_tn: 111135.0000 - val_fn: 5693.0000 - val_accuracy: 0.9387 - val_precision: 0.8715 - val_recall: 0.7706 - val_auc: 0.9692\n",
            "Epoch 133/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2640 - tp: 73452.0000 - fp: 11184.0000 - tn: 444739.0000 - fn: 25709.0000 - accuracy: 0.9335 - precision: 0.8679 - recall: 0.7407 - auc: 0.9622\n",
            "Epoch 00133: val_auc did not improve from 0.97132\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2640 - tp: 73452.0000 - fp: 11184.0000 - tn: 444739.0000 - fn: 25709.0000 - accuracy: 0.9335 - precision: 0.8679 - recall: 0.7407 - auc: 0.9622 - val_loss: 0.2461 - val_tp: 19045.0000 - val_fp: 2488.0000 - val_tn: 111466.0000 - val_fn: 5772.0000 - val_accuracy: 0.9405 - val_precision: 0.8845 - val_recall: 0.7674 - val_auc: 0.9707\n",
            "Epoch 134/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2629 - tp: 73462.0000 - fp: 11126.0000 - tn: 443890.0000 - fn: 25506.0000 - accuracy: 0.9339 - precision: 0.8685 - recall: 0.7423 - auc: 0.9627\n",
            "Epoch 00134: val_auc did not improve from 0.97132\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2629 - tp: 73602.0000 - fp: 11141.0000 - tn: 444782.0000 - fn: 25559.0000 - accuracy: 0.9339 - precision: 0.8685 - recall: 0.7422 - auc: 0.9627 - val_loss: 0.2468 - val_tp: 18256.0000 - val_fp: 1755.0000 - val_tn: 112199.0000 - val_fn: 6561.0000 - val_accuracy: 0.9401 - val_precision: 0.9123 - val_recall: 0.7356 - val_auc: 0.9711\n",
            "Epoch 135/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2627 - tp: 73590.0000 - fp: 11034.0000 - tn: 444616.0000 - fn: 25512.0000 - accuracy: 0.9341 - precision: 0.8696 - recall: 0.7426 - auc: 0.9627\n",
            "Epoch 00135: val_auc did not improve from 0.97132\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2627 - tp: 73631.0000 - fp: 11038.0000 - tn: 444885.0000 - fn: 25530.0000 - accuracy: 0.9341 - precision: 0.8696 - recall: 0.7425 - auc: 0.9627 - val_loss: 0.2493 - val_tp: 18315.0000 - val_fp: 1979.0000 - val_tn: 111975.0000 - val_fn: 6502.0000 - val_accuracy: 0.9389 - val_precision: 0.9025 - val_recall: 0.7380 - val_auc: 0.9700\n",
            "Epoch 136/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2622 - tp: 73563.0000 - fp: 11085.0000 - tn: 444573.0000 - fn: 25531.0000 - accuracy: 0.9340 - precision: 0.8690 - recall: 0.7424 - auc: 0.9628\n",
            "Epoch 00136: val_auc did not improve from 0.97132\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2622 - tp: 73611.0000 - fp: 11094.0000 - tn: 444829.0000 - fn: 25550.0000 - accuracy: 0.9340 - precision: 0.8690 - recall: 0.7423 - auc: 0.9628 - val_loss: 0.2563 - val_tp: 19766.0000 - val_fp: 3560.0000 - val_tn: 110394.0000 - val_fn: 5051.0000 - val_accuracy: 0.9379 - val_precision: 0.8474 - val_recall: 0.7965 - val_auc: 0.9685\n",
            "Epoch 137/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2618 - tp: 73651.0000 - fp: 11111.0000 - tn: 444104.0000 - fn: 25374.0000 - accuracy: 0.9342 - precision: 0.8689 - recall: 0.7438 - auc: 0.9630\n",
            "Epoch 00137: val_auc improved from 0.97132 to 0.97176, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2618 - tp: 73757.0000 - fp: 11135.0000 - tn: 444788.0000 - fn: 25404.0000 - accuracy: 0.9342 - precision: 0.8688 - recall: 0.7438 - auc: 0.9629 - val_loss: 0.2456 - val_tp: 18374.0000 - val_fp: 1873.0000 - val_tn: 112081.0000 - val_fn: 6443.0000 - val_accuracy: 0.9401 - val_precision: 0.9075 - val_recall: 0.7404 - val_auc: 0.9718\n",
            "Epoch 138/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2615 - tp: 73458.0000 - fp: 10875.0000 - tn: 444365.0000 - fn: 25542.0000 - accuracy: 0.9343 - precision: 0.8710 - recall: 0.7420 - auc: 0.9630\n",
            "Epoch 00138: val_auc did not improve from 0.97176\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2615 - tp: 73582.0000 - fp: 10897.0000 - tn: 445026.0000 - fn: 25579.0000 - accuracy: 0.9343 - precision: 0.8710 - recall: 0.7420 - auc: 0.9630 - val_loss: 0.2525 - val_tp: 20604.0000 - val_fp: 4387.0000 - val_tn: 109567.0000 - val_fn: 4213.0000 - val_accuracy: 0.9380 - val_precision: 0.8245 - val_recall: 0.8302 - val_auc: 0.9708\n",
            "Epoch 139/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2614 - tp: 73650.0000 - fp: 10876.0000 - tn: 444983.0000 - fn: 25499.0000 - accuracy: 0.9345 - precision: 0.8713 - recall: 0.7428 - auc: 0.9629\n",
            "Epoch 00139: val_auc did not improve from 0.97176\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2614 - tp: 73659.0000 - fp: 10876.0000 - tn: 445047.0000 - fn: 25502.0000 - accuracy: 0.9345 - precision: 0.8713 - recall: 0.7428 - auc: 0.9629 - val_loss: 0.2498 - val_tp: 18175.0000 - val_fp: 2014.0000 - val_tn: 111940.0000 - val_fn: 6642.0000 - val_accuracy: 0.9376 - val_precision: 0.9002 - val_recall: 0.7324 - val_auc: 0.9698\n",
            "Epoch 140/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2605 - tp: 73623.0000 - fp: 11007.0000 - tn: 444424.0000 - fn: 25442.0000 - accuracy: 0.9343 - precision: 0.8699 - recall: 0.7432 - auc: 0.9632\n",
            "Epoch 00140: val_auc did not improve from 0.97176\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2605 - tp: 73695.0000 - fp: 11017.0000 - tn: 444906.0000 - fn: 25466.0000 - accuracy: 0.9343 - precision: 0.8699 - recall: 0.7432 - auc: 0.9632 - val_loss: 0.2458 - val_tp: 19904.0000 - val_fp: 3225.0000 - val_tn: 110729.0000 - val_fn: 4913.0000 - val_accuracy: 0.9414 - val_precision: 0.8606 - val_recall: 0.8020 - val_auc: 0.9713\n",
            "Epoch 141/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2595 - tp: 73937.0000 - fp: 10919.0000 - tn: 444520.0000 - fn: 25120.0000 - accuracy: 0.9350 - precision: 0.8713 - recall: 0.7464 - auc: 0.9637\n",
            "Epoch 00141: val_auc did not improve from 0.97176\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2595 - tp: 74016.0000 - fp: 10938.0000 - tn: 444985.0000 - fn: 25145.0000 - accuracy: 0.9350 - precision: 0.8712 - recall: 0.7464 - auc: 0.9637 - val_loss: 0.2549 - val_tp: 20042.0000 - val_fp: 3743.0000 - val_tn: 110211.0000 - val_fn: 4775.0000 - val_accuracy: 0.9386 - val_precision: 0.8426 - val_recall: 0.8076 - val_auc: 0.9703\n",
            "Epoch 142/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2599 - tp: 73794.0000 - fp: 11075.0000 - tn: 443910.0000 - fn: 25205.0000 - accuracy: 0.9345 - precision: 0.8695 - recall: 0.7454 - auc: 0.9634\n",
            "Epoch 00142: val_auc improved from 0.97176 to 0.97185, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2599 - tp: 73921.0000 - fp: 11108.0000 - tn: 444815.0000 - fn: 25240.0000 - accuracy: 0.9345 - precision: 0.8694 - recall: 0.7455 - auc: 0.9634 - val_loss: 0.2453 - val_tp: 19664.0000 - val_fp: 2998.0000 - val_tn: 110956.0000 - val_fn: 5153.0000 - val_accuracy: 0.9413 - val_precision: 0.8677 - val_recall: 0.7924 - val_auc: 0.9718\n",
            "Epoch 143/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2599 - tp: 73969.0000 - fp: 11122.0000 - tn: 444742.0000 - fn: 25175.0000 - accuracy: 0.9346 - precision: 0.8693 - recall: 0.7461 - auc: 0.9632\n",
            "Epoch 00143: val_auc improved from 0.97185 to 0.97211, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2600 - tp: 73979.0000 - fp: 11124.0000 - tn: 444799.0000 - fn: 25182.0000 - accuracy: 0.9346 - precision: 0.8693 - recall: 0.7460 - auc: 0.9632 - val_loss: 0.2424 - val_tp: 18661.0000 - val_fp: 1930.0000 - val_tn: 112024.0000 - val_fn: 6156.0000 - val_accuracy: 0.9417 - val_precision: 0.9063 - val_recall: 0.7519 - val_auc: 0.9721\n",
            "Epoch 144/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2588 - tp: 73949.0000 - fp: 10914.0000 - tn: 445009.0000 - fn: 25212.0000 - accuracy: 0.9349 - precision: 0.8714 - recall: 0.7457 - auc: 0.9636\n",
            "Epoch 00144: val_auc did not improve from 0.97211\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2588 - tp: 73949.0000 - fp: 10914.0000 - tn: 445009.0000 - fn: 25212.0000 - accuracy: 0.9349 - precision: 0.8714 - recall: 0.7457 - auc: 0.9636 - val_loss: 0.2528 - val_tp: 17668.0000 - val_fp: 1600.0000 - val_tn: 112354.0000 - val_fn: 7149.0000 - val_accuracy: 0.9370 - val_precision: 0.9170 - val_recall: 0.7119 - val_auc: 0.9698\n",
            "Epoch 145/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2592 - tp: 73675.0000 - fp: 11037.0000 - tn: 444195.0000 - fn: 25333.0000 - accuracy: 0.9344 - precision: 0.8697 - recall: 0.7441 - auc: 0.9633\n",
            "Epoch 00145: val_auc improved from 0.97211 to 0.97241, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2592 - tp: 73788.0000 - fp: 11056.0000 - tn: 444867.0000 - fn: 25373.0000 - accuracy: 0.9344 - precision: 0.8697 - recall: 0.7441 - auc: 0.9633 - val_loss: 0.2423 - val_tp: 18463.0000 - val_fp: 1937.0000 - val_tn: 112017.0000 - val_fn: 6354.0000 - val_accuracy: 0.9403 - val_precision: 0.9050 - val_recall: 0.7440 - val_auc: 0.9724\n",
            "Epoch 146/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2581 - tp: 74096.0000 - fp: 11057.0000 - tn: 444578.0000 - fn: 25021.0000 - accuracy: 0.9350 - precision: 0.8702 - recall: 0.7476 - auc: 0.9639\n",
            "Epoch 00146: val_auc did not improve from 0.97241\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2581 - tp: 74130.0000 - fp: 11073.0000 - tn: 444850.0000 - fn: 25031.0000 - accuracy: 0.9350 - precision: 0.8700 - recall: 0.7476 - auc: 0.9639 - val_loss: 0.2575 - val_tp: 20112.0000 - val_fp: 4333.0000 - val_tn: 109621.0000 - val_fn: 4705.0000 - val_accuracy: 0.9349 - val_precision: 0.8227 - val_recall: 0.8104 - val_auc: 0.9671\n",
            "Epoch 147/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2584 - tp: 73858.0000 - fp: 11075.0000 - tn: 444143.0000 - fn: 25164.0000 - accuracy: 0.9346 - precision: 0.8696 - recall: 0.7459 - auc: 0.9634\n",
            "Epoch 00147: val_auc did not improve from 0.97241\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2584 - tp: 73962.0000 - fp: 11084.0000 - tn: 444839.0000 - fn: 25199.0000 - accuracy: 0.9346 - precision: 0.8697 - recall: 0.7459 - auc: 0.9634 - val_loss: 0.2456 - val_tp: 18625.0000 - val_fp: 2217.0000 - val_tn: 111737.0000 - val_fn: 6192.0000 - val_accuracy: 0.9394 - val_precision: 0.8936 - val_recall: 0.7505 - val_auc: 0.9701\n",
            "Epoch 148/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2578 - tp: 73917.0000 - fp: 10902.0000 - tn: 444548.0000 - fn: 25129.0000 - accuracy: 0.9350 - precision: 0.8715 - recall: 0.7463 - auc: 0.9636\n",
            "Epoch 00148: val_auc improved from 0.97241 to 0.97258, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2578 - tp: 73995.0000 - fp: 10911.0000 - tn: 445012.0000 - fn: 25166.0000 - accuracy: 0.9350 - precision: 0.8715 - recall: 0.7462 - auc: 0.9636 - val_loss: 0.2395 - val_tp: 19408.0000 - val_fp: 2585.0000 - val_tn: 111369.0000 - val_fn: 5409.0000 - val_accuracy: 0.9424 - val_precision: 0.8825 - val_recall: 0.7820 - val_auc: 0.9726\n",
            "Epoch 149/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2572 - tp: 74093.0000 - fp: 11011.0000 - tn: 444853.0000 - fn: 25051.0000 - accuracy: 0.9350 - precision: 0.8706 - recall: 0.7473 - auc: 0.9639\n",
            "Epoch 00149: val_auc did not improve from 0.97258\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2572 - tp: 74106.0000 - fp: 11013.0000 - tn: 444910.0000 - fn: 25055.0000 - accuracy: 0.9350 - precision: 0.8706 - recall: 0.7473 - auc: 0.9639 - val_loss: 0.2413 - val_tp: 18465.0000 - val_fp: 1838.0000 - val_tn: 112116.0000 - val_fn: 6352.0000 - val_accuracy: 0.9410 - val_precision: 0.9095 - val_recall: 0.7440 - val_auc: 0.9723\n",
            "Epoch 150/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2566 - tp: 74249.0000 - fp: 11008.0000 - tn: 444857.0000 - fn: 24894.0000 - accuracy: 0.9353 - precision: 0.8709 - recall: 0.7489 - auc: 0.9641\n",
            "Epoch 00150: val_auc did not improve from 0.97258\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2566 - tp: 74261.0000 - fp: 11008.0000 - tn: 444915.0000 - fn: 24900.0000 - accuracy: 0.9353 - precision: 0.8709 - recall: 0.7489 - auc: 0.9641 - val_loss: 0.2564 - val_tp: 16899.0000 - val_fp: 1270.0000 - val_tn: 112684.0000 - val_fn: 7918.0000 - val_accuracy: 0.9338 - val_precision: 0.9301 - val_recall: 0.6809 - val_auc: 0.9714\n",
            "Epoch 151/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2565 - tp: 74218.0000 - fp: 11092.0000 - tn: 444763.0000 - fn: 24935.0000 - accuracy: 0.9351 - precision: 0.8700 - recall: 0.7485 - auc: 0.9641\n",
            "Epoch 00151: val_auc did not improve from 0.97258\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2565 - tp: 74225.0000 - fp: 11098.0000 - tn: 444825.0000 - fn: 24936.0000 - accuracy: 0.9351 - precision: 0.8699 - recall: 0.7485 - auc: 0.9641 - val_loss: 0.2393 - val_tp: 19010.0000 - val_fp: 2180.0000 - val_tn: 111774.0000 - val_fn: 5807.0000 - val_accuracy: 0.9424 - val_precision: 0.8971 - val_recall: 0.7660 - val_auc: 0.9720\n",
            "Epoch 152/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2558 - tp: 74304.0000 - fp: 10821.0000 - tn: 445037.0000 - fn: 24846.0000 - accuracy: 0.9357 - precision: 0.8729 - recall: 0.7494 - auc: 0.9642\n",
            "Epoch 00152: val_auc improved from 0.97258 to 0.97352, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2558 - tp: 74314.0000 - fp: 10823.0000 - tn: 445100.0000 - fn: 24847.0000 - accuracy: 0.9357 - precision: 0.8729 - recall: 0.7494 - auc: 0.9642 - val_loss: 0.2374 - val_tp: 18592.0000 - val_fp: 1752.0000 - val_tn: 112202.0000 - val_fn: 6225.0000 - val_accuracy: 0.9425 - val_precision: 0.9139 - val_recall: 0.7492 - val_auc: 0.9735\n",
            "Epoch 153/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2561 - tp: 74115.0000 - fp: 10944.0000 - tn: 444918.0000 - fn: 25031.0000 - accuracy: 0.9352 - precision: 0.8713 - recall: 0.7475 - auc: 0.9641\n",
            "Epoch 00153: val_auc did not improve from 0.97352\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2561 - tp: 74125.0000 - fp: 10945.0000 - tn: 444978.0000 - fn: 25036.0000 - accuracy: 0.9352 - precision: 0.8713 - recall: 0.7475 - auc: 0.9641 - val_loss: 0.2395 - val_tp: 18395.0000 - val_fp: 1697.0000 - val_tn: 112257.0000 - val_fn: 6422.0000 - val_accuracy: 0.9415 - val_precision: 0.9155 - val_recall: 0.7412 - val_auc: 0.9726\n",
            "Epoch 154/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2558 - tp: 74162.0000 - fp: 10950.0000 - tn: 444488.0000 - fn: 24896.0000 - accuracy: 0.9354 - precision: 0.8713 - recall: 0.7487 - auc: 0.9642\n",
            "Epoch 00154: val_auc did not improve from 0.97352\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2558 - tp: 74238.0000 - fp: 10964.0000 - tn: 444959.0000 - fn: 24923.0000 - accuracy: 0.9353 - precision: 0.8713 - recall: 0.7487 - auc: 0.9642 - val_loss: 0.2445 - val_tp: 18057.0000 - val_fp: 1825.0000 - val_tn: 112129.0000 - val_fn: 6760.0000 - val_accuracy: 0.9381 - val_precision: 0.9082 - val_recall: 0.7276 - val_auc: 0.9719\n",
            "Epoch 155/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2557 - tp: 74248.0000 - fp: 10903.0000 - tn: 445020.0000 - fn: 24913.0000 - accuracy: 0.9355 - precision: 0.8720 - recall: 0.7488 - auc: 0.9641\n",
            "Epoch 00155: val_auc did not improve from 0.97352\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2557 - tp: 74248.0000 - fp: 10903.0000 - tn: 445020.0000 - fn: 24913.0000 - accuracy: 0.9355 - precision: 0.8720 - recall: 0.7488 - auc: 0.9641 - val_loss: 0.2412 - val_tp: 18907.0000 - val_fp: 2237.0000 - val_tn: 111717.0000 - val_fn: 5910.0000 - val_accuracy: 0.9413 - val_precision: 0.8942 - val_recall: 0.7619 - val_auc: 0.9715\n",
            "Epoch 156/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2544 - tp: 74434.0000 - fp: 10937.0000 - tn: 444694.0000 - fn: 24687.0000 - accuracy: 0.9358 - precision: 0.8719 - recall: 0.7509 - auc: 0.9647\n",
            "Epoch 00156: val_auc did not improve from 0.97352\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2544 - tp: 74461.0000 - fp: 10943.0000 - tn: 444980.0000 - fn: 24700.0000 - accuracy: 0.9358 - precision: 0.8719 - recall: 0.7509 - auc: 0.9647 - val_loss: 0.2431 - val_tp: 18081.0000 - val_fp: 1642.0000 - val_tn: 112312.0000 - val_fn: 6736.0000 - val_accuracy: 0.9396 - val_precision: 0.9167 - val_recall: 0.7286 - val_auc: 0.9721\n",
            "Epoch 157/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2545 - tp: 74239.0000 - fp: 11023.0000 - tn: 443977.0000 - fn: 24745.0000 - accuracy: 0.9354 - precision: 0.8707 - recall: 0.7500 - auc: 0.9647\n",
            "Epoch 00157: val_auc did not improve from 0.97352\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2544 - tp: 74378.0000 - fp: 11042.0000 - tn: 444881.0000 - fn: 24783.0000 - accuracy: 0.9355 - precision: 0.8707 - recall: 0.7501 - auc: 0.9647 - val_loss: 0.2374 - val_tp: 18846.0000 - val_fp: 2108.0000 - val_tn: 111846.0000 - val_fn: 5971.0000 - val_accuracy: 0.9418 - val_precision: 0.8994 - val_recall: 0.7594 - val_auc: 0.9729\n",
            "Epoch 158/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2540 - tp: 74394.0000 - fp: 10948.0000 - tn: 444271.0000 - fn: 24627.0000 - accuracy: 0.9358 - precision: 0.8717 - recall: 0.7513 - auc: 0.9647\n",
            "Epoch 00158: val_auc improved from 0.97352 to 0.97410, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2540 - tp: 74510.0000 - fp: 10969.0000 - tn: 444954.0000 - fn: 24651.0000 - accuracy: 0.9358 - precision: 0.8717 - recall: 0.7514 - auc: 0.9647 - val_loss: 0.2343 - val_tp: 19148.0000 - val_fp: 2197.0000 - val_tn: 111757.0000 - val_fn: 5669.0000 - val_accuracy: 0.9433 - val_precision: 0.8971 - val_recall: 0.7716 - val_auc: 0.9741\n",
            "Epoch 159/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2534 - tp: 74448.0000 - fp: 10937.0000 - tn: 444925.0000 - fn: 24698.0000 - accuracy: 0.9358 - precision: 0.8719 - recall: 0.7509 - auc: 0.9649\n",
            "Epoch 00159: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2534 - tp: 74457.0000 - fp: 10938.0000 - tn: 444985.0000 - fn: 24704.0000 - accuracy: 0.9358 - precision: 0.8719 - recall: 0.7509 - auc: 0.9649 - val_loss: 0.2401 - val_tp: 18272.0000 - val_fp: 1796.0000 - val_tn: 112158.0000 - val_fn: 6545.0000 - val_accuracy: 0.9399 - val_precision: 0.9105 - val_recall: 0.7363 - val_auc: 0.9729\n",
            "Epoch 160/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2537 - tp: 74139.0000 - fp: 10891.0000 - tn: 444759.0000 - fn: 24963.0000 - accuracy: 0.9354 - precision: 0.8719 - recall: 0.7481 - auc: 0.9646\n",
            "Epoch 00160: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2537 - tp: 74187.0000 - fp: 10897.0000 - tn: 445026.0000 - fn: 24974.0000 - accuracy: 0.9354 - precision: 0.8719 - recall: 0.7481 - auc: 0.9646 - val_loss: 0.2361 - val_tp: 19008.0000 - val_fp: 2137.0000 - val_tn: 111817.0000 - val_fn: 5809.0000 - val_accuracy: 0.9427 - val_precision: 0.8989 - val_recall: 0.7659 - val_auc: 0.9731\n",
            "Epoch 161/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2539 - tp: 74086.0000 - fp: 10962.0000 - tn: 444272.0000 - fn: 24920.0000 - accuracy: 0.9353 - precision: 0.8711 - recall: 0.7483 - auc: 0.9646\n",
            "Epoch 00161: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2539 - tp: 74207.0000 - fp: 10969.0000 - tn: 444954.0000 - fn: 24954.0000 - accuracy: 0.9353 - precision: 0.8712 - recall: 0.7483 - auc: 0.9646 - val_loss: 0.2360 - val_tp: 19444.0000 - val_fp: 2609.0000 - val_tn: 111345.0000 - val_fn: 5373.0000 - val_accuracy: 0.9425 - val_precision: 0.8817 - val_recall: 0.7835 - val_auc: 0.9728\n",
            "Epoch 162/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2529 - tp: 74346.0000 - fp: 11011.0000 - tn: 444215.0000 - fn: 24668.0000 - accuracy: 0.9356 - precision: 0.8710 - recall: 0.7509 - auc: 0.9650\n",
            "Epoch 00162: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2528 - tp: 74460.0000 - fp: 11029.0000 - tn: 444894.0000 - fn: 24701.0000 - accuracy: 0.9356 - precision: 0.8710 - recall: 0.7509 - auc: 0.9650 - val_loss: 0.2333 - val_tp: 19597.0000 - val_fp: 2562.0000 - val_tn: 111392.0000 - val_fn: 5220.0000 - val_accuracy: 0.9439 - val_precision: 0.8844 - val_recall: 0.7897 - val_auc: 0.9735\n",
            "Epoch 163/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 74335.0000 - fp: 10870.0000 - tn: 444375.0000 - fn: 24660.0000 - accuracy: 0.9359 - precision: 0.8724 - recall: 0.7509 - auc: 0.9650\n",
            "Epoch 00163: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2526 - tp: 74465.0000 - fp: 10879.0000 - tn: 445044.0000 - fn: 24696.0000 - accuracy: 0.9359 - precision: 0.8725 - recall: 0.7510 - auc: 0.9650 - val_loss: 0.2416 - val_tp: 17617.0000 - val_fp: 1287.0000 - val_tn: 112667.0000 - val_fn: 7200.0000 - val_accuracy: 0.9388 - val_precision: 0.9319 - val_recall: 0.7099 - val_auc: 0.9735\n",
            "Epoch 164/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2524 - tp: 74452.0000 - fp: 10820.0000 - tn: 444835.0000 - fn: 24645.0000 - accuracy: 0.9361 - precision: 0.8731 - recall: 0.7513 - auc: 0.9648\n",
            "Epoch 00164: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2524 - tp: 74506.0000 - fp: 10827.0000 - tn: 445096.0000 - fn: 24655.0000 - accuracy: 0.9361 - precision: 0.8731 - recall: 0.7514 - auc: 0.9648 - val_loss: 0.2338 - val_tp: 18871.0000 - val_fp: 2043.0000 - val_tn: 111911.0000 - val_fn: 5946.0000 - val_accuracy: 0.9424 - val_precision: 0.9023 - val_recall: 0.7604 - val_auc: 0.9739\n",
            "Epoch 165/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2521 - tp: 74481.0000 - fp: 10873.0000 - tn: 445050.0000 - fn: 24680.0000 - accuracy: 0.9360 - precision: 0.8726 - recall: 0.7511 - auc: 0.9650\n",
            "Epoch 00165: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2521 - tp: 74481.0000 - fp: 10873.0000 - tn: 445050.0000 - fn: 24680.0000 - accuracy: 0.9360 - precision: 0.8726 - recall: 0.7511 - auc: 0.9650 - val_loss: 0.2434 - val_tp: 20670.0000 - val_fp: 4376.0000 - val_tn: 109578.0000 - val_fn: 4147.0000 - val_accuracy: 0.9386 - val_precision: 0.8253 - val_recall: 0.8329 - val_auc: 0.9725\n",
            "Epoch 166/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2518 - tp: 74469.0000 - fp: 10868.0000 - tn: 444579.0000 - fn: 24580.0000 - accuracy: 0.9361 - precision: 0.8726 - recall: 0.7518 - auc: 0.9651\n",
            "Epoch 00166: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2519 - tp: 74549.0000 - fp: 10883.0000 - tn: 445040.0000 - fn: 24612.0000 - accuracy: 0.9361 - precision: 0.8726 - recall: 0.7518 - auc: 0.9651 - val_loss: 0.2350 - val_tp: 19156.0000 - val_fp: 2252.0000 - val_tn: 111702.0000 - val_fn: 5661.0000 - val_accuracy: 0.9430 - val_precision: 0.8948 - val_recall: 0.7719 - val_auc: 0.9728\n",
            "Epoch 167/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2515 - tp: 74691.0000 - fp: 10853.0000 - tn: 444352.0000 - fn: 24344.0000 - accuracy: 0.9365 - precision: 0.8731 - recall: 0.7542 - auc: 0.9651\n",
            "Epoch 00167: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2515 - tp: 74792.0000 - fp: 10881.0000 - tn: 445042.0000 - fn: 24369.0000 - accuracy: 0.9365 - precision: 0.8730 - recall: 0.7542 - auc: 0.9651 - val_loss: 0.2417 - val_tp: 18396.0000 - val_fp: 1882.0000 - val_tn: 112072.0000 - val_fn: 6421.0000 - val_accuracy: 0.9402 - val_precision: 0.9072 - val_recall: 0.7413 - val_auc: 0.9709\n",
            "Epoch 168/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2506 - tp: 74690.0000 - fp: 10799.0000 - tn: 445124.0000 - fn: 24471.0000 - accuracy: 0.9365 - precision: 0.8737 - recall: 0.7532 - auc: 0.9655\n",
            "Epoch 00168: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2506 - tp: 74690.0000 - fp: 10799.0000 - tn: 445124.0000 - fn: 24471.0000 - accuracy: 0.9365 - precision: 0.8737 - recall: 0.7532 - auc: 0.9655 - val_loss: 0.2348 - val_tp: 18619.0000 - val_fp: 1756.0000 - val_tn: 112198.0000 - val_fn: 6198.0000 - val_accuracy: 0.9427 - val_precision: 0.9138 - val_recall: 0.7503 - val_auc: 0.9734\n",
            "Epoch 169/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2507 - tp: 74538.0000 - fp: 10842.0000 - tn: 445017.0000 - fn: 24611.0000 - accuracy: 0.9361 - precision: 0.8730 - recall: 0.7518 - auc: 0.9653\n",
            "Epoch 00169: val_auc did not improve from 0.97410\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2507 - tp: 74544.0000 - fp: 10844.0000 - tn: 445079.0000 - fn: 24617.0000 - accuracy: 0.9361 - precision: 0.8730 - recall: 0.7517 - auc: 0.9653 - val_loss: 0.2394 - val_tp: 17987.0000 - val_fp: 1536.0000 - val_tn: 112418.0000 - val_fn: 6830.0000 - val_accuracy: 0.9397 - val_precision: 0.9213 - val_recall: 0.7248 - val_auc: 0.9727\n",
            "Epoch 170/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2503 - tp: 74694.0000 - fp: 10855.0000 - tn: 445068.0000 - fn: 24467.0000 - accuracy: 0.9364 - precision: 0.8731 - recall: 0.7533 - auc: 0.9656\n",
            "Epoch 00170: val_auc improved from 0.97410 to 0.97483, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2503 - tp: 74694.0000 - fp: 10855.0000 - tn: 445068.0000 - fn: 24467.0000 - accuracy: 0.9364 - precision: 0.8731 - recall: 0.7533 - auc: 0.9656 - val_loss: 0.2306 - val_tp: 19055.0000 - val_fp: 2020.0000 - val_tn: 111934.0000 - val_fn: 5762.0000 - val_accuracy: 0.9439 - val_precision: 0.9042 - val_recall: 0.7678 - val_auc: 0.9748\n",
            "Epoch 171/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2496 - tp: 74739.0000 - fp: 10745.0000 - tn: 444487.0000 - fn: 24269.0000 - accuracy: 0.9368 - precision: 0.8743 - recall: 0.7549 - auc: 0.9657\n",
            "Epoch 00171: val_auc did not improve from 0.97483\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2496 - tp: 74852.0000 - fp: 10756.0000 - tn: 445167.0000 - fn: 24309.0000 - accuracy: 0.9368 - precision: 0.8744 - recall: 0.7549 - auc: 0.9657 - val_loss: 0.2325 - val_tp: 19463.0000 - val_fp: 2516.0000 - val_tn: 111438.0000 - val_fn: 5354.0000 - val_accuracy: 0.9433 - val_precision: 0.8855 - val_recall: 0.7843 - val_auc: 0.9742\n",
            "Epoch 172/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2499 - tp: 74776.0000 - fp: 11040.0000 - tn: 444413.0000 - fn: 24267.0000 - accuracy: 0.9363 - precision: 0.8714 - recall: 0.7550 - auc: 0.9655\n",
            "Epoch 00172: val_auc did not improve from 0.97483\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2499 - tp: 74862.0000 - fp: 11051.0000 - tn: 444872.0000 - fn: 24299.0000 - accuracy: 0.9363 - precision: 0.8714 - recall: 0.7550 - auc: 0.9655 - val_loss: 0.2473 - val_tp: 17194.0000 - val_fp: 1165.0000 - val_tn: 112789.0000 - val_fn: 7623.0000 - val_accuracy: 0.9367 - val_precision: 0.9365 - val_recall: 0.6928 - val_auc: 0.9727\n",
            "Epoch 173/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2494 - tp: 74871.0000 - fp: 10977.0000 - tn: 444885.0000 - fn: 24275.0000 - accuracy: 0.9365 - precision: 0.8721 - recall: 0.7552 - auc: 0.9658\n",
            "Epoch 00173: val_auc did not improve from 0.97483\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2494 - tp: 74881.0000 - fp: 10977.0000 - tn: 444946.0000 - fn: 24280.0000 - accuracy: 0.9365 - precision: 0.8721 - recall: 0.7551 - auc: 0.9658 - val_loss: 0.2326 - val_tp: 20026.0000 - val_fp: 3125.0000 - val_tn: 110829.0000 - val_fn: 4791.0000 - val_accuracy: 0.9430 - val_precision: 0.8650 - val_recall: 0.8069 - val_auc: 0.9737\n",
            "Epoch 174/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2490 - tp: 74859.0000 - fp: 10864.0000 - tn: 444581.0000 - fn: 24192.0000 - accuracy: 0.9368 - precision: 0.8733 - recall: 0.7558 - auc: 0.9657\n",
            "Epoch 00174: val_auc did not improve from 0.97483\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2491 - tp: 74940.0000 - fp: 10874.0000 - tn: 445049.0000 - fn: 24221.0000 - accuracy: 0.9368 - precision: 0.8733 - recall: 0.7557 - auc: 0.9657 - val_loss: 0.2380 - val_tp: 18059.0000 - val_fp: 1558.0000 - val_tn: 112396.0000 - val_fn: 6758.0000 - val_accuracy: 0.9401 - val_precision: 0.9206 - val_recall: 0.7277 - val_auc: 0.9722\n",
            "Epoch 175/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2493 - tp: 74664.0000 - fp: 10895.0000 - tn: 444329.0000 - fn: 24352.0000 - accuracy: 0.9364 - precision: 0.8727 - recall: 0.7541 - auc: 0.9658\n",
            "Epoch 00175: val_auc did not improve from 0.97483\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2493 - tp: 74763.0000 - fp: 10901.0000 - tn: 445022.0000 - fn: 24398.0000 - accuracy: 0.9364 - precision: 0.8727 - recall: 0.7540 - auc: 0.9658 - val_loss: 0.2331 - val_tp: 18928.0000 - val_fp: 2002.0000 - val_tn: 111952.0000 - val_fn: 5889.0000 - val_accuracy: 0.9431 - val_precision: 0.9043 - val_recall: 0.7627 - val_auc: 0.9738\n",
            "Epoch 176/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2484 - tp: 74844.0000 - fp: 10922.0000 - tn: 444735.0000 - fn: 24251.0000 - accuracy: 0.9366 - precision: 0.8727 - recall: 0.7553 - auc: 0.9661\n",
            "Epoch 00176: val_auc improved from 0.97483 to 0.97525, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2484 - tp: 74892.0000 - fp: 10925.0000 - tn: 444998.0000 - fn: 24269.0000 - accuracy: 0.9366 - precision: 0.8727 - recall: 0.7553 - auc: 0.9661 - val_loss: 0.2289 - val_tp: 19031.0000 - val_fp: 1942.0000 - val_tn: 112012.0000 - val_fn: 5786.0000 - val_accuracy: 0.9443 - val_precision: 0.9074 - val_recall: 0.7669 - val_auc: 0.9752\n",
            "Epoch 177/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2481 - tp: 74804.0000 - fp: 10632.0000 - tn: 444375.0000 - fn: 24173.0000 - accuracy: 0.9372 - precision: 0.8756 - recall: 0.7558 - auc: 0.9660\n",
            "Epoch 00177: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2481 - tp: 74940.0000 - fp: 10658.0000 - tn: 445265.0000 - fn: 24221.0000 - accuracy: 0.9372 - precision: 0.8755 - recall: 0.7557 - auc: 0.9661 - val_loss: 0.2337 - val_tp: 18300.0000 - val_fp: 1556.0000 - val_tn: 112398.0000 - val_fn: 6517.0000 - val_accuracy: 0.9418 - val_precision: 0.9216 - val_recall: 0.7374 - val_auc: 0.9743\n",
            "Epoch 178/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2482 - tp: 74860.0000 - fp: 10772.0000 - tn: 444865.0000 - fn: 24255.0000 - accuracy: 0.9369 - precision: 0.8742 - recall: 0.7553 - auc: 0.9658\n",
            "Epoch 00178: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2482 - tp: 74894.0000 - fp: 10781.0000 - tn: 445142.0000 - fn: 24267.0000 - accuracy: 0.9369 - precision: 0.8742 - recall: 0.7553 - auc: 0.9658 - val_loss: 0.2343 - val_tp: 19369.0000 - val_fp: 2555.0000 - val_tn: 111399.0000 - val_fn: 5448.0000 - val_accuracy: 0.9423 - val_precision: 0.8835 - val_recall: 0.7805 - val_auc: 0.9733\n",
            "Epoch 179/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2485 - tp: 74785.0000 - fp: 10762.0000 - tn: 445096.0000 - fn: 24365.0000 - accuracy: 0.9367 - precision: 0.8742 - recall: 0.7543 - auc: 0.9658\n",
            "Epoch 00179: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2485 - tp: 74792.0000 - fp: 10763.0000 - tn: 445160.0000 - fn: 24369.0000 - accuracy: 0.9367 - precision: 0.8742 - recall: 0.7542 - auc: 0.9658 - val_loss: 0.2333 - val_tp: 18875.0000 - val_fp: 1992.0000 - val_tn: 111962.0000 - val_fn: 5942.0000 - val_accuracy: 0.9428 - val_precision: 0.9045 - val_recall: 0.7606 - val_auc: 0.9735\n",
            "Epoch 180/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2471 - tp: 75112.0000 - fp: 10683.0000 - tn: 445182.0000 - fn: 24031.0000 - accuracy: 0.9375 - precision: 0.8755 - recall: 0.7576 - auc: 0.9661\n",
            "Epoch 00180: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2470 - tp: 75126.0000 - fp: 10683.0000 - tn: 445240.0000 - fn: 24035.0000 - accuracy: 0.9375 - precision: 0.8755 - recall: 0.7576 - auc: 0.9661 - val_loss: 0.2308 - val_tp: 19066.0000 - val_fp: 1978.0000 - val_tn: 111976.0000 - val_fn: 5751.0000 - val_accuracy: 0.9443 - val_precision: 0.9060 - val_recall: 0.7683 - val_auc: 0.9735\n",
            "Epoch 181/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2474 - tp: 74953.0000 - fp: 10763.0000 - tn: 445100.0000 - fn: 24192.0000 - accuracy: 0.9370 - precision: 0.8744 - recall: 0.7560 - auc: 0.9661\n",
            "Epoch 00181: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2474 - tp: 74966.0000 - fp: 10764.0000 - tn: 445159.0000 - fn: 24195.0000 - accuracy: 0.9370 - precision: 0.8744 - recall: 0.7560 - auc: 0.9661 - val_loss: 0.2315 - val_tp: 20373.0000 - val_fp: 3299.0000 - val_tn: 110655.0000 - val_fn: 4444.0000 - val_accuracy: 0.9442 - val_precision: 0.8606 - val_recall: 0.8209 - val_auc: 0.9749\n",
            "Epoch 182/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2463 - tp: 74909.0000 - fp: 10782.0000 - tn: 444450.0000 - fn: 24099.0000 - accuracy: 0.9371 - precision: 0.8742 - recall: 0.7566 - auc: 0.9667\n",
            "Epoch 00182: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2463 - tp: 75024.0000 - fp: 10797.0000 - tn: 445126.0000 - fn: 24137.0000 - accuracy: 0.9371 - precision: 0.8742 - recall: 0.7566 - auc: 0.9667 - val_loss: 0.2323 - val_tp: 19254.0000 - val_fp: 2351.0000 - val_tn: 111603.0000 - val_fn: 5563.0000 - val_accuracy: 0.9430 - val_precision: 0.8912 - val_recall: 0.7758 - val_auc: 0.9740\n",
            "Epoch 183/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2463 - tp: 74967.0000 - fp: 10851.0000 - tn: 444393.0000 - fn: 24029.0000 - accuracy: 0.9371 - precision: 0.8736 - recall: 0.7573 - auc: 0.9666\n",
            "Epoch 00183: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2463 - tp: 75079.0000 - fp: 10861.0000 - tn: 445062.0000 - fn: 24082.0000 - accuracy: 0.9370 - precision: 0.8736 - recall: 0.7571 - auc: 0.9666 - val_loss: 0.2329 - val_tp: 18532.0000 - val_fp: 1787.0000 - val_tn: 112167.0000 - val_fn: 6285.0000 - val_accuracy: 0.9418 - val_precision: 0.9121 - val_recall: 0.7467 - val_auc: 0.9741\n",
            "Epoch 184/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2461 - tp: 74818.0000 - fp: 10819.0000 - tn: 443982.0000 - fn: 24109.0000 - accuracy: 0.9369 - precision: 0.8737 - recall: 0.7563 - auc: 0.9666\n",
            "Epoch 00184: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2461 - tp: 75001.0000 - fp: 10849.0000 - tn: 445074.0000 - fn: 24160.0000 - accuracy: 0.9369 - precision: 0.8736 - recall: 0.7564 - auc: 0.9666 - val_loss: 0.2304 - val_tp: 19227.0000 - val_fp: 2241.0000 - val_tn: 111713.0000 - val_fn: 5590.0000 - val_accuracy: 0.9436 - val_precision: 0.8956 - val_recall: 0.7748 - val_auc: 0.9737\n",
            "Epoch 185/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2458 - tp: 75179.0000 - fp: 10641.0000 - tn: 445220.0000 - fn: 23968.0000 - accuracy: 0.9376 - precision: 0.8760 - recall: 0.7583 - auc: 0.9665\n",
            "Epoch 00185: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2458 - tp: 75189.0000 - fp: 10645.0000 - tn: 445278.0000 - fn: 23972.0000 - accuracy: 0.9376 - precision: 0.8760 - recall: 0.7583 - auc: 0.9665 - val_loss: 0.2410 - val_tp: 17649.0000 - val_fp: 1243.0000 - val_tn: 112711.0000 - val_fn: 7168.0000 - val_accuracy: 0.9394 - val_precision: 0.9342 - val_recall: 0.7112 - val_auc: 0.9735\n",
            "Epoch 186/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2458 - tp: 75000.0000 - fp: 10887.0000 - tn: 444767.0000 - fn: 24098.0000 - accuracy: 0.9369 - precision: 0.8732 - recall: 0.7568 - auc: 0.9666\n",
            "Epoch 00186: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2458 - tp: 75049.0000 - fp: 10890.0000 - tn: 445033.0000 - fn: 24112.0000 - accuracy: 0.9369 - precision: 0.8733 - recall: 0.7568 - auc: 0.9666 - val_loss: 0.2338 - val_tp: 18308.0000 - val_fp: 1520.0000 - val_tn: 112434.0000 - val_fn: 6509.0000 - val_accuracy: 0.9421 - val_precision: 0.9233 - val_recall: 0.7377 - val_auc: 0.9743\n",
            "Epoch 187/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2455 - tp: 75008.0000 - fp: 10758.0000 - tn: 444483.0000 - fn: 23991.0000 - accuracy: 0.9373 - precision: 0.8746 - recall: 0.7577 - auc: 0.9667\n",
            "Epoch 00187: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2455 - tp: 75140.0000 - fp: 10789.0000 - tn: 445134.0000 - fn: 24021.0000 - accuracy: 0.9373 - precision: 0.8744 - recall: 0.7578 - auc: 0.9667 - val_loss: 0.2277 - val_tp: 20155.0000 - val_fp: 2939.0000 - val_tn: 111015.0000 - val_fn: 4662.0000 - val_accuracy: 0.9452 - val_precision: 0.8727 - val_recall: 0.8121 - val_auc: 0.9747\n",
            "Epoch 188/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2457 - tp: 74836.0000 - fp: 10904.0000 - tn: 443921.0000 - fn: 24067.0000 - accuracy: 0.9368 - precision: 0.8728 - recall: 0.7567 - auc: 0.9666\n",
            "Epoch 00188: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2457 - tp: 75027.0000 - fp: 10924.0000 - tn: 444999.0000 - fn: 24134.0000 - accuracy: 0.9368 - precision: 0.8729 - recall: 0.7566 - auc: 0.9666 - val_loss: 0.2314 - val_tp: 19970.0000 - val_fp: 2991.0000 - val_tn: 110963.0000 - val_fn: 4847.0000 - val_accuracy: 0.9435 - val_precision: 0.8697 - val_recall: 0.8047 - val_auc: 0.9734\n",
            "Epoch 189/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2447 - tp: 75110.0000 - fp: 10795.0000 - tn: 444635.0000 - fn: 23956.0000 - accuracy: 0.9373 - precision: 0.8743 - recall: 0.7582 - auc: 0.9671\n",
            "Epoch 00189: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2447 - tp: 75184.0000 - fp: 10807.0000 - tn: 445116.0000 - fn: 23977.0000 - accuracy: 0.9373 - precision: 0.8743 - recall: 0.7582 - auc: 0.9671 - val_loss: 0.2285 - val_tp: 19530.0000 - val_fp: 2488.0000 - val_tn: 111466.0000 - val_fn: 5287.0000 - val_accuracy: 0.9440 - val_precision: 0.8870 - val_recall: 0.7870 - val_auc: 0.9739\n",
            "Epoch 190/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2440 - tp: 75299.0000 - fp: 10698.0000 - tn: 445163.0000 - fn: 23848.0000 - accuracy: 0.9378 - precision: 0.8756 - recall: 0.7595 - auc: 0.9672\n",
            "Epoch 00190: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2440 - tp: 75312.0000 - fp: 10699.0000 - tn: 445224.0000 - fn: 23849.0000 - accuracy: 0.9378 - precision: 0.8756 - recall: 0.7595 - auc: 0.9672 - val_loss: 0.2284 - val_tp: 19402.0000 - val_fp: 2474.0000 - val_tn: 111480.0000 - val_fn: 5415.0000 - val_accuracy: 0.9432 - val_precision: 0.8869 - val_recall: 0.7818 - val_auc: 0.9743\n",
            "Epoch 191/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2439 - tp: 75201.0000 - fp: 10848.0000 - tn: 443957.0000 - fn: 23722.0000 - accuracy: 0.9376 - precision: 0.8739 - recall: 0.7602 - auc: 0.9673\n",
            "Epoch 00191: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2438 - tp: 75396.0000 - fp: 10878.0000 - tn: 445045.0000 - fn: 23765.0000 - accuracy: 0.9376 - precision: 0.8739 - recall: 0.7603 - auc: 0.9673 - val_loss: 0.2289 - val_tp: 20132.0000 - val_fp: 3084.0000 - val_tn: 110870.0000 - val_fn: 4685.0000 - val_accuracy: 0.9440 - val_precision: 0.8672 - val_recall: 0.8112 - val_auc: 0.9748\n",
            "Epoch 192/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2443 - tp: 75415.0000 - fp: 10832.0000 - tn: 444818.0000 - fn: 23687.0000 - accuracy: 0.9378 - precision: 0.8744 - recall: 0.7610 - auc: 0.9668\n",
            "Epoch 00192: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2443 - tp: 75463.0000 - fp: 10835.0000 - tn: 445088.0000 - fn: 23698.0000 - accuracy: 0.9378 - precision: 0.8744 - recall: 0.7610 - auc: 0.9668 - val_loss: 0.2338 - val_tp: 19018.0000 - val_fp: 2380.0000 - val_tn: 111574.0000 - val_fn: 5799.0000 - val_accuracy: 0.9411 - val_precision: 0.8888 - val_recall: 0.7663 - val_auc: 0.9726\n",
            "Epoch 193/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2437 - tp: 75511.0000 - fp: 10951.0000 - tn: 444908.0000 - fn: 23638.0000 - accuracy: 0.9377 - precision: 0.8733 - recall: 0.7616 - auc: 0.9672\n",
            "Epoch 00193: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2437 - tp: 75521.0000 - fp: 10954.0000 - tn: 444969.0000 - fn: 23640.0000 - accuracy: 0.9377 - precision: 0.8733 - recall: 0.7616 - auc: 0.9672 - val_loss: 0.2292 - val_tp: 19725.0000 - val_fp: 2533.0000 - val_tn: 111421.0000 - val_fn: 5092.0000 - val_accuracy: 0.9451 - val_precision: 0.8862 - val_recall: 0.7948 - val_auc: 0.9733\n",
            "Epoch 194/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2435 - tp: 75512.0000 - fp: 10763.0000 - tn: 445160.0000 - fn: 23649.0000 - accuracy: 0.9380 - precision: 0.8752 - recall: 0.7615 - auc: 0.9671\n",
            "Epoch 00194: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2435 - tp: 75512.0000 - fp: 10763.0000 - tn: 445160.0000 - fn: 23649.0000 - accuracy: 0.9380 - precision: 0.8752 - recall: 0.7615 - auc: 0.9671 - val_loss: 0.2256 - val_tp: 19642.0000 - val_fp: 2535.0000 - val_tn: 111419.0000 - val_fn: 5175.0000 - val_accuracy: 0.9444 - val_precision: 0.8857 - val_recall: 0.7915 - val_auc: 0.9750\n",
            "Epoch 195/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2438 - tp: 75242.0000 - fp: 10647.0000 - tn: 444397.0000 - fn: 23698.0000 - accuracy: 0.9380 - precision: 0.8760 - recall: 0.7605 - auc: 0.9669\n",
            "Epoch 00195: val_auc did not improve from 0.97525\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2438 - tp: 75408.0000 - fp: 10665.0000 - tn: 445258.0000 - fn: 23753.0000 - accuracy: 0.9380 - precision: 0.8761 - recall: 0.7605 - auc: 0.9669 - val_loss: 0.2283 - val_tp: 19272.0000 - val_fp: 2155.0000 - val_tn: 111799.0000 - val_fn: 5545.0000 - val_accuracy: 0.9445 - val_precision: 0.8994 - val_recall: 0.7766 - val_auc: 0.9738\n",
            "Epoch 196/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2430 - tp: 75284.0000 - fp: 10591.0000 - tn: 445061.0000 - fn: 23816.0000 - accuracy: 0.9380 - precision: 0.8767 - recall: 0.7597 - auc: 0.9673\n",
            "Epoch 00196: val_auc improved from 0.97525 to 0.97570, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2430 - tp: 75326.0000 - fp: 10597.0000 - tn: 445326.0000 - fn: 23835.0000 - accuracy: 0.9380 - precision: 0.8767 - recall: 0.7596 - auc: 0.9673 - val_loss: 0.2240 - val_tp: 19895.0000 - val_fp: 2570.0000 - val_tn: 111384.0000 - val_fn: 4922.0000 - val_accuracy: 0.9460 - val_precision: 0.8856 - val_recall: 0.8017 - val_auc: 0.9757\n",
            "Epoch 197/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2433 - tp: 75329.0000 - fp: 10624.0000 - tn: 444836.0000 - fn: 23707.0000 - accuracy: 0.9381 - precision: 0.8764 - recall: 0.7606 - auc: 0.9669\n",
            "Epoch 00197: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2433 - tp: 75423.0000 - fp: 10637.0000 - tn: 445286.0000 - fn: 23738.0000 - accuracy: 0.9381 - precision: 0.8764 - recall: 0.7606 - auc: 0.9669 - val_loss: 0.2309 - val_tp: 20207.0000 - val_fp: 3335.0000 - val_tn: 110619.0000 - val_fn: 4610.0000 - val_accuracy: 0.9427 - val_precision: 0.8583 - val_recall: 0.8142 - val_auc: 0.9746\n",
            "Epoch 198/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2423 - tp: 75557.0000 - fp: 10751.0000 - tn: 444888.0000 - fn: 23556.0000 - accuracy: 0.9382 - precision: 0.8754 - recall: 0.7623 - auc: 0.9675\n",
            "Epoch 00198: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2423 - tp: 75598.0000 - fp: 10759.0000 - tn: 445164.0000 - fn: 23563.0000 - accuracy: 0.9382 - precision: 0.8754 - recall: 0.7624 - auc: 0.9675 - val_loss: 0.2264 - val_tp: 18979.0000 - val_fp: 1876.0000 - val_tn: 112078.0000 - val_fn: 5838.0000 - val_accuracy: 0.9444 - val_precision: 0.9100 - val_recall: 0.7648 - val_auc: 0.9751\n",
            "Epoch 199/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2421 - tp: 75499.0000 - fp: 10749.0000 - tn: 445114.0000 - fn: 23646.0000 - accuracy: 0.9380 - precision: 0.8754 - recall: 0.7615 - auc: 0.9675\n",
            "Epoch 00199: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2421 - tp: 75508.0000 - fp: 10750.0000 - tn: 445173.0000 - fn: 23653.0000 - accuracy: 0.9380 - precision: 0.8754 - recall: 0.7615 - auc: 0.9675 - val_loss: 0.2287 - val_tp: 19791.0000 - val_fp: 2813.0000 - val_tn: 111141.0000 - val_fn: 5026.0000 - val_accuracy: 0.9435 - val_precision: 0.8756 - val_recall: 0.7975 - val_auc: 0.9730\n",
            "Epoch 200/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2417 - tp: 75491.0000 - fp: 10702.0000 - tn: 444513.0000 - fn: 23534.0000 - accuracy: 0.9382 - precision: 0.8758 - recall: 0.7623 - auc: 0.9676\n",
            "Epoch 00200: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2417 - tp: 75598.0000 - fp: 10723.0000 - tn: 445200.0000 - fn: 23563.0000 - accuracy: 0.9382 - precision: 0.8758 - recall: 0.7624 - auc: 0.9676 - val_loss: 0.2288 - val_tp: 19595.0000 - val_fp: 2702.0000 - val_tn: 111252.0000 - val_fn: 5222.0000 - val_accuracy: 0.9429 - val_precision: 0.8788 - val_recall: 0.7896 - val_auc: 0.9738\n",
            "Epoch 201/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2425 - tp: 75219.0000 - fp: 10817.0000 - tn: 445047.0000 - fn: 23925.0000 - accuracy: 0.9374 - precision: 0.8743 - recall: 0.7587 - auc: 0.9674\n",
            "Epoch 00201: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2425 - tp: 75234.0000 - fp: 10817.0000 - tn: 445106.0000 - fn: 23927.0000 - accuracy: 0.9374 - precision: 0.8743 - recall: 0.7587 - auc: 0.9674 - val_loss: 0.2265 - val_tp: 20249.0000 - val_fp: 3295.0000 - val_tn: 110659.0000 - val_fn: 4568.0000 - val_accuracy: 0.9433 - val_precision: 0.8600 - val_recall: 0.8159 - val_auc: 0.9751\n",
            "Epoch 202/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2421 - tp: 75300.0000 - fp: 10643.0000 - tn: 444588.0000 - fn: 23709.0000 - accuracy: 0.9380 - precision: 0.8762 - recall: 0.7605 - auc: 0.9673\n",
            "Epoch 00202: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2421 - tp: 75415.0000 - fp: 10658.0000 - tn: 445265.0000 - fn: 23746.0000 - accuracy: 0.9380 - precision: 0.8762 - recall: 0.7605 - auc: 0.9673 - val_loss: 0.2280 - val_tp: 19147.0000 - val_fp: 2163.0000 - val_tn: 111791.0000 - val_fn: 5670.0000 - val_accuracy: 0.9436 - val_precision: 0.8985 - val_recall: 0.7715 - val_auc: 0.9737\n",
            "Epoch 203/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2416 - tp: 75455.0000 - fp: 10806.0000 - tn: 444633.0000 - fn: 23602.0000 - accuracy: 0.9379 - precision: 0.8747 - recall: 0.7617 - auc: 0.9678\n",
            "Epoch 00203: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2416 - tp: 75532.0000 - fp: 10811.0000 - tn: 445112.0000 - fn: 23629.0000 - accuracy: 0.9380 - precision: 0.8748 - recall: 0.7617 - auc: 0.9678 - val_loss: 0.2311 - val_tp: 18162.0000 - val_fp: 1417.0000 - val_tn: 112537.0000 - val_fn: 6655.0000 - val_accuracy: 0.9418 - val_precision: 0.9276 - val_recall: 0.7318 - val_auc: 0.9740\n",
            "Epoch 204/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2415 - tp: 75546.0000 - fp: 10654.0000 - tn: 445005.0000 - fn: 23547.0000 - accuracy: 0.9383 - precision: 0.8764 - recall: 0.7624 - auc: 0.9675\n",
            "Epoch 00204: val_auc did not improve from 0.97570\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2415 - tp: 75595.0000 - fp: 10657.0000 - tn: 445266.0000 - fn: 23566.0000 - accuracy: 0.9383 - precision: 0.8764 - recall: 0.7623 - auc: 0.9675 - val_loss: 0.2283 - val_tp: 18524.0000 - val_fp: 1692.0000 - val_tn: 112262.0000 - val_fn: 6293.0000 - val_accuracy: 0.9425 - val_precision: 0.9163 - val_recall: 0.7464 - val_auc: 0.9746\n",
            "Epoch 205/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2411 - tp: 75489.0000 - fp: 10737.0000 - tn: 444691.0000 - fn: 23579.0000 - accuracy: 0.9381 - precision: 0.8755 - recall: 0.7620 - auc: 0.9677\n",
            "Epoch 00205: val_auc improved from 0.97570 to 0.97631, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2411 - tp: 75567.0000 - fp: 10751.0000 - tn: 445172.0000 - fn: 23594.0000 - accuracy: 0.9381 - precision: 0.8754 - recall: 0.7621 - auc: 0.9677 - val_loss: 0.2273 - val_tp: 18491.0000 - val_fp: 1543.0000 - val_tn: 112411.0000 - val_fn: 6326.0000 - val_accuracy: 0.9433 - val_precision: 0.9230 - val_recall: 0.7451 - val_auc: 0.9763\n",
            "Epoch 206/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2400 - tp: 75696.0000 - fp: 10734.0000 - tn: 444499.0000 - fn: 23311.0000 - accuracy: 0.9386 - precision: 0.8758 - recall: 0.7646 - auc: 0.9683\n",
            "Epoch 00206: val_auc improved from 0.97631 to 0.97657, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2400 - tp: 75815.0000 - fp: 10753.0000 - tn: 445170.0000 - fn: 23346.0000 - accuracy: 0.9386 - precision: 0.8758 - recall: 0.7646 - auc: 0.9683 - val_loss: 0.2226 - val_tp: 19227.0000 - val_fp: 1941.0000 - val_tn: 112013.0000 - val_fn: 5590.0000 - val_accuracy: 0.9457 - val_precision: 0.9083 - val_recall: 0.7748 - val_auc: 0.9766\n",
            "Epoch 207/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2401 - tp: 75744.0000 - fp: 10586.0000 - tn: 445060.0000 - fn: 23362.0000 - accuracy: 0.9388 - precision: 0.8774 - recall: 0.7643 - auc: 0.9679\n",
            "Epoch 00207: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2401 - tp: 75784.0000 - fp: 10592.0000 - tn: 445331.0000 - fn: 23377.0000 - accuracy: 0.9388 - precision: 0.8774 - recall: 0.7643 - auc: 0.9679 - val_loss: 0.2290 - val_tp: 19894.0000 - val_fp: 2751.0000 - val_tn: 111203.0000 - val_fn: 4923.0000 - val_accuracy: 0.9447 - val_precision: 0.8785 - val_recall: 0.8016 - val_auc: 0.9743\n",
            "Epoch 208/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2411 - tp: 75364.0000 - fp: 10754.0000 - tn: 445169.0000 - fn: 23797.0000 - accuracy: 0.9378 - precision: 0.8751 - recall: 0.7600 - auc: 0.9677\n",
            "Epoch 00208: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2411 - tp: 75364.0000 - fp: 10754.0000 - tn: 445169.0000 - fn: 23797.0000 - accuracy: 0.9378 - precision: 0.8751 - recall: 0.7600 - auc: 0.9677 - val_loss: 0.2260 - val_tp: 19727.0000 - val_fp: 2545.0000 - val_tn: 111409.0000 - val_fn: 5090.0000 - val_accuracy: 0.9450 - val_precision: 0.8857 - val_recall: 0.7949 - val_auc: 0.9748\n",
            "Epoch 209/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2393 - tp: 75651.0000 - fp: 10634.0000 - tn: 444804.0000 - fn: 23407.0000 - accuracy: 0.9386 - precision: 0.8768 - recall: 0.7637 - auc: 0.9684\n",
            "Epoch 00209: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2393 - tp: 75737.0000 - fp: 10650.0000 - tn: 445273.0000 - fn: 23424.0000 - accuracy: 0.9386 - precision: 0.8767 - recall: 0.7638 - auc: 0.9684 - val_loss: 0.2217 - val_tp: 19796.0000 - val_fp: 2532.0000 - val_tn: 111422.0000 - val_fn: 5021.0000 - val_accuracy: 0.9456 - val_precision: 0.8866 - val_recall: 0.7977 - val_auc: 0.9758\n",
            "Epoch 210/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2399 - tp: 75489.0000 - fp: 10637.0000 - tn: 444378.0000 - fn: 23480.0000 - accuracy: 0.9384 - precision: 0.8765 - recall: 0.7628 - auc: 0.9683\n",
            "Epoch 00210: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2399 - tp: 75630.0000 - fp: 10654.0000 - tn: 445269.0000 - fn: 23531.0000 - accuracy: 0.9384 - precision: 0.8765 - recall: 0.7627 - auc: 0.9683 - val_loss: 0.2345 - val_tp: 17325.0000 - val_fp: 1103.0000 - val_tn: 112851.0000 - val_fn: 7492.0000 - val_accuracy: 0.9381 - val_precision: 0.9401 - val_recall: 0.6981 - val_auc: 0.9758\n",
            "Epoch 211/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2397 - tp: 75496.0000 - fp: 10685.0000 - tn: 444768.0000 - fn: 23547.0000 - accuracy: 0.9383 - precision: 0.8760 - recall: 0.7623 - auc: 0.9681\n",
            "Epoch 00211: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2397 - tp: 75586.0000 - fp: 10696.0000 - tn: 445227.0000 - fn: 23575.0000 - accuracy: 0.9383 - precision: 0.8760 - recall: 0.7623 - auc: 0.9681 - val_loss: 0.2222 - val_tp: 19249.0000 - val_fp: 2015.0000 - val_tn: 111939.0000 - val_fn: 5568.0000 - val_accuracy: 0.9454 - val_precision: 0.9052 - val_recall: 0.7756 - val_auc: 0.9762\n",
            "Epoch 212/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2385 - tp: 75979.0000 - fp: 10603.0000 - tn: 445258.0000 - fn: 23168.0000 - accuracy: 0.9392 - precision: 0.8775 - recall: 0.7663 - auc: 0.9686\n",
            "Epoch 00212: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2385 - tp: 75989.0000 - fp: 10604.0000 - tn: 445319.0000 - fn: 23172.0000 - accuracy: 0.9392 - precision: 0.8775 - recall: 0.7663 - auc: 0.9686 - val_loss: 0.2277 - val_tp: 20220.0000 - val_fp: 3261.0000 - val_tn: 110693.0000 - val_fn: 4597.0000 - val_accuracy: 0.9434 - val_precision: 0.8611 - val_recall: 0.8148 - val_auc: 0.9747\n",
            "Epoch 213/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2393 - tp: 75756.0000 - fp: 10759.0000 - tn: 445104.0000 - fn: 23389.0000 - accuracy: 0.9385 - precision: 0.8756 - recall: 0.7641 - auc: 0.9683\n",
            "Epoch 00213: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2393 - tp: 75767.0000 - fp: 10760.0000 - tn: 445163.0000 - fn: 23394.0000 - accuracy: 0.9385 - precision: 0.8756 - recall: 0.7641 - auc: 0.9683 - val_loss: 0.2338 - val_tp: 18058.0000 - val_fp: 1580.0000 - val_tn: 112374.0000 - val_fn: 6759.0000 - val_accuracy: 0.9399 - val_precision: 0.9195 - val_recall: 0.7276 - val_auc: 0.9724\n",
            "Epoch 214/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2394 - tp: 75587.0000 - fp: 10556.0000 - tn: 444885.0000 - fn: 23468.0000 - accuracy: 0.9386 - precision: 0.8775 - recall: 0.7631 - auc: 0.9681\n",
            "Epoch 00214: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2394 - tp: 75667.0000 - fp: 10562.0000 - tn: 445361.0000 - fn: 23494.0000 - accuracy: 0.9386 - precision: 0.8775 - recall: 0.7631 - auc: 0.9681 - val_loss: 0.2344 - val_tp: 17705.0000 - val_fp: 1318.0000 - val_tn: 112636.0000 - val_fn: 7112.0000 - val_accuracy: 0.9393 - val_precision: 0.9307 - val_recall: 0.7134 - val_auc: 0.9752\n",
            "Epoch 215/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2384 - tp: 75920.0000 - fp: 10540.0000 - tn: 445322.0000 - fn: 23226.0000 - accuracy: 0.9392 - precision: 0.8781 - recall: 0.7657 - auc: 0.9685\n",
            "Epoch 00215: val_auc did not improve from 0.97657\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2384 - tp: 75931.0000 - fp: 10540.0000 - tn: 445383.0000 - fn: 23230.0000 - accuracy: 0.9392 - precision: 0.8781 - recall: 0.7657 - auc: 0.9685 - val_loss: 0.2292 - val_tp: 20289.0000 - val_fp: 3241.0000 - val_tn: 110713.0000 - val_fn: 4528.0000 - val_accuracy: 0.9440 - val_precision: 0.8623 - val_recall: 0.8175 - val_auc: 0.9749\n",
            "Epoch 216/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2385 - tp: 75668.0000 - fp: 10545.0000 - tn: 444876.0000 - fn: 23407.0000 - accuracy: 0.9388 - precision: 0.8777 - recall: 0.7637 - auc: 0.9686\n",
            "Epoch 00216: val_auc improved from 0.97657 to 0.97679, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2385 - tp: 75735.0000 - fp: 10561.0000 - tn: 445362.0000 - fn: 23426.0000 - accuracy: 0.9388 - precision: 0.8776 - recall: 0.7638 - auc: 0.9685 - val_loss: 0.2299 - val_tp: 17784.0000 - val_fp: 1221.0000 - val_tn: 112733.0000 - val_fn: 7033.0000 - val_accuracy: 0.9405 - val_precision: 0.9358 - val_recall: 0.7166 - val_auc: 0.9768\n",
            "Epoch 217/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2383 - tp: 75861.0000 - fp: 10593.0000 - tn: 445066.0000 - fn: 23232.0000 - accuracy: 0.9390 - precision: 0.8775 - recall: 0.7656 - auc: 0.9685\n",
            "Epoch 00217: val_auc did not improve from 0.97679\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2383 - tp: 75907.0000 - fp: 10598.0000 - tn: 445325.0000 - fn: 23254.0000 - accuracy: 0.9390 - precision: 0.8775 - recall: 0.7655 - auc: 0.9685 - val_loss: 0.2335 - val_tp: 20971.0000 - val_fp: 3955.0000 - val_tn: 109999.0000 - val_fn: 3846.0000 - val_accuracy: 0.9438 - val_precision: 0.8413 - val_recall: 0.8450 - val_auc: 0.9751\n",
            "Epoch 218/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2383 - tp: 75799.0000 - fp: 10644.0000 - tn: 445215.0000 - fn: 23350.0000 - accuracy: 0.9388 - precision: 0.8769 - recall: 0.7645 - auc: 0.9685\n",
            "Epoch 00218: val_auc did not improve from 0.97679\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2383 - tp: 75807.0000 - fp: 10645.0000 - tn: 445278.0000 - fn: 23354.0000 - accuracy: 0.9387 - precision: 0.8769 - recall: 0.7645 - auc: 0.9685 - val_loss: 0.2253 - val_tp: 19341.0000 - val_fp: 2294.0000 - val_tn: 111660.0000 - val_fn: 5476.0000 - val_accuracy: 0.9440 - val_precision: 0.8940 - val_recall: 0.7793 - val_auc: 0.9747\n",
            "Epoch 219/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2378 - tp: 75909.0000 - fp: 10603.0000 - tn: 445320.0000 - fn: 23252.0000 - accuracy: 0.9390 - precision: 0.8774 - recall: 0.7655 - auc: 0.9688\n",
            "Epoch 00219: val_auc did not improve from 0.97679\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2378 - tp: 75909.0000 - fp: 10603.0000 - tn: 445320.0000 - fn: 23252.0000 - accuracy: 0.9390 - precision: 0.8774 - recall: 0.7655 - auc: 0.9688 - val_loss: 0.2237 - val_tp: 18806.0000 - val_fp: 1682.0000 - val_tn: 112272.0000 - val_fn: 6011.0000 - val_accuracy: 0.9446 - val_precision: 0.9179 - val_recall: 0.7578 - val_auc: 0.9756\n",
            "Epoch 220/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2375 - tp: 76080.0000 - fp: 10607.0000 - tn: 445316.0000 - fn: 23081.0000 - accuracy: 0.9393 - precision: 0.8776 - recall: 0.7672 - auc: 0.9687\n",
            "Epoch 00220: val_auc did not improve from 0.97679\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2375 - tp: 76080.0000 - fp: 10607.0000 - tn: 445316.0000 - fn: 23081.0000 - accuracy: 0.9393 - precision: 0.8776 - recall: 0.7672 - auc: 0.9687 - val_loss: 0.2246 - val_tp: 19704.0000 - val_fp: 2808.0000 - val_tn: 111146.0000 - val_fn: 5113.0000 - val_accuracy: 0.9429 - val_precision: 0.8753 - val_recall: 0.7940 - val_auc: 0.9748\n",
            "Epoch 221/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2372 - tp: 75805.0000 - fp: 10709.0000 - tn: 444081.0000 - fn: 23133.0000 - accuracy: 0.9389 - precision: 0.8762 - recall: 0.7662 - auc: 0.9689\n",
            "Epoch 00221: val_auc did not improve from 0.97679\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2372 - tp: 75965.0000 - fp: 10728.0000 - tn: 445195.0000 - fn: 23196.0000 - accuracy: 0.9389 - precision: 0.8763 - recall: 0.7661 - auc: 0.9689 - val_loss: 0.2265 - val_tp: 18600.0000 - val_fp: 1664.0000 - val_tn: 112290.0000 - val_fn: 6217.0000 - val_accuracy: 0.9432 - val_precision: 0.9179 - val_recall: 0.7495 - val_auc: 0.9757\n",
            "Epoch 222/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2372 - tp: 75979.0000 - fp: 10615.0000 - tn: 445249.0000 - fn: 23165.0000 - accuracy: 0.9391 - precision: 0.8774 - recall: 0.7663 - auc: 0.9687\n",
            "Epoch 00222: val_auc did not improve from 0.97679\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2372 - tp: 75994.0000 - fp: 10615.0000 - tn: 445308.0000 - fn: 23167.0000 - accuracy: 0.9391 - precision: 0.8774 - recall: 0.7664 - auc: 0.9687 - val_loss: 0.2271 - val_tp: 18215.0000 - val_fp: 1478.0000 - val_tn: 112476.0000 - val_fn: 6602.0000 - val_accuracy: 0.9418 - val_precision: 0.9249 - val_recall: 0.7340 - val_auc: 0.9753\n",
            "Epoch 223/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2372 - tp: 76064.0000 - fp: 10720.0000 - tn: 445203.0000 - fn: 23097.0000 - accuracy: 0.9391 - precision: 0.8765 - recall: 0.7671 - auc: 0.9689\n",
            "Epoch 00223: val_auc improved from 0.97679 to 0.97732, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2372 - tp: 76064.0000 - fp: 10720.0000 - tn: 445203.0000 - fn: 23097.0000 - accuracy: 0.9391 - precision: 0.8765 - recall: 0.7671 - auc: 0.9689 - val_loss: 0.2182 - val_tp: 19139.0000 - val_fp: 1700.0000 - val_tn: 112254.0000 - val_fn: 5678.0000 - val_accuracy: 0.9468 - val_precision: 0.9184 - val_recall: 0.7712 - val_auc: 0.9773\n",
            "Epoch 224/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2370 - tp: 75852.0000 - fp: 10679.0000 - tn: 444355.0000 - fn: 23098.0000 - accuracy: 0.9390 - precision: 0.8766 - recall: 0.7666 - auc: 0.9688\n",
            "Epoch 00224: val_auc did not improve from 0.97732\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2370 - tp: 76007.0000 - fp: 10693.0000 - tn: 445230.0000 - fn: 23154.0000 - accuracy: 0.9390 - precision: 0.8767 - recall: 0.7665 - auc: 0.9688 - val_loss: 0.2259 - val_tp: 19075.0000 - val_fp: 2096.0000 - val_tn: 111858.0000 - val_fn: 5742.0000 - val_accuracy: 0.9435 - val_precision: 0.9010 - val_recall: 0.7686 - val_auc: 0.9744\n",
            "Epoch 225/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2364 - tp: 76021.0000 - fp: 10468.0000 - tn: 444765.0000 - fn: 22986.0000 - accuracy: 0.9396 - precision: 0.8790 - recall: 0.7678 - auc: 0.9691\n",
            "Epoch 00225: val_auc did not improve from 0.97732\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2364 - tp: 76142.0000 - fp: 10492.0000 - tn: 445431.0000 - fn: 23019.0000 - accuracy: 0.9396 - precision: 0.8789 - recall: 0.7679 - auc: 0.9691 - val_loss: 0.2208 - val_tp: 19257.0000 - val_fp: 1947.0000 - val_tn: 112007.0000 - val_fn: 5560.0000 - val_accuracy: 0.9459 - val_precision: 0.9082 - val_recall: 0.7760 - val_auc: 0.9760\n",
            "Epoch 226/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2364 - tp: 75981.0000 - fp: 10639.0000 - tn: 444793.0000 - fn: 23083.0000 - accuracy: 0.9392 - precision: 0.8772 - recall: 0.7670 - auc: 0.9689\n",
            "Epoch 00226: val_auc did not improve from 0.97732\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2364 - tp: 76058.0000 - fp: 10644.0000 - tn: 445279.0000 - fn: 23103.0000 - accuracy: 0.9392 - precision: 0.8772 - recall: 0.7670 - auc: 0.9689 - val_loss: 0.2335 - val_tp: 17420.0000 - val_fp: 1111.0000 - val_tn: 112843.0000 - val_fn: 7397.0000 - val_accuracy: 0.9387 - val_precision: 0.9400 - val_recall: 0.7019 - val_auc: 0.9756\n",
            "Epoch 227/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2365 - tp: 76009.0000 - fp: 10430.0000 - tn: 445429.0000 - fn: 23140.0000 - accuracy: 0.9395 - precision: 0.8793 - recall: 0.7666 - auc: 0.9688\n",
            "Epoch 00227: val_auc did not improve from 0.97732\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2365 - tp: 76021.0000 - fp: 10434.0000 - tn: 445489.0000 - fn: 23140.0000 - accuracy: 0.9395 - precision: 0.8793 - recall: 0.7666 - auc: 0.9688 - val_loss: 0.2218 - val_tp: 20496.0000 - val_fp: 3328.0000 - val_tn: 110626.0000 - val_fn: 4321.0000 - val_accuracy: 0.9449 - val_precision: 0.8603 - val_recall: 0.8259 - val_auc: 0.9764\n",
            "Epoch 228/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2361 - tp: 75906.0000 - fp: 10694.0000 - tn: 444302.0000 - fn: 23082.0000 - accuracy: 0.9390 - precision: 0.8765 - recall: 0.7668 - auc: 0.9691\n",
            "Epoch 00228: val_auc improved from 0.97732 to 0.97791, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2361 - tp: 76045.0000 - fp: 10708.0000 - tn: 445215.0000 - fn: 23116.0000 - accuracy: 0.9391 - precision: 0.8766 - recall: 0.7669 - auc: 0.9691 - val_loss: 0.2162 - val_tp: 19846.0000 - val_fp: 2268.0000 - val_tn: 111686.0000 - val_fn: 4971.0000 - val_accuracy: 0.9478 - val_precision: 0.8974 - val_recall: 0.7997 - val_auc: 0.9779\n",
            "Epoch 229/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2352 - tp: 76183.0000 - fp: 10480.0000 - tn: 445443.0000 - fn: 22978.0000 - accuracy: 0.9397 - precision: 0.8791 - recall: 0.7683 - auc: 0.9693\n",
            "Epoch 00229: val_auc did not improve from 0.97791\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2352 - tp: 76183.0000 - fp: 10480.0000 - tn: 445443.0000 - fn: 22978.0000 - accuracy: 0.9397 - precision: 0.8791 - recall: 0.7683 - auc: 0.9693 - val_loss: 0.2211 - val_tp: 19281.0000 - val_fp: 2052.0000 - val_tn: 111902.0000 - val_fn: 5536.0000 - val_accuracy: 0.9453 - val_precision: 0.9038 - val_recall: 0.7769 - val_auc: 0.9762\n",
            "Epoch 230/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2355 - tp: 76143.0000 - fp: 10695.0000 - tn: 444953.0000 - fn: 22961.0000 - accuracy: 0.9393 - precision: 0.8768 - recall: 0.7683 - auc: 0.9693\n",
            "Epoch 00230: val_auc did not improve from 0.97791\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2356 - tp: 76185.0000 - fp: 10705.0000 - tn: 445218.0000 - fn: 22976.0000 - accuracy: 0.9393 - precision: 0.8768 - recall: 0.7683 - auc: 0.9693 - val_loss: 0.2237 - val_tp: 18820.0000 - val_fp: 1803.0000 - val_tn: 112151.0000 - val_fn: 5997.0000 - val_accuracy: 0.9438 - val_precision: 0.9126 - val_recall: 0.7584 - val_auc: 0.9753\n",
            "Epoch 231/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2352 - tp: 76181.0000 - fp: 10634.0000 - tn: 445227.0000 - fn: 22966.0000 - accuracy: 0.9395 - precision: 0.8775 - recall: 0.7684 - auc: 0.9693\n",
            "Epoch 00231: val_auc did not improve from 0.97791\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2351 - tp: 76192.0000 - fp: 10635.0000 - tn: 445288.0000 - fn: 22969.0000 - accuracy: 0.9395 - precision: 0.8775 - recall: 0.7684 - auc: 0.9693 - val_loss: 0.2222 - val_tp: 18821.0000 - val_fp: 1739.0000 - val_tn: 112215.0000 - val_fn: 5996.0000 - val_accuracy: 0.9443 - val_precision: 0.9154 - val_recall: 0.7584 - val_auc: 0.9762\n",
            "Epoch 232/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2355 - tp: 76152.0000 - fp: 10561.0000 - tn: 445298.0000 - fn: 22997.0000 - accuracy: 0.9395 - precision: 0.8782 - recall: 0.7681 - auc: 0.9692\n",
            "Epoch 00232: val_auc did not improve from 0.97791\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2355 - tp: 76162.0000 - fp: 10563.0000 - tn: 445360.0000 - fn: 22999.0000 - accuracy: 0.9395 - precision: 0.8782 - recall: 0.7681 - auc: 0.9692 - val_loss: 0.2204 - val_tp: 20182.0000 - val_fp: 2894.0000 - val_tn: 111060.0000 - val_fn: 4635.0000 - val_accuracy: 0.9457 - val_precision: 0.8746 - val_recall: 0.8132 - val_auc: 0.9756\n",
            "Epoch 233/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2356 - tp: 76129.0000 - fp: 10465.0000 - tn: 445392.0000 - fn: 23022.0000 - accuracy: 0.9397 - precision: 0.8791 - recall: 0.7678 - auc: 0.9690\n",
            "Epoch 00233: val_auc did not improve from 0.97791\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2356 - tp: 76134.0000 - fp: 10465.0000 - tn: 445458.0000 - fn: 23027.0000 - accuracy: 0.9397 - precision: 0.8792 - recall: 0.7678 - auc: 0.9689 - val_loss: 0.2234 - val_tp: 18129.0000 - val_fp: 1254.0000 - val_tn: 112700.0000 - val_fn: 6688.0000 - val_accuracy: 0.9428 - val_precision: 0.9353 - val_recall: 0.7305 - val_auc: 0.9776\n",
            "Epoch 234/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2347 - tp: 76163.0000 - fp: 10422.0000 - tn: 445436.0000 - fn: 22987.0000 - accuracy: 0.9398 - precision: 0.8796 - recall: 0.7682 - auc: 0.9695\n",
            "Epoch 00234: val_auc improved from 0.97791 to 0.97806, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2347 - tp: 76170.0000 - fp: 10422.0000 - tn: 445501.0000 - fn: 22991.0000 - accuracy: 0.9398 - precision: 0.8796 - recall: 0.7681 - auc: 0.9695 - val_loss: 0.2258 - val_tp: 17690.0000 - val_fp: 1085.0000 - val_tn: 112869.0000 - val_fn: 7127.0000 - val_accuracy: 0.9408 - val_precision: 0.9422 - val_recall: 0.7128 - val_auc: 0.9781\n",
            "Epoch 235/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2347 - tp: 76199.0000 - fp: 10587.0000 - tn: 445271.0000 - fn: 22951.0000 - accuracy: 0.9396 - precision: 0.8780 - recall: 0.7685 - auc: 0.9695\n",
            "Epoch 00235: val_auc did not improve from 0.97806\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2347 - tp: 76209.0000 - fp: 10589.0000 - tn: 445334.0000 - fn: 22952.0000 - accuracy: 0.9396 - precision: 0.8780 - recall: 0.7685 - auc: 0.9695 - val_loss: 0.2244 - val_tp: 19970.0000 - val_fp: 2948.0000 - val_tn: 111006.0000 - val_fn: 4847.0000 - val_accuracy: 0.9438 - val_precision: 0.8714 - val_recall: 0.8047 - val_auc: 0.9752\n",
            "Epoch 236/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2347 - tp: 76262.0000 - fp: 10569.0000 - tn: 445354.0000 - fn: 22899.0000 - accuracy: 0.9397 - precision: 0.8783 - recall: 0.7691 - auc: 0.9695\n",
            "Epoch 00236: val_auc did not improve from 0.97806\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2347 - tp: 76262.0000 - fp: 10569.0000 - tn: 445354.0000 - fn: 22899.0000 - accuracy: 0.9397 - precision: 0.8783 - recall: 0.7691 - auc: 0.9695 - val_loss: 0.2175 - val_tp: 19920.0000 - val_fp: 2590.0000 - val_tn: 111364.0000 - val_fn: 4897.0000 - val_accuracy: 0.9460 - val_precision: 0.8849 - val_recall: 0.8027 - val_auc: 0.9771\n",
            "Epoch 237/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2342 - tp: 76351.0000 - fp: 10499.0000 - tn: 445361.0000 - fn: 22797.0000 - accuracy: 0.9400 - precision: 0.8791 - recall: 0.7701 - auc: 0.9695\n",
            "Epoch 00237: val_auc did not improve from 0.97806\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2342 - tp: 76358.0000 - fp: 10501.0000 - tn: 445422.0000 - fn: 22803.0000 - accuracy: 0.9400 - precision: 0.8791 - recall: 0.7700 - auc: 0.9695 - val_loss: 0.2269 - val_tp: 18139.0000 - val_fp: 1435.0000 - val_tn: 112519.0000 - val_fn: 6678.0000 - val_accuracy: 0.9415 - val_precision: 0.9267 - val_recall: 0.7309 - val_auc: 0.9758\n",
            "Epoch 238/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2345 - tp: 76259.0000 - fp: 10517.0000 - tn: 445406.0000 - fn: 22902.0000 - accuracy: 0.9398 - precision: 0.8788 - recall: 0.7690 - auc: 0.9693\n",
            "Epoch 00238: val_auc did not improve from 0.97806\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2345 - tp: 76259.0000 - fp: 10517.0000 - tn: 445406.0000 - fn: 22902.0000 - accuracy: 0.9398 - precision: 0.8788 - recall: 0.7690 - auc: 0.9693 - val_loss: 0.2237 - val_tp: 19983.0000 - val_fp: 2755.0000 - val_tn: 111199.0000 - val_fn: 4834.0000 - val_accuracy: 0.9453 - val_precision: 0.8788 - val_recall: 0.8052 - val_auc: 0.9751\n",
            "Epoch 239/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2345 - tp: 76041.0000 - fp: 10420.0000 - tn: 444807.0000 - fn: 22972.0000 - accuracy: 0.9398 - precision: 0.8795 - recall: 0.7680 - auc: 0.9693\n",
            "Epoch 00239: val_auc did not improve from 0.97806\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2345 - tp: 76150.0000 - fp: 10434.0000 - tn: 445489.0000 - fn: 23011.0000 - accuracy: 0.9397 - precision: 0.8795 - recall: 0.7679 - auc: 0.9693 - val_loss: 0.2168 - val_tp: 19723.0000 - val_fp: 2285.0000 - val_tn: 111669.0000 - val_fn: 5094.0000 - val_accuracy: 0.9468 - val_precision: 0.8962 - val_recall: 0.7947 - val_auc: 0.9769\n",
            "Epoch 240/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2339 - tp: 76365.0000 - fp: 10517.0000 - tn: 445344.0000 - fn: 22782.0000 - accuracy: 0.9400 - precision: 0.8790 - recall: 0.7702 - auc: 0.9696\n",
            "Epoch 00240: val_auc improved from 0.97806 to 0.97859, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2339 - tp: 76377.0000 - fp: 10520.0000 - tn: 445403.0000 - fn: 22784.0000 - accuracy: 0.9400 - precision: 0.8789 - recall: 0.7702 - auc: 0.9696 - val_loss: 0.2124 - val_tp: 19628.0000 - val_fp: 2026.0000 - val_tn: 111928.0000 - val_fn: 5189.0000 - val_accuracy: 0.9480 - val_precision: 0.9064 - val_recall: 0.7909 - val_auc: 0.9786\n",
            "Epoch 241/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2348 - tp: 76245.0000 - fp: 10554.0000 - tn: 445369.0000 - fn: 22916.0000 - accuracy: 0.9397 - precision: 0.8784 - recall: 0.7689 - auc: 0.9691\n",
            "Epoch 00241: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2348 - tp: 76245.0000 - fp: 10554.0000 - tn: 445369.0000 - fn: 22916.0000 - accuracy: 0.9397 - precision: 0.8784 - recall: 0.7689 - auc: 0.9691 - val_loss: 0.2187 - val_tp: 19879.0000 - val_fp: 2612.0000 - val_tn: 111342.0000 - val_fn: 4938.0000 - val_accuracy: 0.9456 - val_precision: 0.8839 - val_recall: 0.8010 - val_auc: 0.9763\n",
            "Epoch 242/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2331 - tp: 76323.0000 - fp: 10549.0000 - tn: 444487.0000 - fn: 22625.0000 - accuracy: 0.9401 - precision: 0.8786 - recall: 0.7713 - auc: 0.9698\n",
            "Epoch 00242: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2331 - tp: 76479.0000 - fp: 10564.0000 - tn: 445359.0000 - fn: 22682.0000 - accuracy: 0.9401 - precision: 0.8786 - recall: 0.7713 - auc: 0.9698 - val_loss: 0.2174 - val_tp: 18754.0000 - val_fp: 1500.0000 - val_tn: 112454.0000 - val_fn: 6063.0000 - val_accuracy: 0.9455 - val_precision: 0.9259 - val_recall: 0.7557 - val_auc: 0.9777\n",
            "Epoch 243/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2331 - tp: 76542.0000 - fp: 10543.0000 - tn: 445380.0000 - fn: 22619.0000 - accuracy: 0.9403 - precision: 0.8789 - recall: 0.7719 - auc: 0.9698\n",
            "Epoch 00243: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2331 - tp: 76542.0000 - fp: 10543.0000 - tn: 445380.0000 - fn: 22619.0000 - accuracy: 0.9403 - precision: 0.8789 - recall: 0.7719 - auc: 0.9698 - val_loss: 0.2176 - val_tp: 19710.0000 - val_fp: 2275.0000 - val_tn: 111679.0000 - val_fn: 5107.0000 - val_accuracy: 0.9468 - val_precision: 0.8965 - val_recall: 0.7942 - val_auc: 0.9767\n",
            "Epoch 244/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2329 - tp: 76475.0000 - fp: 10473.0000 - tn: 445450.0000 - fn: 22686.0000 - accuracy: 0.9403 - precision: 0.8795 - recall: 0.7712 - auc: 0.9699\n",
            "Epoch 00244: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 21s 9ms/step - loss: 0.2329 - tp: 76475.0000 - fp: 10473.0000 - tn: 445450.0000 - fn: 22686.0000 - accuracy: 0.9403 - precision: 0.8795 - recall: 0.7712 - auc: 0.9699 - val_loss: 0.2193 - val_tp: 18897.0000 - val_fp: 1721.0000 - val_tn: 112233.0000 - val_fn: 5920.0000 - val_accuracy: 0.9449 - val_precision: 0.9165 - val_recall: 0.7615 - val_auc: 0.9770\n",
            "Epoch 245/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2332 - tp: 76337.0000 - fp: 10563.0000 - tn: 445072.0000 - fn: 22780.0000 - accuracy: 0.9399 - precision: 0.8784 - recall: 0.7702 - auc: 0.9697\n",
            "Epoch 00245: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2332 - tp: 76368.0000 - fp: 10575.0000 - tn: 445348.0000 - fn: 22793.0000 - accuracy: 0.9399 - precision: 0.8784 - recall: 0.7701 - auc: 0.9697 - val_loss: 0.2151 - val_tp: 19740.0000 - val_fp: 2231.0000 - val_tn: 111723.0000 - val_fn: 5077.0000 - val_accuracy: 0.9473 - val_precision: 0.8985 - val_recall: 0.7954 - val_auc: 0.9773\n",
            "Epoch 246/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2330 - tp: 76457.0000 - fp: 10582.0000 - tn: 445068.0000 - fn: 22645.0000 - accuracy: 0.9401 - precision: 0.8784 - recall: 0.7715 - auc: 0.9697\n",
            "Epoch 00246: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2330 - tp: 76500.0000 - fp: 10588.0000 - tn: 445335.0000 - fn: 22661.0000 - accuracy: 0.9401 - precision: 0.8784 - recall: 0.7715 - auc: 0.9697 - val_loss: 0.2266 - val_tp: 20595.0000 - val_fp: 3662.0000 - val_tn: 110292.0000 - val_fn: 4222.0000 - val_accuracy: 0.9432 - val_precision: 0.8490 - val_recall: 0.8299 - val_auc: 0.9748\n",
            "Epoch 247/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2324 - tp: 76492.0000 - fp: 10547.0000 - tn: 445316.0000 - fn: 22653.0000 - accuracy: 0.9402 - precision: 0.8788 - recall: 0.7715 - auc: 0.9701\n",
            "Epoch 00247: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2324 - tp: 76505.0000 - fp: 10547.0000 - tn: 445376.0000 - fn: 22656.0000 - accuracy: 0.9402 - precision: 0.8788 - recall: 0.7715 - auc: 0.9701 - val_loss: 0.2167 - val_tp: 19735.0000 - val_fp: 2323.0000 - val_tn: 111631.0000 - val_fn: 5082.0000 - val_accuracy: 0.9466 - val_precision: 0.8947 - val_recall: 0.7952 - val_auc: 0.9770\n",
            "Epoch 248/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2325 - tp: 76559.0000 - fp: 10433.0000 - tn: 445206.0000 - fn: 22554.0000 - accuracy: 0.9405 - precision: 0.8801 - recall: 0.7724 - auc: 0.9699\n",
            "Epoch 00248: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2325 - tp: 76596.0000 - fp: 10446.0000 - tn: 445477.0000 - fn: 22565.0000 - accuracy: 0.9405 - precision: 0.8800 - recall: 0.7724 - auc: 0.9699 - val_loss: 0.2268 - val_tp: 21239.0000 - val_fp: 4262.0000 - val_tn: 109692.0000 - val_fn: 3578.0000 - val_accuracy: 0.9435 - val_precision: 0.8329 - val_recall: 0.8558 - val_auc: 0.9764\n",
            "Epoch 249/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2327 - tp: 76415.0000 - fp: 10505.0000 - tn: 445357.0000 - fn: 22731.0000 - accuracy: 0.9401 - precision: 0.8791 - recall: 0.7707 - auc: 0.9699\n",
            "Epoch 00249: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2327 - tp: 76424.0000 - fp: 10506.0000 - tn: 445417.0000 - fn: 22737.0000 - accuracy: 0.9401 - precision: 0.8791 - recall: 0.7707 - auc: 0.9699 - val_loss: 0.2191 - val_tp: 19239.0000 - val_fp: 2094.0000 - val_tn: 111860.0000 - val_fn: 5578.0000 - val_accuracy: 0.9447 - val_precision: 0.9018 - val_recall: 0.7752 - val_auc: 0.9761\n",
            "Epoch 250/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2327 - tp: 76492.0000 - fp: 10646.0000 - tn: 445217.0000 - fn: 22653.0000 - accuracy: 0.9400 - precision: 0.8778 - recall: 0.7715 - auc: 0.9698\n",
            "Epoch 00250: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2327 - tp: 76499.0000 - fp: 10647.0000 - tn: 445276.0000 - fn: 22662.0000 - accuracy: 0.9400 - precision: 0.8778 - recall: 0.7715 - auc: 0.9698 - val_loss: 0.2237 - val_tp: 18157.0000 - val_fp: 1329.0000 - val_tn: 112625.0000 - val_fn: 6660.0000 - val_accuracy: 0.9424 - val_precision: 0.9318 - val_recall: 0.7316 - val_auc: 0.9767\n",
            "Epoch 251/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2318 - tp: 76515.0000 - fp: 10448.0000 - tn: 445475.0000 - fn: 22646.0000 - accuracy: 0.9404 - precision: 0.8799 - recall: 0.7716 - auc: 0.9702\n",
            "Epoch 00251: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2318 - tp: 76515.0000 - fp: 10448.0000 - tn: 445475.0000 - fn: 22646.0000 - accuracy: 0.9404 - precision: 0.8799 - recall: 0.7716 - auc: 0.9702 - val_loss: 0.2277 - val_tp: 18643.0000 - val_fp: 1759.0000 - val_tn: 112195.0000 - val_fn: 6174.0000 - val_accuracy: 0.9428 - val_precision: 0.9138 - val_recall: 0.7512 - val_auc: 0.9748\n",
            "Epoch 252/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2319 - tp: 76395.0000 - fp: 10490.0000 - tn: 445433.0000 - fn: 22766.0000 - accuracy: 0.9401 - precision: 0.8793 - recall: 0.7704 - auc: 0.9702\n",
            "Epoch 00252: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2319 - tp: 76395.0000 - fp: 10490.0000 - tn: 445433.0000 - fn: 22766.0000 - accuracy: 0.9401 - precision: 0.8793 - recall: 0.7704 - auc: 0.9702 - val_loss: 0.2183 - val_tp: 19247.0000 - val_fp: 1932.0000 - val_tn: 112022.0000 - val_fn: 5570.0000 - val_accuracy: 0.9459 - val_precision: 0.9088 - val_recall: 0.7756 - val_auc: 0.9766\n",
            "Epoch 253/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2321 - tp: 76505.0000 - fp: 10497.0000 - tn: 445364.0000 - fn: 22642.0000 - accuracy: 0.9403 - precision: 0.8793 - recall: 0.7716 - auc: 0.9700\n",
            "Epoch 00253: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2321 - tp: 76518.0000 - fp: 10498.0000 - tn: 445425.0000 - fn: 22643.0000 - accuracy: 0.9403 - precision: 0.8794 - recall: 0.7717 - auc: 0.9700 - val_loss: 0.2159 - val_tp: 19437.0000 - val_fp: 1995.0000 - val_tn: 111959.0000 - val_fn: 5380.0000 - val_accuracy: 0.9469 - val_precision: 0.9069 - val_recall: 0.7832 - val_auc: 0.9767\n",
            "Epoch 254/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2318 - tp: 76637.0000 - fp: 10405.0000 - tn: 445455.0000 - fn: 22511.0000 - accuracy: 0.9407 - precision: 0.8805 - recall: 0.7730 - auc: 0.9699\n",
            "Epoch 00254: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2318 - tp: 76648.0000 - fp: 10407.0000 - tn: 445516.0000 - fn: 22513.0000 - accuracy: 0.9407 - precision: 0.8805 - recall: 0.7730 - auc: 0.9699 - val_loss: 0.2177 - val_tp: 19155.0000 - val_fp: 1889.0000 - val_tn: 112065.0000 - val_fn: 5662.0000 - val_accuracy: 0.9456 - val_precision: 0.9102 - val_recall: 0.7718 - val_auc: 0.9770\n",
            "Epoch 255/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2309 - tp: 76427.0000 - fp: 10434.0000 - tn: 444367.0000 - fn: 22500.0000 - accuracy: 0.9405 - precision: 0.8799 - recall: 0.7726 - auc: 0.9706\n",
            "Epoch 00255: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2309 - tp: 76616.0000 - fp: 10465.0000 - tn: 445458.0000 - fn: 22545.0000 - accuracy: 0.9405 - precision: 0.8798 - recall: 0.7726 - auc: 0.9706 - val_loss: 0.2158 - val_tp: 19627.0000 - val_fp: 2170.0000 - val_tn: 111784.0000 - val_fn: 5190.0000 - val_accuracy: 0.9470 - val_precision: 0.9004 - val_recall: 0.7909 - val_auc: 0.9770\n",
            "Epoch 256/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2320 - tp: 76328.0000 - fp: 10350.0000 - tn: 444872.0000 - fn: 22690.0000 - accuracy: 0.9404 - precision: 0.8806 - recall: 0.7708 - auc: 0.9700\n",
            "Epoch 00256: val_auc did not improve from 0.97859\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2319 - tp: 76441.0000 - fp: 10362.0000 - tn: 445561.0000 - fn: 22720.0000 - accuracy: 0.9404 - precision: 0.8806 - recall: 0.7709 - auc: 0.9700 - val_loss: 0.2155 - val_tp: 19148.0000 - val_fp: 1793.0000 - val_tn: 112161.0000 - val_fn: 5669.0000 - val_accuracy: 0.9462 - val_precision: 0.9144 - val_recall: 0.7716 - val_auc: 0.9775\n",
            "Epoch 257/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2307 - tp: 76711.0000 - fp: 10423.0000 - tn: 445500.0000 - fn: 22450.0000 - accuracy: 0.9408 - precision: 0.8804 - recall: 0.7736 - auc: 0.9704\n",
            "Epoch 00257: val_auc improved from 0.97859 to 0.97876, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2307 - tp: 76711.0000 - fp: 10423.0000 - tn: 445500.0000 - fn: 22450.0000 - accuracy: 0.9408 - precision: 0.8804 - recall: 0.7736 - auc: 0.9704 - val_loss: 0.2111 - val_tp: 19451.0000 - val_fp: 1834.0000 - val_tn: 112120.0000 - val_fn: 5366.0000 - val_accuracy: 0.9481 - val_precision: 0.9138 - val_recall: 0.7838 - val_auc: 0.9788\n",
            "Epoch 258/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2307 - tp: 76723.0000 - fp: 10586.0000 - tn: 445337.0000 - fn: 22438.0000 - accuracy: 0.9405 - precision: 0.8788 - recall: 0.7737 - auc: 0.9704\n",
            "Epoch 00258: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2307 - tp: 76723.0000 - fp: 10586.0000 - tn: 445337.0000 - fn: 22438.0000 - accuracy: 0.9405 - precision: 0.8788 - recall: 0.7737 - auc: 0.9704 - val_loss: 0.2209 - val_tp: 19047.0000 - val_fp: 1971.0000 - val_tn: 111983.0000 - val_fn: 5770.0000 - val_accuracy: 0.9442 - val_precision: 0.9062 - val_recall: 0.7675 - val_auc: 0.9756\n",
            "Epoch 259/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2304 - tp: 76449.0000 - fp: 10405.0000 - tn: 444407.0000 - fn: 22467.0000 - accuracy: 0.9406 - precision: 0.8802 - recall: 0.7729 - auc: 0.9705\n",
            "Epoch 00259: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2304 - tp: 76638.0000 - fp: 10429.0000 - tn: 445494.0000 - fn: 22523.0000 - accuracy: 0.9406 - precision: 0.8802 - recall: 0.7729 - auc: 0.9705 - val_loss: 0.2144 - val_tp: 19406.0000 - val_fp: 1975.0000 - val_tn: 111979.0000 - val_fn: 5411.0000 - val_accuracy: 0.9468 - val_precision: 0.9076 - val_recall: 0.7820 - val_auc: 0.9781\n",
            "Epoch 260/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2307 - tp: 76684.0000 - fp: 10436.0000 - tn: 445426.0000 - fn: 22462.0000 - accuracy: 0.9407 - precision: 0.8802 - recall: 0.7734 - auc: 0.9703\n",
            "Epoch 00260: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 21s 10ms/step - loss: 0.2307 - tp: 76695.0000 - fp: 10436.0000 - tn: 445487.0000 - fn: 22466.0000 - accuracy: 0.9407 - precision: 0.8802 - recall: 0.7734 - auc: 0.9704 - val_loss: 0.2199 - val_tp: 20546.0000 - val_fp: 3189.0000 - val_tn: 110765.0000 - val_fn: 4271.0000 - val_accuracy: 0.9462 - val_precision: 0.8656 - val_recall: 0.8279 - val_auc: 0.9771\n",
            "Epoch 261/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2304 - tp: 76748.0000 - fp: 10387.0000 - tn: 445536.0000 - fn: 22413.0000 - accuracy: 0.9409 - precision: 0.8808 - recall: 0.7740 - auc: 0.9704\n",
            "Epoch 00261: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2304 - tp: 76748.0000 - fp: 10387.0000 - tn: 445536.0000 - fn: 22413.0000 - accuracy: 0.9409 - precision: 0.8808 - recall: 0.7740 - auc: 0.9704 - val_loss: 0.2131 - val_tp: 20162.0000 - val_fp: 2610.0000 - val_tn: 111344.0000 - val_fn: 4655.0000 - val_accuracy: 0.9476 - val_precision: 0.8854 - val_recall: 0.8124 - val_auc: 0.9776\n",
            "Epoch 262/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2303 - tp: 76633.0000 - fp: 10521.0000 - tn: 444713.0000 - fn: 22373.0000 - accuracy: 0.9407 - precision: 0.8793 - recall: 0.7740 - auc: 0.9704\n",
            "Epoch 00262: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2303 - tp: 76762.0000 - fp: 10530.0000 - tn: 445393.0000 - fn: 22399.0000 - accuracy: 0.9407 - precision: 0.8794 - recall: 0.7741 - auc: 0.9704 - val_loss: 0.2123 - val_tp: 20270.0000 - val_fp: 2629.0000 - val_tn: 111325.0000 - val_fn: 4547.0000 - val_accuracy: 0.9483 - val_precision: 0.8852 - val_recall: 0.8168 - val_auc: 0.9779\n",
            "Epoch 263/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2294 - tp: 76604.0000 - fp: 10399.0000 - tn: 444426.0000 - fn: 22299.0000 - accuracy: 0.9409 - precision: 0.8805 - recall: 0.7745 - auc: 0.9708\n",
            "Epoch 00263: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2294 - tp: 76815.0000 - fp: 10413.0000 - tn: 445510.0000 - fn: 22346.0000 - accuracy: 0.9410 - precision: 0.8806 - recall: 0.7746 - auc: 0.9708 - val_loss: 0.2122 - val_tp: 20274.0000 - val_fp: 2635.0000 - val_tn: 111319.0000 - val_fn: 4543.0000 - val_accuracy: 0.9483 - val_precision: 0.8850 - val_recall: 0.8169 - val_auc: 0.9781\n",
            "Epoch 264/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2305 - tp: 76542.0000 - fp: 10315.0000 - tn: 444687.0000 - fn: 22440.0000 - accuracy: 0.9409 - precision: 0.8812 - recall: 0.7733 - auc: 0.9702\n",
            "Epoch 00264: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2305 - tp: 76689.0000 - fp: 10342.0000 - tn: 445581.0000 - fn: 22472.0000 - accuracy: 0.9409 - precision: 0.8812 - recall: 0.7734 - auc: 0.9702 - val_loss: 0.2200 - val_tp: 20792.0000 - val_fp: 3421.0000 - val_tn: 110533.0000 - val_fn: 4025.0000 - val_accuracy: 0.9463 - val_precision: 0.8587 - val_recall: 0.8378 - val_auc: 0.9763\n",
            "Epoch 265/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2294 - tp: 77001.0000 - fp: 10533.0000 - tn: 445326.0000 - fn: 22148.0000 - accuracy: 0.9411 - precision: 0.8797 - recall: 0.7766 - auc: 0.9708\n",
            "Epoch 00265: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2294 - tp: 77010.0000 - fp: 10534.0000 - tn: 445389.0000 - fn: 22151.0000 - accuracy: 0.9411 - precision: 0.8797 - recall: 0.7766 - auc: 0.9708 - val_loss: 0.2209 - val_tp: 19985.0000 - val_fp: 2918.0000 - val_tn: 111036.0000 - val_fn: 4832.0000 - val_accuracy: 0.9442 - val_precision: 0.8726 - val_recall: 0.8053 - val_auc: 0.9750\n",
            "Epoch 266/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2304 - tp: 76523.0000 - fp: 10361.0000 - tn: 445562.0000 - fn: 22638.0000 - accuracy: 0.9406 - precision: 0.8807 - recall: 0.7717 - auc: 0.9704\n",
            "Epoch 00266: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2304 - tp: 76523.0000 - fp: 10361.0000 - tn: 445562.0000 - fn: 22638.0000 - accuracy: 0.9406 - precision: 0.8807 - recall: 0.7717 - auc: 0.9704 - val_loss: 0.2175 - val_tp: 18985.0000 - val_fp: 1835.0000 - val_tn: 112119.0000 - val_fn: 5832.0000 - val_accuracy: 0.9448 - val_precision: 0.9119 - val_recall: 0.7650 - val_auc: 0.9768\n",
            "Epoch 267/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2299 - tp: 76809.0000 - fp: 10324.0000 - tn: 445329.0000 - fn: 22290.0000 - accuracy: 0.9412 - precision: 0.8815 - recall: 0.7751 - auc: 0.9704\n",
            "Epoch 00267: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2299 - tp: 76857.0000 - fp: 10330.0000 - tn: 445593.0000 - fn: 22304.0000 - accuracy: 0.9412 - precision: 0.8815 - recall: 0.7751 - auc: 0.9704 - val_loss: 0.2144 - val_tp: 19141.0000 - val_fp: 1716.0000 - val_tn: 112238.0000 - val_fn: 5676.0000 - val_accuracy: 0.9467 - val_precision: 0.9177 - val_recall: 0.7713 - val_auc: 0.9781\n",
            "Epoch 268/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2301 - tp: 76773.0000 - fp: 10417.0000 - tn: 445506.0000 - fn: 22388.0000 - accuracy: 0.9409 - precision: 0.8805 - recall: 0.7742 - auc: 0.9706\n",
            "Epoch 00268: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2301 - tp: 76773.0000 - fp: 10417.0000 - tn: 445506.0000 - fn: 22388.0000 - accuracy: 0.9409 - precision: 0.8805 - recall: 0.7742 - auc: 0.9706 - val_loss: 0.2169 - val_tp: 20319.0000 - val_fp: 2920.0000 - val_tn: 111034.0000 - val_fn: 4498.0000 - val_accuracy: 0.9465 - val_precision: 0.8743 - val_recall: 0.8188 - val_auc: 0.9771\n",
            "Epoch 269/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2297 - tp: 76696.0000 - fp: 10358.0000 - tn: 445565.0000 - fn: 22465.0000 - accuracy: 0.9409 - precision: 0.8810 - recall: 0.7734 - auc: 0.9707\n",
            "Epoch 00269: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2297 - tp: 76696.0000 - fp: 10358.0000 - tn: 445565.0000 - fn: 22465.0000 - accuracy: 0.9409 - precision: 0.8810 - recall: 0.7734 - auc: 0.9707 - val_loss: 0.2232 - val_tp: 17909.0000 - val_fp: 1163.0000 - val_tn: 112791.0000 - val_fn: 6908.0000 - val_accuracy: 0.9418 - val_precision: 0.9390 - val_recall: 0.7216 - val_auc: 0.9766\n",
            "Epoch 270/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2290 - tp: 76758.0000 - fp: 10352.0000 - tn: 444859.0000 - fn: 22271.0000 - accuracy: 0.9411 - precision: 0.8812 - recall: 0.7751 - auc: 0.9707\n",
            "Epoch 00270: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2290 - tp: 76862.0000 - fp: 10364.0000 - tn: 445559.0000 - fn: 22299.0000 - accuracy: 0.9412 - precision: 0.8812 - recall: 0.7751 - auc: 0.9708 - val_loss: 0.2145 - val_tp: 18760.0000 - val_fp: 1393.0000 - val_tn: 112561.0000 - val_fn: 6057.0000 - val_accuracy: 0.9463 - val_precision: 0.9309 - val_recall: 0.7559 - val_auc: 0.9787\n",
            "Epoch 271/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2289 - tp: 76850.0000 - fp: 10394.0000 - tn: 445462.0000 - fn: 22302.0000 - accuracy: 0.9411 - precision: 0.8809 - recall: 0.7751 - auc: 0.9707\n",
            "Epoch 00271: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2289 - tp: 76859.0000 - fp: 10396.0000 - tn: 445527.0000 - fn: 22302.0000 - accuracy: 0.9411 - precision: 0.8809 - recall: 0.7751 - auc: 0.9707 - val_loss: 0.2139 - val_tp: 19813.0000 - val_fp: 2199.0000 - val_tn: 111755.0000 - val_fn: 5004.0000 - val_accuracy: 0.9481 - val_precision: 0.9001 - val_recall: 0.7984 - val_auc: 0.9774\n",
            "Epoch 272/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2285 - tp: 76928.0000 - fp: 10432.0000 - tn: 445491.0000 - fn: 22233.0000 - accuracy: 0.9412 - precision: 0.8806 - recall: 0.7758 - auc: 0.9710\n",
            "Epoch 00272: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2285 - tp: 76928.0000 - fp: 10432.0000 - tn: 445491.0000 - fn: 22233.0000 - accuracy: 0.9412 - precision: 0.8806 - recall: 0.7758 - auc: 0.9710 - val_loss: 0.2128 - val_tp: 19166.0000 - val_fp: 1654.0000 - val_tn: 112300.0000 - val_fn: 5651.0000 - val_accuracy: 0.9474 - val_precision: 0.9206 - val_recall: 0.7723 - val_auc: 0.9784\n",
            "Epoch 273/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2295 - tp: 76796.0000 - fp: 10488.0000 - tn: 445369.0000 - fn: 22355.0000 - accuracy: 0.9408 - precision: 0.8798 - recall: 0.7745 - auc: 0.9706\n",
            "Epoch 00273: val_auc did not improve from 0.97876\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2295 - tp: 76804.0000 - fp: 10489.0000 - tn: 445434.0000 - fn: 22357.0000 - accuracy: 0.9408 - precision: 0.8798 - recall: 0.7745 - auc: 0.9706 - val_loss: 0.2147 - val_tp: 19368.0000 - val_fp: 1896.0000 - val_tn: 112058.0000 - val_fn: 5449.0000 - val_accuracy: 0.9471 - val_precision: 0.9108 - val_recall: 0.7804 - val_auc: 0.9772\n",
            "Epoch 274/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2281 - tp: 76802.0000 - fp: 10408.0000 - tn: 444607.0000 - fn: 22167.0000 - accuracy: 0.9412 - precision: 0.8807 - recall: 0.7760 - auc: 0.9711\n",
            "Epoch 00274: val_auc improved from 0.97876 to 0.97880, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2281 - tp: 76952.0000 - fp: 10431.0000 - tn: 445492.0000 - fn: 22209.0000 - accuracy: 0.9412 - precision: 0.8806 - recall: 0.7760 - auc: 0.9711 - val_loss: 0.2104 - val_tp: 19332.0000 - val_fp: 1787.0000 - val_tn: 112167.0000 - val_fn: 5485.0000 - val_accuracy: 0.9476 - val_precision: 0.9154 - val_recall: 0.7790 - val_auc: 0.9788\n",
            "Epoch 275/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2282 - tp: 76648.0000 - fp: 10301.0000 - tn: 444538.0000 - fn: 22241.0000 - accuracy: 0.9412 - precision: 0.8815 - recall: 0.7751 - auc: 0.9709\n",
            "Epoch 00275: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2282 - tp: 76868.0000 - fp: 10326.0000 - tn: 445597.0000 - fn: 22293.0000 - accuracy: 0.9412 - precision: 0.8816 - recall: 0.7752 - auc: 0.9709 - val_loss: 0.2153 - val_tp: 19776.0000 - val_fp: 2334.0000 - val_tn: 111620.0000 - val_fn: 5041.0000 - val_accuracy: 0.9469 - val_precision: 0.8944 - val_recall: 0.7969 - val_auc: 0.9762\n",
            "Epoch 276/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2282 - tp: 76945.0000 - fp: 10404.0000 - tn: 445519.0000 - fn: 22216.0000 - accuracy: 0.9412 - precision: 0.8809 - recall: 0.7760 - auc: 0.9710\n",
            "Epoch 00276: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2282 - tp: 76945.0000 - fp: 10404.0000 - tn: 445519.0000 - fn: 22216.0000 - accuracy: 0.9412 - precision: 0.8809 - recall: 0.7760 - auc: 0.9710 - val_loss: 0.2129 - val_tp: 20370.0000 - val_fp: 2777.0000 - val_tn: 111177.0000 - val_fn: 4447.0000 - val_accuracy: 0.9479 - val_precision: 0.8800 - val_recall: 0.8208 - val_auc: 0.9780\n",
            "Epoch 277/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2277 - tp: 77038.0000 - fp: 10406.0000 - tn: 445453.0000 - fn: 22111.0000 - accuracy: 0.9414 - precision: 0.8810 - recall: 0.7770 - auc: 0.9711\n",
            "Epoch 00277: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2277 - tp: 77044.0000 - fp: 10407.0000 - tn: 445516.0000 - fn: 22117.0000 - accuracy: 0.9414 - precision: 0.8810 - recall: 0.7770 - auc: 0.9711 - val_loss: 0.2109 - val_tp: 20257.0000 - val_fp: 2483.0000 - val_tn: 111471.0000 - val_fn: 4560.0000 - val_accuracy: 0.9492 - val_precision: 0.8908 - val_recall: 0.8163 - val_auc: 0.9786\n",
            "Epoch 278/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2290 - tp: 76585.0000 - fp: 10451.0000 - tn: 444345.0000 - fn: 22347.0000 - accuracy: 0.9408 - precision: 0.8799 - recall: 0.7741 - auc: 0.9706\n",
            "Epoch 00278: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2290 - tp: 76757.0000 - fp: 10479.0000 - tn: 445444.0000 - fn: 22404.0000 - accuracy: 0.9408 - precision: 0.8799 - recall: 0.7741 - auc: 0.9706 - val_loss: 0.2248 - val_tp: 19566.0000 - val_fp: 2908.0000 - val_tn: 111046.0000 - val_fn: 5251.0000 - val_accuracy: 0.9412 - val_precision: 0.8706 - val_recall: 0.7884 - val_auc: 0.9741\n",
            "Epoch 279/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2285 - tp: 76779.0000 - fp: 10291.0000 - tn: 445569.0000 - fn: 22369.0000 - accuracy: 0.9412 - precision: 0.8818 - recall: 0.7744 - auc: 0.9706\n",
            "Epoch 00279: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2285 - tp: 76790.0000 - fp: 10292.0000 - tn: 445631.0000 - fn: 22371.0000 - accuracy: 0.9412 - precision: 0.8818 - recall: 0.7744 - auc: 0.9706 - val_loss: 0.2118 - val_tp: 19295.0000 - val_fp: 1729.0000 - val_tn: 112225.0000 - val_fn: 5522.0000 - val_accuracy: 0.9477 - val_precision: 0.9178 - val_recall: 0.7775 - val_auc: 0.9787\n",
            "Epoch 280/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2282 - tp: 76819.0000 - fp: 10402.0000 - tn: 445247.0000 - fn: 22284.0000 - accuracy: 0.9411 - precision: 0.8807 - recall: 0.7751 - auc: 0.9709\n",
            "Epoch 00280: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2282 - tp: 76866.0000 - fp: 10412.0000 - tn: 445511.0000 - fn: 22295.0000 - accuracy: 0.9411 - precision: 0.8807 - recall: 0.7752 - auc: 0.9709 - val_loss: 0.2154 - val_tp: 19670.0000 - val_fp: 2277.0000 - val_tn: 111677.0000 - val_fn: 5147.0000 - val_accuracy: 0.9465 - val_precision: 0.8963 - val_recall: 0.7926 - val_auc: 0.9764\n",
            "Epoch 281/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2287 - tp: 76635.0000 - fp: 10402.0000 - tn: 444588.0000 - fn: 22359.0000 - accuracy: 0.9409 - precision: 0.8805 - recall: 0.7741 - auc: 0.9706\n",
            "Epoch 00281: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2288 - tp: 76766.0000 - fp: 10443.0000 - tn: 445480.0000 - fn: 22395.0000 - accuracy: 0.9408 - precision: 0.8803 - recall: 0.7742 - auc: 0.9706 - val_loss: 0.2138 - val_tp: 20272.0000 - val_fp: 2681.0000 - val_tn: 111273.0000 - val_fn: 4545.0000 - val_accuracy: 0.9479 - val_precision: 0.8832 - val_recall: 0.8169 - val_auc: 0.9766\n",
            "Epoch 282/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2280 - tp: 76765.0000 - fp: 10262.0000 - tn: 445388.0000 - fn: 22337.0000 - accuracy: 0.9412 - precision: 0.8821 - recall: 0.7746 - auc: 0.9709\n",
            "Epoch 00282: val_auc did not improve from 0.97880\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2280 - tp: 76809.0000 - fp: 10273.0000 - tn: 445650.0000 - fn: 22352.0000 - accuracy: 0.9412 - precision: 0.8820 - recall: 0.7746 - auc: 0.9709 - val_loss: 0.2167 - val_tp: 19518.0000 - val_fp: 2040.0000 - val_tn: 111914.0000 - val_fn: 5299.0000 - val_accuracy: 0.9471 - val_precision: 0.9054 - val_recall: 0.7865 - val_auc: 0.9763\n",
            "Epoch 283/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2274 - tp: 76922.0000 - fp: 10368.0000 - tn: 445280.0000 - fn: 22182.0000 - accuracy: 0.9413 - precision: 0.8812 - recall: 0.7762 - auc: 0.9712\n",
            "Epoch 00283: val_auc improved from 0.97880 to 0.97940, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2274 - tp: 76970.0000 - fp: 10377.0000 - tn: 445546.0000 - fn: 22191.0000 - accuracy: 0.9413 - precision: 0.8812 - recall: 0.7762 - auc: 0.9712 - val_loss: 0.2091 - val_tp: 19370.0000 - val_fp: 1693.0000 - val_tn: 112261.0000 - val_fn: 5447.0000 - val_accuracy: 0.9485 - val_precision: 0.9196 - val_recall: 0.7805 - val_auc: 0.9794\n",
            "Epoch 284/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2268 - tp: 77003.0000 - fp: 10302.0000 - tn: 445621.0000 - fn: 22158.0000 - accuracy: 0.9415 - precision: 0.8820 - recall: 0.7765 - auc: 0.9715\n",
            "Epoch 00284: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2268 - tp: 77003.0000 - fp: 10302.0000 - tn: 445621.0000 - fn: 22158.0000 - accuracy: 0.9415 - precision: 0.8820 - recall: 0.7765 - auc: 0.9715 - val_loss: 0.2101 - val_tp: 19397.0000 - val_fp: 1803.0000 - val_tn: 112151.0000 - val_fn: 5420.0000 - val_accuracy: 0.9480 - val_precision: 0.9150 - val_recall: 0.7816 - val_auc: 0.9786\n",
            "Epoch 285/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2277 - tp: 76791.0000 - fp: 10371.0000 - tn: 444641.0000 - fn: 22181.0000 - accuracy: 0.9412 - precision: 0.8810 - recall: 0.7759 - auc: 0.9710\n",
            "Epoch 00285: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2277 - tp: 76937.0000 - fp: 10393.0000 - tn: 445530.0000 - fn: 22224.0000 - accuracy: 0.9412 - precision: 0.8810 - recall: 0.7759 - auc: 0.9710 - val_loss: 0.2125 - val_tp: 18889.0000 - val_fp: 1616.0000 - val_tn: 112338.0000 - val_fn: 5928.0000 - val_accuracy: 0.9456 - val_precision: 0.9212 - val_recall: 0.7611 - val_auc: 0.9786\n",
            "Epoch 286/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2269 - tp: 77028.0000 - fp: 10451.0000 - tn: 445472.0000 - fn: 22133.0000 - accuracy: 0.9413 - precision: 0.8805 - recall: 0.7768 - auc: 0.9714\n",
            "Epoch 00286: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2269 - tp: 77028.0000 - fp: 10451.0000 - tn: 445472.0000 - fn: 22133.0000 - accuracy: 0.9413 - precision: 0.8805 - recall: 0.7768 - auc: 0.9714 - val_loss: 0.2152 - val_tp: 19344.0000 - val_fp: 2052.0000 - val_tn: 111902.0000 - val_fn: 5473.0000 - val_accuracy: 0.9458 - val_precision: 0.9041 - val_recall: 0.7795 - val_auc: 0.9765\n",
            "Epoch 287/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2269 - tp: 77025.0000 - fp: 10286.0000 - tn: 445637.0000 - fn: 22136.0000 - accuracy: 0.9416 - precision: 0.8822 - recall: 0.7768 - auc: 0.9712\n",
            "Epoch 00287: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2269 - tp: 77025.0000 - fp: 10286.0000 - tn: 445637.0000 - fn: 22136.0000 - accuracy: 0.9416 - precision: 0.8822 - recall: 0.7768 - auc: 0.9712 - val_loss: 0.2130 - val_tp: 18589.0000 - val_fp: 1340.0000 - val_tn: 112614.0000 - val_fn: 6228.0000 - val_accuracy: 0.9455 - val_precision: 0.9328 - val_recall: 0.7490 - val_auc: 0.9791\n",
            "Epoch 288/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2272 - tp: 76897.0000 - fp: 10398.0000 - tn: 445525.0000 - fn: 22264.0000 - accuracy: 0.9412 - precision: 0.8809 - recall: 0.7755 - auc: 0.9712\n",
            "Epoch 00288: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2272 - tp: 76897.0000 - fp: 10398.0000 - tn: 445525.0000 - fn: 22264.0000 - accuracy: 0.9412 - precision: 0.8809 - recall: 0.7755 - auc: 0.9712 - val_loss: 0.2146 - val_tp: 18530.0000 - val_fp: 1318.0000 - val_tn: 112636.0000 - val_fn: 6287.0000 - val_accuracy: 0.9452 - val_precision: 0.9336 - val_recall: 0.7467 - val_auc: 0.9788\n",
            "Epoch 289/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2276 - tp: 76646.0000 - fp: 10421.0000 - tn: 444416.0000 - fn: 22245.0000 - accuracy: 0.9410 - precision: 0.8803 - recall: 0.7751 - auc: 0.9710\n",
            "Epoch 00289: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2277 - tp: 76851.0000 - fp: 10443.0000 - tn: 445480.0000 - fn: 22310.0000 - accuracy: 0.9410 - precision: 0.8804 - recall: 0.7750 - auc: 0.9710 - val_loss: 0.2133 - val_tp: 20427.0000 - val_fp: 2998.0000 - val_tn: 110956.0000 - val_fn: 4390.0000 - val_accuracy: 0.9468 - val_precision: 0.8720 - val_recall: 0.8231 - val_auc: 0.9775\n",
            "Epoch 290/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2266 - tp: 76912.0000 - fp: 10307.0000 - tn: 444918.0000 - fn: 22103.0000 - accuracy: 0.9415 - precision: 0.8818 - recall: 0.7768 - auc: 0.9712\n",
            "Epoch 00290: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2266 - tp: 77028.0000 - fp: 10320.0000 - tn: 445603.0000 - fn: 22133.0000 - accuracy: 0.9415 - precision: 0.8819 - recall: 0.7768 - auc: 0.9712 - val_loss: 0.2145 - val_tp: 18932.0000 - val_fp: 1500.0000 - val_tn: 112454.0000 - val_fn: 5885.0000 - val_accuracy: 0.9468 - val_precision: 0.9266 - val_recall: 0.7629 - val_auc: 0.9780\n",
            "Epoch 291/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2264 - tp: 76992.0000 - fp: 10505.0000 - tn: 444723.0000 - fn: 22020.0000 - accuracy: 0.9413 - precision: 0.8799 - recall: 0.7776 - auc: 0.9714\n",
            "Epoch 00291: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 21s 9ms/step - loss: 0.2264 - tp: 77109.0000 - fp: 10511.0000 - tn: 445412.0000 - fn: 22052.0000 - accuracy: 0.9413 - precision: 0.8800 - recall: 0.7776 - auc: 0.9714 - val_loss: 0.2103 - val_tp: 19735.0000 - val_fp: 2033.0000 - val_tn: 111921.0000 - val_fn: 5082.0000 - val_accuracy: 0.9487 - val_precision: 0.9066 - val_recall: 0.7952 - val_auc: 0.9782\n",
            "Epoch 292/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2255 - tp: 77039.0000 - fp: 10383.0000 - tn: 445480.0000 - fn: 22106.0000 - accuracy: 0.9415 - precision: 0.8812 - recall: 0.7770 - auc: 0.9718\n",
            "Epoch 00292: val_auc did not improve from 0.97940\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2255 - tp: 77049.0000 - fp: 10383.0000 - tn: 445540.0000 - fn: 22112.0000 - accuracy: 0.9415 - precision: 0.8812 - recall: 0.7770 - auc: 0.9718 - val_loss: 0.2103 - val_tp: 20480.0000 - val_fp: 2986.0000 - val_tn: 110968.0000 - val_fn: 4337.0000 - val_accuracy: 0.9472 - val_precision: 0.8728 - val_recall: 0.8252 - val_auc: 0.9785\n",
            "Epoch 293/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2256 - tp: 77142.0000 - fp: 10409.0000 - tn: 445255.0000 - fn: 21946.0000 - accuracy: 0.9417 - precision: 0.8811 - recall: 0.7785 - auc: 0.9717\n",
            "Epoch 00293: val_auc improved from 0.97940 to 0.97981, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2257 - tp: 77185.0000 - fp: 10413.0000 - tn: 445510.0000 - fn: 21976.0000 - accuracy: 0.9417 - precision: 0.8811 - recall: 0.7784 - auc: 0.9717 - val_loss: 0.2074 - val_tp: 19268.0000 - val_fp: 1547.0000 - val_tn: 112407.0000 - val_fn: 5549.0000 - val_accuracy: 0.9489 - val_precision: 0.9257 - val_recall: 0.7764 - val_auc: 0.9798\n",
            "Epoch 294/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2266 - tp: 77050.0000 - fp: 10346.0000 - tn: 445524.0000 - fn: 22088.0000 - accuracy: 0.9416 - precision: 0.8816 - recall: 0.7772 - auc: 0.9712\n",
            "Epoch 00294: val_auc did not improve from 0.97981\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2266 - tp: 77067.0000 - fp: 10346.0000 - tn: 445577.0000 - fn: 22094.0000 - accuracy: 0.9416 - precision: 0.8816 - recall: 0.7772 - auc: 0.9712 - val_loss: 0.2096 - val_tp: 20725.0000 - val_fp: 3082.0000 - val_tn: 110872.0000 - val_fn: 4092.0000 - val_accuracy: 0.9483 - val_precision: 0.8705 - val_recall: 0.8351 - val_auc: 0.9786\n",
            "Epoch 295/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2263 - tp: 77032.0000 - fp: 10329.0000 - tn: 445535.0000 - fn: 22112.0000 - accuracy: 0.9415 - precision: 0.8818 - recall: 0.7770 - auc: 0.9714\n",
            "Epoch 00295: val_auc improved from 0.97981 to 0.97989, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2263 - tp: 77046.0000 - fp: 10331.0000 - tn: 445592.0000 - fn: 22115.0000 - accuracy: 0.9415 - precision: 0.8818 - recall: 0.7770 - auc: 0.9714 - val_loss: 0.2104 - val_tp: 18622.0000 - val_fp: 1272.0000 - val_tn: 112682.0000 - val_fn: 6195.0000 - val_accuracy: 0.9462 - val_precision: 0.9361 - val_recall: 0.7504 - val_auc: 0.9799\n",
            "Epoch 296/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2258 - tp: 77120.0000 - fp: 10318.0000 - tn: 444916.0000 - fn: 21886.0000 - accuracy: 0.9419 - precision: 0.8820 - recall: 0.7789 - auc: 0.9716\n",
            "Epoch 00296: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2258 - tp: 77236.0000 - fp: 10329.0000 - tn: 445594.0000 - fn: 21925.0000 - accuracy: 0.9419 - precision: 0.8820 - recall: 0.7789 - auc: 0.9716 - val_loss: 0.2138 - val_tp: 20519.0000 - val_fp: 2895.0000 - val_tn: 111059.0000 - val_fn: 4298.0000 - val_accuracy: 0.9482 - val_precision: 0.8764 - val_recall: 0.8268 - val_auc: 0.9781\n",
            "Epoch 297/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2255 - tp: 77228.0000 - fp: 10333.0000 - tn: 445119.0000 - fn: 21816.0000 - accuracy: 0.9420 - precision: 0.8820 - recall: 0.7797 - auc: 0.9717\n",
            "Epoch 00297: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2256 - tp: 77316.0000 - fp: 10343.0000 - tn: 445580.0000 - fn: 21845.0000 - accuracy: 0.9420 - precision: 0.8820 - recall: 0.7797 - auc: 0.9716 - val_loss: 0.2185 - val_tp: 21132.0000 - val_fp: 4046.0000 - val_tn: 109908.0000 - val_fn: 3685.0000 - val_accuracy: 0.9443 - val_precision: 0.8393 - val_recall: 0.8515 - val_auc: 0.9770\n",
            "Epoch 298/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2264 - tp: 77048.0000 - fp: 10465.0000 - tn: 444339.0000 - fn: 21876.0000 - accuracy: 0.9416 - precision: 0.8804 - recall: 0.7789 - auc: 0.9713\n",
            "Epoch 00298: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2264 - tp: 77238.0000 - fp: 10492.0000 - tn: 445431.0000 - fn: 21923.0000 - accuracy: 0.9416 - precision: 0.8804 - recall: 0.7789 - auc: 0.9713 - val_loss: 0.2135 - val_tp: 18685.0000 - val_fp: 1320.0000 - val_tn: 112634.0000 - val_fn: 6132.0000 - val_accuracy: 0.9463 - val_precision: 0.9340 - val_recall: 0.7529 - val_auc: 0.9786\n",
            "Epoch 299/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2254 - tp: 77208.0000 - fp: 10431.0000 - tn: 445428.0000 - fn: 21941.0000 - accuracy: 0.9417 - precision: 0.8810 - recall: 0.7787 - auc: 0.9717\n",
            "Epoch 00299: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2254 - tp: 77218.0000 - fp: 10431.0000 - tn: 445492.0000 - fn: 21943.0000 - accuracy: 0.9417 - precision: 0.8810 - recall: 0.7787 - auc: 0.9717 - val_loss: 0.2109 - val_tp: 19917.0000 - val_fp: 2355.0000 - val_tn: 111599.0000 - val_fn: 4900.0000 - val_accuracy: 0.9477 - val_precision: 0.8943 - val_recall: 0.8026 - val_auc: 0.9781\n",
            "Epoch 300/2000\n",
            "2163/2169 [============================>.] - ETA: 0s - loss: 0.2258 - tp: 77012.0000 - fp: 10379.0000 - tn: 444429.0000 - fn: 21908.0000 - accuracy: 0.9417 - precision: 0.8812 - recall: 0.7785 - auc: 0.9715\n",
            "Epoch 00300: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2259 - tp: 77199.0000 - fp: 10413.0000 - tn: 445510.0000 - fn: 21962.0000 - accuracy: 0.9417 - precision: 0.8811 - recall: 0.7785 - auc: 0.9715 - val_loss: 0.2100 - val_tp: 19404.0000 - val_fp: 1895.0000 - val_tn: 112059.0000 - val_fn: 5413.0000 - val_accuracy: 0.9473 - val_precision: 0.9110 - val_recall: 0.7819 - val_auc: 0.9787\n",
            "Epoch 301/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2250 - tp: 77158.0000 - fp: 10354.0000 - tn: 445297.0000 - fn: 21943.0000 - accuracy: 0.9418 - precision: 0.8817 - recall: 0.7786 - auc: 0.9718\n",
            "Epoch 00301: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2250 - tp: 77206.0000 - fp: 10360.0000 - tn: 445563.0000 - fn: 21955.0000 - accuracy: 0.9418 - precision: 0.8817 - recall: 0.7786 - auc: 0.9718 - val_loss: 0.2120 - val_tp: 21046.0000 - val_fp: 3468.0000 - val_tn: 110486.0000 - val_fn: 3771.0000 - val_accuracy: 0.9478 - val_precision: 0.8585 - val_recall: 0.8480 - val_auc: 0.9789\n",
            "Epoch 302/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2251 - tp: 77160.0000 - fp: 10437.0000 - tn: 445216.0000 - fn: 21939.0000 - accuracy: 0.9416 - precision: 0.8809 - recall: 0.7786 - auc: 0.9718\n",
            "Epoch 00302: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2251 - tp: 77208.0000 - fp: 10445.0000 - tn: 445478.0000 - fn: 21953.0000 - accuracy: 0.9416 - precision: 0.8808 - recall: 0.7786 - auc: 0.9718 - val_loss: 0.2236 - val_tp: 18074.0000 - val_fp: 1217.0000 - val_tn: 112737.0000 - val_fn: 6743.0000 - val_accuracy: 0.9426 - val_precision: 0.9369 - val_recall: 0.7283 - val_auc: 0.9776\n",
            "Epoch 303/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2254 - tp: 77163.0000 - fp: 10272.0000 - tn: 445168.0000 - fn: 21893.0000 - accuracy: 0.9420 - precision: 0.8825 - recall: 0.7790 - auc: 0.9715\n",
            "Epoch 00303: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2254 - tp: 77246.0000 - fp: 10285.0000 - tn: 445638.0000 - fn: 21915.0000 - accuracy: 0.9420 - precision: 0.8825 - recall: 0.7790 - auc: 0.9715 - val_loss: 0.2182 - val_tp: 20938.0000 - val_fp: 3697.0000 - val_tn: 110257.0000 - val_fn: 3879.0000 - val_accuracy: 0.9454 - val_precision: 0.8499 - val_recall: 0.8437 - val_auc: 0.9772\n",
            "Epoch 304/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2254 - tp: 77186.0000 - fp: 10269.0000 - tn: 445392.0000 - fn: 21905.0000 - accuracy: 0.9420 - precision: 0.8826 - recall: 0.7789 - auc: 0.9715\n",
            "Epoch 00304: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2254 - tp: 77245.0000 - fp: 10275.0000 - tn: 445648.0000 - fn: 21916.0000 - accuracy: 0.9420 - precision: 0.8826 - recall: 0.7790 - auc: 0.9716 - val_loss: 0.2106 - val_tp: 20164.0000 - val_fp: 2754.0000 - val_tn: 111200.0000 - val_fn: 4653.0000 - val_accuracy: 0.9466 - val_precision: 0.8798 - val_recall: 0.8125 - val_auc: 0.9784\n",
            "Epoch 305/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2248 - tp: 77193.0000 - fp: 10278.0000 - tn: 444730.0000 - fn: 21783.0000 - accuracy: 0.9421 - precision: 0.8825 - recall: 0.7799 - auc: 0.9717\n",
            "Epoch 00305: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2248 - tp: 77332.0000 - fp: 10302.0000 - tn: 445621.0000 - fn: 21829.0000 - accuracy: 0.9421 - precision: 0.8824 - recall: 0.7799 - auc: 0.9717 - val_loss: 0.2141 - val_tp: 18333.0000 - val_fp: 1197.0000 - val_tn: 112757.0000 - val_fn: 6484.0000 - val_accuracy: 0.9446 - val_precision: 0.9387 - val_recall: 0.7387 - val_auc: 0.9788\n",
            "Epoch 306/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2246 - tp: 77236.0000 - fp: 10212.0000 - tn: 445647.0000 - fn: 21913.0000 - accuracy: 0.9421 - precision: 0.8832 - recall: 0.7790 - auc: 0.9719\n",
            "Epoch 00306: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2246 - tp: 77246.0000 - fp: 10214.0000 - tn: 445709.0000 - fn: 21915.0000 - accuracy: 0.9421 - precision: 0.8832 - recall: 0.7790 - auc: 0.9719 - val_loss: 0.2052 - val_tp: 19993.0000 - val_fp: 2140.0000 - val_tn: 111814.0000 - val_fn: 4824.0000 - val_accuracy: 0.9498 - val_precision: 0.9033 - val_recall: 0.8056 - val_auc: 0.9797\n",
            "Epoch 307/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2244 - tp: 77250.0000 - fp: 10213.0000 - tn: 445219.0000 - fn: 21814.0000 - accuracy: 0.9422 - precision: 0.8832 - recall: 0.7798 - auc: 0.9718\n",
            "Epoch 00307: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2244 - tp: 77324.0000 - fp: 10229.0000 - tn: 445694.0000 - fn: 21837.0000 - accuracy: 0.9422 - precision: 0.8832 - recall: 0.7798 - auc: 0.9718 - val_loss: 0.2063 - val_tp: 20026.0000 - val_fp: 2247.0000 - val_tn: 111707.0000 - val_fn: 4791.0000 - val_accuracy: 0.9493 - val_precision: 0.8991 - val_recall: 0.8069 - val_auc: 0.9796\n",
            "Epoch 308/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2242 - tp: 77186.0000 - fp: 10341.0000 - tn: 445518.0000 - fn: 21963.0000 - accuracy: 0.9418 - precision: 0.8819 - recall: 0.7785 - auc: 0.9720\n",
            "Epoch 00308: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2242 - tp: 77195.0000 - fp: 10341.0000 - tn: 445582.0000 - fn: 21966.0000 - accuracy: 0.9418 - precision: 0.8819 - recall: 0.7785 - auc: 0.9720 - val_loss: 0.2191 - val_tp: 18536.0000 - val_fp: 1509.0000 - val_tn: 112445.0000 - val_fn: 6281.0000 - val_accuracy: 0.9439 - val_precision: 0.9247 - val_recall: 0.7469 - val_auc: 0.9770\n",
            "Epoch 309/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2245 - tp: 77321.0000 - fp: 10319.0000 - tn: 445544.0000 - fn: 21824.0000 - accuracy: 0.9421 - precision: 0.8823 - recall: 0.7799 - auc: 0.9719\n",
            "Epoch 00309: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2245 - tp: 77332.0000 - fp: 10320.0000 - tn: 445603.0000 - fn: 21829.0000 - accuracy: 0.9421 - precision: 0.8823 - recall: 0.7799 - auc: 0.9719 - val_loss: 0.2073 - val_tp: 19196.0000 - val_fp: 1547.0000 - val_tn: 112407.0000 - val_fn: 5621.0000 - val_accuracy: 0.9483 - val_precision: 0.9254 - val_recall: 0.7735 - val_auc: 0.9797\n",
            "Epoch 310/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2247 - tp: 77268.0000 - fp: 10396.0000 - tn: 445467.0000 - fn: 21877.0000 - accuracy: 0.9419 - precision: 0.8814 - recall: 0.7793 - auc: 0.9719\n",
            "Epoch 00310: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2247 - tp: 77277.0000 - fp: 10398.0000 - tn: 445525.0000 - fn: 21884.0000 - accuracy: 0.9418 - precision: 0.8814 - recall: 0.7793 - auc: 0.9718 - val_loss: 0.2111 - val_tp: 19476.0000 - val_fp: 1920.0000 - val_tn: 112034.0000 - val_fn: 5341.0000 - val_accuracy: 0.9477 - val_precision: 0.9103 - val_recall: 0.7848 - val_auc: 0.9779\n",
            "Epoch 311/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2251 - tp: 77290.0000 - fp: 10330.0000 - tn: 445529.0000 - fn: 21859.0000 - accuracy: 0.9420 - precision: 0.8821 - recall: 0.7795 - auc: 0.9716\n",
            "Epoch 00311: val_auc did not improve from 0.97989\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2251 - tp: 77301.0000 - fp: 10331.0000 - tn: 445592.0000 - fn: 21860.0000 - accuracy: 0.9420 - precision: 0.8821 - recall: 0.7796 - auc: 0.9716 - val_loss: 0.2064 - val_tp: 19623.0000 - val_fp: 1880.0000 - val_tn: 112074.0000 - val_fn: 5194.0000 - val_accuracy: 0.9490 - val_precision: 0.9126 - val_recall: 0.7907 - val_auc: 0.9793\n",
            "Epoch 312/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2242 - tp: 77407.0000 - fp: 10329.0000 - tn: 445594.0000 - fn: 21754.0000 - accuracy: 0.9422 - precision: 0.8823 - recall: 0.7806 - auc: 0.9719\n",
            "Epoch 00312: val_auc improved from 0.97989 to 0.98023, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2242 - tp: 77407.0000 - fp: 10329.0000 - tn: 445594.0000 - fn: 21754.0000 - accuracy: 0.9422 - precision: 0.8823 - recall: 0.7806 - auc: 0.9719 - val_loss: 0.2048 - val_tp: 19507.0000 - val_fp: 1676.0000 - val_tn: 112278.0000 - val_fn: 5310.0000 - val_accuracy: 0.9497 - val_precision: 0.9209 - val_recall: 0.7860 - val_auc: 0.9802\n",
            "Epoch 313/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2241 - tp: 77126.0000 - fp: 10211.0000 - tn: 445435.0000 - fn: 21980.0000 - accuracy: 0.9420 - precision: 0.8831 - recall: 0.7782 - auc: 0.9719\n",
            "Epoch 00313: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2241 - tp: 77167.0000 - fp: 10218.0000 - tn: 445705.0000 - fn: 21994.0000 - accuracy: 0.9420 - precision: 0.8831 - recall: 0.7782 - auc: 0.9719 - val_loss: 0.2126 - val_tp: 18545.0000 - val_fp: 1241.0000 - val_tn: 112713.0000 - val_fn: 6272.0000 - val_accuracy: 0.9459 - val_precision: 0.9373 - val_recall: 0.7473 - val_auc: 0.9788\n",
            "Epoch 314/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2240 - tp: 77314.0000 - fp: 10302.0000 - tn: 444942.0000 - fn: 21682.0000 - accuracy: 0.9423 - precision: 0.8824 - recall: 0.7810 - auc: 0.9720\n",
            "Epoch 00314: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2240 - tp: 77448.0000 - fp: 10314.0000 - tn: 445609.0000 - fn: 21713.0000 - accuracy: 0.9423 - precision: 0.8825 - recall: 0.7810 - auc: 0.9720 - val_loss: 0.2094 - val_tp: 20160.0000 - val_fp: 2499.0000 - val_tn: 111455.0000 - val_fn: 4657.0000 - val_accuracy: 0.9484 - val_precision: 0.8897 - val_recall: 0.8123 - val_auc: 0.9781\n",
            "Epoch 315/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2241 - tp: 77162.0000 - fp: 10258.0000 - tn: 444961.0000 - fn: 21859.0000 - accuracy: 0.9421 - precision: 0.8827 - recall: 0.7792 - auc: 0.9719\n",
            "Epoch 00315: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2241 - tp: 77275.0000 - fp: 10270.0000 - tn: 445653.0000 - fn: 21886.0000 - accuracy: 0.9421 - precision: 0.8827 - recall: 0.7793 - auc: 0.9720 - val_loss: 0.2092 - val_tp: 19572.0000 - val_fp: 1914.0000 - val_tn: 112040.0000 - val_fn: 5245.0000 - val_accuracy: 0.9484 - val_precision: 0.9109 - val_recall: 0.7887 - val_auc: 0.9788\n",
            "Epoch 316/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2234 - tp: 77416.0000 - fp: 10309.0000 - tn: 445614.0000 - fn: 21745.0000 - accuracy: 0.9423 - precision: 0.8825 - recall: 0.7807 - auc: 0.9721\n",
            "Epoch 00316: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2234 - tp: 77416.0000 - fp: 10309.0000 - tn: 445614.0000 - fn: 21745.0000 - accuracy: 0.9423 - precision: 0.8825 - recall: 0.7807 - auc: 0.9721 - val_loss: 0.2090 - val_tp: 21071.0000 - val_fp: 3392.0000 - val_tn: 110562.0000 - val_fn: 3746.0000 - val_accuracy: 0.9486 - val_precision: 0.8613 - val_recall: 0.8491 - val_auc: 0.9793\n",
            "Epoch 317/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2240 - tp: 77275.0000 - fp: 10253.0000 - tn: 445194.0000 - fn: 21774.0000 - accuracy: 0.9422 - precision: 0.8829 - recall: 0.7802 - auc: 0.9719\n",
            "Epoch 00317: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2239 - tp: 77367.0000 - fp: 10259.0000 - tn: 445664.0000 - fn: 21794.0000 - accuracy: 0.9423 - precision: 0.8829 - recall: 0.7802 - auc: 0.9720 - val_loss: 0.2104 - val_tp: 19370.0000 - val_fp: 1917.0000 - val_tn: 112037.0000 - val_fn: 5447.0000 - val_accuracy: 0.9469 - val_precision: 0.9099 - val_recall: 0.7805 - val_auc: 0.9786\n",
            "Epoch 318/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2231 - tp: 77553.0000 - fp: 10265.0000 - tn: 445597.0000 - fn: 21593.0000 - accuracy: 0.9426 - precision: 0.8831 - recall: 0.7822 - auc: 0.9723\n",
            "Epoch 00318: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2231 - tp: 77564.0000 - fp: 10266.0000 - tn: 445657.0000 - fn: 21597.0000 - accuracy: 0.9426 - precision: 0.8831 - recall: 0.7822 - auc: 0.9723 - val_loss: 0.2122 - val_tp: 18708.0000 - val_fp: 1371.0000 - val_tn: 112583.0000 - val_fn: 6109.0000 - val_accuracy: 0.9461 - val_precision: 0.9317 - val_recall: 0.7538 - val_auc: 0.9800\n",
            "Epoch 319/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2236 - tp: 77443.0000 - fp: 10300.0000 - tn: 445623.0000 - fn: 21718.0000 - accuracy: 0.9423 - precision: 0.8826 - recall: 0.7810 - auc: 0.9720\n",
            "Epoch 00319: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2236 - tp: 77443.0000 - fp: 10300.0000 - tn: 445623.0000 - fn: 21718.0000 - accuracy: 0.9423 - precision: 0.8826 - recall: 0.7810 - auc: 0.9720 - val_loss: 0.2095 - val_tp: 19568.0000 - val_fp: 1962.0000 - val_tn: 111992.0000 - val_fn: 5249.0000 - val_accuracy: 0.9480 - val_precision: 0.9089 - val_recall: 0.7885 - val_auc: 0.9790\n",
            "Epoch 320/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2231 - tp: 77438.0000 - fp: 10220.0000 - tn: 445642.0000 - fn: 21708.0000 - accuracy: 0.9425 - precision: 0.8834 - recall: 0.7811 - auc: 0.9721\n",
            "Epoch 00320: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2231 - tp: 77448.0000 - fp: 10221.0000 - tn: 445702.0000 - fn: 21713.0000 - accuracy: 0.9425 - precision: 0.8834 - recall: 0.7810 - auc: 0.9721 - val_loss: 0.2072 - val_tp: 19808.0000 - val_fp: 2125.0000 - val_tn: 111829.0000 - val_fn: 5009.0000 - val_accuracy: 0.9486 - val_precision: 0.9031 - val_recall: 0.7982 - val_auc: 0.9793\n",
            "Epoch 321/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2235 - tp: 77161.0000 - fp: 10246.0000 - tn: 444749.0000 - fn: 21828.0000 - accuracy: 0.9421 - precision: 0.8828 - recall: 0.7795 - auc: 0.9721\n",
            "Epoch 00321: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2235 - tp: 77291.0000 - fp: 10256.0000 - tn: 445667.0000 - fn: 21870.0000 - accuracy: 0.9421 - precision: 0.8829 - recall: 0.7794 - auc: 0.9721 - val_loss: 0.2062 - val_tp: 19388.0000 - val_fp: 1668.0000 - val_tn: 112286.0000 - val_fn: 5429.0000 - val_accuracy: 0.9489 - val_precision: 0.9208 - val_recall: 0.7812 - val_auc: 0.9800\n",
            "Epoch 322/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2230 - tp: 77441.0000 - fp: 10260.0000 - tn: 445599.0000 - fn: 21708.0000 - accuracy: 0.9424 - precision: 0.8830 - recall: 0.7811 - auc: 0.9722\n",
            "Epoch 00322: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2230 - tp: 77448.0000 - fp: 10262.0000 - tn: 445661.0000 - fn: 21713.0000 - accuracy: 0.9424 - precision: 0.8830 - recall: 0.7810 - auc: 0.9721 - val_loss: 0.2048 - val_tp: 20031.0000 - val_fp: 2095.0000 - val_tn: 111859.0000 - val_fn: 4786.0000 - val_accuracy: 0.9504 - val_precision: 0.9053 - val_recall: 0.8071 - val_auc: 0.9799\n",
            "Epoch 323/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2229 - tp: 77531.0000 - fp: 10202.0000 - tn: 445653.0000 - fn: 21622.0000 - accuracy: 0.9427 - precision: 0.8837 - recall: 0.7819 - auc: 0.9724\n",
            "Epoch 00323: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2229 - tp: 77536.0000 - fp: 10203.0000 - tn: 445720.0000 - fn: 21625.0000 - accuracy: 0.9427 - precision: 0.8837 - recall: 0.7819 - auc: 0.9724 - val_loss: 0.2043 - val_tp: 19587.0000 - val_fp: 1838.0000 - val_tn: 112116.0000 - val_fn: 5230.0000 - val_accuracy: 0.9491 - val_precision: 0.9142 - val_recall: 0.7893 - val_auc: 0.9802\n",
            "Epoch 324/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2231 - tp: 77182.0000 - fp: 10424.0000 - tn: 444824.0000 - fn: 21810.0000 - accuracy: 0.9418 - precision: 0.8810 - recall: 0.7797 - auc: 0.9722\n",
            "Epoch 00324: val_auc did not improve from 0.98023\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2231 - tp: 77308.0000 - fp: 10439.0000 - tn: 445484.0000 - fn: 21853.0000 - accuracy: 0.9418 - precision: 0.8810 - recall: 0.7796 - auc: 0.9722 - val_loss: 0.2060 - val_tp: 19797.0000 - val_fp: 2125.0000 - val_tn: 111829.0000 - val_fn: 5020.0000 - val_accuracy: 0.9485 - val_precision: 0.9031 - val_recall: 0.7977 - val_auc: 0.9796\n",
            "Epoch 325/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2233 - tp: 77339.0000 - fp: 10195.0000 - tn: 445450.0000 - fn: 21768.0000 - accuracy: 0.9424 - precision: 0.8835 - recall: 0.7804 - auc: 0.9720\n",
            "Epoch 00325: val_auc improved from 0.98023 to 0.98052, saving model to ./best_model.h5\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2234 - tp: 77375.0000 - fp: 10207.0000 - tn: 445716.0000 - fn: 21786.0000 - accuracy: 0.9424 - precision: 0.8835 - recall: 0.7803 - auc: 0.9720 - val_loss: 0.2053 - val_tp: 19361.0000 - val_fp: 1596.0000 - val_tn: 112358.0000 - val_fn: 5456.0000 - val_accuracy: 0.9492 - val_precision: 0.9238 - val_recall: 0.7802 - val_auc: 0.9805\n",
            "Epoch 326/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2223 - tp: 77511.0000 - fp: 10340.0000 - tn: 445583.0000 - fn: 21650.0000 - accuracy: 0.9424 - precision: 0.8823 - recall: 0.7817 - auc: 0.9725\n",
            "Epoch 00326: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2223 - tp: 77511.0000 - fp: 10340.0000 - tn: 445583.0000 - fn: 21650.0000 - accuracy: 0.9424 - precision: 0.8823 - recall: 0.7817 - auc: 0.9725 - val_loss: 0.2035 - val_tp: 20000.0000 - val_fp: 2092.0000 - val_tn: 111862.0000 - val_fn: 4817.0000 - val_accuracy: 0.9502 - val_precision: 0.9053 - val_recall: 0.8059 - val_auc: 0.9801\n",
            "Epoch 327/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2227 - tp: 77408.0000 - fp: 10277.0000 - tn: 445377.0000 - fn: 21690.0000 - accuracy: 0.9424 - precision: 0.8828 - recall: 0.7811 - auc: 0.9721\n",
            "Epoch 00327: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2227 - tp: 77452.0000 - fp: 10282.0000 - tn: 445641.0000 - fn: 21709.0000 - accuracy: 0.9424 - precision: 0.8828 - recall: 0.7811 - auc: 0.9721 - val_loss: 0.2035 - val_tp: 20199.0000 - val_fp: 2247.0000 - val_tn: 111707.0000 - val_fn: 4618.0000 - val_accuracy: 0.9505 - val_precision: 0.8999 - val_recall: 0.8139 - val_auc: 0.9800\n",
            "Epoch 328/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2224 - tp: 77538.0000 - fp: 10245.0000 - tn: 445620.0000 - fn: 21605.0000 - accuracy: 0.9426 - precision: 0.8833 - recall: 0.7821 - auc: 0.9723\n",
            "Epoch 00328: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2224 - tp: 77550.0000 - fp: 10245.0000 - tn: 445678.0000 - fn: 21611.0000 - accuracy: 0.9426 - precision: 0.8833 - recall: 0.7821 - auc: 0.9723 - val_loss: 0.2044 - val_tp: 19359.0000 - val_fp: 1553.0000 - val_tn: 112401.0000 - val_fn: 5458.0000 - val_accuracy: 0.9495 - val_precision: 0.9257 - val_recall: 0.7801 - val_auc: 0.9803\n",
            "Epoch 329/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2229 - tp: 77354.0000 - fp: 10516.0000 - tn: 445407.0000 - fn: 21807.0000 - accuracy: 0.9418 - precision: 0.8803 - recall: 0.7801 - auc: 0.9722\n",
            "Epoch 00329: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 19s 9ms/step - loss: 0.2229 - tp: 77354.0000 - fp: 10516.0000 - tn: 445407.0000 - fn: 21807.0000 - accuracy: 0.9418 - precision: 0.8803 - recall: 0.7801 - auc: 0.9722 - val_loss: 0.2060 - val_tp: 20578.0000 - val_fp: 2798.0000 - val_tn: 111156.0000 - val_fn: 4239.0000 - val_accuracy: 0.9493 - val_precision: 0.8803 - val_recall: 0.8292 - val_auc: 0.9793\n",
            "Epoch 330/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2218 - tp: 77669.0000 - fp: 10225.0000 - tn: 445413.0000 - fn: 21445.0000 - accuracy: 0.9429 - precision: 0.8837 - recall: 0.7836 - auc: 0.9724\n",
            "Epoch 00330: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2218 - tp: 77707.0000 - fp: 10234.0000 - tn: 445689.0000 - fn: 21454.0000 - accuracy: 0.9429 - precision: 0.8836 - recall: 0.7836 - auc: 0.9724 - val_loss: 0.2091 - val_tp: 20290.0000 - val_fp: 2562.0000 - val_tn: 111392.0000 - val_fn: 4527.0000 - val_accuracy: 0.9489 - val_precision: 0.8879 - val_recall: 0.8176 - val_auc: 0.9780\n",
            "Epoch 331/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2224 - tp: 77428.0000 - fp: 10312.0000 - tn: 444940.0000 - fn: 21560.0000 - accuracy: 0.9425 - precision: 0.8825 - recall: 0.7822 - auc: 0.9723\n",
            "Epoch 00331: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2224 - tp: 77557.0000 - fp: 10320.0000 - tn: 445603.0000 - fn: 21604.0000 - accuracy: 0.9425 - precision: 0.8826 - recall: 0.7821 - auc: 0.9723 - val_loss: 0.2069 - val_tp: 18995.0000 - val_fp: 1390.0000 - val_tn: 112564.0000 - val_fn: 5822.0000 - val_accuracy: 0.9480 - val_precision: 0.9318 - val_recall: 0.7654 - val_auc: 0.9802\n",
            "Epoch 332/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2225 - tp: 77542.0000 - fp: 10320.0000 - tn: 445603.0000 - fn: 21619.0000 - accuracy: 0.9425 - precision: 0.8825 - recall: 0.7820 - auc: 0.9723\n",
            "Epoch 00332: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2225 - tp: 77542.0000 - fp: 10320.0000 - tn: 445603.0000 - fn: 21619.0000 - accuracy: 0.9425 - precision: 0.8825 - recall: 0.7820 - auc: 0.9723 - val_loss: 0.2044 - val_tp: 19414.0000 - val_fp: 1663.0000 - val_tn: 112291.0000 - val_fn: 5403.0000 - val_accuracy: 0.9491 - val_precision: 0.9211 - val_recall: 0.7823 - val_auc: 0.9800\n",
            "Epoch 333/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2227 - tp: 77455.0000 - fp: 10292.0000 - tn: 445565.0000 - fn: 21696.0000 - accuracy: 0.9424 - precision: 0.8827 - recall: 0.7812 - auc: 0.9722\n",
            "Epoch 00333: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2227 - tp: 77461.0000 - fp: 10294.0000 - tn: 445629.0000 - fn: 21700.0000 - accuracy: 0.9424 - precision: 0.8827 - recall: 0.7812 - auc: 0.9722 - val_loss: 0.2081 - val_tp: 18952.0000 - val_fp: 1462.0000 - val_tn: 112492.0000 - val_fn: 5865.0000 - val_accuracy: 0.9472 - val_precision: 0.9284 - val_recall: 0.7637 - val_auc: 0.9797\n",
            "Epoch 334/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2222 - tp: 77415.0000 - fp: 10148.0000 - tn: 445087.0000 - fn: 21590.0000 - accuracy: 0.9427 - precision: 0.8841 - recall: 0.7819 - auc: 0.9723\n",
            "Epoch 00334: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2223 - tp: 77538.0000 - fp: 10169.0000 - tn: 445754.0000 - fn: 21623.0000 - accuracy: 0.9427 - precision: 0.8841 - recall: 0.7819 - auc: 0.9723 - val_loss: 0.2066 - val_tp: 19153.0000 - val_fp: 1614.0000 - val_tn: 112340.0000 - val_fn: 5664.0000 - val_accuracy: 0.9476 - val_precision: 0.9223 - val_recall: 0.7718 - val_auc: 0.9799\n",
            "Epoch 335/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2219 - tp: 77450.0000 - fp: 10300.0000 - tn: 445128.0000 - fn: 21618.0000 - accuracy: 0.9424 - precision: 0.8826 - recall: 0.7818 - auc: 0.9726\n",
            "Epoch 00335: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2218 - tp: 77526.0000 - fp: 10309.0000 - tn: 445614.0000 - fn: 21635.0000 - accuracy: 0.9425 - precision: 0.8826 - recall: 0.7818 - auc: 0.9726 - val_loss: 0.2052 - val_tp: 20091.0000 - val_fp: 2361.0000 - val_tn: 111593.0000 - val_fn: 4726.0000 - val_accuracy: 0.9489 - val_precision: 0.8948 - val_recall: 0.8096 - val_auc: 0.9792\n",
            "Epoch 336/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2225 - tp: 77475.0000 - fp: 10312.0000 - tn: 445546.0000 - fn: 21675.0000 - accuracy: 0.9424 - precision: 0.8825 - recall: 0.7814 - auc: 0.9722\n",
            "Epoch 00336: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2225 - tp: 77485.0000 - fp: 10315.0000 - tn: 445608.0000 - fn: 21676.0000 - accuracy: 0.9424 - precision: 0.8825 - recall: 0.7814 - auc: 0.9722 - val_loss: 0.2035 - val_tp: 19910.0000 - val_fp: 2106.0000 - val_tn: 111848.0000 - val_fn: 4907.0000 - val_accuracy: 0.9495 - val_precision: 0.9043 - val_recall: 0.8023 - val_auc: 0.9804\n",
            "Epoch 337/2000\n",
            "2165/2169 [============================>.] - ETA: 0s - loss: 0.2213 - tp: 77584.0000 - fp: 10368.0000 - tn: 444851.0000 - fn: 21437.0000 - accuracy: 0.9426 - precision: 0.8821 - recall: 0.7835 - auc: 0.9727\n",
            "Epoch 00337: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2213 - tp: 77696.0000 - fp: 10388.0000 - tn: 445535.0000 - fn: 21465.0000 - accuracy: 0.9426 - precision: 0.8821 - recall: 0.7835 - auc: 0.9727 - val_loss: 0.2035 - val_tp: 20216.0000 - val_fp: 2217.0000 - val_tn: 111737.0000 - val_fn: 4601.0000 - val_accuracy: 0.9509 - val_precision: 0.9012 - val_recall: 0.8146 - val_auc: 0.9801\n",
            "Epoch 338/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2213 - tp: 77620.0000 - fp: 10256.0000 - tn: 445667.0000 - fn: 21541.0000 - accuracy: 0.9427 - precision: 0.8833 - recall: 0.7828 - auc: 0.9727\n",
            "Epoch 00338: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2213 - tp: 77620.0000 - fp: 10256.0000 - tn: 445667.0000 - fn: 21541.0000 - accuracy: 0.9427 - precision: 0.8833 - recall: 0.7828 - auc: 0.9727 - val_loss: 0.2134 - val_tp: 21096.0000 - val_fp: 3677.0000 - val_tn: 110277.0000 - val_fn: 3721.0000 - val_accuracy: 0.9467 - val_precision: 0.8516 - val_recall: 0.8501 - val_auc: 0.9780\n",
            "Epoch 339/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2217 - tp: 77654.0000 - fp: 10376.0000 - tn: 445488.0000 - fn: 21490.0000 - accuracy: 0.9426 - precision: 0.8821 - recall: 0.7832 - auc: 0.9725\n",
            "Epoch 00339: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2217 - tp: 77665.0000 - fp: 10379.0000 - tn: 445544.0000 - fn: 21496.0000 - accuracy: 0.9426 - precision: 0.8821 - recall: 0.7832 - auc: 0.9725 - val_loss: 0.2041 - val_tp: 20005.0000 - val_fp: 2198.0000 - val_tn: 111756.0000 - val_fn: 4812.0000 - val_accuracy: 0.9495 - val_precision: 0.9010 - val_recall: 0.8061 - val_auc: 0.9796\n",
            "Epoch 340/2000\n",
            "2168/2169 [============================>.] - ETA: 0s - loss: 0.2217 - tp: 77641.0000 - fp: 10404.0000 - tn: 445454.0000 - fn: 21509.0000 - accuracy: 0.9425 - precision: 0.8818 - recall: 0.7831 - auc: 0.9726\n",
            "Epoch 00340: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2217 - tp: 77651.0000 - fp: 10406.0000 - tn: 445517.0000 - fn: 21510.0000 - accuracy: 0.9425 - precision: 0.8818 - recall: 0.7831 - auc: 0.9726 - val_loss: 0.2028 - val_tp: 19826.0000 - val_fp: 1889.0000 - val_tn: 112065.0000 - val_fn: 4991.0000 - val_accuracy: 0.9504 - val_precision: 0.9130 - val_recall: 0.7989 - val_auc: 0.9802\n",
            "Epoch 341/2000\n",
            "2167/2169 [============================>.] - ETA: 0s - loss: 0.2220 - tp: 77543.0000 - fp: 10334.0000 - tn: 445324.0000 - fn: 21551.0000 - accuracy: 0.9425 - precision: 0.8824 - recall: 0.7825 - auc: 0.9723\n",
            "Epoch 00341: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2221 - tp: 77596.0000 - fp: 10341.0000 - tn: 445582.0000 - fn: 21565.0000 - accuracy: 0.9425 - precision: 0.8824 - recall: 0.7825 - auc: 0.9723 - val_loss: 0.2038 - val_tp: 20189.0000 - val_fp: 2338.0000 - val_tn: 111616.0000 - val_fn: 4628.0000 - val_accuracy: 0.9498 - val_precision: 0.8962 - val_recall: 0.8135 - val_auc: 0.9797\n",
            "Epoch 342/2000\n",
            "2166/2169 [============================>.] - ETA: 0s - loss: 0.2218 - tp: 77591.0000 - fp: 10303.0000 - tn: 445128.0000 - fn: 21474.0000 - accuracy: 0.9427 - precision: 0.8828 - recall: 0.7832 - auc: 0.9724\n",
            "Epoch 00342: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2218 - tp: 77673.0000 - fp: 10313.0000 - tn: 445610.0000 - fn: 21488.0000 - accuracy: 0.9427 - precision: 0.8828 - recall: 0.7833 - auc: 0.9725 - val_loss: 0.2053 - val_tp: 19955.0000 - val_fp: 2118.0000 - val_tn: 111836.0000 - val_fn: 4862.0000 - val_accuracy: 0.9497 - val_precision: 0.9040 - val_recall: 0.8041 - val_auc: 0.9787\n",
            "Epoch 343/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2213 - tp: 77701.0000 - fp: 10259.0000 - tn: 445664.0000 - fn: 21460.0000 - accuracy: 0.9429 - precision: 0.8834 - recall: 0.7836 - auc: 0.9727\n",
            "Epoch 00343: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2213 - tp: 77701.0000 - fp: 10259.0000 - tn: 445664.0000 - fn: 21460.0000 - accuracy: 0.9429 - precision: 0.8834 - recall: 0.7836 - auc: 0.9727 - val_loss: 0.2183 - val_tp: 21274.0000 - val_fp: 4078.0000 - val_tn: 109876.0000 - val_fn: 3543.0000 - val_accuracy: 0.9451 - val_precision: 0.8391 - val_recall: 0.8572 - val_auc: 0.9778\n",
            "Epoch 344/2000\n",
            "2164/2169 [============================>.] - ETA: 0s - loss: 0.2213 - tp: 77488.0000 - fp: 10230.0000 - tn: 444798.0000 - fn: 21468.0000 - accuracy: 0.9428 - precision: 0.8834 - recall: 0.7831 - auc: 0.9726\n",
            "Epoch 00344: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2213 - tp: 77647.0000 - fp: 10249.0000 - tn: 445674.0000 - fn: 21514.0000 - accuracy: 0.9428 - precision: 0.8834 - recall: 0.7830 - auc: 0.9726 - val_loss: 0.2031 - val_tp: 19496.0000 - val_fp: 1675.0000 - val_tn: 112279.0000 - val_fn: 5321.0000 - val_accuracy: 0.9496 - val_precision: 0.9209 - val_recall: 0.7856 - val_auc: 0.9803\n",
            "Epoch 345/2000\n",
            "2169/2169 [==============================] - ETA: 0s - loss: 0.2207 - tp: 77598.0000 - fp: 10085.0000 - tn: 445838.0000 - fn: 21563.0000 - accuracy: 0.9430 - precision: 0.8850 - recall: 0.7825 - auc: 0.9727\n",
            "Epoch 00345: val_auc did not improve from 0.98052\n",
            "2169/2169 [==============================] - 20s 9ms/step - loss: 0.2207 - tp: 77598.0000 - fp: 10085.0000 - tn: 445838.0000 - fn: 21563.0000 - accuracy: 0.9430 - precision: 0.8850 - recall: 0.7825 - auc: 0.9727 - val_loss: 0.2039 - val_tp: 19420.0000 - val_fp: 1691.0000 - val_tn: 112263.0000 - val_fn: 5397.0000 - val_accuracy: 0.9489 - val_precision: 0.9199 - val_recall: 0.7825 - val_auc: 0.9801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "74ffadef-aa29-4395-a35a-7443b3ac4362"
      },
      "source": [
        "def plot_metrics(history):\n",
        "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch,  history.history[metric], color='b', label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color='b', linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "plot_metrics(history)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjbrzEe2ISI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "73d52cab-23dd-42a2-fb41-64f3624bb874"
      },
      "source": [
        "# Save model weights to drive\n",
        "!cp -r best_model.h5 '/content/gdrive/My Drive/Kaggle/best_model_20200803_METRICS.h5'\n",
        "\n",
        "new_model = tf.keras.models.load_model('./best_model.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "#new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Kaggle/best_model_20200802_METRICS.h5', \n",
        "#                                        custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 428,033\n",
            "Trainable params: 427,009\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAzqpDLIS0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty some RAM space\n",
        "indices_0_new = None\n",
        "data_backup = None\n",
        "dataset_transaction = None\n",
        "X_to_train = None\n",
        "Y_to_train = None\n",
        "Y_train = None\n",
        "X_train = None"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n",
        "  test_size = len(Y_test)\n",
        "  y_pred = (model.predict(X_test[:test_size], batch_size=128)>threshold).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        " \n",
        "  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)\n",
        "\n",
        "pre = []\n",
        "re = []\n",
        "f1 = []\n",
        "threshold_value = []\n",
        "\n",
        "for i in range(90):\n",
        "  threshold_value.append(0.1+i*0.01)\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_test, Y_test, test_size=len(Y_test), threshold=threshold_value[-1])\n",
        "  pre.append(temp_pre)\n",
        "  re.append(temp_re)\n",
        "  f1.append(temp_f1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CP4zuNU6WUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2a5b7823-2900-4103-f914-7540c847173c"
      },
      "source": [
        "plt.plot(threshold_value, f1, 'b')\n",
        "plt.plot(threshold_value, pre, 'r')\n",
        "plt.plot(threshold_value, re, '--')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe0fa3d4160>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2H3PvUGb8KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "b58a6921-db1c-4cbf-fc44-3ef47922d161"
      },
      "source": [
        "#BATCH_SIZE = 256\n",
        "baseline_results = new_model.evaluate(X_test, Y_test,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(new_model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "predictions = new_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "plot_cm(Y_test, predictions, p=0.6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss :  0.1283738762140274\n",
            "tp :  530.0\n",
            "fp :  226.0\n",
            "tn :  19109.0\n",
            "fn :  135.0\n",
            "accuracy :  0.9819499850273132\n",
            "precision :  0.7010582089424133\n",
            "recall :  0.7969924807548523\n",
            "auc :  0.9863584637641907\n",
            "\n",
            "Legitimate Transactions Detected (True Negatives):  19175\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  160\n",
            "Fraudulent Transactions Missed (False Negatives):  152\n",
            "Fraudulent Transactions Detected (True Positives):  513\n",
            "Total Fraudulent Transactions:  665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUuAUMBb_9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "a12068cd-2381-48db-8133-9c6ccf99d9cd"
      },
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,30])\n",
        "  plt.ylim([70,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "train_prediction = new_model.predict(X_train, batch_size=BATCH_SIZE, verbose=0)\n",
        "plot_roc(\"Train Baseline\", Y_train, train_prediction, color='b')\n",
        "plot_roc(\"Test Baseline\", Y_test, predictions, color='b', linestyle='--')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe0fa577400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43d7bce2-6a9f-403f-fa51-312ba9f6fb96"
      },
      "source": [
        "prediction = np.squeeze(predictions, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist((prediction>0.9).astype('int'), bins=[0,1,2])\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.85).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.08% 3.32%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "e16894a0-4e1a-47e1-ef25-e4f6905e66cc"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content\")\n",
        "test_transaction = pd.read_csv('test_transaction.csv')\n",
        "test_identity = pd.read_csv('test_identity.csv', names=saved_columns, header=0)\n",
        "test_identity.head(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Android 6.0.1</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x720</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>LGLS676 Build/MXB48T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ie 11.0 for tablet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Trident/7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SM-G9650 Build/R16NW</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  ...  id_38  DeviceType                   DeviceInfo\n",
              "0        3663586  -45.0  280290.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "1        3663588    0.0    3579.0  ...      T      mobile         LGLS676 Build/MXB48T\n",
              "2        3663597   -5.0  185210.0  ...      F     desktop                  Trident/7.0\n",
              "3        3663601  -45.0  252944.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "4        3663602  -95.0  328680.0  ...      F      mobile         SM-G9650 Build/R16NW\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8CCDZ5bl8V6p",
        "colab": {}
      },
      "source": [
        "dataset_transaction = None\n",
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = test_identity.pop(column)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVs1d5a_wMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e9b28869-ab36-412d-cb51-a80f52a71219"
      },
      "source": [
        "test_identity.head(5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>24.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  id_03  ...  id_36  id_37  id_38  DeviceType\n",
              "0        3663586  -45.0  280290.0    NaN  ...      F      T      F      mobile\n",
              "1        3663588    0.0    3579.0    0.0  ...      F      T      T      mobile\n",
              "2        3663597   -5.0  185210.0    NaN  ...      T      T      F     desktop\n",
              "3        3663601  -45.0  252944.0    0.0  ...      F      T      F      mobile\n",
              "4        3663602  -95.0  328680.0    NaN  ...      F      T      F      mobile\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "du0_nSm48V63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7406aa4f-1d93-41e5-fd0d-a442ee098a43"
      },
      "source": [
        "merged_data = pd.merge(left=test_transaction, right=test_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "TransactionID = merged_data.pop('TransactionID')\n",
        "test_transaction = None\n",
        "merged_data.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506691, 428)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoViKsx6cZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d32e75ae-9788-4628-f4b4-be6f5893346a"
      },
      "source": [
        "test_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "float_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = []\n",
        "for column in skip_int_columns:\n",
        "  int_columns_test.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "print(len(float_columns_test), len(int_columns_test), len(obj_columns_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzQZ6nR6wOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_normalization(X, indices, cache_min, cache_max, cache_mean):\n",
        "  X_out = copy.copy(X)\n",
        "  X_out[indices] = (X_out[indices] - cache_mean)/(cache_max - cache_min)\n",
        "  X_out[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return X_out.astype('float16')  \n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM75lh_6lhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in float_columns_test:\n",
        "  # Set to float 16\n",
        "  test_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjog0oM7p4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns_test:\n",
        "  # Set to int 32\n",
        "  test_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMTT8KB74NL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05344c3b-4126-4ddd-c917-f6cb00848be4"
      },
      "source": [
        "encoded_column = 0\n",
        "for column in obj_columns_test:\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(cache[column])\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.transform(test_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(cache[column])))])\n",
        "  test_transaction = pd.concat([test_transaction, pd_encoded], axis=1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "\n",
        "\n",
        "for column in obj_columns_test:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "for column in to_remove:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_OOqFi8HrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ccac32f-e37d-4f20-f2ab-930529f3b432"
      },
      "source": [
        "# Check if we have the same shape with the X_train\n",
        "#print(test_transaction.shape, X_train.shape)\n",
        "print(test_transaction.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506691, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9vDvpDZdpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the prediction and submit the output\n",
        "result = (new_model.predict(test_transaction)>0.7).astype('int8')\n",
        "result_pd = pd.DataFrame(result, columns=['isFraud'])\n",
        "data_to_file = pd.concat([TransactionID, result_pd], axis=1)\n",
        "data_to_file.head(5)\n",
        "data_to_file.to_csv(\"./submission.csv\", index=False)\n",
        "data_to_file.to_csv('/content/gdrive/My Drive/Kaggle/submission.csv', index=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9099XTi4s2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "89435338-c8d9-4ec2-a0af-3ef4c8c1a5dd"
      },
      "source": [
        "!kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"New submission with model_20200802 using Keras METRICS and LeakyRelu and with threshold 0.7\""
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 4.83M/4.83M [00:01<00:00, 3.03MB/s]\n",
            "Successfully submitted to IEEE-CIS Fraud Detection"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICp4sPm6brq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3e2nvzrHir4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fe4ae314-d9e2-483a-9baa-e8b9302f14bf"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohc = OneHotEncoder()\n",
        "a = {'a': ['Null', 'A', 'B', 'C', 'D']}\n",
        "df = pd.DataFrame(a)\n",
        "df\n",
        "encoded = ohc.fit_transform(df['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsaGKlzMUlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69213f3a-42cb-4edc-cbc8-5bc6c0bb5a7e"
      },
      "source": [
        "b = {'a': ['Null', 'A', 'B', 'C', 'E']}\n",
        "df_b = pd.DataFrame(b)\n",
        "ohc_b = OneHotEncoder(handle_unknown='ignore')\n",
        "ohc_b.fit(df['a'].values.reshape(-1,1))\n",
        "encoded_b = ohc_b.transform(df_b['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded_b = pd.DataFrame(encoded_b.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvykuaRPMpZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "20c01d2a-8a3b-41b6-a7ae-b6d7a36f327f"
      },
      "source": [
        "for column in obj_columns:\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(dataset_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 5\n",
            "P_emaildomain 60\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_RMIz3NTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2e5b2844-8bdd-45ee-f5ec-83cd54c9cba7"
      },
      "source": [
        "for column in obj_columns_test:\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(test_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 4\n",
            "P_emaildomain 61\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}