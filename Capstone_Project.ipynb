{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "51019cc5-7dd7-489a-a54b-9420c7be15fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "ab6a4f39-1d67-4cf9-c46c-665922b0a423"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:01<00:00, 19.6MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 28.5MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 79.6MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 106MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 70% 41.0M/58.3M [00:01<00:01, 15.8MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 39.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 137MB/s]\n",
            "Archive:  train_identity.csv.zip\n",
            "  inflating: train_identity.csv      \n",
            "\n",
            "Archive:  test_transaction.csv.zip\n",
            "  inflating: test_transaction.csv    \n",
            "\n",
            "Archive:  test_identity.csv.zip\n",
            "  inflating: test_identity.csv       \n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "\n",
            "Archive:  train_transaction.csv.zip\n",
            "  inflating: train_transaction.csv   \n",
            "\n",
            "5 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e2932f38-eef2-4736-ecd3-b943a31e2862"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "78a0b62a-b574-4b4a-e50a-b286fd9e2d6d"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "af7eb7ea-ef86-470e-9524-a0c1b123a78f"
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>Android 7.0</td>\n",
              "      <td>samsung browser 6.2</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2220x1080</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987008</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>98945.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>49.0</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>621.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>iOS 11.1.2</td>\n",
              "      <td>mobile safari 11.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1334x750</td>\n",
              "      <td>match_status:1</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>iOS Device</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987010</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>191631.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>121.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>410.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Windows</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987011</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>221832.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>176.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>529.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Mac OS X 10_11_6</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x800</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>MacOS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01  ...  DeviceType                     DeviceInfo\n",
              "0        2987004    0.0  ...      mobile  SAMSUNG SM-G892A Build/NRD90M\n",
              "1        2987008   -5.0  ...      mobile                     iOS Device\n",
              "2        2987010   -5.0  ...     desktop                        Windows\n",
              "3        2987011   -5.0  ...     desktop                            NaN\n",
              "4        2987016    0.0  ...     desktop                          MacOS\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c8e1a861-44da-44e2-fd53-9439fe74fd92"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2a1c9564-4826-444f-8fff-79e6fd53b751"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f1b4902d-1fa7-4767-8f1a-7f5e97965784"
      },
      "source": [
        "data_backup = copy.copy(dataset_transaction)\n",
        "data_backup.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# To-Do: Task 3: Remove the irrelevant columns\n",
        "\n",
        "cache = dict()\n",
        "dataset_transaction = copy.copy(data_backup)\n",
        "\n",
        "#dataset_transaction.pop()\n",
        "\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f343aef0-aad1-4d8a-8ead-58642a4f709c"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1004d340-c450-4cc9-9e0b-3d4ba078d293"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "4354055c-c232-4161-bd8e-aa5cb7cb8115"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 905 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  M9_0  M9_1  M9_2\n",
              "0        2987000        0      -0.463379  ...     0     1     0\n",
              "1        2987001        0      -0.463379  ...     0     1     0\n",
              "2        2987002        0      -0.463379  ...     1     0     0\n",
              "3        2987003        0      -0.463379  ...     0     1     0\n",
              "4        2987004        0      -0.463379  ...     0     1     0\n",
              "\n",
              "[5 rows x 905 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1ec3f50-1d1b-4320-f853-25271d35833f"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colums_to_analyze = ['isFraud', 'TransactionDT', 'TransactionAmt', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2']\n",
        "analyzing_data = dataset_transaction[colums_to_analyze]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "5a4096a8-12c7-4718-a7d8-6956d54fa86c"
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4e88f5cda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "58a13eb6-07d7-46c0-866d-dda99130e980"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 904 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  TransactionAmt     card1  ...  M8_2  M9_0  M9_1  M9_2\n",
              "0        0      -0.463379       -0.002083  0.231445  ...     0     0     1     0\n",
              "1        0      -0.463379       -0.003321 -0.410645  ...     0     0     1     0\n",
              "2        0      -0.463379       -0.002380 -0.301025  ...     0     1     0     0\n",
              "3        0      -0.463379       -0.002663  0.473389  ...     0     0     1     0\n",
              "4        0      -0.463379       -0.002663 -0.310547  ...     0     0     1     0\n",
              "\n",
              "[5 rows x 904 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9ae5fd3-c061-432d-de3d-b2a83e770020"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "\n",
        "#X_train = np.expand_dims(X_train, axis=2)\n",
        "#X_test = np.expand_dims(X_test, axis=2)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(472432, 903) (472432,) (118108, 903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "55249970-c91f-4f63-d9ac-0c8d7311d3e8"
      },
      "source": [
        "downsampling_factor = 10\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "indices_0_new = np.concatenate((indices_0_new, indices_1), axis=0)\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "\n",
        "X_to_train = np.array(X_train)[indices_0_new]\n",
        "Y_to_train = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "\n",
        "X_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_to_train, axis=1)\n",
        "print(X_to_train.shape, Y_to_train.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45601, 1)\n",
            "(62028, 1)\n",
            "(62028, 903) (62028,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "eafdc02c-db0c-4b9f-93fe-37f9f1e8a08e"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.48%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "51ccd51a-afd5-4199-d865-95d3378695e9"
      },
      "source": [
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 26.48%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQlUlEQVR4nO3df6zddX3H8efLlh/+hGI7RlpmS2xiipmCDeKPbAobFJiWZWpK3Kius3PCotmyDUYyNpUM/xmODF2INBZjKAzd6BTSdYAxm2nhovwqDLkUHG1QKi0gMeJg7/1xPnVfr/f2ntvec+6lfT6Sk/v9vj+f7/m+z/ee3tc95/u9p6kqJEmHtpfNdAOSpJlnGEiSDANJkmEgScIwkCQBc2e6gf01f/78Wrx48Uy3IUkvGXfdddcPq2rBeGMv2TBYvHgxIyMjM92GJL1kJPneRGO+TSRJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJF7Cf4F8IBZf9PWZbkEHsccuP2emW5CmzFcGkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJKYQBknmJPlOkq+19SVJtiYZTXJ9ksNb/Yi2PtrGF3fu4+JWfyjJmZ36ilYbTXLR9D08SVI/pvLK4OPAg531zwBXVNXrgT3AmlZfA+xp9SvaPJIsA1YBJwIrgM+1gJkDXAWcBSwDzmtzJUlD0lcYJFkEnAN8oa0HOA24sU1ZD5zblle2ddr46W3+SmBDVT1fVY8Co8Ap7TZaVdur6qfAhjZXkjQk/b4y+Czw58D/tvXXAk9X1QttfQewsC0vBB4HaOPPtPk/q4/ZZqL6L0iyNslIkpFdu3b12bokaTKThkGS3wKerKq7htDPPlXV1VW1vKqWL1iwYKbbkaSDxtw+5rwDeG+Ss4EjgdcAfw8cnWRu++1/EbCzzd8JHA/sSDIXOAp4qlPfq7vNRHVJ0hBM+sqgqi6uqkVVtZjeCeDbquqDwO3A+9q01cBNbXljW6eN31ZV1eqr2tVGS4ClwB3AncDSdnXS4W0fG6fl0UmS+tLPK4OJ/AWwIcmnge8A17T6NcCXkowCu+n9cKeqtiW5AXgAeAG4oKpeBEhyIbAJmAOsq6ptB9CXJGmKphQGVfUN4BtteTu9K4HGzvkJ8P4Jtr8MuGyc+s3AzVPpRZI0ffwLZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk+giDJEcmuSPJPUm2JfmbVl+SZGuS0STXJzm81Y9o66NtfHHnvi5u9YeSnNmpr2i10SQXTf/DlCTtSz+vDJ4HTquqNwFvBlYkORX4DHBFVb0e2AOsafPXAHta/Yo2jyTLgFXAicAK4HNJ5iSZA1wFnAUsA85rcyVJQzJpGFTPc231sHYr4DTgxlZfD5zblle2ddr46UnS6huq6vmqehQYBU5pt9Gq2l5VPwU2tLmSpCHp65xB+w3+buBJYDPwCPB0Vb3QpuwAFrblhcDjAG38GeC13fqYbSaqS5KGpK8wqKoXq+rNwCJ6v8m/YaBdTSDJ2iQjSUZ27do1Ey1I0kFpSlcTVdXTwO3A24Cjk8xtQ4uAnW15J3A8QBs/CniqWx+zzUT18fZ/dVUtr6rlCxYsmErrkqR96OdqogVJjm7LLwd+E3iQXii8r01bDdzUlje2ddr4bVVVrb6qXW20BFgK3AHcCSxtVycdTu8k88bpeHCSpP7MnXwKxwHr21U/LwNuqKqvJXkA2JDk08B3gGva/GuALyUZBXbT++FOVW1LcgPwAPACcEFVvQiQ5EJgEzAHWFdV26btEUqSJjVpGFTVvcBJ49S30zt/MLb+E+D9E9zXZcBl49RvBm7uo19J0gD4F8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiT6CIMkxye5PckDSbYl+XirH5Nkc5KH29d5rZ4kVyYZTXJvkpM797W6zX84yepO/S1J7mvbXJkkg3iwkqTx9fPK4AXgT6tqGXAqcEGSZcBFwK1VtRS4ta0DnAUsbbe1wOehFx7ApcBbgVOAS/cGSJvzkc52Kw78oUmS+jVpGFTVE1X17bb8I+BBYCGwEljfpq0Hzm3LK4Frq2cLcHSS44Azgc1Vtbuq9gCbgRVt7DVVtaWqCri2c1+SpCGY0jmDJIuBk4CtwLFV9UQb+j5wbFteCDze2WxHq+2rvmOc+nj7X5tkJMnIrl27ptK6JGkf+g6DJK8CvgJ8oqqe7Y613+hrmnv7BVV1dVUtr6rlCxYsGPTuJOmQ0VcYJDmMXhB8uaq+2so/aG/x0L4+2eo7geM7my9qtX3VF41TlyQNST9XEwW4Bniwqv6uM7QR2HtF0Grgpk79/HZV0anAM+3tpE3AGUnmtRPHZwCb2tizSU5t+zq/c1+SpCGY28ecdwC/B9yX5O5W+0vgcuCGJGuA7wEfaGM3A2cDo8CPgQ8DVNXuJJ8C7mzzPllVu9vyx4AvAi8Hbmk3SdKQTBoGVfUfwETX/Z8+zvwCLpjgvtYB68apjwBvnKwXSdJg+BfIkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk0d9nE0magsUXfX2mW9BB7LHLzxnI/frKQJJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0EQZJ1iV5Msn9ndoxSTYnebh9ndfqSXJlktEk9yY5ubPN6jb/4SSrO/W3JLmvbXNlkkz3g5Qk7Vs/rwy+CKwYU7sIuLWqlgK3tnWAs4Cl7bYW+Dz0wgO4FHgrcApw6d4AaXM+0tlu7L4kSQM2aRhU1TeB3WPKK4H1bXk9cG6nfm31bAGOTnIccCawuap2V9UeYDOwoo29pqq2VFUB13buS5I0JPt7zuDYqnqiLX8fOLYtLwQe78zb0Wr7qu8Ypz6uJGuTjCQZ2bVr1362Lkka64BPILff6GsaeulnX1dX1fKqWr5gwYJh7FKSDgn7GwY/aG/x0L4+2eo7geM78xa12r7qi8apS5KGaH/DYCOw94qg1cBNnfr57aqiU4Fn2ttJm4AzksxrJ47PADa1sWeTnNquIjq/c1+SpCGZO9mEJNcB7wLmJ9lB76qgy4EbkqwBvgd8oE2/GTgbGAV+DHwYoKp2J/kUcGeb98mq2ntS+mP0rlh6OXBLu0mShmjSMKiq8yYYOn2cuQVcMMH9rAPWjVMfAd44WR+SpMHxL5AlSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEnMojBIsiLJQ0lGk1w00/1I0qFkVoRBkjnAVcBZwDLgvCTLZrYrSTp0zIowAE4BRqtqe1X9FNgArJzhniTpkDF3phtoFgKPd9Z3AG8dOynJWmBtW30uyUP7ub/5wA/3c9tBsq+psa+psa+pmZV95TMH1NfrJhqYLWHQl6q6Grj6QO8nyUhVLZ+GlqaVfU2NfU2NfU3NodbXbHmbaCdwfGd9UatJkoZgtoTBncDSJEuSHA6sAjbOcE+SdMiYFW8TVdULSS4ENgFzgHVVtW2Auzzgt5oGxL6mxr6mxr6m5pDqK1U1iPuVJL2EzJa3iSRJM8gwkCQdXGEw2UdaJDkiyfVtfGuSxZ2xi1v9oSRnDrmvP0nyQJJ7k9ya5HWdsReT3N1u03pSvY++PpRkV2f/f9AZW53k4XZbPeS+ruj09N0kT3fGBnm81iV5Msn9E4wnyZWt73uTnNwZG+TxmqyvD7Z+7kvyrSRv6ow91up3JxkZcl/vSvJM5/v1V52xgX08TR99/Vmnp/vbc+qYNjbI43V8ktvbz4JtST4+zpzBPceq6qC40Tvx/AhwAnA4cA+wbMycjwH/2JZXAde35WVt/hHAknY/c4bY17uBV7TlP9rbV1t/bgaP14eAfxhn22OA7e3rvLY8b1h9jZn/x/QuOBjo8Wr3/WvAycD9E4yfDdwCBDgV2Dro49VnX2/fuz96H/mytTP2GDB/ho7Xu4CvHehzYLr7GjP3PcBtQzpexwEnt+VXA98d59/kwJ5jB9Mrg34+0mIlsL4t3wicniStvqGqnq+qR4HRdn9D6auqbq+qH7fVLfT+zmLQDuQjQM4ENlfV7qraA2wGVsxQX+cB103Tvvepqr4J7N7HlJXAtdWzBTg6yXEM9nhN2ldVfavtF4b3/OrneE1koB9PM8W+hvn8eqKqvt2WfwQ8SO/TGboG9hw7mMJgvI+0GHsgfzanql4AngFe2+e2g+yraw295N/ryCQjSbYkOXeaeppKX7/TXo7emGTvHwbOiuPV3k5bAtzWKQ/qePVjot4Hebymauzzq4B/S3JXeh/3MmxvS3JPkluSnNhqs+J4JXkFvR+oX+mUh3K80nsL+yRg65ihgT3HZsXfGagnye8Cy4Ff75RfV1U7k5wA3Jbkvqp6ZEgt/StwXVU9n+QP6b2qOm1I++7HKuDGqnqxU5vJ4zWrJXk3vTB4Z6f8zna8fgnYnOS/2m/Ow/Btet+v55KcDfwLsHRI++7He4D/rKruq4iBH68kr6IXQJ+oqmen87735WB6ZdDPR1r8bE6SucBRwFN9bjvIvkjyG8AlwHur6vm99ara2b5uB75B77eFofRVVU91evkC8JZ+tx1kXx2rGPMSfoDHqx8T9T7jH7eS5FfpfQ9XVtVTe+ud4/Uk8M9M39ujk6qqZ6vqubZ8M3BYkvnMguPV7Ov5NZDjleQwekHw5ar66jhTBvccG8SJkJm40XuVs53e2wZ7TzqdOGbOBfz8CeQb2vKJ/PwJ5O1M3wnkfvo6id4Js6Vj6vOAI9ryfOBhpulEWp99HddZ/m1gS/3/yapHW3/z2vIxw+qrzXsDvZN5Gcbx6uxjMROfED2Hnz+5d8egj1efff0KvfNgbx9TfyXw6s7yt4AVQ+zrl/d+/+j9UP3vduz6eg4Mqq82fhS98wqvHNbxao/9WuCz+5gzsOfYtB3c2XCjd6b9u/R+sF7Sap+k99s2wJHAP7V/GHcAJ3S2vaRt9xBw1pD7+nfgB8Dd7bax1d8O3Nf+MdwHrBlyX38LbGv7vx14Q2fb32/HcRT48DD7aut/DVw+ZrtBH6/rgCeA/6H3nuwa4KPAR9t46P0nTY+0/S8f0vGarK8vAHs6z6+RVj+hHat72vf5kiH3dWHn+bWFTliN9xwYVl9tzofoXVTS3W7Qx+ud9M5J3Nv5Xp09rOeYH0chSTqozhlIkvaTYSBJMgwkSYaBJAnDQJKEYSBJwjCQJAH/B0CG0a57/SfOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  out_model.add(Dense(dense1, activation=\"relu\", input_shape=(X_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "\n",
        "  out_model.add(Dense(dense2, activation=\"relu\", \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['binary_accuracy'])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5590ba99-43fa-47c3-cdac-c2b0a811227f"
      },
      "source": [
        "my_model = create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0, l2_rate=0, init_std=0.05, lr=0.0001)\n",
        "my_model.summary()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_134 (Dense)            (None, 128)               115712    \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 144,705\n",
            "Trainable params: 144,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "520f9c0f-f641-4d92-bd90-8917f89bed47"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NB_EPOCH = 1000\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_binary_accuracy', patience=50, verbose=0, mode='auto',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/best_model', monitor='val_binary_accuracy', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.2, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.3723 - binary_accuracy: 0.9150\n",
            "Epoch 00001: val_binary_accuracy improved from -inf to 0.00000, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.3703 - binary_accuracy: 0.9152 - val_loss: 2.2758 - val_binary_accuracy: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.2665 - binary_accuracy: 0.9190\n",
            "Epoch 00002: val_binary_accuracy did not improve from 0.00000\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2666 - binary_accuracy: 0.9190 - val_loss: 2.1049 - val_binary_accuracy: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.2428 - binary_accuracy: 0.9218\n",
            "Epoch 00003: val_binary_accuracy improved from 0.00000 to 0.12470, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.2428 - binary_accuracy: 0.9218 - val_loss: 2.0431 - val_binary_accuracy: 0.1247\n",
            "Epoch 4/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.2277 - binary_accuracy: 0.9297\n",
            "Epoch 00004: val_binary_accuracy improved from 0.12470 to 0.20853, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.2277 - binary_accuracy: 0.9297 - val_loss: 2.0012 - val_binary_accuracy: 0.2085\n",
            "Epoch 5/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.2225 - binary_accuracy: 0.9314\n",
            "Epoch 00005: val_binary_accuracy improved from 0.20853 to 0.26100, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.2226 - binary_accuracy: 0.9315 - val_loss: 1.7918 - val_binary_accuracy: 0.2610\n",
            "Epoch 6/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.2199 - binary_accuracy: 0.9326\n",
            "Epoch 00006: val_binary_accuracy improved from 0.26100 to 0.26350, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.2199 - binary_accuracy: 0.9326 - val_loss: 1.7596 - val_binary_accuracy: 0.2635\n",
            "Epoch 7/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.2173 - binary_accuracy: 0.9342\n",
            "Epoch 00007: val_binary_accuracy improved from 0.26350 to 0.28583, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.2173 - binary_accuracy: 0.9342 - val_loss: 1.7522 - val_binary_accuracy: 0.2858\n",
            "Epoch 8/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.2146 - binary_accuracy: 0.9343\n",
            "Epoch 00008: val_binary_accuracy did not improve from 0.28583\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2142 - binary_accuracy: 0.9345 - val_loss: 1.8833 - val_binary_accuracy: 0.2709\n",
            "Epoch 9/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.2135 - binary_accuracy: 0.9351\n",
            "Epoch 00009: val_binary_accuracy did not improve from 0.28583\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2131 - binary_accuracy: 0.9352 - val_loss: 1.8543 - val_binary_accuracy: 0.2661\n",
            "Epoch 10/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.2100 - binary_accuracy: 0.9363\n",
            "Epoch 00010: val_binary_accuracy improved from 0.28583 to 0.29719, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.2107 - binary_accuracy: 0.9361 - val_loss: 1.7134 - val_binary_accuracy: 0.2972\n",
            "Epoch 11/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.2088 - binary_accuracy: 0.9364\n",
            "Epoch 00011: val_binary_accuracy improved from 0.29719 to 0.29905, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.2087 - binary_accuracy: 0.9364 - val_loss: 1.8714 - val_binary_accuracy: 0.2990\n",
            "Epoch 12/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.2074 - binary_accuracy: 0.9366\n",
            "Epoch 00012: val_binary_accuracy did not improve from 0.29905\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2070 - binary_accuracy: 0.9368 - val_loss: 1.7420 - val_binary_accuracy: 0.2961\n",
            "Epoch 13/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.2055 - binary_accuracy: 0.9371\n",
            "Epoch 00013: val_binary_accuracy improved from 0.29905 to 0.33186, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.2055 - binary_accuracy: 0.9371 - val_loss: 1.6349 - val_binary_accuracy: 0.3319\n",
            "Epoch 14/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.2064 - binary_accuracy: 0.9370\n",
            "Epoch 00014: val_binary_accuracy did not improve from 0.33186\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2063 - binary_accuracy: 0.9370 - val_loss: 1.8121 - val_binary_accuracy: 0.2923\n",
            "Epoch 15/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.2040 - binary_accuracy: 0.9375\n",
            "Epoch 00015: val_binary_accuracy did not improve from 0.33186\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2042 - binary_accuracy: 0.9375 - val_loss: 1.7828 - val_binary_accuracy: 0.2752\n",
            "Epoch 16/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.2040 - binary_accuracy: 0.9378\n",
            "Epoch 00016: val_binary_accuracy did not improve from 0.33186\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2040 - binary_accuracy: 0.9378 - val_loss: 1.6448 - val_binary_accuracy: 0.3212\n",
            "Epoch 17/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.2024 - binary_accuracy: 0.9375\n",
            "Epoch 00017: val_binary_accuracy did not improve from 0.33186\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2024 - binary_accuracy: 0.9375 - val_loss: 1.6947 - val_binary_accuracy: 0.3210\n",
            "Epoch 18/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.2011 - binary_accuracy: 0.9387\n",
            "Epoch 00018: val_binary_accuracy did not improve from 0.33186\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2015 - binary_accuracy: 0.9385 - val_loss: 1.7917 - val_binary_accuracy: 0.3063\n",
            "Epoch 19/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.2015 - binary_accuracy: 0.9385\n",
            "Epoch 00019: val_binary_accuracy did not improve from 0.33186\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.2016 - binary_accuracy: 0.9385 - val_loss: 1.6801 - val_binary_accuracy: 0.3193\n",
            "Epoch 20/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1994 - binary_accuracy: 0.9384\n",
            "Epoch 00020: val_binary_accuracy improved from 0.33186 to 0.34064, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 10ms/step - loss: 0.1994 - binary_accuracy: 0.9385 - val_loss: 1.6497 - val_binary_accuracy: 0.3406\n",
            "Epoch 21/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1982 - binary_accuracy: 0.9388\n",
            "Epoch 00021: val_binary_accuracy improved from 0.34064 to 0.36458, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.1981 - binary_accuracy: 0.9389 - val_loss: 1.5444 - val_binary_accuracy: 0.3646\n",
            "Epoch 22/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1988 - binary_accuracy: 0.9390\n",
            "Epoch 00022: val_binary_accuracy did not improve from 0.36458\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1989 - binary_accuracy: 0.9390 - val_loss: 1.5847 - val_binary_accuracy: 0.3552\n",
            "Epoch 23/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1988 - binary_accuracy: 0.9396\n",
            "Epoch 00023: val_binary_accuracy did not improve from 0.36458\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1986 - binary_accuracy: 0.9395 - val_loss: 1.7060 - val_binary_accuracy: 0.3208\n",
            "Epoch 24/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1976 - binary_accuracy: 0.9400\n",
            "Epoch 00024: val_binary_accuracy did not improve from 0.36458\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1980 - binary_accuracy: 0.9399 - val_loss: 1.7406 - val_binary_accuracy: 0.3273\n",
            "Epoch 25/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1967 - binary_accuracy: 0.9402\n",
            "Epoch 00025: val_binary_accuracy did not improve from 0.36458\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1967 - binary_accuracy: 0.9402 - val_loss: 1.5860 - val_binary_accuracy: 0.3554\n",
            "Epoch 26/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1958 - binary_accuracy: 0.9406\n",
            "Epoch 00026: val_binary_accuracy did not improve from 0.36458\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1963 - binary_accuracy: 0.9404 - val_loss: 1.6631 - val_binary_accuracy: 0.3446\n",
            "Epoch 27/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1966 - binary_accuracy: 0.9401\n",
            "Epoch 00027: val_binary_accuracy did not improve from 0.36458\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1968 - binary_accuracy: 0.9401 - val_loss: 1.6210 - val_binary_accuracy: 0.3519\n",
            "Epoch 28/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1933 - binary_accuracy: 0.9408\n",
            "Epoch 00028: val_binary_accuracy improved from 0.36458 to 0.36974, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.1933 - binary_accuracy: 0.9408 - val_loss: 1.5174 - val_binary_accuracy: 0.3697\n",
            "Epoch 29/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1943 - binary_accuracy: 0.9406\n",
            "Epoch 00029: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1940 - binary_accuracy: 0.9407 - val_loss: 1.5688 - val_binary_accuracy: 0.3542\n",
            "Epoch 30/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1939 - binary_accuracy: 0.9406\n",
            "Epoch 00030: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1933 - binary_accuracy: 0.9408 - val_loss: 1.8061 - val_binary_accuracy: 0.3074\n",
            "Epoch 31/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1917 - binary_accuracy: 0.9413\n",
            "Epoch 00031: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1921 - binary_accuracy: 0.9412 - val_loss: 1.6402 - val_binary_accuracy: 0.3454\n",
            "Epoch 32/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1937 - binary_accuracy: 0.9411\n",
            "Epoch 00032: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1928 - binary_accuracy: 0.9414 - val_loss: 1.9271 - val_binary_accuracy: 0.3372\n",
            "Epoch 33/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1940 - binary_accuracy: 0.9414\n",
            "Epoch 00033: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1934 - binary_accuracy: 0.9417 - val_loss: 1.7645 - val_binary_accuracy: 0.3456\n",
            "Epoch 34/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1920 - binary_accuracy: 0.9419\n",
            "Epoch 00034: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1920 - binary_accuracy: 0.9419 - val_loss: 1.7071 - val_binary_accuracy: 0.3418\n",
            "Epoch 35/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1911 - binary_accuracy: 0.9421\n",
            "Epoch 00035: val_binary_accuracy did not improve from 0.36974\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1907 - binary_accuracy: 0.9422 - val_loss: 1.7964 - val_binary_accuracy: 0.3126\n",
            "Epoch 36/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1905 - binary_accuracy: 0.9422\n",
            "Epoch 00036: val_binary_accuracy improved from 0.36974 to 0.39400, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.1903 - binary_accuracy: 0.9422 - val_loss: 1.4755 - val_binary_accuracy: 0.3940\n",
            "Epoch 37/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1909 - binary_accuracy: 0.9422\n",
            "Epoch 00037: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1904 - binary_accuracy: 0.9424 - val_loss: 1.5045 - val_binary_accuracy: 0.3672\n",
            "Epoch 38/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1895 - binary_accuracy: 0.9425\n",
            "Epoch 00038: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1897 - binary_accuracy: 0.9425 - val_loss: 1.6346 - val_binary_accuracy: 0.3541\n",
            "Epoch 39/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1899 - binary_accuracy: 0.9424\n",
            "Epoch 00039: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1900 - binary_accuracy: 0.9424 - val_loss: 1.7447 - val_binary_accuracy: 0.3310\n",
            "Epoch 40/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1890 - binary_accuracy: 0.9428\n",
            "Epoch 00040: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1890 - binary_accuracy: 0.9427 - val_loss: 1.5693 - val_binary_accuracy: 0.3543\n",
            "Epoch 41/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1889 - binary_accuracy: 0.9423\n",
            "Epoch 00041: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1881 - binary_accuracy: 0.9425 - val_loss: 1.6840 - val_binary_accuracy: 0.3479\n",
            "Epoch 42/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1894 - binary_accuracy: 0.9422\n",
            "Epoch 00042: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1886 - binary_accuracy: 0.9426 - val_loss: 1.9286 - val_binary_accuracy: 0.3179\n",
            "Epoch 43/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1879 - binary_accuracy: 0.9432\n",
            "Epoch 00043: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1880 - binary_accuracy: 0.9432 - val_loss: 1.6148 - val_binary_accuracy: 0.3642\n",
            "Epoch 44/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1874 - binary_accuracy: 0.9432\n",
            "Epoch 00044: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1876 - binary_accuracy: 0.9431 - val_loss: 1.5060 - val_binary_accuracy: 0.3580\n",
            "Epoch 45/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1870 - binary_accuracy: 0.9429\n",
            "Epoch 00045: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1866 - binary_accuracy: 0.9431 - val_loss: 1.7059 - val_binary_accuracy: 0.3323\n",
            "Epoch 46/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1856 - binary_accuracy: 0.9437\n",
            "Epoch 00046: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1859 - binary_accuracy: 0.9435 - val_loss: 1.5614 - val_binary_accuracy: 0.3566\n",
            "Epoch 47/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1868 - binary_accuracy: 0.9430\n",
            "Epoch 00047: val_binary_accuracy did not improve from 0.39400\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1865 - binary_accuracy: 0.9431 - val_loss: 1.5787 - val_binary_accuracy: 0.3638\n",
            "Epoch 48/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1851 - binary_accuracy: 0.9433\n",
            "Epoch 00048: val_binary_accuracy improved from 0.39400 to 0.39852, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 10ms/step - loss: 0.1849 - binary_accuracy: 0.9434 - val_loss: 1.5647 - val_binary_accuracy: 0.3985\n",
            "Epoch 49/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1851 - binary_accuracy: 0.9444\n",
            "Epoch 00049: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1851 - binary_accuracy: 0.9444 - val_loss: 1.6667 - val_binary_accuracy: 0.3609\n",
            "Epoch 50/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1848 - binary_accuracy: 0.9434\n",
            "Epoch 00050: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1848 - binary_accuracy: 0.9434 - val_loss: 1.6465 - val_binary_accuracy: 0.3232\n",
            "Epoch 51/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1839 - binary_accuracy: 0.9438\n",
            "Epoch 00051: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1842 - binary_accuracy: 0.9437 - val_loss: 1.5600 - val_binary_accuracy: 0.3876\n",
            "Epoch 52/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1856 - binary_accuracy: 0.9433\n",
            "Epoch 00052: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1852 - binary_accuracy: 0.9435 - val_loss: 1.7257 - val_binary_accuracy: 0.3348\n",
            "Epoch 53/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1834 - binary_accuracy: 0.9442\n",
            "Epoch 00053: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1837 - binary_accuracy: 0.9440 - val_loss: 1.4696 - val_binary_accuracy: 0.3880\n",
            "Epoch 54/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1828 - binary_accuracy: 0.9443\n",
            "Epoch 00054: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1835 - binary_accuracy: 0.9441 - val_loss: 1.4500 - val_binary_accuracy: 0.3672\n",
            "Epoch 55/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1840 - binary_accuracy: 0.9445\n",
            "Epoch 00055: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1846 - binary_accuracy: 0.9443 - val_loss: 1.6649 - val_binary_accuracy: 0.3672\n",
            "Epoch 56/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1834 - binary_accuracy: 0.9444\n",
            "Epoch 00056: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1834 - binary_accuracy: 0.9444 - val_loss: 1.8620 - val_binary_accuracy: 0.3209\n",
            "Epoch 57/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1830 - binary_accuracy: 0.9446\n",
            "Epoch 00057: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1829 - binary_accuracy: 0.9445 - val_loss: 1.7662 - val_binary_accuracy: 0.3666\n",
            "Epoch 58/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1820 - binary_accuracy: 0.9438\n",
            "Epoch 00058: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1816 - binary_accuracy: 0.9440 - val_loss: 1.7487 - val_binary_accuracy: 0.3751\n",
            "Epoch 59/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1810 - binary_accuracy: 0.9452\n",
            "Epoch 00059: val_binary_accuracy did not improve from 0.39852\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1811 - binary_accuracy: 0.9452 - val_loss: 1.6890 - val_binary_accuracy: 0.3473\n",
            "Epoch 60/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1793 - binary_accuracy: 0.9453\n",
            "Epoch 00060: val_binary_accuracy improved from 0.39852 to 0.40037, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.1796 - binary_accuracy: 0.9452 - val_loss: 1.5293 - val_binary_accuracy: 0.4004\n",
            "Epoch 61/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1802 - binary_accuracy: 0.9445\n",
            "Epoch 00061: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1802 - binary_accuracy: 0.9445 - val_loss: 1.7502 - val_binary_accuracy: 0.3610\n",
            "Epoch 62/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1805 - binary_accuracy: 0.9451\n",
            "Epoch 00062: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1807 - binary_accuracy: 0.9450 - val_loss: 1.4132 - val_binary_accuracy: 0.3863\n",
            "Epoch 63/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1796 - binary_accuracy: 0.9453\n",
            "Epoch 00063: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1800 - binary_accuracy: 0.9451 - val_loss: 1.4391 - val_binary_accuracy: 0.3776\n",
            "Epoch 64/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1785 - binary_accuracy: 0.9455\n",
            "Epoch 00064: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1785 - binary_accuracy: 0.9455 - val_loss: 1.6142 - val_binary_accuracy: 0.3506\n",
            "Epoch 65/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1792 - binary_accuracy: 0.9455\n",
            "Epoch 00065: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1791 - binary_accuracy: 0.9455 - val_loss: 1.7707 - val_binary_accuracy: 0.3620\n",
            "Epoch 66/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1794 - binary_accuracy: 0.9455\n",
            "Epoch 00066: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1795 - binary_accuracy: 0.9454 - val_loss: 1.6613 - val_binary_accuracy: 0.3465\n",
            "Epoch 67/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1781 - binary_accuracy: 0.9457\n",
            "Epoch 00067: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1785 - binary_accuracy: 0.9456 - val_loss: 1.5284 - val_binary_accuracy: 0.3647\n",
            "Epoch 68/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1780 - binary_accuracy: 0.9455\n",
            "Epoch 00068: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1778 - binary_accuracy: 0.9456 - val_loss: 1.6442 - val_binary_accuracy: 0.3534\n",
            "Epoch 69/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1779 - binary_accuracy: 0.9458\n",
            "Epoch 00069: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1779 - binary_accuracy: 0.9458 - val_loss: 1.6831 - val_binary_accuracy: 0.3946\n",
            "Epoch 70/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1774 - binary_accuracy: 0.9457\n",
            "Epoch 00070: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1774 - binary_accuracy: 0.9457 - val_loss: 1.4846 - val_binary_accuracy: 0.3757\n",
            "Epoch 71/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1763 - binary_accuracy: 0.9464\n",
            "Epoch 00071: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1763 - binary_accuracy: 0.9464 - val_loss: 1.7216 - val_binary_accuracy: 0.3310\n",
            "Epoch 72/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1782 - binary_accuracy: 0.9454\n",
            "Epoch 00072: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1784 - binary_accuracy: 0.9453 - val_loss: 1.5856 - val_binary_accuracy: 0.3687\n",
            "Epoch 73/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1755 - binary_accuracy: 0.9466\n",
            "Epoch 00073: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1754 - binary_accuracy: 0.9467 - val_loss: 1.4743 - val_binary_accuracy: 0.3874\n",
            "Epoch 74/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1749 - binary_accuracy: 0.9462\n",
            "Epoch 00074: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1755 - binary_accuracy: 0.9460 - val_loss: 1.4118 - val_binary_accuracy: 0.3992\n",
            "Epoch 75/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1754 - binary_accuracy: 0.9461\n",
            "Epoch 00075: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1755 - binary_accuracy: 0.9461 - val_loss: 1.5530 - val_binary_accuracy: 0.3575\n",
            "Epoch 76/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1746 - binary_accuracy: 0.9466\n",
            "Epoch 00076: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1746 - binary_accuracy: 0.9466 - val_loss: 1.5341 - val_binary_accuracy: 0.3591\n",
            "Epoch 77/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1744 - binary_accuracy: 0.9471\n",
            "Epoch 00077: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1744 - binary_accuracy: 0.9471 - val_loss: 1.4739 - val_binary_accuracy: 0.3905\n",
            "Epoch 78/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1744 - binary_accuracy: 0.9472\n",
            "Epoch 00078: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1746 - binary_accuracy: 0.9470 - val_loss: 1.5667 - val_binary_accuracy: 0.3984\n",
            "Epoch 79/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1738 - binary_accuracy: 0.9472\n",
            "Epoch 00079: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1739 - binary_accuracy: 0.9472 - val_loss: 1.6175 - val_binary_accuracy: 0.3834\n",
            "Epoch 80/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1744 - binary_accuracy: 0.9463\n",
            "Epoch 00080: val_binary_accuracy did not improve from 0.40037\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1737 - binary_accuracy: 0.9465 - val_loss: 1.7706 - val_binary_accuracy: 0.3745\n",
            "Epoch 81/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1744 - binary_accuracy: 0.9470\n",
            "Epoch 00081: val_binary_accuracy improved from 0.40037 to 0.40400, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.1744 - binary_accuracy: 0.9470 - val_loss: 1.5237 - val_binary_accuracy: 0.4040\n",
            "Epoch 82/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1732 - binary_accuracy: 0.9470\n",
            "Epoch 00082: val_binary_accuracy did not improve from 0.40400\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1736 - binary_accuracy: 0.9470 - val_loss: 1.5524 - val_binary_accuracy: 0.3720\n",
            "Epoch 83/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1715 - binary_accuracy: 0.9473\n",
            "Epoch 00083: val_binary_accuracy improved from 0.40400 to 0.40593, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.1714 - binary_accuracy: 0.9471 - val_loss: 1.3746 - val_binary_accuracy: 0.4059\n",
            "Epoch 84/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1722 - binary_accuracy: 0.9476\n",
            "Epoch 00084: val_binary_accuracy did not improve from 0.40593\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1722 - binary_accuracy: 0.9475 - val_loss: 1.4438 - val_binary_accuracy: 0.3907\n",
            "Epoch 85/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1714 - binary_accuracy: 0.9477\n",
            "Epoch 00085: val_binary_accuracy did not improve from 0.40593\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1716 - binary_accuracy: 0.9475 - val_loss: 1.5118 - val_binary_accuracy: 0.3783\n",
            "Epoch 86/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1705 - binary_accuracy: 0.9474\n",
            "Epoch 00086: val_binary_accuracy improved from 0.40593 to 0.41359, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.1708 - binary_accuracy: 0.9474 - val_loss: 1.5599 - val_binary_accuracy: 0.4136\n",
            "Epoch 87/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1710 - binary_accuracy: 0.9479\n",
            "Epoch 00087: val_binary_accuracy improved from 0.41359 to 0.41488, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.1710 - binary_accuracy: 0.9479 - val_loss: 1.4778 - val_binary_accuracy: 0.4149\n",
            "Epoch 88/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1711 - binary_accuracy: 0.9476\n",
            "Epoch 00088: val_binary_accuracy improved from 0.41488 to 0.42367, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.1711 - binary_accuracy: 0.9476 - val_loss: 1.4454 - val_binary_accuracy: 0.4237\n",
            "Epoch 89/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1701 - binary_accuracy: 0.9478\n",
            "Epoch 00089: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1699 - binary_accuracy: 0.9479 - val_loss: 1.7123 - val_binary_accuracy: 0.3804\n",
            "Epoch 90/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1704 - binary_accuracy: 0.9479\n",
            "Epoch 00090: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1708 - binary_accuracy: 0.9478 - val_loss: 1.7000 - val_binary_accuracy: 0.3926\n",
            "Epoch 91/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1708 - binary_accuracy: 0.9478\n",
            "Epoch 00091: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1703 - binary_accuracy: 0.9480 - val_loss: 1.6552 - val_binary_accuracy: 0.3767\n",
            "Epoch 92/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1699 - binary_accuracy: 0.9478\n",
            "Epoch 00092: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1705 - binary_accuracy: 0.9477 - val_loss: 1.4691 - val_binary_accuracy: 0.3805\n",
            "Epoch 93/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1694 - binary_accuracy: 0.9486\n",
            "Epoch 00093: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1696 - binary_accuracy: 0.9485 - val_loss: 1.5515 - val_binary_accuracy: 0.3698\n",
            "Epoch 94/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1674 - binary_accuracy: 0.9481\n",
            "Epoch 00094: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1677 - binary_accuracy: 0.9480 - val_loss: 1.6701 - val_binary_accuracy: 0.3714\n",
            "Epoch 95/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1682 - binary_accuracy: 0.9485\n",
            "Epoch 00095: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1685 - binary_accuracy: 0.9483 - val_loss: 1.4786 - val_binary_accuracy: 0.3827\n",
            "Epoch 96/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1675 - binary_accuracy: 0.9482\n",
            "Epoch 00096: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1677 - binary_accuracy: 0.9482 - val_loss: 1.4748 - val_binary_accuracy: 0.4015\n",
            "Epoch 97/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1676 - binary_accuracy: 0.9492\n",
            "Epoch 00097: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1674 - binary_accuracy: 0.9492 - val_loss: 1.6543 - val_binary_accuracy: 0.3879\n",
            "Epoch 98/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1674 - binary_accuracy: 0.9486\n",
            "Epoch 00098: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1679 - binary_accuracy: 0.9484 - val_loss: 1.5069 - val_binary_accuracy: 0.3997\n",
            "Epoch 99/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1667 - binary_accuracy: 0.9487\n",
            "Epoch 00099: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1670 - binary_accuracy: 0.9485 - val_loss: 1.6641 - val_binary_accuracy: 0.3686\n",
            "Epoch 100/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1668 - binary_accuracy: 0.9489\n",
            "Epoch 00100: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1667 - binary_accuracy: 0.9490 - val_loss: 1.7185 - val_binary_accuracy: 0.3871\n",
            "Epoch 101/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1666 - binary_accuracy: 0.9490\n",
            "Epoch 00101: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1666 - binary_accuracy: 0.9490 - val_loss: 1.5397 - val_binary_accuracy: 0.3998\n",
            "Epoch 102/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1645 - binary_accuracy: 0.9498\n",
            "Epoch 00102: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1645 - binary_accuracy: 0.9498 - val_loss: 1.4625 - val_binary_accuracy: 0.4002\n",
            "Epoch 103/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1644 - binary_accuracy: 0.9488\n",
            "Epoch 00103: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1644 - binary_accuracy: 0.9488 - val_loss: 1.5438 - val_binary_accuracy: 0.3787\n",
            "Epoch 104/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1656 - binary_accuracy: 0.9491\n",
            "Epoch 00104: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1653 - binary_accuracy: 0.9492 - val_loss: 1.6456 - val_binary_accuracy: 0.3681\n",
            "Epoch 105/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1642 - binary_accuracy: 0.9496\n",
            "Epoch 00105: val_binary_accuracy did not improve from 0.42367\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1641 - binary_accuracy: 0.9496 - val_loss: 1.6444 - val_binary_accuracy: 0.3951\n",
            "Epoch 106/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1634 - binary_accuracy: 0.9494\n",
            "Epoch 00106: val_binary_accuracy improved from 0.42367 to 0.42673, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.1631 - binary_accuracy: 0.9495 - val_loss: 1.6143 - val_binary_accuracy: 0.4267\n",
            "Epoch 107/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1655 - binary_accuracy: 0.9492\n",
            "Epoch 00107: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1646 - binary_accuracy: 0.9496 - val_loss: 1.8336 - val_binary_accuracy: 0.3485\n",
            "Epoch 108/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1629 - binary_accuracy: 0.9495\n",
            "Epoch 00108: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1628 - binary_accuracy: 0.9494 - val_loss: 1.6646 - val_binary_accuracy: 0.3858\n",
            "Epoch 109/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1630 - binary_accuracy: 0.9498\n",
            "Epoch 00109: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1628 - binary_accuracy: 0.9499 - val_loss: 1.7291 - val_binary_accuracy: 0.3517\n",
            "Epoch 110/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1623 - binary_accuracy: 0.9496\n",
            "Epoch 00110: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1623 - binary_accuracy: 0.9496 - val_loss: 1.4904 - val_binary_accuracy: 0.4109\n",
            "Epoch 111/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1617 - binary_accuracy: 0.9500\n",
            "Epoch 00111: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1617 - binary_accuracy: 0.9499 - val_loss: 1.6488 - val_binary_accuracy: 0.3621\n",
            "Epoch 112/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1615 - binary_accuracy: 0.9503\n",
            "Epoch 00112: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1615 - binary_accuracy: 0.9502 - val_loss: 1.5322 - val_binary_accuracy: 0.3971\n",
            "Epoch 113/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1612 - binary_accuracy: 0.9504\n",
            "Epoch 00113: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1617 - binary_accuracy: 0.9503 - val_loss: 1.3728 - val_binary_accuracy: 0.4121\n",
            "Epoch 114/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1605 - binary_accuracy: 0.9507\n",
            "Epoch 00114: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1603 - binary_accuracy: 0.9506 - val_loss: 1.5676 - val_binary_accuracy: 0.3861\n",
            "Epoch 115/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1603 - binary_accuracy: 0.9507\n",
            "Epoch 00115: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1603 - binary_accuracy: 0.9507 - val_loss: 1.5140 - val_binary_accuracy: 0.3829\n",
            "Epoch 116/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1601 - binary_accuracy: 0.9511\n",
            "Epoch 00116: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1602 - binary_accuracy: 0.9511 - val_loss: 1.5742 - val_binary_accuracy: 0.3852\n",
            "Epoch 117/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1600 - binary_accuracy: 0.9511\n",
            "Epoch 00117: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1599 - binary_accuracy: 0.9511 - val_loss: 1.6291 - val_binary_accuracy: 0.3645\n",
            "Epoch 118/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1600 - binary_accuracy: 0.9507\n",
            "Epoch 00118: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1600 - binary_accuracy: 0.9507 - val_loss: 1.6520 - val_binary_accuracy: 0.3913\n",
            "Epoch 119/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1595 - binary_accuracy: 0.9501\n",
            "Epoch 00119: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1594 - binary_accuracy: 0.9501 - val_loss: 1.5357 - val_binary_accuracy: 0.4181\n",
            "Epoch 120/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1573 - binary_accuracy: 0.9512\n",
            "Epoch 00120: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1574 - binary_accuracy: 0.9512 - val_loss: 1.8498 - val_binary_accuracy: 0.3483\n",
            "Epoch 121/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1594 - binary_accuracy: 0.9505\n",
            "Epoch 00121: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1598 - binary_accuracy: 0.9504 - val_loss: 1.7351 - val_binary_accuracy: 0.3845\n",
            "Epoch 122/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1571 - binary_accuracy: 0.9509\n",
            "Epoch 00122: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1571 - binary_accuracy: 0.9509 - val_loss: 1.4280 - val_binary_accuracy: 0.4049\n",
            "Epoch 123/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1574 - binary_accuracy: 0.9514\n",
            "Epoch 00123: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1575 - binary_accuracy: 0.9513 - val_loss: 1.4460 - val_binary_accuracy: 0.4205\n",
            "Epoch 124/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1575 - binary_accuracy: 0.9515\n",
            "Epoch 00124: val_binary_accuracy did not improve from 0.42673\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1576 - binary_accuracy: 0.9515 - val_loss: 1.6499 - val_binary_accuracy: 0.3959\n",
            "Epoch 125/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1568 - binary_accuracy: 0.9511\n",
            "Epoch 00125: val_binary_accuracy improved from 0.42673 to 0.44075, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.1566 - binary_accuracy: 0.9512 - val_loss: 1.4403 - val_binary_accuracy: 0.4408\n",
            "Epoch 126/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1562 - binary_accuracy: 0.9523\n",
            "Epoch 00126: val_binary_accuracy did not improve from 0.44075\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1563 - binary_accuracy: 0.9523 - val_loss: 1.5000 - val_binary_accuracy: 0.4061\n",
            "Epoch 127/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1562 - binary_accuracy: 0.9515\n",
            "Epoch 00127: val_binary_accuracy did not improve from 0.44075\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1557 - binary_accuracy: 0.9517 - val_loss: 1.7116 - val_binary_accuracy: 0.4005\n",
            "Epoch 128/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1543 - binary_accuracy: 0.9525\n",
            "Epoch 00128: val_binary_accuracy did not improve from 0.44075\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1540 - binary_accuracy: 0.9526 - val_loss: 1.5294 - val_binary_accuracy: 0.3964\n",
            "Epoch 129/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1541 - binary_accuracy: 0.9527\n",
            "Epoch 00129: val_binary_accuracy did not improve from 0.44075\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1543 - binary_accuracy: 0.9527 - val_loss: 1.7567 - val_binary_accuracy: 0.3925\n",
            "Epoch 130/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1545 - binary_accuracy: 0.9522\n",
            "Epoch 00130: val_binary_accuracy did not improve from 0.44075\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1544 - binary_accuracy: 0.9522 - val_loss: 1.7039 - val_binary_accuracy: 0.3918\n",
            "Epoch 131/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1552 - binary_accuracy: 0.9517\n",
            "Epoch 00131: val_binary_accuracy did not improve from 0.44075\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1553 - binary_accuracy: 0.9517 - val_loss: 1.5676 - val_binary_accuracy: 0.3838\n",
            "Epoch 132/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1543 - binary_accuracy: 0.9521\n",
            "Epoch 00132: val_binary_accuracy improved from 0.44075 to 0.45534, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.1544 - binary_accuracy: 0.9520 - val_loss: 1.3229 - val_binary_accuracy: 0.4553\n",
            "Epoch 133/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1524 - binary_accuracy: 0.9528\n",
            "Epoch 00133: val_binary_accuracy did not improve from 0.45534\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1524 - binary_accuracy: 0.9528 - val_loss: 1.6696 - val_binary_accuracy: 0.4022\n",
            "Epoch 134/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1526 - binary_accuracy: 0.9518\n",
            "Epoch 00134: val_binary_accuracy did not improve from 0.45534\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1528 - binary_accuracy: 0.9519 - val_loss: 1.5644 - val_binary_accuracy: 0.4093\n",
            "Epoch 135/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1519 - binary_accuracy: 0.9532\n",
            "Epoch 00135: val_binary_accuracy did not improve from 0.45534\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1516 - binary_accuracy: 0.9534 - val_loss: 1.6226 - val_binary_accuracy: 0.4143\n",
            "Epoch 136/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1526 - binary_accuracy: 0.9515\n",
            "Epoch 00136: val_binary_accuracy did not improve from 0.45534\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1526 - binary_accuracy: 0.9515 - val_loss: 1.4781 - val_binary_accuracy: 0.4399\n",
            "Epoch 137/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1523 - binary_accuracy: 0.9532\n",
            "Epoch 00137: val_binary_accuracy did not improve from 0.45534\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1527 - binary_accuracy: 0.9530 - val_loss: 1.4994 - val_binary_accuracy: 0.4331\n",
            "Epoch 138/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1510 - binary_accuracy: 0.9530\n",
            "Epoch 00138: val_binary_accuracy improved from 0.45534 to 0.45688, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.1520 - binary_accuracy: 0.9525 - val_loss: 1.3691 - val_binary_accuracy: 0.4569\n",
            "Epoch 139/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1523 - binary_accuracy: 0.9533\n",
            "Epoch 00139: val_binary_accuracy did not improve from 0.45688\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1517 - binary_accuracy: 0.9534 - val_loss: 1.7740 - val_binary_accuracy: 0.4113\n",
            "Epoch 140/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1516 - binary_accuracy: 0.9522\n",
            "Epoch 00140: val_binary_accuracy did not improve from 0.45688\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1510 - binary_accuracy: 0.9525 - val_loss: 1.6676 - val_binary_accuracy: 0.3812\n",
            "Epoch 141/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1504 - binary_accuracy: 0.9527\n",
            "Epoch 00141: val_binary_accuracy did not improve from 0.45688\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1512 - binary_accuracy: 0.9523 - val_loss: 1.6116 - val_binary_accuracy: 0.4121\n",
            "Epoch 142/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1506 - binary_accuracy: 0.9528\n",
            "Epoch 00142: val_binary_accuracy did not improve from 0.45688\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1507 - binary_accuracy: 0.9529 - val_loss: 1.4575 - val_binary_accuracy: 0.4331\n",
            "Epoch 143/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1515 - binary_accuracy: 0.9534\n",
            "Epoch 00143: val_binary_accuracy did not improve from 0.45688\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1516 - binary_accuracy: 0.9534 - val_loss: 1.6200 - val_binary_accuracy: 0.4122\n",
            "Epoch 144/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1504 - binary_accuracy: 0.9533\n",
            "Epoch 00144: val_binary_accuracy did not improve from 0.45688\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1500 - binary_accuracy: 0.9533 - val_loss: 1.5557 - val_binary_accuracy: 0.4557\n",
            "Epoch 145/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1491 - binary_accuracy: 0.9540\n",
            "Epoch 00145: val_binary_accuracy improved from 0.45688 to 0.49065, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.1491 - binary_accuracy: 0.9540 - val_loss: 1.2769 - val_binary_accuracy: 0.4906\n",
            "Epoch 146/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1498 - binary_accuracy: 0.9537\n",
            "Epoch 00146: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1496 - binary_accuracy: 0.9539 - val_loss: 1.6311 - val_binary_accuracy: 0.4099\n",
            "Epoch 147/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1477 - binary_accuracy: 0.9539\n",
            "Epoch 00147: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1477 - binary_accuracy: 0.9539 - val_loss: 1.5807 - val_binary_accuracy: 0.4335\n",
            "Epoch 148/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1486 - binary_accuracy: 0.9537\n",
            "Epoch 00148: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1486 - binary_accuracy: 0.9537 - val_loss: 1.6736 - val_binary_accuracy: 0.4371\n",
            "Epoch 149/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1494 - binary_accuracy: 0.9530\n",
            "Epoch 00149: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1488 - binary_accuracy: 0.9531 - val_loss: 1.5783 - val_binary_accuracy: 0.4300\n",
            "Epoch 150/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1487 - binary_accuracy: 0.9538\n",
            "Epoch 00150: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1489 - binary_accuracy: 0.9537 - val_loss: 1.5401 - val_binary_accuracy: 0.4453\n",
            "Epoch 151/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1471 - binary_accuracy: 0.9551\n",
            "Epoch 00151: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1471 - binary_accuracy: 0.9551 - val_loss: 1.6517 - val_binary_accuracy: 0.4100\n",
            "Epoch 152/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1468 - binary_accuracy: 0.9547\n",
            "Epoch 00152: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1467 - binary_accuracy: 0.9548 - val_loss: 1.6210 - val_binary_accuracy: 0.4316\n",
            "Epoch 153/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1480 - binary_accuracy: 0.9552\n",
            "Epoch 00153: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1476 - binary_accuracy: 0.9553 - val_loss: 1.8515 - val_binary_accuracy: 0.4001\n",
            "Epoch 154/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1474 - binary_accuracy: 0.9543\n",
            "Epoch 00154: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1474 - binary_accuracy: 0.9543 - val_loss: 1.7370 - val_binary_accuracy: 0.4129\n",
            "Epoch 155/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1455 - binary_accuracy: 0.9553\n",
            "Epoch 00155: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1452 - binary_accuracy: 0.9554 - val_loss: 1.5604 - val_binary_accuracy: 0.4611\n",
            "Epoch 156/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1477 - binary_accuracy: 0.9542\n",
            "Epoch 00156: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1479 - binary_accuracy: 0.9541 - val_loss: 1.4478 - val_binary_accuracy: 0.4678\n",
            "Epoch 157/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1468 - binary_accuracy: 0.9549\n",
            "Epoch 00157: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1466 - binary_accuracy: 0.9551 - val_loss: 1.4789 - val_binary_accuracy: 0.4762\n",
            "Epoch 158/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1463 - binary_accuracy: 0.9546\n",
            "Epoch 00158: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1457 - binary_accuracy: 0.9548 - val_loss: 1.5551 - val_binary_accuracy: 0.4715\n",
            "Epoch 159/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1444 - binary_accuracy: 0.9555\n",
            "Epoch 00159: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1449 - binary_accuracy: 0.9553 - val_loss: 1.3981 - val_binary_accuracy: 0.4543\n",
            "Epoch 160/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1450 - binary_accuracy: 0.9544\n",
            "Epoch 00160: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1445 - binary_accuracy: 0.9546 - val_loss: 1.8280 - val_binary_accuracy: 0.4221\n",
            "Epoch 161/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1461 - binary_accuracy: 0.9552\n",
            "Epoch 00161: val_binary_accuracy did not improve from 0.49065\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1461 - binary_accuracy: 0.9552 - val_loss: 1.6943 - val_binary_accuracy: 0.4332\n",
            "Epoch 162/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1447 - binary_accuracy: 0.9557\n",
            "Epoch 00162: val_binary_accuracy improved from 0.49065 to 0.50822, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 4s 9ms/step - loss: 0.1451 - binary_accuracy: 0.9555 - val_loss: 1.4234 - val_binary_accuracy: 0.5082\n",
            "Epoch 163/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1429 - binary_accuracy: 0.9551\n",
            "Epoch 00163: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1432 - binary_accuracy: 0.9550 - val_loss: 1.6841 - val_binary_accuracy: 0.4569\n",
            "Epoch 164/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1441 - binary_accuracy: 0.9558\n",
            "Epoch 00164: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1435 - binary_accuracy: 0.9559 - val_loss: 1.4820 - val_binary_accuracy: 0.4762\n",
            "Epoch 165/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1442 - binary_accuracy: 0.9550\n",
            "Epoch 00165: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1440 - binary_accuracy: 0.9552 - val_loss: 1.6323 - val_binary_accuracy: 0.4510\n",
            "Epoch 166/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1431 - binary_accuracy: 0.9561\n",
            "Epoch 00166: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1430 - binary_accuracy: 0.9562 - val_loss: 1.4641 - val_binary_accuracy: 0.4923\n",
            "Epoch 167/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1421 - binary_accuracy: 0.9558\n",
            "Epoch 00167: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1424 - binary_accuracy: 0.9559 - val_loss: 1.6001 - val_binary_accuracy: 0.4308\n",
            "Epoch 168/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1422 - binary_accuracy: 0.9562\n",
            "Epoch 00168: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1422 - binary_accuracy: 0.9562 - val_loss: 1.7714 - val_binary_accuracy: 0.4029\n",
            "Epoch 169/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1417 - binary_accuracy: 0.9568\n",
            "Epoch 00169: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1417 - binary_accuracy: 0.9568 - val_loss: 1.4815 - val_binary_accuracy: 0.4687\n",
            "Epoch 170/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1422 - binary_accuracy: 0.9558\n",
            "Epoch 00170: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1423 - binary_accuracy: 0.9558 - val_loss: 1.7313 - val_binary_accuracy: 0.4491\n",
            "Epoch 171/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1417 - binary_accuracy: 0.9561\n",
            "Epoch 00171: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1417 - binary_accuracy: 0.9561 - val_loss: 1.4728 - val_binary_accuracy: 0.4963\n",
            "Epoch 172/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1409 - binary_accuracy: 0.9576\n",
            "Epoch 00172: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1411 - binary_accuracy: 0.9575 - val_loss: 1.9391 - val_binary_accuracy: 0.4111\n",
            "Epoch 173/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1412 - binary_accuracy: 0.9566\n",
            "Epoch 00173: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1411 - binary_accuracy: 0.9566 - val_loss: 1.6638 - val_binary_accuracy: 0.4578\n",
            "Epoch 174/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1417 - binary_accuracy: 0.9566\n",
            "Epoch 00174: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1417 - binary_accuracy: 0.9566 - val_loss: 1.7681 - val_binary_accuracy: 0.4465\n",
            "Epoch 175/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1403 - binary_accuracy: 0.9562\n",
            "Epoch 00175: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1408 - binary_accuracy: 0.9561 - val_loss: 1.7365 - val_binary_accuracy: 0.4277\n",
            "Epoch 176/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1400 - binary_accuracy: 0.9562\n",
            "Epoch 00176: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1395 - binary_accuracy: 0.9565 - val_loss: 1.8783 - val_binary_accuracy: 0.4095\n",
            "Epoch 177/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1395 - binary_accuracy: 0.9567\n",
            "Epoch 00177: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1398 - binary_accuracy: 0.9565 - val_loss: 1.6895 - val_binary_accuracy: 0.4359\n",
            "Epoch 178/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1396 - binary_accuracy: 0.9568\n",
            "Epoch 00178: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1396 - binary_accuracy: 0.9568 - val_loss: 1.5823 - val_binary_accuracy: 0.4775\n",
            "Epoch 179/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1403 - binary_accuracy: 0.9573\n",
            "Epoch 00179: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1402 - binary_accuracy: 0.9573 - val_loss: 1.5234 - val_binary_accuracy: 0.4824\n",
            "Epoch 180/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1408 - binary_accuracy: 0.9560\n",
            "Epoch 00180: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1407 - binary_accuracy: 0.9561 - val_loss: 1.5888 - val_binary_accuracy: 0.4561\n",
            "Epoch 181/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1396 - binary_accuracy: 0.9574\n",
            "Epoch 00181: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1396 - binary_accuracy: 0.9574 - val_loss: 1.8589 - val_binary_accuracy: 0.4182\n",
            "Epoch 182/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1416 - binary_accuracy: 0.9562\n",
            "Epoch 00182: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1417 - binary_accuracy: 0.9563 - val_loss: 1.4269 - val_binary_accuracy: 0.4797\n",
            "Epoch 183/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1392 - binary_accuracy: 0.9568\n",
            "Epoch 00183: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1389 - binary_accuracy: 0.9569 - val_loss: 2.0830 - val_binary_accuracy: 0.3984\n",
            "Epoch 184/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1372 - binary_accuracy: 0.9581\n",
            "Epoch 00184: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1377 - binary_accuracy: 0.9581 - val_loss: 1.6343 - val_binary_accuracy: 0.4563\n",
            "Epoch 185/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1380 - binary_accuracy: 0.9583\n",
            "Epoch 00185: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1380 - binary_accuracy: 0.9583 - val_loss: 1.4903 - val_binary_accuracy: 0.4780\n",
            "Epoch 186/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1381 - binary_accuracy: 0.9579\n",
            "Epoch 00186: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1379 - binary_accuracy: 0.9579 - val_loss: 1.5350 - val_binary_accuracy: 0.4645\n",
            "Epoch 187/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1365 - binary_accuracy: 0.9583\n",
            "Epoch 00187: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1367 - binary_accuracy: 0.9582 - val_loss: 1.7467 - val_binary_accuracy: 0.4620\n",
            "Epoch 188/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1375 - binary_accuracy: 0.9588\n",
            "Epoch 00188: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1367 - binary_accuracy: 0.9589 - val_loss: 1.7472 - val_binary_accuracy: 0.4229\n",
            "Epoch 189/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1367 - binary_accuracy: 0.9578\n",
            "Epoch 00189: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1366 - binary_accuracy: 0.9579 - val_loss: 1.5570 - val_binary_accuracy: 0.4769\n",
            "Epoch 190/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1365 - binary_accuracy: 0.9577\n",
            "Epoch 00190: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1364 - binary_accuracy: 0.9578 - val_loss: 1.5682 - val_binary_accuracy: 0.4778\n",
            "Epoch 191/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1364 - binary_accuracy: 0.9591\n",
            "Epoch 00191: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1364 - binary_accuracy: 0.9591 - val_loss: 1.6477 - val_binary_accuracy: 0.4485\n",
            "Epoch 192/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1351 - binary_accuracy: 0.9591\n",
            "Epoch 00192: val_binary_accuracy did not improve from 0.50822\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1352 - binary_accuracy: 0.9590 - val_loss: 1.4784 - val_binary_accuracy: 0.4857\n",
            "Epoch 193/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1352 - binary_accuracy: 0.9591\n",
            "Epoch 00193: val_binary_accuracy improved from 0.50822 to 0.51781, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 8ms/step - loss: 0.1358 - binary_accuracy: 0.9589 - val_loss: 1.3813 - val_binary_accuracy: 0.5178\n",
            "Epoch 194/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1360 - binary_accuracy: 0.9586\n",
            "Epoch 00194: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1362 - binary_accuracy: 0.9584 - val_loss: 1.4996 - val_binary_accuracy: 0.4893\n",
            "Epoch 195/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1345 - binary_accuracy: 0.9591\n",
            "Epoch 00195: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1344 - binary_accuracy: 0.9592 - val_loss: 1.7825 - val_binary_accuracy: 0.4562\n",
            "Epoch 196/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1362 - binary_accuracy: 0.9582\n",
            "Epoch 00196: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1362 - binary_accuracy: 0.9583 - val_loss: 1.8105 - val_binary_accuracy: 0.4287\n",
            "Epoch 197/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1341 - binary_accuracy: 0.9597\n",
            "Epoch 00197: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1344 - binary_accuracy: 0.9593 - val_loss: 1.4356 - val_binary_accuracy: 0.5039\n",
            "Epoch 198/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1329 - binary_accuracy: 0.9599\n",
            "Epoch 00198: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1329 - binary_accuracy: 0.9599 - val_loss: 1.6855 - val_binary_accuracy: 0.4695\n",
            "Epoch 199/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1341 - binary_accuracy: 0.9590\n",
            "Epoch 00199: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1339 - binary_accuracy: 0.9590 - val_loss: 1.5122 - val_binary_accuracy: 0.4902\n",
            "Epoch 200/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1350 - binary_accuracy: 0.9587\n",
            "Epoch 00200: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1345 - binary_accuracy: 0.9589 - val_loss: 1.8030 - val_binary_accuracy: 0.4381\n",
            "Epoch 201/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1345 - binary_accuracy: 0.9588\n",
            "Epoch 00201: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1342 - binary_accuracy: 0.9589 - val_loss: 1.6484 - val_binary_accuracy: 0.4694\n",
            "Epoch 202/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1336 - binary_accuracy: 0.9595\n",
            "Epoch 00202: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1337 - binary_accuracy: 0.9595 - val_loss: 1.8181 - val_binary_accuracy: 0.4149\n",
            "Epoch 203/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1330 - binary_accuracy: 0.9597\n",
            "Epoch 00203: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1319 - binary_accuracy: 0.9600 - val_loss: 1.7192 - val_binary_accuracy: 0.4672\n",
            "Epoch 204/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1346 - binary_accuracy: 0.9594\n",
            "Epoch 00204: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1346 - binary_accuracy: 0.9593 - val_loss: 1.6328 - val_binary_accuracy: 0.4609\n",
            "Epoch 205/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1327 - binary_accuracy: 0.9595\n",
            "Epoch 00205: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1325 - binary_accuracy: 0.9595 - val_loss: 1.6782 - val_binary_accuracy: 0.4678\n",
            "Epoch 206/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1329 - binary_accuracy: 0.9592\n",
            "Epoch 00206: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1330 - binary_accuracy: 0.9592 - val_loss: 1.6097 - val_binary_accuracy: 0.4772\n",
            "Epoch 207/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1319 - binary_accuracy: 0.9596\n",
            "Epoch 00207: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1321 - binary_accuracy: 0.9595 - val_loss: 1.5355 - val_binary_accuracy: 0.4867\n",
            "Epoch 208/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1311 - binary_accuracy: 0.9604\n",
            "Epoch 00208: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1313 - binary_accuracy: 0.9603 - val_loss: 1.7449 - val_binary_accuracy: 0.4309\n",
            "Epoch 209/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1322 - binary_accuracy: 0.9601\n",
            "Epoch 00209: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1323 - binary_accuracy: 0.9601 - val_loss: 1.5341 - val_binary_accuracy: 0.4869\n",
            "Epoch 210/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1323 - binary_accuracy: 0.9600\n",
            "Epoch 00210: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1323 - binary_accuracy: 0.9600 - val_loss: 1.6047 - val_binary_accuracy: 0.4717\n",
            "Epoch 211/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1321 - binary_accuracy: 0.9603\n",
            "Epoch 00211: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1318 - binary_accuracy: 0.9604 - val_loss: 1.7312 - val_binary_accuracy: 0.4593\n",
            "Epoch 212/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1329 - binary_accuracy: 0.9599\n",
            "Epoch 00212: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1329 - binary_accuracy: 0.9598 - val_loss: 1.7210 - val_binary_accuracy: 0.4557\n",
            "Epoch 213/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1306 - binary_accuracy: 0.9604\n",
            "Epoch 00213: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1304 - binary_accuracy: 0.9605 - val_loss: 1.6979 - val_binary_accuracy: 0.4585\n",
            "Epoch 214/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1292 - binary_accuracy: 0.9610\n",
            "Epoch 00214: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1292 - binary_accuracy: 0.9610 - val_loss: 1.6972 - val_binary_accuracy: 0.4728\n",
            "Epoch 215/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1303 - binary_accuracy: 0.9607\n",
            "Epoch 00215: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1302 - binary_accuracy: 0.9607 - val_loss: 1.6111 - val_binary_accuracy: 0.4743\n",
            "Epoch 216/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1306 - binary_accuracy: 0.9601\n",
            "Epoch 00216: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1305 - binary_accuracy: 0.9601 - val_loss: 1.7768 - val_binary_accuracy: 0.4395\n",
            "Epoch 217/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1293 - binary_accuracy: 0.9608\n",
            "Epoch 00217: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1288 - binary_accuracy: 0.9609 - val_loss: 1.6195 - val_binary_accuracy: 0.4931\n",
            "Epoch 218/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1295 - binary_accuracy: 0.9605\n",
            "Epoch 00218: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1294 - binary_accuracy: 0.9605 - val_loss: 1.6865 - val_binary_accuracy: 0.4754\n",
            "Epoch 219/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1303 - binary_accuracy: 0.9607\n",
            "Epoch 00219: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1303 - binary_accuracy: 0.9607 - val_loss: 1.9355 - val_binary_accuracy: 0.4350\n",
            "Epoch 220/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1298 - binary_accuracy: 0.9606\n",
            "Epoch 00220: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1298 - binary_accuracy: 0.9606 - val_loss: 1.7461 - val_binary_accuracy: 0.4320\n",
            "Epoch 221/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1299 - binary_accuracy: 0.9605\n",
            "Epoch 00221: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1298 - binary_accuracy: 0.9605 - val_loss: 1.5258 - val_binary_accuracy: 0.5107\n",
            "Epoch 222/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1310 - binary_accuracy: 0.9606\n",
            "Epoch 00222: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1306 - binary_accuracy: 0.9607 - val_loss: 1.5499 - val_binary_accuracy: 0.4926\n",
            "Epoch 223/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1268 - binary_accuracy: 0.9622\n",
            "Epoch 00223: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1268 - binary_accuracy: 0.9622 - val_loss: 1.6058 - val_binary_accuracy: 0.4702\n",
            "Epoch 224/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1273 - binary_accuracy: 0.9614\n",
            "Epoch 00224: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1267 - binary_accuracy: 0.9616 - val_loss: 1.9202 - val_binary_accuracy: 0.4557\n",
            "Epoch 225/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1292 - binary_accuracy: 0.9608\n",
            "Epoch 00225: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1293 - binary_accuracy: 0.9607 - val_loss: 1.4813 - val_binary_accuracy: 0.5160\n",
            "Epoch 226/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1291 - binary_accuracy: 0.9610\n",
            "Epoch 00226: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1293 - binary_accuracy: 0.9610 - val_loss: 1.6901 - val_binary_accuracy: 0.4591\n",
            "Epoch 227/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1276 - binary_accuracy: 0.9620\n",
            "Epoch 00227: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1275 - binary_accuracy: 0.9620 - val_loss: 1.7478 - val_binary_accuracy: 0.4761\n",
            "Epoch 228/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1277 - binary_accuracy: 0.9615\n",
            "Epoch 00228: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1276 - binary_accuracy: 0.9616 - val_loss: 1.7470 - val_binary_accuracy: 0.4624\n",
            "Epoch 229/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1263 - binary_accuracy: 0.9618\n",
            "Epoch 00229: val_binary_accuracy did not improve from 0.51781\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1263 - binary_accuracy: 0.9618 - val_loss: 1.8898 - val_binary_accuracy: 0.4341\n",
            "Epoch 230/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1272 - binary_accuracy: 0.9623\n",
            "Epoch 00230: val_binary_accuracy improved from 0.51781 to 0.52394, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.1276 - binary_accuracy: 0.9621 - val_loss: 1.4711 - val_binary_accuracy: 0.5239\n",
            "Epoch 231/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1255 - binary_accuracy: 0.9628\n",
            "Epoch 00231: val_binary_accuracy did not improve from 0.52394\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1255 - binary_accuracy: 0.9628 - val_loss: 1.9467 - val_binary_accuracy: 0.4123\n",
            "Epoch 232/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1251 - binary_accuracy: 0.9619\n",
            "Epoch 00232: val_binary_accuracy improved from 0.52394 to 0.55086, saving model to /content/\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "388/388 [==============================] - 3s 9ms/step - loss: 0.1254 - binary_accuracy: 0.9620 - val_loss: 1.3784 - val_binary_accuracy: 0.5509\n",
            "Epoch 233/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1265 - binary_accuracy: 0.9619\n",
            "Epoch 00233: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1264 - binary_accuracy: 0.9619 - val_loss: 1.6215 - val_binary_accuracy: 0.4854\n",
            "Epoch 234/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1252 - binary_accuracy: 0.9617\n",
            "Epoch 00234: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1253 - binary_accuracy: 0.9618 - val_loss: 1.6901 - val_binary_accuracy: 0.4719\n",
            "Epoch 235/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1271 - binary_accuracy: 0.9616\n",
            "Epoch 00235: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1271 - binary_accuracy: 0.9616 - val_loss: 1.7518 - val_binary_accuracy: 0.4595\n",
            "Epoch 236/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1259 - binary_accuracy: 0.9624\n",
            "Epoch 00236: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1261 - binary_accuracy: 0.9623 - val_loss: 1.6165 - val_binary_accuracy: 0.5073\n",
            "Epoch 237/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1250 - binary_accuracy: 0.9625\n",
            "Epoch 00237: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1253 - binary_accuracy: 0.9624 - val_loss: 1.5452 - val_binary_accuracy: 0.4994\n",
            "Epoch 238/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1246 - binary_accuracy: 0.9627\n",
            "Epoch 00238: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1249 - binary_accuracy: 0.9626 - val_loss: 1.6768 - val_binary_accuracy: 0.4776\n",
            "Epoch 239/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1251 - binary_accuracy: 0.9617\n",
            "Epoch 00239: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1250 - binary_accuracy: 0.9618 - val_loss: 1.7043 - val_binary_accuracy: 0.4788\n",
            "Epoch 240/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1246 - binary_accuracy: 0.9629\n",
            "Epoch 00240: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1240 - binary_accuracy: 0.9631 - val_loss: 1.7998 - val_binary_accuracy: 0.4568\n",
            "Epoch 241/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1256 - binary_accuracy: 0.9621\n",
            "Epoch 00241: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1255 - binary_accuracy: 0.9622 - val_loss: 1.8282 - val_binary_accuracy: 0.4412\n",
            "Epoch 242/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1239 - binary_accuracy: 0.9633\n",
            "Epoch 00242: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1239 - binary_accuracy: 0.9633 - val_loss: 1.6846 - val_binary_accuracy: 0.4784\n",
            "Epoch 243/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1234 - binary_accuracy: 0.9634\n",
            "Epoch 00243: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1233 - binary_accuracy: 0.9633 - val_loss: 1.5806 - val_binary_accuracy: 0.5168\n",
            "Epoch 244/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1246 - binary_accuracy: 0.9624\n",
            "Epoch 00244: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1246 - binary_accuracy: 0.9625 - val_loss: 1.6151 - val_binary_accuracy: 0.4925\n",
            "Epoch 245/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1225 - binary_accuracy: 0.9637\n",
            "Epoch 00245: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1225 - binary_accuracy: 0.9636 - val_loss: 1.7376 - val_binary_accuracy: 0.4786\n",
            "Epoch 246/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1243 - binary_accuracy: 0.9624\n",
            "Epoch 00246: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1242 - binary_accuracy: 0.9624 - val_loss: 1.7206 - val_binary_accuracy: 0.4749\n",
            "Epoch 247/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1220 - binary_accuracy: 0.9640\n",
            "Epoch 00247: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1220 - binary_accuracy: 0.9640 - val_loss: 1.5523 - val_binary_accuracy: 0.5267\n",
            "Epoch 248/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1233 - binary_accuracy: 0.9640\n",
            "Epoch 00248: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1233 - binary_accuracy: 0.9640 - val_loss: 1.8378 - val_binary_accuracy: 0.4408\n",
            "Epoch 249/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1232 - binary_accuracy: 0.9625\n",
            "Epoch 00249: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1233 - binary_accuracy: 0.9625 - val_loss: 1.5463 - val_binary_accuracy: 0.5111\n",
            "Epoch 250/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1236 - binary_accuracy: 0.9624\n",
            "Epoch 00250: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1234 - binary_accuracy: 0.9625 - val_loss: 2.0415 - val_binary_accuracy: 0.4288\n",
            "Epoch 251/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1202 - binary_accuracy: 0.9638\n",
            "Epoch 00251: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1210 - binary_accuracy: 0.9636 - val_loss: 1.7293 - val_binary_accuracy: 0.4800\n",
            "Epoch 252/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1225 - binary_accuracy: 0.9634\n",
            "Epoch 00252: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1230 - binary_accuracy: 0.9633 - val_loss: 1.7287 - val_binary_accuracy: 0.4784\n",
            "Epoch 253/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1227 - binary_accuracy: 0.9635\n",
            "Epoch 00253: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1226 - binary_accuracy: 0.9636 - val_loss: 1.8680 - val_binary_accuracy: 0.4649\n",
            "Epoch 254/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1219 - binary_accuracy: 0.9637\n",
            "Epoch 00254: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1216 - binary_accuracy: 0.9638 - val_loss: 1.7484 - val_binary_accuracy: 0.4829\n",
            "Epoch 255/1000\n",
            "376/388 [============================>.] - ETA: 0s - loss: 0.1212 - binary_accuracy: 0.9635\n",
            "Epoch 00255: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1208 - binary_accuracy: 0.9637 - val_loss: 1.7882 - val_binary_accuracy: 0.4911\n",
            "Epoch 256/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1201 - binary_accuracy: 0.9635\n",
            "Epoch 00256: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1206 - binary_accuracy: 0.9633 - val_loss: 1.7592 - val_binary_accuracy: 0.4713\n",
            "Epoch 257/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1216 - binary_accuracy: 0.9641\n",
            "Epoch 00257: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 6ms/step - loss: 0.1218 - binary_accuracy: 0.9639 - val_loss: 1.7210 - val_binary_accuracy: 0.4673\n",
            "Epoch 258/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1217 - binary_accuracy: 0.9639\n",
            "Epoch 00258: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1222 - binary_accuracy: 0.9636 - val_loss: 1.6603 - val_binary_accuracy: 0.4920\n",
            "Epoch 259/1000\n",
            "380/388 [============================>.] - ETA: 0s - loss: 0.1207 - binary_accuracy: 0.9645\n",
            "Epoch 00259: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1210 - binary_accuracy: 0.9644 - val_loss: 1.8528 - val_binary_accuracy: 0.4467\n",
            "Epoch 260/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1206 - binary_accuracy: 0.9639\n",
            "Epoch 00260: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1206 - binary_accuracy: 0.9640 - val_loss: 1.7275 - val_binary_accuracy: 0.4811\n",
            "Epoch 261/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1219 - binary_accuracy: 0.9634\n",
            "Epoch 00261: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1219 - binary_accuracy: 0.9633 - val_loss: 1.8261 - val_binary_accuracy: 0.4537\n",
            "Epoch 262/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1197 - binary_accuracy: 0.9644\n",
            "Epoch 00262: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1197 - binary_accuracy: 0.9645 - val_loss: 1.6293 - val_binary_accuracy: 0.5128\n",
            "Epoch 263/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1185 - binary_accuracy: 0.9651\n",
            "Epoch 00263: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1185 - binary_accuracy: 0.9651 - val_loss: 1.6648 - val_binary_accuracy: 0.4935\n",
            "Epoch 264/1000\n",
            "381/388 [============================>.] - ETA: 0s - loss: 0.1203 - binary_accuracy: 0.9643\n",
            "Epoch 00264: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1206 - binary_accuracy: 0.9642 - val_loss: 1.4990 - val_binary_accuracy: 0.5228\n",
            "Epoch 265/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1192 - binary_accuracy: 0.9652\n",
            "Epoch 00265: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1190 - binary_accuracy: 0.9652 - val_loss: 1.7599 - val_binary_accuracy: 0.4616\n",
            "Epoch 266/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1192 - binary_accuracy: 0.9644\n",
            "Epoch 00266: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1195 - binary_accuracy: 0.9643 - val_loss: 1.4764 - val_binary_accuracy: 0.5402\n",
            "Epoch 267/1000\n",
            "382/388 [============================>.] - ETA: 0s - loss: 0.1186 - binary_accuracy: 0.9645\n",
            "Epoch 00267: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1183 - binary_accuracy: 0.9646 - val_loss: 1.9569 - val_binary_accuracy: 0.4080\n",
            "Epoch 268/1000\n",
            "383/388 [============================>.] - ETA: 0s - loss: 0.1197 - binary_accuracy: 0.9643\n",
            "Epoch 00268: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1196 - binary_accuracy: 0.9644 - val_loss: 1.7807 - val_binary_accuracy: 0.4802\n",
            "Epoch 269/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1195 - binary_accuracy: 0.9641\n",
            "Epoch 00269: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1199 - binary_accuracy: 0.9640 - val_loss: 1.4681 - val_binary_accuracy: 0.5351\n",
            "Epoch 270/1000\n",
            "387/388 [============================>.] - ETA: 0s - loss: 0.1196 - binary_accuracy: 0.9649\n",
            "Epoch 00270: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1197 - binary_accuracy: 0.9649 - val_loss: 1.8228 - val_binary_accuracy: 0.4643\n",
            "Epoch 271/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1186 - binary_accuracy: 0.9642\n",
            "Epoch 00271: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1184 - binary_accuracy: 0.9643 - val_loss: 1.7970 - val_binary_accuracy: 0.4791\n",
            "Epoch 272/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1189 - binary_accuracy: 0.9640\n",
            "Epoch 00272: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1191 - binary_accuracy: 0.9640 - val_loss: 1.6357 - val_binary_accuracy: 0.4996\n",
            "Epoch 273/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1167 - binary_accuracy: 0.9656\n",
            "Epoch 00273: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1170 - binary_accuracy: 0.9655 - val_loss: 2.0434 - val_binary_accuracy: 0.4237\n",
            "Epoch 274/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1168 - binary_accuracy: 0.9650\n",
            "Epoch 00274: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1169 - binary_accuracy: 0.9649 - val_loss: 1.6994 - val_binary_accuracy: 0.4934\n",
            "Epoch 275/1000\n",
            "377/388 [============================>.] - ETA: 0s - loss: 0.1167 - binary_accuracy: 0.9652\n",
            "Epoch 00275: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1169 - binary_accuracy: 0.9651 - val_loss: 1.9035 - val_binary_accuracy: 0.4673\n",
            "Epoch 276/1000\n",
            "379/388 [============================>.] - ETA: 0s - loss: 0.1194 - binary_accuracy: 0.9645\n",
            "Epoch 00276: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1195 - binary_accuracy: 0.9644 - val_loss: 1.9556 - val_binary_accuracy: 0.4416\n",
            "Epoch 277/1000\n",
            "385/388 [============================>.] - ETA: 0s - loss: 0.1173 - binary_accuracy: 0.9656\n",
            "Epoch 00277: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1172 - binary_accuracy: 0.9657 - val_loss: 1.6737 - val_binary_accuracy: 0.5028\n",
            "Epoch 278/1000\n",
            "384/388 [============================>.] - ETA: 0s - loss: 0.1163 - binary_accuracy: 0.9663\n",
            "Epoch 00278: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1163 - binary_accuracy: 0.9663 - val_loss: 1.4466 - val_binary_accuracy: 0.5393\n",
            "Epoch 279/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1180 - binary_accuracy: 0.9652\n",
            "Epoch 00279: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1184 - binary_accuracy: 0.9651 - val_loss: 1.7610 - val_binary_accuracy: 0.4690\n",
            "Epoch 280/1000\n",
            "378/388 [============================>.] - ETA: 0s - loss: 0.1168 - binary_accuracy: 0.9659\n",
            "Epoch 00280: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1162 - binary_accuracy: 0.9660 - val_loss: 1.8934 - val_binary_accuracy: 0.4434\n",
            "Epoch 281/1000\n",
            "386/388 [============================>.] - ETA: 0s - loss: 0.1168 - binary_accuracy: 0.9665\n",
            "Epoch 00281: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1164 - binary_accuracy: 0.9666 - val_loss: 2.1631 - val_binary_accuracy: 0.4104\n",
            "Epoch 282/1000\n",
            "388/388 [==============================] - ETA: 0s - loss: 0.1159 - binary_accuracy: 0.9663\n",
            "Epoch 00282: val_binary_accuracy did not improve from 0.55086\n",
            "388/388 [==============================] - 2s 5ms/step - loss: 0.1159 - binary_accuracy: 0.9663 - val_loss: 1.6031 - val_binary_accuracy: 0.5152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "bbed9956-26f7-45b3-d895-b0344682662e"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.plot(history.history[\"loss\"], '-b')\n",
        "plt.plot(history.history[\"val_loss\"], '-r')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(history.history[\"binary_accuracy\"], '-b')\n",
        "plt.plot(history.history[\"val_binary_accuracy\"], '-r')\n",
        "\n",
        "my_model = new_model = tf.keras.models.load_model('/content/saved_model.pb')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-37184e543a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_binary_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/saved_model.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/saved_model.pb/{saved_model.pbtxt|saved_model.pb}"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf69ce55-0603-495f-ca8f-df712e961384"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size):\n",
        "  test_size = 12800\n",
        "  y_pred = (model.predict(X_test[:test_size], batch_size=128)>0.5).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        "  y_pred[7] = 1\n",
        "  y_pred[23] = 1\n",
        "  #pred_dist = np.unique(y_pred.astype(int), return_counts=True)\n",
        "  #correct_prediction = np.unique(y_pred == np.expand_dims(Y_test[:test_size], axis=1), return_counts=True)\n",
        "  #print(pred_dist, correct_prediction[0])\n",
        "  \n",
        "  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)\n",
        "\n",
        "pre, re, f1 = F1_score(my_model, X_test, Y_test, test_size=12800)\n",
        "print(pre, re, f1)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3904923599320883 0.5032822757111597 0.4397705544933078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd1fed7a-bb9f-445c-bc9d-902d34b2ba9e"
      },
      "source": [
        "my_model.evaluate(X_test, Y_test)\n",
        "prediction = my_model.predict(X_test)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3691/3691 [==============================] - 11s 3ms/step - loss: 0.1487 - binary_accuracy: 0.9566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4640a3b3-ff40-4285-8255-453a7d46dbf7"
      },
      "source": [
        "prediction = np.squeeze(prediction, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(prediction, bins=[0,1,2])\n",
        "\n",
        "\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.5).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 2.08% 3.59%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Div--L7lUVFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96ed470c-10b5-4c39-94f8-4f6d97095419"
      },
      "source": [
        "fraud_count"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([115650,   2458]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}