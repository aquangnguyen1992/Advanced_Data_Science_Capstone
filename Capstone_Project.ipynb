{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "97476974-879c-465b-8351-7a0739f223f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fcf15b55-35c6-4906-a15d-92a97746a672"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "58e5fded-5e42-4aa0-82e8-314a3e4cb337"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)\n",
        "saved_columns= np.array(dataset_identity.columns)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "66b0a8a5-2fa8-453a-b120-531fa30c6c96"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e324d2d1-f75c-4274-8b2c-ee68e250b93f"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4MWdmZ8wBEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = dataset_identity.pop(column)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS60VEEMwgHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3c2d315-5daf-4721-c868-63c49aca6ea3"
      },
      "source": [
        "merged_data = pd.merge(left=dataset_transaction, right=dataset_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "dataset_transaction = None\n",
        "dataset_identity = None\n",
        "merged_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 430)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIAS8CbdwwET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "bd57e387-118c-4bbd-abf1-d299e2900288"
      },
      "source": [
        "merged_data.tail(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>590535</th>\n",
              "      <td>3577535</td>\n",
              "      <td>0</td>\n",
              "      <td>15811047</td>\n",
              "      <td>49.00</td>\n",
              "      <td>W</td>\n",
              "      <td>6550</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>226.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>272.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590536</th>\n",
              "      <td>3577536</td>\n",
              "      <td>0</td>\n",
              "      <td>15811049</td>\n",
              "      <td>39.50</td>\n",
              "      <td>W</td>\n",
              "      <td>10444</td>\n",
              "      <td>225.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>224.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>204.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590537</th>\n",
              "      <td>3577537</td>\n",
              "      <td>0</td>\n",
              "      <td>15811079</td>\n",
              "      <td>30.95</td>\n",
              "      <td>W</td>\n",
              "      <td>12037</td>\n",
              "      <td>595.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>224.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>231.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590538</th>\n",
              "      <td>3577538</td>\n",
              "      <td>0</td>\n",
              "      <td>15811088</td>\n",
              "      <td>117.00</td>\n",
              "      <td>W</td>\n",
              "      <td>7826</td>\n",
              "      <td>481.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>224.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>387.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>aol.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590539</th>\n",
              "      <td>3577539</td>\n",
              "      <td>0</td>\n",
              "      <td>15811131</td>\n",
              "      <td>279.95</td>\n",
              "      <td>W</td>\n",
              "      <td>15066</td>\n",
              "      <td>170.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>299.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "590535        3577535        0       15811047  ...    NaN   NaN         NaN\n",
              "590536        3577536        0       15811049  ...    NaN   NaN         NaN\n",
              "590537        3577537        0       15811079  ...    NaN   NaN         NaN\n",
              "590538        3577538        0       15811088  ...    NaN   NaN         NaN\n",
              "590539        3577539        0       15811131  ...    NaN   NaN         NaN\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a287bcf5-00e7-4b23-c4fe-180e72ab70e0"
      },
      "source": [
        "dataset_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "dataset_identity = None\n",
        "\n",
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "cache = dict()\n",
        "print(len(float_columns), len(int_columns), len(obj_columns))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  max_X = np.max(X_temp)\n",
        "  min_X = np.min(X_temp)\n",
        "  mean_X = np.mean(X_temp)\n",
        "  X_out.iloc[indices] = (X_temp - mean_X)/(max_X - min_X)\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0.0\n",
        "\n",
        "  return min_X, max_X, mean_X, X_out.astype('float16')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "3809c310-b5e0-4b8e-a6b8-a5905eb58256"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>32.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  id_37 id_38  DeviceType\n",
              "0        2987000        0          86400  ...    NaN   NaN         NaN\n",
              "1        2987001        0          86401  ...    NaN   NaN         NaN\n",
              "2        2987002        0          86469  ...    NaN   NaN         NaN\n",
              "3        2987003        0          86499  ...    NaN   NaN         NaN\n",
              "4        2987004        0          86506  ...      T     T      mobile\n",
              "\n",
              "[5 rows x 430 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float32')\n",
        "\n",
        "  # Code the NaN column for every features\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)\n",
        "  dataset_transaction[column].astype('float16')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d3387553-d5f4-4336-d93f-8b423a2c1ad0"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 829 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  id_26_NaN_Code  id_32_NaN_Code\n",
              "0        2987000        0  ...               1               1\n",
              "1        2987001        0  ...               1               1\n",
              "2        2987002        0  ...               1               1\n",
              "3        2987003        0  ...               1               1\n",
              "4        2987004        0  ...               1               0\n",
              "\n",
              "[5 rows x 829 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('float32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "3fe27bd9-cc22-4f80-de1a-2d6bab407f7d"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "      <th>id_01_NaN_Code</th>\n",
              "      <th>id_02_NaN_Code</th>\n",
              "      <th>id_03_NaN_Code</th>\n",
              "      <th>id_04_NaN_Code</th>\n",
              "      <th>id_05_NaN_Code</th>\n",
              "      <th>id_06_NaN_Code</th>\n",
              "      <th>id_07_NaN_Code</th>\n",
              "      <th>id_08_NaN_Code</th>\n",
              "      <th>id_09_NaN_Code</th>\n",
              "      <th>id_10_NaN_Code</th>\n",
              "      <th>id_11_NaN_Code</th>\n",
              "      <th>id_13_NaN_Code</th>\n",
              "      <th>id_14_NaN_Code</th>\n",
              "      <th>id_17_NaN_Code</th>\n",
              "      <th>id_18_NaN_Code</th>\n",
              "      <th>id_19_NaN_Code</th>\n",
              "      <th>id_20_NaN_Code</th>\n",
              "      <th>id_21_NaN_Code</th>\n",
              "      <th>id_22_NaN_Code</th>\n",
              "      <th>id_24_NaN_Code</th>\n",
              "      <th>id_25_NaN_Code</th>\n",
              "      <th>id_26_NaN_Code</th>\n",
              "      <th>id_32_NaN_Code</th>\n",
              "      <th>TransactionDT_NaN_Code</th>\n",
              "      <th>card1_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>362.500000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>231.875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>169.625000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>42.343750</td>\n",
              "      <td>69.8125</td>\n",
              "      <td>41.625</td>\n",
              "      <td>146.0</td>\n",
              "      <td>0.561035</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>118.500000</td>\n",
              "      <td>231.875</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>169.625000</td>\n",
              "      <td>28.343750</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>42.343750</td>\n",
              "      <td>69.8125</td>\n",
              "      <td>41.625</td>\n",
              "      <td>146.0</td>\n",
              "      <td>0.561035</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>231.875</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>169.625000</td>\n",
              "      <td>28.343750</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>42.343750</td>\n",
              "      <td>69.8125</td>\n",
              "      <td>41.625</td>\n",
              "      <td>146.0</td>\n",
              "      <td>0.561035</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>118.500000</td>\n",
              "      <td>231.875</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>69.8125</td>\n",
              "      <td>41.625</td>\n",
              "      <td>146.0</td>\n",
              "      <td>0.561035</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>118.500000</td>\n",
              "      <td>231.875</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>169.625000</td>\n",
              "      <td>28.343750</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>42.343750</td>\n",
              "      <td>69.8125</td>\n",
              "      <td>41.625</td>\n",
              "      <td>146.0</td>\n",
              "      <td>0.561035</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 831 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  TransactionDT_NaN_Code  card1_NaN_Code\n",
              "0        2987000        0  ...                       0               0\n",
              "1        2987001        0  ...                       0               0\n",
              "2        2987002        0  ...                       0               0\n",
              "3        2987003        0  ...                       0               0\n",
              "4        2987004        0  ...                       0               0\n",
              "\n",
              "[5 rows x 831 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14ce02d6-fd46-4bce-c60c-9c231a045e7f"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoded_column = 0\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "  cache[column] = dataset_transaction[column].values.reshape(-1,1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuvQmMmLRnM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "5288afe8-d74f-4392-b86b-c9ebeb9688a7"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1011 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 1011 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddacf39b-b303-4040-d7b6-e8f7a696e8e8"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoc4TuIx1zoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0e5273d8-477c-4949-dba7-9fa6b9285f6b"
      },
      "source": [
        "out = ['isFraud']\n",
        "for column in dataset_transaction.columns:\n",
        "  if column.find('R_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "  if column.find('P_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "print(out)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['isFraud', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'P_emaildomain_5', 'P_emaildomain_6', 'P_emaildomain_7', 'P_emaildomain_8', 'P_emaildomain_9', 'P_emaildomain_10', 'P_emaildomain_11', 'P_emaildomain_12', 'P_emaildomain_13', 'P_emaildomain_14', 'P_emaildomain_15', 'P_emaildomain_16', 'P_emaildomain_17', 'P_emaildomain_18', 'P_emaildomain_19', 'P_emaildomain_20', 'P_emaildomain_21', 'P_emaildomain_22', 'P_emaildomain_23', 'P_emaildomain_24', 'P_emaildomain_25', 'P_emaildomain_26', 'P_emaildomain_27', 'P_emaildomain_28', 'P_emaildomain_29', 'P_emaildomain_30', 'P_emaildomain_31', 'P_emaildomain_32', 'P_emaildomain_33', 'P_emaildomain_34', 'P_emaildomain_35', 'P_emaildomain_36', 'P_emaildomain_37', 'P_emaildomain_38', 'P_emaildomain_39', 'P_emaildomain_40', 'P_emaildomain_41', 'P_emaildomain_42', 'P_emaildomain_43', 'P_emaildomain_44', 'P_emaildomain_45', 'P_emaildomain_46', 'P_emaildomain_47', 'P_emaildomain_48', 'P_emaildomain_49', 'P_emaildomain_50', 'P_emaildomain_51', 'P_emaildomain_52', 'P_emaildomain_53', 'P_emaildomain_54', 'P_emaildomain_55', 'P_emaildomain_56', 'P_emaildomain_57', 'P_emaildomain_58', 'P_emaildomain_59', 'R_emaildomain_0', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'R_emaildomain_4', 'R_emaildomain_5', 'R_emaildomain_6', 'R_emaildomain_7', 'R_emaildomain_8', 'R_emaildomain_9', 'R_emaildomain_10', 'R_emaildomain_11', 'R_emaildomain_12', 'R_emaildomain_13', 'R_emaildomain_14', 'R_emaildomain_15', 'R_emaildomain_16', 'R_emaildomain_17', 'R_emaildomain_18', 'R_emaildomain_19', 'R_emaildomain_20', 'R_emaildomain_21', 'R_emaildomain_22', 'R_emaildomain_23', 'R_emaildomain_24', 'R_emaildomain_25', 'R_emaildomain_26', 'R_emaildomain_27', 'R_emaildomain_28', 'R_emaildomain_29', 'R_emaildomain_30', 'R_emaildomain_31', 'R_emaildomain_32', 'R_emaildomain_33', 'R_emaildomain_34', 'R_emaildomain_35', 'R_emaildomain_36', 'R_emaildomain_37', 'R_emaildomain_38', 'R_emaildomain_39', 'R_emaildomain_40', 'R_emaildomain_41', 'R_emaildomain_42', 'R_emaildomain_43', 'R_emaildomain_44', 'R_emaildomain_45', 'R_emaildomain_46', 'R_emaildomain_47', 'R_emaildomain_48', 'R_emaildomain_49', 'R_emaildomain_50', 'R_emaildomain_51', 'R_emaildomain_52', 'R_emaildomain_53', 'R_emaildomain_54', 'R_emaildomain_55', 'R_emaildomain_56', 'R_emaildomain_57', 'R_emaildomain_58', 'R_emaildomain_59', 'R_emaildomain_60']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#columns_to_analyze = ['isFraud', 'DeviceType_0', 'DeviceType_1', 'DeviceType_2', 'id_15_0', 'id_15_1', 'id_15_2', 'id_15_3']#, 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2', 'card1', 'card2', 'card3']\n",
        "columns_to_analyze = out\n",
        "\n",
        "analyzing_data = dataset_transaction[columns_to_analyze]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "to_display = False\n",
        "\n",
        "if to_display:\n",
        "  mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD5CKASq2rzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "7930537f-cabe-41aa-ef17-b88809cc3afc"
      },
      "source": [
        "# Remove the weak correlation features\n",
        "col = corr.columns\n",
        "is_fraud = np.where(col=='isFraud')[0][0]\n",
        "col = col.to_list()\n",
        "col.pop(is_fraud)\n",
        "to_remove = []\n",
        "for each_col in col:\n",
        "  if abs(corr['isFraud'][each_col]) < 0.05: # Weak correlation\n",
        "    to_remove.append(each_col)\n",
        "    a = dataset_transaction.pop(each_col)\n",
        "print(len(to_remove))\n",
        "analyzing_data = None\n",
        "\n",
        "\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 893 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  DeviceType_1  DeviceType_2\n",
              "0        2987000        0  ...             0             0\n",
              "1        2987001        0  ...             0             0\n",
              "2        2987002        0  ...             0             0\n",
              "3        2987003        0  ...             0             0\n",
              "4        2987004        0  ...             0             1\n",
              "\n",
              "[5 rows x 893 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "919603a6-43da-4056-e1b8-25b85752f4eb"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "a = dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>id_15_0</th>\n",
              "      <th>id_15_1</th>\n",
              "      <th>id_15_2</th>\n",
              "      <th>id_15_3</th>\n",
              "      <th>id_16_0</th>\n",
              "      <th>id_16_1</th>\n",
              "      <th>id_16_2</th>\n",
              "      <th>id_23_0</th>\n",
              "      <th>id_23_1</th>\n",
              "      <th>id_23_2</th>\n",
              "      <th>id_23_3</th>\n",
              "      <th>id_27_0</th>\n",
              "      <th>id_27_1</th>\n",
              "      <th>id_27_2</th>\n",
              "      <th>id_28_0</th>\n",
              "      <th>id_28_1</th>\n",
              "      <th>id_28_2</th>\n",
              "      <th>id_29_0</th>\n",
              "      <th>id_29_1</th>\n",
              "      <th>id_29_2</th>\n",
              "      <th>id_34_0</th>\n",
              "      <th>id_34_1</th>\n",
              "      <th>id_34_2</th>\n",
              "      <th>id_34_3</th>\n",
              "      <th>id_34_4</th>\n",
              "      <th>id_35_0</th>\n",
              "      <th>id_35_1</th>\n",
              "      <th>id_35_2</th>\n",
              "      <th>id_36_0</th>\n",
              "      <th>id_36_1</th>\n",
              "      <th>id_36_2</th>\n",
              "      <th>id_37_0</th>\n",
              "      <th>id_37_1</th>\n",
              "      <th>id_37_2</th>\n",
              "      <th>id_38_0</th>\n",
              "      <th>id_38_1</th>\n",
              "      <th>id_38_2</th>\n",
              "      <th>DeviceType_0</th>\n",
              "      <th>DeviceType_1</th>\n",
              "      <th>DeviceType_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 892 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  ...  DeviceType_1  DeviceType_2\n",
              "0        0      -0.463379  ...             0             0\n",
              "1        0      -0.463379  ...             0             0\n",
              "2        0      -0.463379  ...             0             0\n",
              "3        0      -0.463379  ...             0             0\n",
              "4        0      -0.463379  ...             0             1\n",
              "\n",
              "[5 rows x 892 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)\n",
        "X_train = X\n",
        "Y_train = Y\n",
        "\n",
        "#test_size = 20000\n",
        "#indices = np.random.randint(0, len(Y), size=(test_size,))\n",
        "#X_test = np.array(X_train)[indices]\n",
        "#Y_test = np.array(Y_train)[indices]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "cf1d4991-5c18-4894-9da7-f2912e0f6378"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77fee93e-e4bd-439c-85a7-1ab1c88e1a0f"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 5\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_new = np.array(X_train)[indices_0_new]\n",
        "Y_new = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_new, test_size=0.05)\n",
        "\n",
        "X_to_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_train, axis=1)\n",
        "\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[2]))\n",
        "Y_test = np.squeeze(Y_test, axis=1)\n",
        "\n",
        "\n",
        "print(X_to_train.shape, X_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569877, 1)\n",
            "(659162, 891) (34693, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7033a9c0-3a5a-4dcc-f9e5-58b1ad602a55"
      },
      "source": [
        "X_new = None\n",
        "Y_new = None\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "X_train = None\n",
        "Y_train = None\n",
        "X = None\n",
        "Y = None\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 17.87%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW+UlEQVR4nO3df4xndX3v8efr8suflcXdUgLUgdxNzGLagBukSFqVBhaoLk3vNRBbFkulVmw0Nr0XS1IaTVP8p3pJvTZEiZAYkaKtVOHSLWBMSxYZuMCCFFhWLBCUlUWQmIvF+75/fD8rh7nzmZ3Zme93xp3nI/lmzvfz+Zxz3vP5np3XfM/5ztlUFZIkzeY/LXcBkqSVy5CQJHUZEpKkLkNCktRlSEiSug5c7gKW2tq1a2tqamq5y5Cknyt33nnnD6pq3cz2/S4kpqammJ6eXu4yJOnnSpLvztbu6SZJUpchIUnqMiQkSV373TWJxZi6+OvLXYL2Y49edtZylyAtmO8kJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkrnmHRJIDkvzvJF9rz49JcnuSHUm+lOTg1n5Ie76j9U8NtvHR1v5gktMH7Zta244kFw/aZ92HJGkyFvJO4kPAA4PnnwA+WVX/GXgGuKC1XwA809o/2caRZANwDnAcsAn4ny14DgA+DZwBbADObWPn2ockaQLmFRJJjgLOAj7bngd4B3BdG3IVcHZb3tye0/pPbeM3A9dU1QtV9R1gB3Bie+yoqp1V9RPgGmDzXvYhSZqA+b6T+BTw34D/256/HvhhVb3Ynj8OHNmWjwQeA2j9z7bxP2ufsU6vfa59SJImYK8hkeS3gKeq6s4J1LNPklyYZDrJ9K5du5a7HEnab8znncRbgXcleZTRqaB3AP8DODTJgW3MUcATbfkJ4GiA1v864Olh+4x1eu1Pz7GPl6mqK6pqY1VtXLdu3Ty+JUnSfOw1JKrqo1V1VFVNMbrwfEtVvQe4FfgvbdgW4Ktt+fr2nNZ/S1VVaz+nffrpGGA98C3gDmB9+yTTwW0f17d1evuQJE3AYv5O4r8DH0myg9H1g8+19s8Br2/tHwEuBqiq+4FrgW8D/wu4qKp+2q45fBC4idGnp65tY+fahyRpAg7c+5CXVNU3gG+05Z2MPpk0c8z/Af5rZ/2/BP5ylvYbgBtmaZ91H5KkyfAvriVJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKlrryGR5Ogktyb5dpL7k3yotR+WZGuSh9vXNa09SS5PsiPJvUlOGGxrSxv/cJItg/Y3J9ne1rk8SebahyRpMubzTuJF4E+qagNwEnBRkg3AxcDNVbUeuLk9BzgDWN8eFwKfgdEPfOBS4C3AicClgx/6nwHeN1hvU2vv7UOSNAF7DYmqerKq7mrLPwIeAI4ENgNXtWFXAWe35c3A1TWyDTg0yRHA6cDWqtpdVc8AW4FNre8XqmpbVRVw9YxtzbYPSdIELOiaRJIp4HjgduDwqnqydX0POLwtHwk8Nljt8dY2V/vjs7Qzxz5m1nVhkukk07t27VrItyRJmsO8QyLJa4AvAx+uqueGfe0dQC1xbS8z1z6q6oqq2lhVG9etWzfOMiRpVZlXSCQ5iFFAfKGqvtKav99OFdG+PtXanwCOHqx+VGubq/2oWdrn2ockaQLm8+mmAJ8DHqiqvx50XQ/s+YTSFuCrg/bz2qecTgKebaeMbgJOS7KmXbA+Dbip9T2X5KS2r/NmbGu2fUiSJuDAeYx5K/B7wPYkd7e2PwMuA65NcgHwXeDdre8G4ExgB/Bj4L0AVbU7yceBO9q4j1XV7rb8AeDzwCuBG9uDOfYhSZqAvYZEVf0LkE73qbOML+CizrauBK6cpX0aeNMs7U/Ptg9J0mT4F9eSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqSu+dyWQ9ISmLr468tdgvZjj1521li26zsJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6VnxIJNmU5MEkO5JcvNz1SNJqsqJDIskBwKeBM4ANwLlJNixvVZK0eqzokABOBHZU1c6q+glwDbB5mWuSpFVjpf8f10cCjw2ePw68ZeagJBcCF7anzyd5cB/3txb4wT6uO07WtTDWtTDWtTArsq58YtF1vWG2xpUeEvNSVVcAVyx2O0mmq2rjEpS0pKxrYaxrYaxrYVZbXSv9dNMTwNGD50e1NknSBKz0kLgDWJ/kmCQHA+cA1y9zTZK0aqzo001V9WKSDwI3AQcAV1bV/WPc5aJPWY2JdS2MdS2MdS3MqqorVTWO7UqS9gMr/XSTJGkZGRKSpK5VExJ7u71HkkOSfKn1355katD30db+YJLTJ1zXR5J8O8m9SW5O8oZB30+T3N0eS3pBfx51nZ9k12D/fzDo25Lk4fbYMuG6Pjmo6aEkPxz0jWW+klyZ5Kkk93X6k+TyVvO9SU4Y9I1zrvZW13taPduT3JbkVwd9j7b2u5NMT7iutyV5dvBa/fmgb2y36ZlHXX86qOm+djwd1vrGOV9HJ7m1/Ry4P8mHZhkzvmOsqvb7B6OL3o8AxwIHA/cAG2aM+QDwt235HOBLbXlDG38IcEzbzgETrOvtwKva8h/tqas9f34Z5+t84G9mWfcwYGf7uqYtr5lUXTPG/zGjDzuMe75+HTgBuK/TfyZwIxDgJOD2cc/VPOs6ec/+GN365vZB36PA2mWar7cBX1vs67/Udc0Y+07glgnN1xHACW35tcBDs/x7HNsxtlreSczn9h6bgava8nXAqUnS2q+pqheq6jvAjra9idRVVbdW1Y/b022M/lZk3BZzO5TTga1VtbuqngG2ApuWqa5zgS8u0b67quqbwO45hmwGrq6RbcChSY5gvHO117qq6ra2X5jcsTWf+eoZ6216FljXRI4tgKp6sqruass/Ah5gdDeKobEdY6slJGa7vcfMSf7ZmKp6EXgWeP081x1nXUMXMPptYY9XJJlOsi3J2UtU00Lq+p321va6JHv+6HFFzFc7LXcMcMugeVzztTe9usc5Vws189gq4J+S3JnRbW8m7deS3JPkxiTHtbYVMV9JXsXoB+2XB80Tma+MToMfD9w+o2tsx9iK/jsJvSTJ7wIbgd8YNL+hqp5IcixwS5LtVfXIhEr6R+CLVfVCkj9k9C7sHRPa93ycA1xXVT8dtC3nfK1YSd7OKCROGTSf0ubqF4GtSf6t/aY9CXcxeq2eT3Im8A/A+gntez7eCfxrVQ3fdYx9vpK8hlEwfbiqnlvKbc9ltbyTmM/tPX42JsmBwOuAp+e57jjrIslvApcA76qqF/a0V9UT7etO4BuMfsOYSF1V9fSgls8Cb57vuuOsa+AcZpwOGON87U2v7mW/7UySX2H0+m2uqqf3tA/m6ing71m6U6x7VVXPVdXzbfkG4KAka1kB89XMdWyNZb6SHMQoIL5QVV+ZZcj4jrFxXGhZaQ9G75h2Mjr9sOeC13EzxlzEyy9cX9uWj+PlF653snQXrudT1/GMLtatn9G+BjikLa8FHmaJLuLNs64jBsu/DWyrly6UfafVt6YtHzaputq4NzK6kJhJzFfb5hT9C7Fn8fKLit8a91zNs65fZnSN7eQZ7a8GXjtYvg3YNMG6fmnPa8foh+2/t7mb1+s/rrpa/+sYXbd49aTmq33vVwOfmmPM2I6xJZvclf5gdPX/IUY/cC9pbR9j9Ns5wCuAv2v/aL4FHDtY95K23oPAGROu65+B7wN3t8f1rf1kYHv7h7IduGDCdf0VcH/b/63AGwfr/n6bxx3AeydZV3v+F8BlM9Yb23wx+q3ySeA/GJ3zvQB4P/D+1h9G/3nWI23fGyc0V3ur67PAM4Nja7q1H9vm6Z72Gl8y4bo+ODi2tjEIsdle/0nV1cacz+iDLMP1xj1fpzC65nHv4LU6c1LHmLflkCR1rZZrEpKkfWBISJK6DAlJUtd+93cSa9eurampqeUuQ5J+rtx5550/qKp1M9v3u5CYmppienpJ768lSfu9JN+drd3TTZKkLkNCktRlSEiSuva7axKLMXXx15e7BO3HHr3srOUuQVow30lIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUtc8hkeToJLcm+XaS+5N8qLUflmRrkofb1zWtPUkuT7Ijyb1JThhsa0sb/3CSLYP2NyfZ3ta5PEkW881KkhZmMe8kXgT+pKo2ACcBFyXZAFwM3FxV64Gb23OAM4D17XEh8BkYhQpwKfAW4ETg0j3B0sa8b7DepkXUK0laoH0Oiap6sqruass/Ah4AjgQ2A1e1YVcBZ7flzcDVNbINODTJEcDpwNaq2l1VzwBbgU2t7xeqaltVFXD1YFuSpAlYkmsSSaaA44HbgcOr6snW9T3g8LZ8JPDYYLXHW9tc7Y/P0j7b/i9MMp1keteuXYv6XiRJL1l0SCR5DfBl4MNV9dywr70DqMXuY2+q6oqq2lhVG9etWzfu3UnSqrGokEhyEKOA+EJVfaU1f7+dKqJ9faq1PwEcPVj9qNY2V/tRs7RLkiZkMZ9uCvA54IGq+utB1/XAnk8obQG+Omg/r33K6STg2XZa6ibgtCRr2gXr04CbWt9zSU5q+zpvsC1J0gQcuIh13wr8HrA9yd2t7c+Ay4Brk1wAfBd4d+u7ATgT2AH8GHgvQFXtTvJx4I427mNVtbstfwD4PPBK4Mb2kCRNyD6HRFX9C9D7u4VTZxlfwEWdbV0JXDlL+zTwpn2tUZK0OP7FtSSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2LCokkVyZ5Ksl9g7bDkmxN8nD7uqa1J8nlSXYkuTfJCYN1trTxDyfZMmh/c5LtbZ3Lk2Qx9UqSFmax7yQ+D2ya0XYxcHNVrQdubs8BzgDWt8eFwGdgFCrApcBbgBOBS/cESxvzvsF6M/clSRqjRYVEVX0T2D2jeTNwVVu+Cjh70H51jWwDDk1yBHA6sLWqdlfVM8BWYFPr+4Wq2lZVBVw92JYkaQLGcU3i8Kp6si1/Dzi8LR8JPDYY93hrm6v98Vna/z9JLkwynWR6165di/8OJEnAmC9ct3cANc59tP1cUVUbq2rjunXrxr07SVo1xhES32+nimhfn2rtTwBHD8Yd1drmaj9qlnZJ0oSMIySuB/Z8QmkL8NVB+3ntU04nAc+201I3AaclWdMuWJ8G3NT6nktyUvtU03mDbUmSJuDAxayc5IvA24C1SR5n9Cmly4Brk1wAfBd4dxt+A3AmsAP4MfBegKraneTjwB1t3Meqas/F8A8w+gTVK4Eb20OSNCGLComqOrfTdeosYwu4qLOdK4ErZ2mfBt60mBolSfvOv7iWJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6lrUR2Alzd/UxV9f7hK0H3v0srPGsl3fSUiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6VnxIJNmU5MEkO5JcvNz1SNJqsqJDIskBwKeBM4ANwLlJNixvVZK0eqzokABOBHZU1c6q+glwDbB5mWuSpFXjwOUuYC+OBB4bPH8ceMvMQUkuBC5sT59P8uA+7m8t8IN9XHecrGthrGthrGthVmRd+cSi63rDbI0rPSTmpaquAK5Y7HaSTFfVxiUoaUlZ18JY18JY18KstrpW+ummJ4CjB8+Pam2SpAlY6SFxB7A+yTFJDgbOAa5f5pokadVY0aebqurFJB8EbgIOAK6sqvvHuMtFn7IaE+taGOtaGOtamFVVV6pqHNuVJO0HVvrpJknSMjIkJEldqyYk9nZ7jySHJPlS6789ydSg76Ot/cEkp0+4ro8k+XaSe5PcnOQNg76fJrm7PZb0gv486jo/ya7B/v9g0LclycPtsWXCdX1yUNNDSX446BvLfCW5MslTSe7r9CfJ5a3me5OcMOgb51ztra73tHq2J7ktya8O+h5t7XcnmZ5wXW9L8uzgtfrzQd/YbtMzj7r+dFDTfe14Oqz1jXO+jk5ya/s5cH+SD80yZnzHWFXt9w9GF70fAY4FDgbuATbMGPMB4G/b8jnAl9ryhjb+EOCYtp0DJljX24FXteU/2lNXe/78Ms7X+cDfzLLuYcDO9nVNW14zqbpmjP9jRh92GPd8/TpwAnBfp/9M4EYgwEnA7eOeq3nWdfKe/TG69c3tg75HgbXLNF9vA7622Nd/qeuaMfadwC0Tmq8jgBPa8muBh2b59zi2Y2y1vJOYz+09NgNXteXrgFOTpLVfU1UvVNV3gB1texOpq6puraoft6fbGP2tyLgt5nYopwNbq2p3VT0DbAU2LVNd5wJfXKJ9d1XVN4HdcwzZDFxdI9uAQ5McwXjnaq91VdVtbb8wuWNrPvPVM9bb9CywrokcWwBV9WRV3dWWfwQ8wOhuFENjO8ZWS0jMdnuPmZP8szFV9SLwLPD6ea47zrqGLmD028Ier0gynWRbkrOXqKaF1PU77a3tdUn2/NHjipivdlruGOCWQfO45mtvenWPc64WauaxVcA/Jbkzo9veTNqvJbknyY1JjmttK2K+kryK0Q/aLw+aJzJfGZ0GPx64fUbX2I6xFf13EnpJkt8FNgK/MWh+Q1U9keRY4JYk26vqkQmV9I/AF6vqhSR/yOhd2DsmtO/5OAe4rqp+OmhbzvlasZK8nVFInDJoPqXN1S8CW5P8W/tNexLuYvRaPZ/kTOAfgPUT2vd8vBP416oavusY+3wleQ2jYPpwVT23lNuey2p5JzGf23v8bEySA4HXAU/Pc91x1kWS3wQuAd5VVS/saa+qJ9rXncA3GP2GMZG6qurpQS2fBd4833XHWdfAOcw4HTDG+dqbXt3LftuZJL/C6PXbXFVP72kfzNVTwN+zdKdY96qqnquq59vyDcBBSdayAuarmevYGst8JTmIUUB8oaq+MsuQ8R1j47jQstIejN4x7WR0+mHPBa/jZoy5iJdfuL62LR/Hyy9c72TpLlzPp67jGV2sWz+jfQ1wSFteCzzMEl3Em2ddRwyWfxvYVi9dKPtOq29NWz5sUnW1cW9kdCExk5ivts0p+hdiz+LlFxW/Ne65mmddv8zoGtvJM9pfDbx2sHwbsGmCdf3SnteO0Q/bf29zN6/Xf1x1tf7XMbpu8epJzVf73q8GPjXHmLEdY0s2uSv9wejq/0OMfuBe0to+xui3c4BXAH/X/tF8Czh2sO4lbb0HgTMmXNc/A98H7m6P61v7ycD29g9lO3DBhOv6K+D+tv9bgTcO1v39No87gPdOsq72/C+Ay2asN7b5YvRb5ZPAfzA653sB8H7g/a0/jP7zrEfavjdOaK72VtdngWcGx9Z0az+2zdM97TW+ZMJ1fXBwbG1jEGKzvf6TqquNOZ/RB1mG6417vk5hdM3j3sFrdeakjjFvyyFJ6lot1yQkSfvAkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnq+n8ZgZm2Yu/QKwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  \n",
        "  out_model.add(Dense(dense1, activation='relu',\n",
        "                      input_shape=(X_to_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(dense2, activation='relu', \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(int(dense2/2), activation='relu', \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(int(dense2/2), activation='relu',\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[METRICS])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "41701d5a-5e82-4d67-84e1-46c77933d7b8"
      },
      "source": [
        "#my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=1e-4, l2_rate=5e-4, init_std=0.1, lr=0.00008)\n",
        "my_model = create_model(dense1=256, dense2=256, dropout_rate=0.4, l1_rate=5e-5, l2_rate=1e-5, init_std=0.1, lr=0.0001)\n",
        "my_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 477,825\n",
            "Trainable params: 476,545\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6118e8de-ed3f-4e6b-ee7c-b771b5f49d08"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "NB_EPOCH = 2000\n",
        "PATIENCE = 25\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', patience=PATIENCE, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./best_model.h5', monitor='val_auc', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.01, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 1.5121 - tp: 49707.0000 - fp: 68481.0000 - tn: 466915.0000 - fn: 66673.0000 - accuracy: 0.7926 - precision: 0.4206 - recall: 0.4271 - auc: 0.7227\n",
            "Epoch 00001: val_auc improved from -inf to 0.84770, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 15s 12ms/step - loss: 1.5119 - tp: 49759.0000 - fp: 68504.0000 - tn: 467534.0000 - fn: 66773.0000 - accuracy: 0.7927 - precision: 0.4207 - recall: 0.4270 - auc: 0.7228 - val_loss: 1.2378 - val_tp: 357.0000 - val_fp: 33.0000 - val_tn: 5400.0000 - val_fn: 802.0000 - val_accuracy: 0.8733 - val_precision: 0.9154 - val_recall: 0.3080 - val_auc: 0.8477\n",
            "Epoch 2/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 1.1676 - tp: 44489.0000 - fp: 12206.0000 - tn: 522344.0000 - fn: 71713.0000 - accuracy: 0.8710 - precision: 0.7847 - recall: 0.3829 - auc: 0.8284\n",
            "Epoch 00002: val_auc improved from 0.84770 to 0.86230, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 1.1675 - tp: 44620.0000 - fp: 12246.0000 - tn: 523792.0000 - fn: 71912.0000 - accuracy: 0.8710 - precision: 0.7847 - recall: 0.3829 - auc: 0.8284 - val_loss: 1.0640 - val_tp: 502.0000 - val_fp: 113.0000 - val_tn: 5320.0000 - val_fn: 657.0000 - val_accuracy: 0.8832 - val_precision: 0.8163 - val_recall: 0.4331 - val_auc: 0.8623\n",
            "Epoch 3/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 1.0160 - tp: 48935.0000 - fp: 13030.0000 - tn: 522350.0000 - fn: 67461.0000 - accuracy: 0.8765 - precision: 0.7897 - recall: 0.4204 - auc: 0.8477\n",
            "Epoch 00003: val_auc improved from 0.86230 to 0.86943, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 1.0159 - tp: 48989.0000 - fp: 13041.0000 - tn: 522997.0000 - fn: 67543.0000 - accuracy: 0.8765 - precision: 0.7898 - recall: 0.4204 - auc: 0.8477 - val_loss: 0.9381 - val_tp: 575.0000 - val_fp: 148.0000 - val_tn: 5285.0000 - val_fn: 584.0000 - val_accuracy: 0.8890 - val_precision: 0.7953 - val_recall: 0.4961 - val_auc: 0.8694\n",
            "Epoch 4/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.8889 - tp: 51787.0000 - fp: 13414.0000 - tn: 522381.0000 - fn: 64706.0000 - accuracy: 0.8802 - precision: 0.7943 - recall: 0.4446 - auc: 0.8576\n",
            "Epoch 00004: val_auc improved from 0.86943 to 0.87806, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.8889 - tp: 51807.0000 - fp: 13428.0000 - tn: 522610.0000 - fn: 64725.0000 - accuracy: 0.8802 - precision: 0.7942 - recall: 0.4446 - auc: 0.8576 - val_loss: 0.8144 - val_tp: 543.0000 - val_fp: 116.0000 - val_tn: 5317.0000 - val_fn: 616.0000 - val_accuracy: 0.8890 - val_precision: 0.8240 - val_recall: 0.4685 - val_auc: 0.8781\n",
            "Epoch 5/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.7744 - tp: 53846.0000 - fp: 13846.0000 - tn: 522192.0000 - fn: 62686.0000 - accuracy: 0.8827 - precision: 0.7955 - recall: 0.4621 - auc: 0.8634\n",
            "Epoch 00005: val_auc improved from 0.87806 to 0.88104, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.7744 - tp: 53846.0000 - fp: 13846.0000 - tn: 522192.0000 - fn: 62686.0000 - accuracy: 0.8827 - precision: 0.7955 - recall: 0.4621 - auc: 0.8634 - val_loss: 0.7043 - val_tp: 536.0000 - val_fp: 85.0000 - val_tn: 5348.0000 - val_fn: 623.0000 - val_accuracy: 0.8926 - val_precision: 0.8631 - val_recall: 0.4625 - val_auc: 0.8810\n",
            "Epoch 6/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.6756 - tp: 56139.0000 - fp: 14111.0000 - tn: 521706.0000 - fn: 60332.0000 - accuracy: 0.8859 - precision: 0.7991 - recall: 0.4820 - auc: 0.8689\n",
            "Epoch 00006: val_auc improved from 0.88104 to 0.88220, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.6756 - tp: 56165.0000 - fp: 14115.0000 - tn: 521923.0000 - fn: 60367.0000 - accuracy: 0.8859 - precision: 0.7992 - recall: 0.4820 - auc: 0.8689 - val_loss: 0.6200 - val_tp: 535.0000 - val_fp: 75.0000 - val_tn: 5358.0000 - val_fn: 624.0000 - val_accuracy: 0.8940 - val_precision: 0.8770 - val_recall: 0.4616 - val_auc: 0.8822\n",
            "Epoch 7/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.6008 - tp: 57091.0000 - fp: 14053.0000 - tn: 520485.0000 - fn: 59123.0000 - accuracy: 0.8876 - precision: 0.8025 - recall: 0.4913 - auc: 0.8727\n",
            "Epoch 00007: val_auc improved from 0.88220 to 0.88691, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.6008 - tp: 57248.0000 - fp: 14114.0000 - tn: 521924.0000 - fn: 59284.0000 - accuracy: 0.8875 - precision: 0.8022 - recall: 0.4913 - auc: 0.8726 - val_loss: 0.5804 - val_tp: 714.0000 - val_fp: 286.0000 - val_tn: 5147.0000 - val_fn: 445.0000 - val_accuracy: 0.8891 - val_precision: 0.7140 - val_recall: 0.6160 - val_auc: 0.8869\n",
            "Epoch 8/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.5474 - tp: 58645.0000 - fp: 14355.0000 - tn: 521683.0000 - fn: 57887.0000 - accuracy: 0.8893 - precision: 0.8034 - recall: 0.5033 - auc: 0.8760\n",
            "Epoch 00008: val_auc did not improve from 0.88691\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.5474 - tp: 58645.0000 - fp: 14355.0000 - tn: 521683.0000 - fn: 57887.0000 - accuracy: 0.8893 - precision: 0.8034 - recall: 0.5033 - auc: 0.8760 - val_loss: 0.5235 - val_tp: 619.0000 - val_fp: 147.0000 - val_tn: 5286.0000 - val_fn: 540.0000 - val_accuracy: 0.8958 - val_precision: 0.8081 - val_recall: 0.5341 - val_auc: 0.8821\n",
            "Epoch 9/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.5100 - tp: 59337.0000 - fp: 14010.0000 - tn: 520547.0000 - fn: 56858.0000 - accuracy: 0.8911 - precision: 0.8090 - recall: 0.5107 - auc: 0.8787\n",
            "Epoch 00009: val_auc improved from 0.88691 to 0.89350, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.5100 - tp: 59512.0000 - fp: 14043.0000 - tn: 521995.0000 - fn: 57020.0000 - accuracy: 0.8911 - precision: 0.8091 - recall: 0.5107 - auc: 0.8787 - val_loss: 0.4794 - val_tp: 667.0000 - val_fp: 137.0000 - val_tn: 5296.0000 - val_fn: 492.0000 - val_accuracy: 0.9046 - val_precision: 0.8296 - val_recall: 0.5755 - val_auc: 0.8935\n",
            "Epoch 10/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.4828 - tp: 60498.0000 - fp: 14053.0000 - tn: 521750.0000 - fn: 55987.0000 - accuracy: 0.8926 - precision: 0.8115 - recall: 0.5194 - auc: 0.8818\n",
            "Epoch 00010: val_auc improved from 0.89350 to 0.89572, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.4828 - tp: 60521.0000 - fp: 14057.0000 - tn: 521981.0000 - fn: 56011.0000 - accuracy: 0.8926 - precision: 0.8115 - recall: 0.5194 - auc: 0.8818 - val_loss: 0.4566 - val_tp: 624.0000 - val_fp: 99.0000 - val_tn: 5334.0000 - val_fn: 535.0000 - val_accuracy: 0.9038 - val_precision: 0.8631 - val_recall: 0.5384 - val_auc: 0.8957\n",
            "Epoch 11/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.4635 - tp: 61309.0000 - fp: 13972.0000 - tn: 521420.0000 - fn: 55075.0000 - accuracy: 0.8941 - precision: 0.8144 - recall: 0.5268 - auc: 0.8840\n",
            "Epoch 00011: val_auc did not improve from 0.89572\n",
            "1275/1275 [==============================] - 15s 11ms/step - loss: 0.4635 - tp: 61389.0000 - fp: 13989.0000 - tn: 522049.0000 - fn: 55143.0000 - accuracy: 0.8941 - precision: 0.8144 - recall: 0.5268 - auc: 0.8840 - val_loss: 0.4459 - val_tp: 621.0000 - val_fp: 108.0000 - val_tn: 5325.0000 - val_fn: 538.0000 - val_accuracy: 0.9020 - val_precision: 0.8519 - val_recall: 0.5358 - val_auc: 0.8926\n",
            "Epoch 12/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.4472 - tp: 62397.0000 - fp: 14038.0000 - tn: 521768.0000 - fn: 54085.0000 - accuracy: 0.8956 - precision: 0.8163 - recall: 0.5357 - auc: 0.8871\n",
            "Epoch 00012: val_auc improved from 0.89572 to 0.89782, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.4472 - tp: 62423.0000 - fp: 14046.0000 - tn: 521992.0000 - fn: 54109.0000 - accuracy: 0.8956 - precision: 0.8163 - recall: 0.5357 - auc: 0.8871 - val_loss: 0.4255 - val_tp: 671.0000 - val_fp: 148.0000 - val_tn: 5285.0000 - val_fn: 488.0000 - val_accuracy: 0.9035 - val_precision: 0.8193 - val_recall: 0.5789 - val_auc: 0.8978\n",
            "Epoch 13/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.4344 - tp: 63371.0000 - fp: 14176.0000 - tn: 521862.0000 - fn: 53161.0000 - accuracy: 0.8968 - precision: 0.8172 - recall: 0.5438 - auc: 0.8895\n",
            "Epoch 00013: val_auc improved from 0.89782 to 0.90403, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.4344 - tp: 63371.0000 - fp: 14176.0000 - tn: 521862.0000 - fn: 53161.0000 - accuracy: 0.8968 - precision: 0.8172 - recall: 0.5438 - auc: 0.8895 - val_loss: 0.4141 - val_tp: 665.0000 - val_fp: 132.0000 - val_tn: 5301.0000 - val_fn: 494.0000 - val_accuracy: 0.9050 - val_precision: 0.8344 - val_recall: 0.5738 - val_auc: 0.9040\n",
            "Epoch 14/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.4234 - tp: 63929.0000 - fp: 13885.0000 - tn: 521092.0000 - fn: 52358.0000 - accuracy: 0.8983 - precision: 0.8216 - recall: 0.5498 - auc: 0.8915\n",
            "Epoch 00014: val_auc did not improve from 0.90403\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.4234 - tp: 64050.0000 - fp: 13916.0000 - tn: 522122.0000 - fn: 52482.0000 - accuracy: 0.8983 - precision: 0.8215 - recall: 0.5496 - auc: 0.8915 - val_loss: 0.4252 - val_tp: 742.0000 - val_fp: 267.0000 - val_tn: 5166.0000 - val_fn: 417.0000 - val_accuracy: 0.8962 - val_precision: 0.7354 - val_recall: 0.6402 - val_auc: 0.8990\n",
            "Epoch 15/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.4143 - tp: 64773.0000 - fp: 14386.0000 - tn: 521652.0000 - fn: 51759.0000 - accuracy: 0.8986 - precision: 0.8183 - recall: 0.5558 - auc: 0.8939\n",
            "Epoch 00015: val_auc improved from 0.90403 to 0.90679, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.4143 - tp: 64773.0000 - fp: 14386.0000 - tn: 521652.0000 - fn: 51759.0000 - accuracy: 0.8986 - precision: 0.8183 - recall: 0.5558 - auc: 0.8939 - val_loss: 0.3969 - val_tp: 588.0000 - val_fp: 71.0000 - val_tn: 5362.0000 - val_fn: 571.0000 - val_accuracy: 0.9026 - val_precision: 0.8923 - val_recall: 0.5073 - val_auc: 0.9068\n",
            "Epoch 16/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.4052 - tp: 65530.0000 - fp: 14110.0000 - tn: 521928.0000 - fn: 51002.0000 - accuracy: 0.9002 - precision: 0.8228 - recall: 0.5623 - auc: 0.8967\n",
            "Epoch 00016: val_auc improved from 0.90679 to 0.90890, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.4052 - tp: 65530.0000 - fp: 14110.0000 - tn: 521928.0000 - fn: 51002.0000 - accuracy: 0.9002 - precision: 0.8228 - recall: 0.5623 - auc: 0.8967 - val_loss: 0.3849 - val_tp: 672.0000 - val_fp: 115.0000 - val_tn: 5318.0000 - val_fn: 487.0000 - val_accuracy: 0.9087 - val_precision: 0.8539 - val_recall: 0.5798 - val_auc: 0.9089\n",
            "Epoch 17/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3979 - tp: 66142.0000 - fp: 14103.0000 - tn: 521713.0000 - fn: 50330.0000 - accuracy: 0.9012 - precision: 0.8243 - recall: 0.5679 - auc: 0.8987\n",
            "Epoch 00017: val_auc did not improve from 0.90890\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3979 - tp: 66169.0000 - fp: 14108.0000 - tn: 521930.0000 - fn: 50363.0000 - accuracy: 0.9012 - precision: 0.8243 - recall: 0.5678 - auc: 0.8987 - val_loss: 0.3890 - val_tp: 676.0000 - val_fp: 148.0000 - val_tn: 5285.0000 - val_fn: 483.0000 - val_accuracy: 0.9043 - val_precision: 0.8204 - val_recall: 0.5833 - val_auc: 0.9033\n",
            "Epoch 18/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.3909 - tp: 66771.0000 - fp: 14044.0000 - tn: 521338.0000 - fn: 49623.0000 - accuracy: 0.9023 - precision: 0.8262 - recall: 0.5737 - auc: 0.9011\n",
            "Epoch 00018: val_auc improved from 0.90890 to 0.91448, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3909 - tp: 66850.0000 - fp: 14065.0000 - tn: 521973.0000 - fn: 49682.0000 - accuracy: 0.9023 - precision: 0.8262 - recall: 0.5737 - auc: 0.9011 - val_loss: 0.3710 - val_tp: 700.0000 - val_fp: 132.0000 - val_tn: 5301.0000 - val_fn: 459.0000 - val_accuracy: 0.9103 - val_precision: 0.8413 - val_recall: 0.6040 - val_auc: 0.9145\n",
            "Epoch 19/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.3847 - tp: 67172.0000 - fp: 13783.0000 - tn: 521615.0000 - fn: 49206.0000 - accuracy: 0.9034 - precision: 0.8297 - recall: 0.5772 - auc: 0.9030\n",
            "Epoch 00019: val_auc improved from 0.91448 to 0.91559, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3847 - tp: 67260.0000 - fp: 13792.0000 - tn: 522246.0000 - fn: 49272.0000 - accuracy: 0.9034 - precision: 0.8298 - recall: 0.5772 - auc: 0.9030 - val_loss: 0.3763 - val_tp: 765.0000 - val_fp: 237.0000 - val_tn: 5196.0000 - val_fn: 394.0000 - val_accuracy: 0.9043 - val_precision: 0.7635 - val_recall: 0.6601 - val_auc: 0.9156\n",
            "Epoch 20/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.3784 - tp: 67991.0000 - fp: 14073.0000 - tn: 520442.0000 - fn: 48246.0000 - accuracy: 0.9042 - precision: 0.8285 - recall: 0.5849 - auc: 0.9053\n",
            "Epoch 00020: val_auc improved from 0.91559 to 0.91715, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3783 - tp: 68190.0000 - fp: 14121.0000 - tn: 521917.0000 - fn: 48342.0000 - accuracy: 0.9043 - precision: 0.8284 - recall: 0.5852 - auc: 0.9053 - val_loss: 0.3629 - val_tp: 720.0000 - val_fp: 146.0000 - val_tn: 5287.0000 - val_fn: 439.0000 - val_accuracy: 0.9113 - val_precision: 0.8314 - val_recall: 0.6212 - val_auc: 0.9172\n",
            "Epoch 21/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3727 - tp: 68939.0000 - fp: 13992.0000 - tn: 521824.0000 - fn: 47533.0000 - accuracy: 0.9057 - precision: 0.8313 - recall: 0.5919 - auc: 0.9072\n",
            "Epoch 00021: val_auc improved from 0.91715 to 0.91762, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3728 - tp: 68974.0000 - fp: 13997.0000 - tn: 522041.0000 - fn: 47558.0000 - accuracy: 0.9057 - precision: 0.8313 - recall: 0.5919 - auc: 0.9072 - val_loss: 0.3720 - val_tp: 796.0000 - val_fp: 286.0000 - val_tn: 5147.0000 - val_fn: 363.0000 - val_accuracy: 0.9015 - val_precision: 0.7357 - val_recall: 0.6868 - val_auc: 0.9176\n",
            "Epoch 22/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3674 - tp: 69370.0000 - fp: 13998.0000 - tn: 521813.0000 - fn: 47107.0000 - accuracy: 0.9063 - precision: 0.8321 - recall: 0.5956 - auc: 0.9092\n",
            "Epoch 00022: val_auc improved from 0.91762 to 0.91933, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3675 - tp: 69401.0000 - fp: 14001.0000 - tn: 522037.0000 - fn: 47131.0000 - accuracy: 0.9063 - precision: 0.8321 - recall: 0.5956 - auc: 0.9092 - val_loss: 0.3529 - val_tp: 735.0000 - val_fp: 163.0000 - val_tn: 5270.0000 - val_fn: 424.0000 - val_accuracy: 0.9110 - val_precision: 0.8185 - val_recall: 0.6342 - val_auc: 0.9193\n",
            "Epoch 23/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.3625 - tp: 70000.0000 - fp: 13960.0000 - tn: 520602.0000 - fn: 46190.0000 - accuracy: 0.9076 - precision: 0.8337 - recall: 0.6025 - auc: 0.9111\n",
            "Epoch 00023: val_auc improved from 0.91933 to 0.92387, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3625 - tp: 70198.0000 - fp: 14007.0000 - tn: 522031.0000 - fn: 46334.0000 - accuracy: 0.9075 - precision: 0.8337 - recall: 0.6024 - auc: 0.9110 - val_loss: 0.3469 - val_tp: 762.0000 - val_fp: 169.0000 - val_tn: 5264.0000 - val_fn: 397.0000 - val_accuracy: 0.9141 - val_precision: 0.8185 - val_recall: 0.6575 - val_auc: 0.9239\n",
            "Epoch 24/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.3576 - tp: 70790.0000 - fp: 14028.0000 - tn: 522010.0000 - fn: 45742.0000 - accuracy: 0.9084 - precision: 0.8346 - recall: 0.6075 - auc: 0.9129\n",
            "Epoch 00024: val_auc improved from 0.92387 to 0.92786, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3576 - tp: 70790.0000 - fp: 14028.0000 - tn: 522010.0000 - fn: 45742.0000 - accuracy: 0.9084 - precision: 0.8346 - recall: 0.6075 - auc: 0.9129 - val_loss: 0.3379 - val_tp: 694.0000 - val_fp: 96.0000 - val_tn: 5337.0000 - val_fn: 465.0000 - val_accuracy: 0.9149 - val_precision: 0.8785 - val_recall: 0.5988 - val_auc: 0.9279\n",
            "Epoch 25/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3534 - tp: 71152.0000 - fp: 14073.0000 - tn: 521740.0000 - fn: 45323.0000 - accuracy: 0.9089 - precision: 0.8349 - recall: 0.6109 - auc: 0.9146\n",
            "Epoch 00025: val_auc did not improve from 0.92786\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3535 - tp: 71184.0000 - fp: 14080.0000 - tn: 521958.0000 - fn: 45348.0000 - accuracy: 0.9089 - precision: 0.8349 - recall: 0.6109 - auc: 0.9146 - val_loss: 0.3373 - val_tp: 682.0000 - val_fp: 91.0000 - val_tn: 5342.0000 - val_fn: 477.0000 - val_accuracy: 0.9138 - val_precision: 0.8823 - val_recall: 0.5884 - val_auc: 0.9239\n",
            "Epoch 26/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3491 - tp: 71482.0000 - fp: 13821.0000 - tn: 521998.0000 - fn: 44987.0000 - accuracy: 0.9098 - precision: 0.8380 - recall: 0.6137 - auc: 0.9165\n",
            "Epoch 00026: val_auc did not improve from 0.92786\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3492 - tp: 71512.0000 - fp: 13825.0000 - tn: 522213.0000 - fn: 45020.0000 - accuracy: 0.9098 - precision: 0.8380 - recall: 0.6137 - auc: 0.9165 - val_loss: 0.3413 - val_tp: 766.0000 - val_fp: 174.0000 - val_tn: 5259.0000 - val_fn: 393.0000 - val_accuracy: 0.9140 - val_precision: 0.8149 - val_recall: 0.6609 - val_auc: 0.9250\n",
            "Epoch 27/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.3453 - tp: 72049.0000 - fp: 14068.0000 - tn: 521970.0000 - fn: 44483.0000 - accuracy: 0.9103 - precision: 0.8366 - recall: 0.6183 - auc: 0.9182\n",
            "Epoch 00027: val_auc improved from 0.92786 to 0.92920, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3453 - tp: 72049.0000 - fp: 14068.0000 - tn: 521970.0000 - fn: 44483.0000 - accuracy: 0.9103 - precision: 0.8366 - recall: 0.6183 - auc: 0.9182 - val_loss: 0.3367 - val_tp: 785.0000 - val_fp: 188.0000 - val_tn: 5245.0000 - val_fn: 374.0000 - val_accuracy: 0.9147 - val_precision: 0.8068 - val_recall: 0.6773 - val_auc: 0.9292\n",
            "Epoch 28/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3419 - tp: 72459.0000 - fp: 13921.0000 - tn: 521880.0000 - fn: 44028.0000 - accuracy: 0.9112 - precision: 0.8388 - recall: 0.6220 - auc: 0.9192\n",
            "Epoch 00028: val_auc did not improve from 0.92920\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3419 - tp: 72485.0000 - fp: 13924.0000 - tn: 522114.0000 - fn: 44047.0000 - accuracy: 0.9112 - precision: 0.8389 - recall: 0.6220 - auc: 0.9192 - val_loss: 0.3926 - val_tp: 897.0000 - val_fp: 500.0000 - val_tn: 4933.0000 - val_fn: 262.0000 - val_accuracy: 0.8844 - val_precision: 0.6421 - val_recall: 0.7739 - val_auc: 0.9223\n",
            "Epoch 29/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3376 - tp: 73090.0000 - fp: 14030.0000 - tn: 521771.0000 - fn: 43397.0000 - accuracy: 0.9120 - precision: 0.8390 - recall: 0.6275 - auc: 0.9214\n",
            "Epoch 00029: val_auc improved from 0.92920 to 0.93210, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3376 - tp: 73118.0000 - fp: 14042.0000 - tn: 521996.0000 - fn: 43414.0000 - accuracy: 0.9120 - precision: 0.8389 - recall: 0.6274 - auc: 0.9214 - val_loss: 0.3245 - val_tp: 762.0000 - val_fp: 170.0000 - val_tn: 5263.0000 - val_fn: 397.0000 - val_accuracy: 0.9140 - val_precision: 0.8176 - val_recall: 0.6575 - val_auc: 0.9321\n",
            "Epoch 30/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3344 - tp: 73374.0000 - fp: 13847.0000 - tn: 521973.0000 - fn: 43094.0000 - accuracy: 0.9127 - precision: 0.8412 - recall: 0.6300 - auc: 0.9224\n",
            "Epoch 00030: val_auc did not improve from 0.93210\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3344 - tp: 73415.0000 - fp: 13853.0000 - tn: 522185.0000 - fn: 43117.0000 - accuracy: 0.9127 - precision: 0.8413 - recall: 0.6300 - auc: 0.9224 - val_loss: 0.3213 - val_tp: 713.0000 - val_fp: 124.0000 - val_tn: 5309.0000 - val_fn: 446.0000 - val_accuracy: 0.9135 - val_precision: 0.8519 - val_recall: 0.6152 - val_auc: 0.9304\n",
            "Epoch 31/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.3308 - tp: 73940.0000 - fp: 13825.0000 - tn: 522213.0000 - fn: 42592.0000 - accuracy: 0.9135 - precision: 0.8425 - recall: 0.6345 - auc: 0.9244\n",
            "Epoch 00031: val_auc did not improve from 0.93210\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3308 - tp: 73940.0000 - fp: 13825.0000 - tn: 522213.0000 - fn: 42592.0000 - accuracy: 0.9135 - precision: 0.8425 - recall: 0.6345 - auc: 0.9244 - val_loss: 0.3287 - val_tp: 731.0000 - val_fp: 133.0000 - val_tn: 5300.0000 - val_fn: 428.0000 - val_accuracy: 0.9149 - val_precision: 0.8461 - val_recall: 0.6307 - val_auc: 0.9294\n",
            "Epoch 32/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3279 - tp: 74645.0000 - fp: 13951.0000 - tn: 521862.0000 - fn: 41830.0000 - accuracy: 0.9145 - precision: 0.8425 - recall: 0.6409 - auc: 0.9253\n",
            "Epoch 00032: val_auc improved from 0.93210 to 0.93319, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3279 - tp: 74676.0000 - fp: 13954.0000 - tn: 522084.0000 - fn: 41856.0000 - accuracy: 0.9145 - precision: 0.8426 - recall: 0.6408 - auc: 0.9253 - val_loss: 0.3224 - val_tp: 804.0000 - val_fp: 210.0000 - val_tn: 5223.0000 - val_fn: 355.0000 - val_accuracy: 0.9143 - val_precision: 0.7929 - val_recall: 0.6937 - val_auc: 0.9332\n",
            "Epoch 33/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3250 - tp: 74667.0000 - fp: 13724.0000 - tn: 522081.0000 - fn: 41816.0000 - accuracy: 0.9149 - precision: 0.8447 - recall: 0.6410 - auc: 0.9267\n",
            "Epoch 00033: val_auc improved from 0.93319 to 0.93652, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3250 - tp: 74695.0000 - fp: 13731.0000 - tn: 522307.0000 - fn: 41837.0000 - accuracy: 0.9148 - precision: 0.8447 - recall: 0.6410 - auc: 0.9267 - val_loss: 0.3138 - val_tp: 781.0000 - val_fp: 140.0000 - val_tn: 5293.0000 - val_fn: 378.0000 - val_accuracy: 0.9214 - val_precision: 0.8480 - val_recall: 0.6739 - val_auc: 0.9365\n",
            "Epoch 34/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.3219 - tp: 75078.0000 - fp: 13680.0000 - tn: 521717.0000 - fn: 41301.0000 - accuracy: 0.9156 - precision: 0.8459 - recall: 0.6451 - auc: 0.9280\n",
            "Epoch 00034: val_auc improved from 0.93652 to 0.93749, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3219 - tp: 75177.0000 - fp: 13688.0000 - tn: 522350.0000 - fn: 41355.0000 - accuracy: 0.9157 - precision: 0.8460 - recall: 0.6451 - auc: 0.9280 - val_loss: 0.3099 - val_tp: 765.0000 - val_fp: 131.0000 - val_tn: 5302.0000 - val_fn: 394.0000 - val_accuracy: 0.9204 - val_precision: 0.8538 - val_recall: 0.6601 - val_auc: 0.9375\n",
            "Epoch 35/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.3194 - tp: 75498.0000 - fp: 13912.0000 - tn: 522126.0000 - fn: 41034.0000 - accuracy: 0.9158 - precision: 0.8444 - recall: 0.6479 - auc: 0.9292\n",
            "Epoch 00035: val_auc did not improve from 0.93749\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3194 - tp: 75498.0000 - fp: 13912.0000 - tn: 522126.0000 - fn: 41034.0000 - accuracy: 0.9158 - precision: 0.8444 - recall: 0.6479 - auc: 0.9292 - val_loss: 0.3087 - val_tp: 772.0000 - val_fp: 132.0000 - val_tn: 5301.0000 - val_fn: 387.0000 - val_accuracy: 0.9213 - val_precision: 0.8540 - val_recall: 0.6661 - val_auc: 0.9361\n",
            "Epoch 36/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.3167 - tp: 76126.0000 - fp: 13814.0000 - tn: 522224.0000 - fn: 40406.0000 - accuracy: 0.9169 - precision: 0.8464 - recall: 0.6533 - auc: 0.9303\n",
            "Epoch 00036: val_auc improved from 0.93749 to 0.94222, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3167 - tp: 76126.0000 - fp: 13814.0000 - tn: 522224.0000 - fn: 40406.0000 - accuracy: 0.9169 - precision: 0.8464 - recall: 0.6533 - auc: 0.9303 - val_loss: 0.3055 - val_tp: 819.0000 - val_fp: 161.0000 - val_tn: 5272.0000 - val_fn: 340.0000 - val_accuracy: 0.9240 - val_precision: 0.8357 - val_recall: 0.7066 - val_auc: 0.9422\n",
            "Epoch 37/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.3141 - tp: 76152.0000 - fp: 13631.0000 - tn: 522179.0000 - fn: 40326.0000 - accuracy: 0.9173 - precision: 0.8482 - recall: 0.6538 - auc: 0.9315\n",
            "Epoch 00037: val_auc did not improve from 0.94222\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3141 - tp: 76186.0000 - fp: 13636.0000 - tn: 522402.0000 - fn: 40346.0000 - accuracy: 0.9173 - precision: 0.8482 - recall: 0.6538 - auc: 0.9315 - val_loss: 0.3403 - val_tp: 923.0000 - val_fp: 402.0000 - val_tn: 5031.0000 - val_fn: 236.0000 - val_accuracy: 0.9032 - val_precision: 0.6966 - val_recall: 0.7964 - val_auc: 0.9390\n",
            "Epoch 38/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.3121 - tp: 76361.0000 - fp: 13701.0000 - tn: 521711.0000 - fn: 40003.0000 - accuracy: 0.9176 - precision: 0.8479 - recall: 0.6562 - auc: 0.9321\n",
            "Epoch 00038: val_auc did not improve from 0.94222\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3121 - tp: 76476.0000 - fp: 13712.0000 - tn: 522326.0000 - fn: 40056.0000 - accuracy: 0.9176 - precision: 0.8480 - recall: 0.6563 - auc: 0.9322 - val_loss: 0.3014 - val_tp: 797.0000 - val_fp: 123.0000 - val_tn: 5310.0000 - val_fn: 362.0000 - val_accuracy: 0.9264 - val_precision: 0.8663 - val_recall: 0.6877 - val_auc: 0.9396\n",
            "Epoch 39/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.3092 - tp: 77024.0000 - fp: 13737.0000 - tn: 521230.0000 - fn: 39273.0000 - accuracy: 0.9186 - precision: 0.8486 - recall: 0.6623 - auc: 0.9336\n",
            "Epoch 00039: val_auc did not improve from 0.94222\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3092 - tp: 77178.0000 - fp: 13767.0000 - tn: 522271.0000 - fn: 39354.0000 - accuracy: 0.9186 - precision: 0.8486 - recall: 0.6623 - auc: 0.9336 - val_loss: 0.3063 - val_tp: 834.0000 - val_fp: 205.0000 - val_tn: 5228.0000 - val_fn: 325.0000 - val_accuracy: 0.9196 - val_precision: 0.8027 - val_recall: 0.7196 - val_auc: 0.9416\n",
            "Epoch 40/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.3066 - tp: 77389.0000 - fp: 13668.0000 - tn: 520867.0000 - fn: 38828.0000 - accuracy: 0.9193 - precision: 0.8499 - recall: 0.6659 - auc: 0.9350\n",
            "Epoch 00040: val_auc improved from 0.94222 to 0.94253, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3066 - tp: 77599.0000 - fp: 13713.0000 - tn: 522325.0000 - fn: 38933.0000 - accuracy: 0.9193 - precision: 0.8498 - recall: 0.6659 - auc: 0.9350 - val_loss: 0.3173 - val_tp: 901.0000 - val_fp: 304.0000 - val_tn: 5129.0000 - val_fn: 258.0000 - val_accuracy: 0.9147 - val_precision: 0.7477 - val_recall: 0.7774 - val_auc: 0.9425\n",
            "Epoch 41/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.3040 - tp: 77825.0000 - fp: 13834.0000 - tn: 521133.0000 - fn: 38472.0000 - accuracy: 0.9197 - precision: 0.8491 - recall: 0.6692 - auc: 0.9363\n",
            "Epoch 00041: val_auc improved from 0.94253 to 0.94816, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3040 - tp: 77983.0000 - fp: 13864.0000 - tn: 522174.0000 - fn: 38549.0000 - accuracy: 0.9197 - precision: 0.8491 - recall: 0.6692 - auc: 0.9363 - val_loss: 0.2955 - val_tp: 825.0000 - val_fp: 160.0000 - val_tn: 5273.0000 - val_fn: 334.0000 - val_accuracy: 0.9251 - val_precision: 0.8376 - val_recall: 0.7118 - val_auc: 0.9482\n",
            "Epoch 42/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.3018 - tp: 78083.0000 - fp: 13611.0000 - tn: 521783.0000 - fn: 38299.0000 - accuracy: 0.9204 - precision: 0.8516 - recall: 0.6709 - auc: 0.9371\n",
            "Epoch 00042: val_auc did not improve from 0.94816\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.3018 - tp: 78185.0000 - fp: 13626.0000 - tn: 522412.0000 - fn: 38347.0000 - accuracy: 0.9204 - precision: 0.8516 - recall: 0.6709 - auc: 0.9371 - val_loss: 0.3013 - val_tp: 911.0000 - val_fp: 259.0000 - val_tn: 5174.0000 - val_fn: 248.0000 - val_accuracy: 0.9231 - val_precision: 0.7786 - val_recall: 0.7860 - val_auc: 0.9474\n",
            "Epoch 43/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2998 - tp: 78527.0000 - fp: 13690.0000 - tn: 522348.0000 - fn: 38005.0000 - accuracy: 0.9208 - precision: 0.8515 - recall: 0.6739 - auc: 0.9379\n",
            "Epoch 00043: val_auc did not improve from 0.94816\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2998 - tp: 78527.0000 - fp: 13690.0000 - tn: 522348.0000 - fn: 38005.0000 - accuracy: 0.9208 - precision: 0.8515 - recall: 0.6739 - auc: 0.9379 - val_loss: 0.2835 - val_tp: 801.0000 - val_fp: 119.0000 - val_tn: 5314.0000 - val_fn: 358.0000 - val_accuracy: 0.9276 - val_precision: 0.8707 - val_recall: 0.6911 - val_auc: 0.9480\n",
            "Epoch 44/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2978 - tp: 78737.0000 - fp: 13597.0000 - tn: 521381.0000 - fn: 37549.0000 - accuracy: 0.9215 - precision: 0.8527 - recall: 0.6771 - auc: 0.9392\n",
            "Epoch 00044: val_auc improved from 0.94816 to 0.95100, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2978 - tp: 78908.0000 - fp: 13626.0000 - tn: 522412.0000 - fn: 37624.0000 - accuracy: 0.9215 - precision: 0.8527 - recall: 0.6771 - auc: 0.9392 - val_loss: 0.2924 - val_tp: 914.0000 - val_fp: 262.0000 - val_tn: 5171.0000 - val_fn: 245.0000 - val_accuracy: 0.9231 - val_precision: 0.7772 - val_recall: 0.7886 - val_auc: 0.9510\n",
            "Epoch 45/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2963 - tp: 79238.0000 - fp: 13924.0000 - tn: 521032.0000 - fn: 37070.0000 - accuracy: 0.9217 - precision: 0.8505 - recall: 0.6813 - auc: 0.9397\n",
            "Epoch 00045: val_auc did not improve from 0.95100\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2962 - tp: 79398.0000 - fp: 13952.0000 - tn: 522086.0000 - fn: 37134.0000 - accuracy: 0.9217 - precision: 0.8505 - recall: 0.6813 - auc: 0.9398 - val_loss: 0.2939 - val_tp: 867.0000 - val_fp: 227.0000 - val_tn: 5206.0000 - val_fn: 292.0000 - val_accuracy: 0.9213 - val_precision: 0.7925 - val_recall: 0.7481 - val_auc: 0.9483\n",
            "Epoch 46/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2945 - tp: 79215.0000 - fp: 13778.0000 - tn: 520772.0000 - fn: 36987.0000 - accuracy: 0.9220 - precision: 0.8518 - recall: 0.6817 - auc: 0.9406\n",
            "Epoch 00046: val_auc did not improve from 0.95100\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2945 - tp: 79432.0000 - fp: 13814.0000 - tn: 522224.0000 - fn: 37100.0000 - accuracy: 0.9220 - precision: 0.8519 - recall: 0.6816 - auc: 0.9406 - val_loss: 0.2866 - val_tp: 839.0000 - val_fp: 171.0000 - val_tn: 5262.0000 - val_fn: 320.0000 - val_accuracy: 0.9255 - val_precision: 0.8307 - val_recall: 0.7239 - val_auc: 0.9490\n",
            "Epoch 47/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2925 - tp: 79601.0000 - fp: 13639.0000 - tn: 521747.0000 - fn: 36789.0000 - accuracy: 0.9226 - precision: 0.8537 - recall: 0.6839 - auc: 0.9416\n",
            "Epoch 00047: val_auc did not improve from 0.95100\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2925 - tp: 79700.0000 - fp: 13653.0000 - tn: 522385.0000 - fn: 36832.0000 - accuracy: 0.9226 - precision: 0.8537 - recall: 0.6839 - auc: 0.9416 - val_loss: 0.2903 - val_tp: 900.0000 - val_fp: 234.0000 - val_tn: 5199.0000 - val_fn: 259.0000 - val_accuracy: 0.9252 - val_precision: 0.7937 - val_recall: 0.7765 - val_auc: 0.9500\n",
            "Epoch 48/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2902 - tp: 80195.0000 - fp: 13625.0000 - tn: 522413.0000 - fn: 36337.0000 - accuracy: 0.9234 - precision: 0.8548 - recall: 0.6882 - auc: 0.9428\n",
            "Epoch 00048: val_auc improved from 0.95100 to 0.95149, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2902 - tp: 80195.0000 - fp: 13625.0000 - tn: 522413.0000 - fn: 36337.0000 - accuracy: 0.9234 - precision: 0.8548 - recall: 0.6882 - auc: 0.9428 - val_loss: 0.2796 - val_tp: 867.0000 - val_fp: 183.0000 - val_tn: 5250.0000 - val_fn: 292.0000 - val_accuracy: 0.9279 - val_precision: 0.8257 - val_recall: 0.7481 - val_auc: 0.9515\n",
            "Epoch 49/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2888 - tp: 80185.0000 - fp: 13553.0000 - tn: 521827.0000 - fn: 36211.0000 - accuracy: 0.9236 - precision: 0.8554 - recall: 0.6889 - auc: 0.9433\n",
            "Epoch 00049: val_auc improved from 0.95149 to 0.95198, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2889 - tp: 80275.0000 - fp: 13576.0000 - tn: 522462.0000 - fn: 36257.0000 - accuracy: 0.9236 - precision: 0.8553 - recall: 0.6889 - auc: 0.9433 - val_loss: 0.2946 - val_tp: 921.0000 - val_fp: 253.0000 - val_tn: 5180.0000 - val_fn: 238.0000 - val_accuracy: 0.9255 - val_precision: 0.7845 - val_recall: 0.7947 - val_auc: 0.9520\n",
            "Epoch 50/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2869 - tp: 80590.0000 - fp: 13445.0000 - tn: 522366.0000 - fn: 35887.0000 - accuracy: 0.9244 - precision: 0.8570 - recall: 0.6919 - auc: 0.9441\n",
            "Epoch 00050: val_auc did not improve from 0.95198\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2869 - tp: 80627.0000 - fp: 13446.0000 - tn: 522592.0000 - fn: 35905.0000 - accuracy: 0.9244 - precision: 0.8571 - recall: 0.6919 - auc: 0.9441 - val_loss: 0.2903 - val_tp: 850.0000 - val_fp: 177.0000 - val_tn: 5256.0000 - val_fn: 309.0000 - val_accuracy: 0.9263 - val_precision: 0.8277 - val_recall: 0.7334 - val_auc: 0.9495\n",
            "Epoch 51/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2859 - tp: 80456.0000 - fp: 13322.0000 - tn: 522494.0000 - fn: 36016.0000 - accuracy: 0.9244 - precision: 0.8579 - recall: 0.6908 - auc: 0.9445\n",
            "Epoch 00051: val_auc improved from 0.95198 to 0.95263, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2860 - tp: 80498.0000 - fp: 13326.0000 - tn: 522712.0000 - fn: 36034.0000 - accuracy: 0.9244 - precision: 0.8580 - recall: 0.6908 - auc: 0.9445 - val_loss: 0.2885 - val_tp: 908.0000 - val_fp: 252.0000 - val_tn: 5181.0000 - val_fn: 251.0000 - val_accuracy: 0.9237 - val_precision: 0.7828 - val_recall: 0.7834 - val_auc: 0.9526\n",
            "Epoch 52/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2843 - tp: 80864.0000 - fp: 13490.0000 - tn: 522548.0000 - fn: 35668.0000 - accuracy: 0.9247 - precision: 0.8570 - recall: 0.6939 - auc: 0.9454\n",
            "Epoch 00052: val_auc improved from 0.95263 to 0.95718, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2843 - tp: 80864.0000 - fp: 13490.0000 - tn: 522548.0000 - fn: 35668.0000 - accuracy: 0.9247 - precision: 0.8570 - recall: 0.6939 - auc: 0.9454 - val_loss: 0.2675 - val_tp: 825.0000 - val_fp: 101.0000 - val_tn: 5332.0000 - val_fn: 334.0000 - val_accuracy: 0.9340 - val_precision: 0.8909 - val_recall: 0.7118 - val_auc: 0.9572\n",
            "Epoch 53/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2821 - tp: 81276.0000 - fp: 13297.0000 - tn: 522504.0000 - fn: 35211.0000 - accuracy: 0.9256 - precision: 0.8594 - recall: 0.6977 - auc: 0.9465\n",
            "Epoch 00053: val_auc did not improve from 0.95718\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2821 - tp: 81310.0000 - fp: 13305.0000 - tn: 522733.0000 - fn: 35222.0000 - accuracy: 0.9256 - precision: 0.8594 - recall: 0.6977 - auc: 0.9465 - val_loss: 0.2816 - val_tp: 890.0000 - val_fp: 224.0000 - val_tn: 5209.0000 - val_fn: 269.0000 - val_accuracy: 0.9252 - val_precision: 0.7989 - val_recall: 0.7679 - val_auc: 0.9539\n",
            "Epoch 54/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2810 - tp: 81393.0000 - fp: 13343.0000 - tn: 521616.0000 - fn: 34912.0000 - accuracy: 0.9259 - precision: 0.8592 - recall: 0.6998 - auc: 0.9470\n",
            "Epoch 00054: val_auc did not improve from 0.95718\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2811 - tp: 81547.0000 - fp: 13374.0000 - tn: 522664.0000 - fn: 34985.0000 - accuracy: 0.9259 - precision: 0.8591 - recall: 0.6998 - auc: 0.9470 - val_loss: 0.2918 - val_tp: 923.0000 - val_fp: 251.0000 - val_tn: 5182.0000 - val_fn: 236.0000 - val_accuracy: 0.9261 - val_precision: 0.7862 - val_recall: 0.7964 - val_auc: 0.9543\n",
            "Epoch 55/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2800 - tp: 81657.0000 - fp: 13503.0000 - tn: 521886.0000 - fn: 34730.0000 - accuracy: 0.9260 - precision: 0.8581 - recall: 0.7016 - auc: 0.9475\n",
            "Epoch 00055: val_auc improved from 0.95718 to 0.96099, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2799 - tp: 81755.0000 - fp: 13513.0000 - tn: 522525.0000 - fn: 34777.0000 - accuracy: 0.9260 - precision: 0.8582 - recall: 0.7016 - auc: 0.9475 - val_loss: 0.2767 - val_tp: 928.0000 - val_fp: 222.0000 - val_tn: 5211.0000 - val_fn: 231.0000 - val_accuracy: 0.9313 - val_precision: 0.8070 - val_recall: 0.8007 - val_auc: 0.9610\n",
            "Epoch 56/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2782 - tp: 82008.0000 - fp: 13350.0000 - tn: 522688.0000 - fn: 34524.0000 - accuracy: 0.9266 - precision: 0.8600 - recall: 0.7037 - auc: 0.9484\n",
            "Epoch 00056: val_auc did not improve from 0.96099\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2782 - tp: 82008.0000 - fp: 13350.0000 - tn: 522688.0000 - fn: 34524.0000 - accuracy: 0.9266 - precision: 0.8600 - recall: 0.7037 - auc: 0.9484 - val_loss: 0.2655 - val_tp: 853.0000 - val_fp: 140.0000 - val_tn: 5293.0000 - val_fn: 306.0000 - val_accuracy: 0.9323 - val_precision: 0.8590 - val_recall: 0.7360 - val_auc: 0.9593\n",
            "Epoch 57/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2765 - tp: 82474.0000 - fp: 13535.0000 - tn: 521848.0000 - fn: 33919.0000 - accuracy: 0.9272 - precision: 0.8590 - recall: 0.7086 - auc: 0.9493\n",
            "Epoch 00057: val_auc did not improve from 0.96099\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2765 - tp: 82563.0000 - fp: 13552.0000 - tn: 522486.0000 - fn: 33969.0000 - accuracy: 0.9272 - precision: 0.8590 - recall: 0.7085 - auc: 0.9492 - val_loss: 0.2671 - val_tp: 768.0000 - val_fp: 67.0000 - val_tn: 5366.0000 - val_fn: 391.0000 - val_accuracy: 0.9305 - val_precision: 0.9198 - val_recall: 0.6626 - val_auc: 0.9556\n",
            "Epoch 58/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2758 - tp: 82317.0000 - fp: 13424.0000 - tn: 521959.0000 - fn: 34076.0000 - accuracy: 0.9271 - precision: 0.8598 - recall: 0.7072 - auc: 0.9494\n",
            "Epoch 00058: val_auc did not improve from 0.96099\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2758 - tp: 82420.0000 - fp: 13445.0000 - tn: 522593.0000 - fn: 34112.0000 - accuracy: 0.9271 - precision: 0.8598 - recall: 0.7073 - auc: 0.9494 - val_loss: 0.2579 - val_tp: 852.0000 - val_fp: 107.0000 - val_tn: 5326.0000 - val_fn: 307.0000 - val_accuracy: 0.9372 - val_precision: 0.8884 - val_recall: 0.7351 - val_auc: 0.9604\n",
            "Epoch 59/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2745 - tp: 82368.0000 - fp: 13391.0000 - tn: 521162.0000 - fn: 33831.0000 - accuracy: 0.9274 - precision: 0.8602 - recall: 0.7089 - auc: 0.9503\n",
            "Epoch 00059: val_auc improved from 0.96099 to 0.96150, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2745 - tp: 82605.0000 - fp: 13420.0000 - tn: 522618.0000 - fn: 33927.0000 - accuracy: 0.9274 - precision: 0.8602 - recall: 0.7089 - auc: 0.9503 - val_loss: 0.2767 - val_tp: 908.0000 - val_fp: 209.0000 - val_tn: 5224.0000 - val_fn: 251.0000 - val_accuracy: 0.9302 - val_precision: 0.8129 - val_recall: 0.7834 - val_auc: 0.9615\n",
            "Epoch 60/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2729 - tp: 82827.0000 - fp: 13420.0000 - tn: 522618.0000 - fn: 33705.0000 - accuracy: 0.9278 - precision: 0.8606 - recall: 0.7108 - auc: 0.9510\n",
            "Epoch 00060: val_auc improved from 0.96150 to 0.96251, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2729 - tp: 82827.0000 - fp: 13420.0000 - tn: 522618.0000 - fn: 33705.0000 - accuracy: 0.9278 - precision: 0.8606 - recall: 0.7108 - auc: 0.9510 - val_loss: 0.2631 - val_tp: 926.0000 - val_fp: 217.0000 - val_tn: 5216.0000 - val_fn: 233.0000 - val_accuracy: 0.9317 - val_precision: 0.8101 - val_recall: 0.7990 - val_auc: 0.9625\n",
            "Epoch 61/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2723 - tp: 82948.0000 - fp: 13530.0000 - tn: 521437.0000 - fn: 33349.0000 - accuracy: 0.9280 - precision: 0.8598 - recall: 0.7132 - auc: 0.9514\n",
            "Epoch 00061: val_auc did not improve from 0.96251\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2723 - tp: 83115.0000 - fp: 13564.0000 - tn: 522474.0000 - fn: 33417.0000 - accuracy: 0.9280 - precision: 0.8597 - recall: 0.7132 - auc: 0.9514 - val_loss: 0.2792 - val_tp: 942.0000 - val_fp: 253.0000 - val_tn: 5180.0000 - val_fn: 217.0000 - val_accuracy: 0.9287 - val_precision: 0.7883 - val_recall: 0.8128 - val_auc: 0.9617\n",
            "Epoch 62/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2706 - tp: 83366.0000 - fp: 13520.0000 - tn: 521866.0000 - fn: 33024.0000 - accuracy: 0.9286 - precision: 0.8605 - recall: 0.7163 - auc: 0.9521\n",
            "Epoch 00062: val_auc improved from 0.96251 to 0.96326, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2706 - tp: 83457.0000 - fp: 13535.0000 - tn: 522503.0000 - fn: 33075.0000 - accuracy: 0.9286 - precision: 0.8605 - recall: 0.7162 - auc: 0.9521 - val_loss: 0.2642 - val_tp: 909.0000 - val_fp: 183.0000 - val_tn: 5250.0000 - val_fn: 250.0000 - val_accuracy: 0.9343 - val_precision: 0.8324 - val_recall: 0.7843 - val_auc: 0.9633\n",
            "Epoch 63/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2692 - tp: 83392.0000 - fp: 13309.0000 - tn: 522098.0000 - fn: 32977.0000 - accuracy: 0.9290 - precision: 0.8624 - recall: 0.7166 - auc: 0.9526\n",
            "Epoch 00063: val_auc improved from 0.96326 to 0.96543, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2692 - tp: 83498.0000 - fp: 13318.0000 - tn: 522720.0000 - fn: 33034.0000 - accuracy: 0.9290 - precision: 0.8624 - recall: 0.7165 - auc: 0.9526 - val_loss: 0.2495 - val_tp: 865.0000 - val_fp: 111.0000 - val_tn: 5322.0000 - val_fn: 294.0000 - val_accuracy: 0.9386 - val_precision: 0.8863 - val_recall: 0.7463 - val_auc: 0.9654\n",
            "Epoch 64/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2677 - tp: 83684.0000 - fp: 13211.0000 - tn: 522603.0000 - fn: 32790.0000 - accuracy: 0.9295 - precision: 0.8637 - recall: 0.7185 - auc: 0.9534\n",
            "Epoch 00064: val_auc did not improve from 0.96543\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2678 - tp: 83722.0000 - fp: 13218.0000 - tn: 522820.0000 - fn: 32810.0000 - accuracy: 0.9295 - precision: 0.8636 - recall: 0.7184 - auc: 0.9534 - val_loss: 0.2731 - val_tp: 943.0000 - val_fp: 218.0000 - val_tn: 5215.0000 - val_fn: 216.0000 - val_accuracy: 0.9342 - val_precision: 0.8122 - val_recall: 0.8136 - val_auc: 0.9638\n",
            "Epoch 65/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2679 - tp: 83690.0000 - fp: 13552.0000 - tn: 521406.0000 - fn: 32616.0000 - accuracy: 0.9291 - precision: 0.8606 - recall: 0.7196 - auc: 0.9534\n",
            "Epoch 00065: val_auc did not improve from 0.96543\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2679 - tp: 83850.0000 - fp: 13569.0000 - tn: 522469.0000 - fn: 32682.0000 - accuracy: 0.9291 - precision: 0.8607 - recall: 0.7195 - auc: 0.9534 - val_loss: 0.2605 - val_tp: 879.0000 - val_fp: 135.0000 - val_tn: 5298.0000 - val_fn: 280.0000 - val_accuracy: 0.9370 - val_precision: 0.8669 - val_recall: 0.7584 - val_auc: 0.9638\n",
            "Epoch 66/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2667 - tp: 83881.0000 - fp: 13384.0000 - tn: 521965.0000 - fn: 32546.0000 - accuracy: 0.9295 - precision: 0.8624 - recall: 0.7205 - auc: 0.9539\n",
            "Epoch 00066: val_auc did not improve from 0.96543\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2667 - tp: 83961.0000 - fp: 13401.0000 - tn: 522637.0000 - fn: 32571.0000 - accuracy: 0.9296 - precision: 0.8624 - recall: 0.7205 - auc: 0.9539 - val_loss: 0.2526 - val_tp: 857.0000 - val_fp: 111.0000 - val_tn: 5322.0000 - val_fn: 302.0000 - val_accuracy: 0.9373 - val_precision: 0.8853 - val_recall: 0.7394 - val_auc: 0.9647\n",
            "Epoch 67/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2647 - tp: 84373.0000 - fp: 13455.0000 - tn: 522353.0000 - fn: 32107.0000 - accuracy: 0.9302 - precision: 0.8625 - recall: 0.7244 - auc: 0.9549\n",
            "Epoch 00067: val_auc improved from 0.96543 to 0.96550, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2647 - tp: 84412.0000 - fp: 13463.0000 - tn: 522575.0000 - fn: 32120.0000 - accuracy: 0.9301 - precision: 0.8624 - recall: 0.7244 - auc: 0.9549 - val_loss: 0.2701 - val_tp: 969.0000 - val_fp: 276.0000 - val_tn: 5157.0000 - val_fn: 190.0000 - val_accuracy: 0.9293 - val_precision: 0.7783 - val_recall: 0.8361 - val_auc: 0.9655\n",
            "Epoch 68/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2642 - tp: 84258.0000 - fp: 13437.0000 - tn: 522601.0000 - fn: 32274.0000 - accuracy: 0.9300 - precision: 0.8625 - recall: 0.7230 - auc: 0.9551\n",
            "Epoch 00068: val_auc improved from 0.96550 to 0.96584, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2642 - tp: 84258.0000 - fp: 13437.0000 - tn: 522601.0000 - fn: 32274.0000 - accuracy: 0.9300 - precision: 0.8625 - recall: 0.7230 - auc: 0.9551 - val_loss: 0.2423 - val_tp: 885.0000 - val_fp: 134.0000 - val_tn: 5299.0000 - val_fn: 274.0000 - val_accuracy: 0.9381 - val_precision: 0.8685 - val_recall: 0.7636 - val_auc: 0.9658\n",
            "Epoch 69/2000\n",
            "1270/1275 [============================>.] - ETA: 0s - loss: 0.2634 - tp: 84038.0000 - fp: 13115.0000 - tn: 521037.0000 - fn: 32050.0000 - accuracy: 0.9305 - precision: 0.8650 - recall: 0.7239 - auc: 0.9554\n",
            "Epoch 00069: val_auc improved from 0.96584 to 0.96692, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2634 - tp: 84361.0000 - fp: 13158.0000 - tn: 522880.0000 - fn: 32171.0000 - accuracy: 0.9305 - precision: 0.8651 - recall: 0.7239 - auc: 0.9554 - val_loss: 0.2507 - val_tp: 916.0000 - val_fp: 170.0000 - val_tn: 5263.0000 - val_fn: 243.0000 - val_accuracy: 0.9373 - val_precision: 0.8435 - val_recall: 0.7903 - val_auc: 0.9669\n",
            "Epoch 70/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2627 - tp: 84206.0000 - fp: 13309.0000 - tn: 521242.0000 - fn: 31995.0000 - accuracy: 0.9304 - precision: 0.8635 - recall: 0.7247 - auc: 0.9557\n",
            "Epoch 00070: val_auc did not improve from 0.96692\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2628 - tp: 84431.0000 - fp: 13337.0000 - tn: 522701.0000 - fn: 32101.0000 - accuracy: 0.9304 - precision: 0.8636 - recall: 0.7245 - auc: 0.9557 - val_loss: 0.2503 - val_tp: 882.0000 - val_fp: 119.0000 - val_tn: 5314.0000 - val_fn: 277.0000 - val_accuracy: 0.9399 - val_precision: 0.8811 - val_recall: 0.7610 - val_auc: 0.9660\n",
            "Epoch 71/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2611 - tp: 84648.0000 - fp: 13257.0000 - tn: 521704.0000 - fn: 31655.0000 - accuracy: 0.9310 - precision: 0.8646 - recall: 0.7278 - auc: 0.9566\n",
            "Epoch 00071: val_auc did not improve from 0.96692\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2611 - tp: 84825.0000 - fp: 13285.0000 - tn: 522753.0000 - fn: 31707.0000 - accuracy: 0.9311 - precision: 0.8646 - recall: 0.7279 - auc: 0.9566 - val_loss: 0.2488 - val_tp: 905.0000 - val_fp: 136.0000 - val_tn: 5297.0000 - val_fn: 254.0000 - val_accuracy: 0.9408 - val_precision: 0.8694 - val_recall: 0.7808 - val_auc: 0.9655\n",
            "Epoch 72/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2603 - tp: 84689.0000 - fp: 13025.0000 - tn: 521529.0000 - fn: 31509.0000 - accuracy: 0.9316 - precision: 0.8667 - recall: 0.7288 - auc: 0.9568\n",
            "Epoch 00072: val_auc improved from 0.96692 to 0.96890, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2604 - tp: 84935.0000 - fp: 13062.0000 - tn: 522976.0000 - fn: 31597.0000 - accuracy: 0.9316 - precision: 0.8667 - recall: 0.7289 - auc: 0.9568 - val_loss: 0.2585 - val_tp: 942.0000 - val_fp: 225.0000 - val_tn: 5208.0000 - val_fn: 217.0000 - val_accuracy: 0.9329 - val_precision: 0.8072 - val_recall: 0.8128 - val_auc: 0.9689\n",
            "Epoch 73/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2598 - tp: 85083.0000 - fp: 13167.0000 - tn: 522215.0000 - fn: 31311.0000 - accuracy: 0.9318 - precision: 0.8660 - recall: 0.7310 - auc: 0.9570\n",
            "Epoch 00073: val_auc improved from 0.96890 to 0.96964, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2598 - tp: 85183.0000 - fp: 13196.0000 - tn: 522842.0000 - fn: 31349.0000 - accuracy: 0.9317 - precision: 0.8659 - recall: 0.7310 - auc: 0.9570 - val_loss: 0.2684 - val_tp: 1010.0000 - val_fp: 352.0000 - val_tn: 5081.0000 - val_fn: 149.0000 - val_accuracy: 0.9240 - val_precision: 0.7416 - val_recall: 0.8714 - val_auc: 0.9696\n",
            "Epoch 74/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2589 - tp: 85251.0000 - fp: 13299.0000 - tn: 522739.0000 - fn: 31281.0000 - accuracy: 0.9317 - precision: 0.8651 - recall: 0.7316 - auc: 0.9574\n",
            "Epoch 00074: val_auc did not improve from 0.96964\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2589 - tp: 85251.0000 - fp: 13299.0000 - tn: 522739.0000 - fn: 31281.0000 - accuracy: 0.9317 - precision: 0.8651 - recall: 0.7316 - auc: 0.9574 - val_loss: 0.2689 - val_tp: 955.0000 - val_fp: 259.0000 - val_tn: 5174.0000 - val_fn: 204.0000 - val_accuracy: 0.9298 - val_precision: 0.7867 - val_recall: 0.8240 - val_auc: 0.9643\n",
            "Epoch 75/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2588 - tp: 85320.0000 - fp: 13280.0000 - tn: 522758.0000 - fn: 31212.0000 - accuracy: 0.9318 - precision: 0.8653 - recall: 0.7322 - auc: 0.9574\n",
            "Epoch 00075: val_auc did not improve from 0.96964\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2588 - tp: 85320.0000 - fp: 13280.0000 - tn: 522758.0000 - fn: 31212.0000 - accuracy: 0.9318 - precision: 0.8653 - recall: 0.7322 - auc: 0.9574 - val_loss: 0.2482 - val_tp: 908.0000 - val_fp: 163.0000 - val_tn: 5270.0000 - val_fn: 251.0000 - val_accuracy: 0.9372 - val_precision: 0.8478 - val_recall: 0.7834 - val_auc: 0.9686\n",
            "Epoch 76/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2575 - tp: 85427.0000 - fp: 13251.0000 - tn: 522560.0000 - fn: 31050.0000 - accuracy: 0.9321 - precision: 0.8657 - recall: 0.7334 - auc: 0.9581\n",
            "Epoch 00076: val_auc improved from 0.96964 to 0.97014, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2575 - tp: 85463.0000 - fp: 13258.0000 - tn: 522780.0000 - fn: 31069.0000 - accuracy: 0.9321 - precision: 0.8657 - recall: 0.7334 - auc: 0.9581 - val_loss: 0.2420 - val_tp: 932.0000 - val_fp: 137.0000 - val_tn: 5296.0000 - val_fn: 227.0000 - val_accuracy: 0.9448 - val_precision: 0.8718 - val_recall: 0.8041 - val_auc: 0.9701\n",
            "Epoch 77/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2555 - tp: 85714.0000 - fp: 13245.0000 - tn: 521732.0000 - fn: 30573.0000 - accuracy: 0.9327 - precision: 0.8662 - recall: 0.7371 - auc: 0.9590\n",
            "Epoch 00077: val_auc did not improve from 0.97014\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2555 - tp: 85892.0000 - fp: 13269.0000 - tn: 522769.0000 - fn: 30640.0000 - accuracy: 0.9327 - precision: 0.8662 - recall: 0.7371 - auc: 0.9590 - val_loss: 0.2828 - val_tp: 1002.0000 - val_fp: 363.0000 - val_tn: 5070.0000 - val_fn: 157.0000 - val_accuracy: 0.9211 - val_precision: 0.7341 - val_recall: 0.8645 - val_auc: 0.9666\n",
            "Epoch 78/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2552 - tp: 85764.0000 - fp: 13163.0000 - tn: 522657.0000 - fn: 30704.0000 - accuracy: 0.9327 - precision: 0.8669 - recall: 0.7364 - auc: 0.9592\n",
            "Epoch 00078: val_auc did not improve from 0.97014\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2552 - tp: 85809.0000 - fp: 13165.0000 - tn: 522873.0000 - fn: 30723.0000 - accuracy: 0.9327 - precision: 0.8670 - recall: 0.7364 - auc: 0.9592 - val_loss: 0.2506 - val_tp: 940.0000 - val_fp: 180.0000 - val_tn: 5253.0000 - val_fn: 219.0000 - val_accuracy: 0.9395 - val_precision: 0.8393 - val_recall: 0.8110 - val_auc: 0.9678\n",
            "Epoch 79/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2544 - tp: 85680.0000 - fp: 12984.0000 - tn: 521998.0000 - fn: 30602.0000 - accuracy: 0.9331 - precision: 0.8684 - recall: 0.7368 - auc: 0.9594\n",
            "Epoch 00079: val_auc improved from 0.97014 to 0.97093, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2545 - tp: 85860.0000 - fp: 13010.0000 - tn: 523028.0000 - fn: 30672.0000 - accuracy: 0.9331 - precision: 0.8684 - recall: 0.7368 - auc: 0.9594 - val_loss: 0.2356 - val_tp: 868.0000 - val_fp: 93.0000 - val_tn: 5340.0000 - val_fn: 291.0000 - val_accuracy: 0.9417 - val_precision: 0.9032 - val_recall: 0.7489 - val_auc: 0.9709\n",
            "Epoch 80/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2536 - tp: 86274.0000 - fp: 13069.0000 - tn: 522969.0000 - fn: 30258.0000 - accuracy: 0.9336 - precision: 0.8684 - recall: 0.7403 - auc: 0.9596\n",
            "Epoch 00080: val_auc did not improve from 0.97093\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2536 - tp: 86274.0000 - fp: 13069.0000 - tn: 522969.0000 - fn: 30258.0000 - accuracy: 0.9336 - precision: 0.8684 - recall: 0.7403 - auc: 0.9596 - val_loss: 0.2507 - val_tp: 929.0000 - val_fp: 182.0000 - val_tn: 5251.0000 - val_fn: 230.0000 - val_accuracy: 0.9375 - val_precision: 0.8362 - val_recall: 0.8016 - val_auc: 0.9695\n",
            "Epoch 81/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2536 - tp: 86138.0000 - fp: 13185.0000 - tn: 522853.0000 - fn: 30394.0000 - accuracy: 0.9332 - precision: 0.8673 - recall: 0.7392 - auc: 0.9599\n",
            "Epoch 00081: val_auc did not improve from 0.97093\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2536 - tp: 86138.0000 - fp: 13185.0000 - tn: 522853.0000 - fn: 30394.0000 - accuracy: 0.9332 - precision: 0.8673 - recall: 0.7392 - auc: 0.9599 - val_loss: 0.2402 - val_tp: 940.0000 - val_fp: 177.0000 - val_tn: 5256.0000 - val_fn: 219.0000 - val_accuracy: 0.9399 - val_precision: 0.8415 - val_recall: 0.8110 - val_auc: 0.9705\n",
            "Epoch 82/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2526 - tp: 86162.0000 - fp: 12980.0000 - tn: 522818.0000 - fn: 30328.0000 - accuracy: 0.9336 - precision: 0.8691 - recall: 0.7397 - auc: 0.9601\n",
            "Epoch 00082: val_auc improved from 0.97093 to 0.97212, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2526 - tp: 86192.0000 - fp: 12988.0000 - tn: 523050.0000 - fn: 30340.0000 - accuracy: 0.9336 - precision: 0.8690 - recall: 0.7396 - auc: 0.9601 - val_loss: 0.2352 - val_tp: 925.0000 - val_fp: 168.0000 - val_tn: 5265.0000 - val_fn: 234.0000 - val_accuracy: 0.9390 - val_precision: 0.8463 - val_recall: 0.7981 - val_auc: 0.9721\n",
            "Epoch 83/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2522 - tp: 86081.0000 - fp: 13114.0000 - tn: 522267.0000 - fn: 30314.0000 - accuracy: 0.9334 - precision: 0.8678 - recall: 0.7396 - auc: 0.9606\n",
            "Epoch 00083: val_auc did not improve from 0.97212\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2522 - tp: 86173.0000 - fp: 13132.0000 - tn: 522906.0000 - fn: 30359.0000 - accuracy: 0.9334 - precision: 0.8678 - recall: 0.7395 - auc: 0.9605 - val_loss: 0.2685 - val_tp: 975.0000 - val_fp: 285.0000 - val_tn: 5148.0000 - val_fn: 184.0000 - val_accuracy: 0.9289 - val_precision: 0.7738 - val_recall: 0.8412 - val_auc: 0.9661\n",
            "Epoch 84/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2504 - tp: 86656.0000 - fp: 13035.0000 - tn: 521947.0000 - fn: 29626.0000 - accuracy: 0.9345 - precision: 0.8692 - recall: 0.7452 - auc: 0.9612\n",
            "Epoch 00084: val_auc did not improve from 0.97212\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2504 - tp: 86848.0000 - fp: 13051.0000 - tn: 522987.0000 - fn: 29684.0000 - accuracy: 0.9345 - precision: 0.8694 - recall: 0.7453 - auc: 0.9612 - val_loss: 0.2449 - val_tp: 953.0000 - val_fp: 201.0000 - val_tn: 5232.0000 - val_fn: 206.0000 - val_accuracy: 0.9383 - val_precision: 0.8258 - val_recall: 0.8223 - val_auc: 0.9706\n",
            "Epoch 85/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2505 - tp: 86614.0000 - fp: 13131.0000 - tn: 522907.0000 - fn: 29918.0000 - accuracy: 0.9340 - precision: 0.8684 - recall: 0.7433 - auc: 0.9611\n",
            "Epoch 00085: val_auc did not improve from 0.97212\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2505 - tp: 86614.0000 - fp: 13131.0000 - tn: 522907.0000 - fn: 29918.0000 - accuracy: 0.9340 - precision: 0.8684 - recall: 0.7433 - auc: 0.9611 - val_loss: 0.2546 - val_tp: 963.0000 - val_fp: 190.0000 - val_tn: 5243.0000 - val_fn: 196.0000 - val_accuracy: 0.9414 - val_precision: 0.8352 - val_recall: 0.8309 - val_auc: 0.9717\n",
            "Epoch 86/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2494 - tp: 86743.0000 - fp: 12970.0000 - tn: 522836.0000 - fn: 29739.0000 - accuracy: 0.9345 - precision: 0.8699 - recall: 0.7447 - auc: 0.9616\n",
            "Epoch 00086: val_auc did not improve from 0.97212\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2494 - tp: 86778.0000 - fp: 12974.0000 - tn: 523064.0000 - fn: 29754.0000 - accuracy: 0.9345 - precision: 0.8699 - recall: 0.7447 - auc: 0.9616 - val_loss: 0.2488 - val_tp: 991.0000 - val_fp: 241.0000 - val_tn: 5192.0000 - val_fn: 168.0000 - val_accuracy: 0.9380 - val_precision: 0.8044 - val_recall: 0.8550 - val_auc: 0.9705\n",
            "Epoch 87/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2485 - tp: 87109.0000 - fp: 13256.0000 - tn: 522552.0000 - fn: 29371.0000 - accuracy: 0.9347 - precision: 0.8679 - recall: 0.7478 - auc: 0.9620\n",
            "Epoch 00087: val_auc did not improve from 0.97212\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2485 - tp: 87149.0000 - fp: 13263.0000 - tn: 522775.0000 - fn: 29383.0000 - accuracy: 0.9346 - precision: 0.8679 - recall: 0.7479 - auc: 0.9620 - val_loss: 0.2620 - val_tp: 991.0000 - val_fp: 304.0000 - val_tn: 5129.0000 - val_fn: 168.0000 - val_accuracy: 0.9284 - val_precision: 0.7653 - val_recall: 0.8550 - val_auc: 0.9695\n",
            "Epoch 88/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2481 - tp: 87119.0000 - fp: 13086.0000 - tn: 522705.0000 - fn: 29378.0000 - accuracy: 0.9349 - precision: 0.8694 - recall: 0.7478 - auc: 0.9623\n",
            "Epoch 00088: val_auc did not improve from 0.97212\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2481 - tp: 87144.0000 - fp: 13093.0000 - tn: 522945.0000 - fn: 29388.0000 - accuracy: 0.9349 - precision: 0.8694 - recall: 0.7478 - auc: 0.9623 - val_loss: 0.2624 - val_tp: 954.0000 - val_fp: 269.0000 - val_tn: 5164.0000 - val_fn: 205.0000 - val_accuracy: 0.9281 - val_precision: 0.7800 - val_recall: 0.8231 - val_auc: 0.9673\n",
            "Epoch 89/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2469 - tp: 87287.0000 - fp: 13065.0000 - tn: 522973.0000 - fn: 29245.0000 - accuracy: 0.9352 - precision: 0.8698 - recall: 0.7490 - auc: 0.9628\n",
            "Epoch 00089: val_auc improved from 0.97212 to 0.97360, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2469 - tp: 87287.0000 - fp: 13065.0000 - tn: 522973.0000 - fn: 29245.0000 - accuracy: 0.9352 - precision: 0.8698 - recall: 0.7490 - auc: 0.9628 - val_loss: 0.2358 - val_tp: 945.0000 - val_fp: 144.0000 - val_tn: 5289.0000 - val_fn: 214.0000 - val_accuracy: 0.9457 - val_precision: 0.8678 - val_recall: 0.8154 - val_auc: 0.9736\n",
            "Epoch 90/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2466 - tp: 87116.0000 - fp: 13113.0000 - tn: 521830.0000 - fn: 29205.0000 - accuracy: 0.9350 - precision: 0.8692 - recall: 0.7489 - auc: 0.9630\n",
            "Epoch 00090: val_auc improved from 0.97360 to 0.97386, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2466 - tp: 87280.0000 - fp: 13136.0000 - tn: 522902.0000 - fn: 29252.0000 - accuracy: 0.9350 - precision: 0.8692 - recall: 0.7490 - auc: 0.9630 - val_loss: 0.2411 - val_tp: 957.0000 - val_fp: 199.0000 - val_tn: 5234.0000 - val_fn: 202.0000 - val_accuracy: 0.9392 - val_precision: 0.8279 - val_recall: 0.8257 - val_auc: 0.9739\n",
            "Epoch 91/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2451 - tp: 87666.0000 - fp: 13042.0000 - tn: 522770.0000 - fn: 28810.0000 - accuracy: 0.9358 - precision: 0.8705 - recall: 0.7527 - auc: 0.9636\n",
            "Epoch 00091: val_auc did not improve from 0.97386\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2451 - tp: 87700.0000 - fp: 13047.0000 - tn: 522991.0000 - fn: 28832.0000 - accuracy: 0.9358 - precision: 0.8705 - recall: 0.7526 - auc: 0.9636 - val_loss: 0.2562 - val_tp: 969.0000 - val_fp: 229.0000 - val_tn: 5204.0000 - val_fn: 190.0000 - val_accuracy: 0.9364 - val_precision: 0.8088 - val_recall: 0.8361 - val_auc: 0.9712\n",
            "Epoch 92/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2452 - tp: 87750.0000 - fp: 12944.0000 - tn: 522858.0000 - fn: 28736.0000 - accuracy: 0.9361 - precision: 0.8715 - recall: 0.7533 - auc: 0.9634\n",
            "Epoch 00092: val_auc did not improve from 0.97386\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2452 - tp: 87785.0000 - fp: 12950.0000 - tn: 523088.0000 - fn: 28747.0000 - accuracy: 0.9361 - precision: 0.8714 - recall: 0.7533 - auc: 0.9634 - val_loss: 0.2384 - val_tp: 942.0000 - val_fp: 171.0000 - val_tn: 5262.0000 - val_fn: 217.0000 - val_accuracy: 0.9411 - val_precision: 0.8464 - val_recall: 0.8128 - val_auc: 0.9714\n",
            "Epoch 93/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2440 - tp: 87841.0000 - fp: 13093.0000 - tn: 522711.0000 - fn: 28643.0000 - accuracy: 0.9360 - precision: 0.8703 - recall: 0.7541 - auc: 0.9639\n",
            "Epoch 00093: val_auc did not improve from 0.97386\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2440 - tp: 87876.0000 - fp: 13096.0000 - tn: 522942.0000 - fn: 28656.0000 - accuracy: 0.9360 - precision: 0.8703 - recall: 0.7541 - auc: 0.9639 - val_loss: 0.2421 - val_tp: 948.0000 - val_fp: 175.0000 - val_tn: 5258.0000 - val_fn: 211.0000 - val_accuracy: 0.9414 - val_precision: 0.8442 - val_recall: 0.8179 - val_auc: 0.9714\n",
            "Epoch 94/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2440 - tp: 87838.0000 - fp: 13215.0000 - tn: 522600.0000 - fn: 28635.0000 - accuracy: 0.9358 - precision: 0.8692 - recall: 0.7541 - auc: 0.9640\n",
            "Epoch 00094: val_auc did not improve from 0.97386\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2440 - tp: 87878.0000 - fp: 13218.0000 - tn: 522820.0000 - fn: 28654.0000 - accuracy: 0.9358 - precision: 0.8693 - recall: 0.7541 - auc: 0.9640 - val_loss: 0.2464 - val_tp: 994.0000 - val_fp: 244.0000 - val_tn: 5189.0000 - val_fn: 165.0000 - val_accuracy: 0.9380 - val_precision: 0.8029 - val_recall: 0.8576 - val_auc: 0.9713\n",
            "Epoch 95/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2439 - tp: 87688.0000 - fp: 12989.0000 - tn: 522815.0000 - fn: 28796.0000 - accuracy: 0.9359 - precision: 0.8710 - recall: 0.7528 - auc: 0.9640\n",
            "Epoch 00095: val_auc improved from 0.97386 to 0.97498, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2439 - tp: 87724.0000 - fp: 12999.0000 - tn: 523039.0000 - fn: 28808.0000 - accuracy: 0.9359 - precision: 0.8709 - recall: 0.7528 - auc: 0.9640 - val_loss: 0.2383 - val_tp: 978.0000 - val_fp: 189.0000 - val_tn: 5244.0000 - val_fn: 181.0000 - val_accuracy: 0.9439 - val_precision: 0.8380 - val_recall: 0.8438 - val_auc: 0.9750\n",
            "Epoch 96/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2427 - tp: 88077.0000 - fp: 13120.0000 - tn: 522280.0000 - fn: 28299.0000 - accuracy: 0.9365 - precision: 0.8704 - recall: 0.7568 - auc: 0.9645\n",
            "Epoch 00096: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2427 - tp: 88201.0000 - fp: 13132.0000 - tn: 522906.0000 - fn: 28331.0000 - accuracy: 0.9365 - precision: 0.8704 - recall: 0.7569 - auc: 0.9645 - val_loss: 0.2562 - val_tp: 1010.0000 - val_fp: 298.0000 - val_tn: 5135.0000 - val_fn: 149.0000 - val_accuracy: 0.9322 - val_precision: 0.7722 - val_recall: 0.8714 - val_auc: 0.9719\n",
            "Epoch 97/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2427 - tp: 88175.0000 - fp: 13036.0000 - tn: 523002.0000 - fn: 28357.0000 - accuracy: 0.9366 - precision: 0.8712 - recall: 0.7567 - auc: 0.9644\n",
            "Epoch 00097: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2427 - tp: 88175.0000 - fp: 13036.0000 - tn: 523002.0000 - fn: 28357.0000 - accuracy: 0.9366 - precision: 0.8712 - recall: 0.7567 - auc: 0.9644 - val_loss: 0.2375 - val_tp: 932.0000 - val_fp: 169.0000 - val_tn: 5264.0000 - val_fn: 227.0000 - val_accuracy: 0.9399 - val_precision: 0.8465 - val_recall: 0.8041 - val_auc: 0.9701\n",
            "Epoch 98/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2416 - tp: 87988.0000 - fp: 12867.0000 - tn: 521696.0000 - fn: 28201.0000 - accuracy: 0.9369 - precision: 0.8724 - recall: 0.7573 - auc: 0.9648\n",
            "Epoch 00098: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2415 - tp: 88262.0000 - fp: 12906.0000 - tn: 523132.0000 - fn: 28270.0000 - accuracy: 0.9369 - precision: 0.8724 - recall: 0.7574 - auc: 0.9648 - val_loss: 0.2504 - val_tp: 1010.0000 - val_fp: 265.0000 - val_tn: 5168.0000 - val_fn: 149.0000 - val_accuracy: 0.9372 - val_precision: 0.7922 - val_recall: 0.8714 - val_auc: 0.9740\n",
            "Epoch 99/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2408 - tp: 88043.0000 - fp: 12911.0000 - tn: 522050.0000 - fn: 28260.0000 - accuracy: 0.9368 - precision: 0.8721 - recall: 0.7570 - auc: 0.9653\n",
            "Epoch 00099: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2408 - tp: 88212.0000 - fp: 12933.0000 - tn: 523105.0000 - fn: 28320.0000 - accuracy: 0.9368 - precision: 0.8721 - recall: 0.7570 - auc: 0.9653 - val_loss: 0.2561 - val_tp: 995.0000 - val_fp: 273.0000 - val_tn: 5160.0000 - val_fn: 164.0000 - val_accuracy: 0.9337 - val_precision: 0.7847 - val_recall: 0.8585 - val_auc: 0.9729\n",
            "Epoch 100/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2411 - tp: 88252.0000 - fp: 13013.0000 - tn: 522352.0000 - fn: 28159.0000 - accuracy: 0.9368 - precision: 0.8715 - recall: 0.7581 - auc: 0.9651\n",
            "Epoch 00100: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2411 - tp: 88345.0000 - fp: 13035.0000 - tn: 523003.0000 - fn: 28187.0000 - accuracy: 0.9368 - precision: 0.8714 - recall: 0.7581 - auc: 0.9651 - val_loss: 0.2428 - val_tp: 973.0000 - val_fp: 220.0000 - val_tn: 5213.0000 - val_fn: 186.0000 - val_accuracy: 0.9384 - val_precision: 0.8156 - val_recall: 0.8395 - val_auc: 0.9725\n",
            "Epoch 101/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2397 - tp: 88464.0000 - fp: 12921.0000 - tn: 522033.0000 - fn: 27846.0000 - accuracy: 0.9374 - precision: 0.8726 - recall: 0.7606 - auc: 0.9657\n",
            "Epoch 00101: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2398 - tp: 88626.0000 - fp: 12950.0000 - tn: 523088.0000 - fn: 27906.0000 - accuracy: 0.9374 - precision: 0.8725 - recall: 0.7605 - auc: 0.9656 - val_loss: 0.2391 - val_tp: 976.0000 - val_fp: 192.0000 - val_tn: 5241.0000 - val_fn: 183.0000 - val_accuracy: 0.9431 - val_precision: 0.8356 - val_recall: 0.8421 - val_auc: 0.9739\n",
            "Epoch 102/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2397 - tp: 88455.0000 - fp: 12793.0000 - tn: 522170.0000 - fn: 27846.0000 - accuracy: 0.9376 - precision: 0.8736 - recall: 0.7606 - auc: 0.9657\n",
            "Epoch 00102: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2397 - tp: 88634.0000 - fp: 12811.0000 - tn: 523227.0000 - fn: 27898.0000 - accuracy: 0.9376 - precision: 0.8737 - recall: 0.7606 - auc: 0.9657 - val_loss: 0.2319 - val_tp: 919.0000 - val_fp: 149.0000 - val_tn: 5284.0000 - val_fn: 240.0000 - val_accuracy: 0.9410 - val_precision: 0.8605 - val_recall: 0.7929 - val_auc: 0.9738\n",
            "Epoch 103/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2394 - tp: 88212.0000 - fp: 12811.0000 - tn: 522181.0000 - fn: 28060.0000 - accuracy: 0.9372 - precision: 0.8732 - recall: 0.7587 - auc: 0.9659\n",
            "Epoch 00103: val_auc did not improve from 0.97498\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2395 - tp: 88406.0000 - fp: 12827.0000 - tn: 523211.0000 - fn: 28126.0000 - accuracy: 0.9372 - precision: 0.8733 - recall: 0.7586 - auc: 0.9659 - val_loss: 0.2336 - val_tp: 962.0000 - val_fp: 141.0000 - val_tn: 5292.0000 - val_fn: 197.0000 - val_accuracy: 0.9487 - val_precision: 0.8722 - val_recall: 0.8300 - val_auc: 0.9739\n",
            "Epoch 104/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2379 - tp: 88563.0000 - fp: 12578.0000 - tn: 522378.0000 - fn: 27745.0000 - accuracy: 0.9381 - precision: 0.8756 - recall: 0.7615 - auc: 0.9665\n",
            "Epoch 00104: val_auc improved from 0.97498 to 0.97607, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2378 - tp: 88738.0000 - fp: 12616.0000 - tn: 523422.0000 - fn: 27794.0000 - accuracy: 0.9381 - precision: 0.8755 - recall: 0.7615 - auc: 0.9665 - val_loss: 0.2419 - val_tp: 1011.0000 - val_fp: 237.0000 - val_tn: 5196.0000 - val_fn: 148.0000 - val_accuracy: 0.9416 - val_precision: 0.8101 - val_recall: 0.8723 - val_auc: 0.9761\n",
            "Epoch 105/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2386 - tp: 88381.0000 - fp: 12937.0000 - tn: 521621.0000 - fn: 27813.0000 - accuracy: 0.9374 - precision: 0.8723 - recall: 0.7606 - auc: 0.9662\n",
            "Epoch 00105: val_auc improved from 0.97607 to 0.97712, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2387 - tp: 88618.0000 - fp: 12961.0000 - tn: 523077.0000 - fn: 27914.0000 - accuracy: 0.9374 - precision: 0.8724 - recall: 0.7605 - auc: 0.9662 - val_loss: 0.2396 - val_tp: 997.0000 - val_fp: 227.0000 - val_tn: 5206.0000 - val_fn: 162.0000 - val_accuracy: 0.9410 - val_precision: 0.8145 - val_recall: 0.8602 - val_auc: 0.9771\n",
            "Epoch 106/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2365 - tp: 89156.0000 - fp: 12657.0000 - tn: 522328.0000 - fn: 27123.0000 - accuracy: 0.9389 - precision: 0.8757 - recall: 0.7667 - auc: 0.9670\n",
            "Epoch 00106: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2365 - tp: 89342.0000 - fp: 12677.0000 - tn: 523361.0000 - fn: 27190.0000 - accuracy: 0.9389 - precision: 0.8757 - recall: 0.7667 - auc: 0.9670 - val_loss: 0.2193 - val_tp: 916.0000 - val_fp: 100.0000 - val_tn: 5333.0000 - val_fn: 243.0000 - val_accuracy: 0.9480 - val_precision: 0.9016 - val_recall: 0.7903 - val_auc: 0.9770\n",
            "Epoch 107/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2370 - tp: 88666.0000 - fp: 12829.0000 - tn: 522122.0000 - fn: 27647.0000 - accuracy: 0.9379 - precision: 0.8736 - recall: 0.7623 - auc: 0.9669\n",
            "Epoch 00107: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2371 - tp: 88832.0000 - fp: 12859.0000 - tn: 523179.0000 - fn: 27700.0000 - accuracy: 0.9378 - precision: 0.8735 - recall: 0.7623 - auc: 0.9668 - val_loss: 0.2284 - val_tp: 917.0000 - val_fp: 117.0000 - val_tn: 5316.0000 - val_fn: 242.0000 - val_accuracy: 0.9455 - val_precision: 0.8868 - val_recall: 0.7912 - val_auc: 0.9726\n",
            "Epoch 108/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2367 - tp: 89024.0000 - fp: 12909.0000 - tn: 522898.0000 - fn: 27457.0000 - accuracy: 0.9381 - precision: 0.8734 - recall: 0.7643 - auc: 0.9669\n",
            "Epoch 00108: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2367 - tp: 89064.0000 - fp: 12914.0000 - tn: 523124.0000 - fn: 27468.0000 - accuracy: 0.9381 - precision: 0.8734 - recall: 0.7643 - auc: 0.9669 - val_loss: 0.2396 - val_tp: 987.0000 - val_fp: 230.0000 - val_tn: 5203.0000 - val_fn: 172.0000 - val_accuracy: 0.9390 - val_precision: 0.8110 - val_recall: 0.8516 - val_auc: 0.9735\n",
            "Epoch 109/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2360 - tp: 88962.0000 - fp: 12799.0000 - tn: 521763.0000 - fn: 27228.0000 - accuracy: 0.9385 - precision: 0.8742 - recall: 0.7657 - auc: 0.9673\n",
            "Epoch 00109: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2360 - tp: 89222.0000 - fp: 12843.0000 - tn: 523195.0000 - fn: 27310.0000 - accuracy: 0.9385 - precision: 0.8742 - recall: 0.7656 - auc: 0.9673 - val_loss: 0.2318 - val_tp: 980.0000 - val_fp: 204.0000 - val_tn: 5229.0000 - val_fn: 179.0000 - val_accuracy: 0.9419 - val_precision: 0.8277 - val_recall: 0.8456 - val_auc: 0.9743\n",
            "Epoch 110/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2362 - tp: 89190.0000 - fp: 12947.0000 - tn: 523091.0000 - fn: 27342.0000 - accuracy: 0.9383 - precision: 0.8732 - recall: 0.7654 - auc: 0.9673\n",
            "Epoch 00110: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2362 - tp: 89190.0000 - fp: 12947.0000 - tn: 523091.0000 - fn: 27342.0000 - accuracy: 0.9383 - precision: 0.8732 - recall: 0.7654 - auc: 0.9673 - val_loss: 0.2568 - val_tp: 1029.0000 - val_fp: 313.0000 - val_tn: 5120.0000 - val_fn: 130.0000 - val_accuracy: 0.9328 - val_precision: 0.7668 - val_recall: 0.8878 - val_auc: 0.9735\n",
            "Epoch 111/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2354 - tp: 89319.0000 - fp: 12851.0000 - tn: 522954.0000 - fn: 27164.0000 - accuracy: 0.9387 - precision: 0.8742 - recall: 0.7668 - auc: 0.9675\n",
            "Epoch 00111: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2353 - tp: 89357.0000 - fp: 12856.0000 - tn: 523182.0000 - fn: 27175.0000 - accuracy: 0.9387 - precision: 0.8742 - recall: 0.7668 - auc: 0.9675 - val_loss: 0.2249 - val_tp: 936.0000 - val_fp: 134.0000 - val_tn: 5299.0000 - val_fn: 223.0000 - val_accuracy: 0.9458 - val_precision: 0.8748 - val_recall: 0.8076 - val_auc: 0.9739\n",
            "Epoch 112/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2346 - tp: 89338.0000 - fp: 12907.0000 - tn: 522060.0000 - fn: 26959.0000 - accuracy: 0.9388 - precision: 0.8738 - recall: 0.7682 - auc: 0.9679\n",
            "Epoch 00112: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2346 - tp: 89518.0000 - fp: 12931.0000 - tn: 523107.0000 - fn: 27014.0000 - accuracy: 0.9388 - precision: 0.8738 - recall: 0.7682 - auc: 0.9679 - val_loss: 0.2393 - val_tp: 1002.0000 - val_fp: 226.0000 - val_tn: 5207.0000 - val_fn: 157.0000 - val_accuracy: 0.9419 - val_precision: 0.8160 - val_recall: 0.8645 - val_auc: 0.9762\n",
            "Epoch 113/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2344 - tp: 89336.0000 - fp: 12730.0000 - tn: 522642.0000 - fn: 27068.0000 - accuracy: 0.9389 - precision: 0.8753 - recall: 0.7675 - auc: 0.9677\n",
            "Epoch 00113: val_auc did not improve from 0.97712\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2343 - tp: 89440.0000 - fp: 12745.0000 - tn: 523293.0000 - fn: 27092.0000 - accuracy: 0.9390 - precision: 0.8753 - recall: 0.7675 - auc: 0.9677 - val_loss: 0.2386 - val_tp: 1012.0000 - val_fp: 248.0000 - val_tn: 5185.0000 - val_fn: 147.0000 - val_accuracy: 0.9401 - val_precision: 0.8032 - val_recall: 0.8732 - val_auc: 0.9767\n",
            "Epoch 114/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2340 - tp: 89522.0000 - fp: 12915.0000 - tn: 522489.0000 - fn: 26850.0000 - accuracy: 0.9390 - precision: 0.8739 - recall: 0.7693 - auc: 0.9680\n",
            "Epoch 00114: val_auc improved from 0.97712 to 0.97742, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2340 - tp: 89647.0000 - fp: 12926.0000 - tn: 523112.0000 - fn: 26885.0000 - accuracy: 0.9390 - precision: 0.8740 - recall: 0.7693 - auc: 0.9680 - val_loss: 0.2219 - val_tp: 999.0000 - val_fp: 197.0000 - val_tn: 5236.0000 - val_fn: 160.0000 - val_accuracy: 0.9458 - val_precision: 0.8353 - val_recall: 0.8619 - val_auc: 0.9774\n",
            "Epoch 115/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2333 - tp: 90031.0000 - fp: 13044.0000 - tn: 522773.0000 - fn: 26440.0000 - accuracy: 0.9395 - precision: 0.8735 - recall: 0.7730 - auc: 0.9683\n",
            "Epoch 00115: val_auc did not improve from 0.97742\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2333 - tp: 90071.0000 - fp: 13045.0000 - tn: 522993.0000 - fn: 26461.0000 - accuracy: 0.9395 - precision: 0.8735 - recall: 0.7729 - auc: 0.9683 - val_loss: 0.2317 - val_tp: 991.0000 - val_fp: 200.0000 - val_tn: 5233.0000 - val_fn: 168.0000 - val_accuracy: 0.9442 - val_precision: 0.8321 - val_recall: 0.8550 - val_auc: 0.9770\n",
            "Epoch 116/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2326 - tp: 90120.0000 - fp: 13033.0000 - tn: 522784.0000 - fn: 26351.0000 - accuracy: 0.9396 - precision: 0.8737 - recall: 0.7738 - auc: 0.9687\n",
            "Epoch 00116: val_auc did not improve from 0.97742\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2326 - tp: 90166.0000 - fp: 13036.0000 - tn: 523002.0000 - fn: 26366.0000 - accuracy: 0.9396 - precision: 0.8737 - recall: 0.7737 - auc: 0.9687 - val_loss: 0.2324 - val_tp: 949.0000 - val_fp: 168.0000 - val_tn: 5265.0000 - val_fn: 210.0000 - val_accuracy: 0.9427 - val_precision: 0.8496 - val_recall: 0.8188 - val_auc: 0.9733\n",
            "Epoch 117/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2326 - tp: 89982.0000 - fp: 12839.0000 - tn: 522964.0000 - fn: 26503.0000 - accuracy: 0.9397 - precision: 0.8751 - recall: 0.7725 - auc: 0.9685\n",
            "Epoch 00117: val_auc did not improve from 0.97742\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2326 - tp: 90015.0000 - fp: 12854.0000 - tn: 523184.0000 - fn: 26517.0000 - accuracy: 0.9397 - precision: 0.8750 - recall: 0.7724 - auc: 0.9685 - val_loss: 0.2207 - val_tp: 961.0000 - val_fp: 158.0000 - val_tn: 5275.0000 - val_fn: 198.0000 - val_accuracy: 0.9460 - val_precision: 0.8588 - val_recall: 0.8292 - val_auc: 0.9762\n",
            "Epoch 118/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2319 - tp: 89903.0000 - fp: 12837.0000 - tn: 523201.0000 - fn: 26629.0000 - accuracy: 0.9395 - precision: 0.8751 - recall: 0.7715 - auc: 0.9691\n",
            "Epoch 00118: val_auc improved from 0.97742 to 0.97817, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2319 - tp: 89903.0000 - fp: 12837.0000 - tn: 523201.0000 - fn: 26629.0000 - accuracy: 0.9395 - precision: 0.8751 - recall: 0.7715 - auc: 0.9691 - val_loss: 0.2190 - val_tp: 965.0000 - val_fp: 152.0000 - val_tn: 5281.0000 - val_fn: 194.0000 - val_accuracy: 0.9475 - val_precision: 0.8639 - val_recall: 0.8326 - val_auc: 0.9782\n",
            "Epoch 119/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2318 - tp: 89958.0000 - fp: 12958.0000 - tn: 522420.0000 - fn: 26440.0000 - accuracy: 0.9396 - precision: 0.8741 - recall: 0.7728 - auc: 0.9689\n",
            "Epoch 00119: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2318 - tp: 90060.0000 - fp: 12975.0000 - tn: 523063.0000 - fn: 26472.0000 - accuracy: 0.9396 - precision: 0.8741 - recall: 0.7728 - auc: 0.9689 - val_loss: 0.2375 - val_tp: 962.0000 - val_fp: 177.0000 - val_tn: 5256.0000 - val_fn: 197.0000 - val_accuracy: 0.9433 - val_precision: 0.8446 - val_recall: 0.8300 - val_auc: 0.9734\n",
            "Epoch 120/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2311 - tp: 89726.0000 - fp: 12848.0000 - tn: 521675.0000 - fn: 26503.0000 - accuracy: 0.9395 - precision: 0.8747 - recall: 0.7720 - auc: 0.9692\n",
            "Epoch 00120: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2311 - tp: 89967.0000 - fp: 12884.0000 - tn: 523154.0000 - fn: 26565.0000 - accuracy: 0.9395 - precision: 0.8747 - recall: 0.7720 - auc: 0.9692 - val_loss: 0.2410 - val_tp: 996.0000 - val_fp: 223.0000 - val_tn: 5210.0000 - val_fn: 163.0000 - val_accuracy: 0.9414 - val_precision: 0.8171 - val_recall: 0.8594 - val_auc: 0.9760\n",
            "Epoch 121/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2301 - tp: 90301.0000 - fp: 12750.0000 - tn: 523053.0000 - fn: 26184.0000 - accuracy: 0.9403 - precision: 0.8763 - recall: 0.7752 - auc: 0.9695\n",
            "Epoch 00121: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2301 - tp: 90338.0000 - fp: 12761.0000 - tn: 523277.0000 - fn: 26194.0000 - accuracy: 0.9403 - precision: 0.8762 - recall: 0.7752 - auc: 0.9695 - val_loss: 0.2153 - val_tp: 995.0000 - val_fp: 173.0000 - val_tn: 5260.0000 - val_fn: 164.0000 - val_accuracy: 0.9489 - val_precision: 0.8519 - val_recall: 0.8585 - val_auc: 0.9780\n",
            "Epoch 122/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2308 - tp: 90367.0000 - fp: 12848.0000 - tn: 522956.0000 - fn: 26117.0000 - accuracy: 0.9403 - precision: 0.8755 - recall: 0.7758 - auc: 0.9693\n",
            "Epoch 00122: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2308 - tp: 90405.0000 - fp: 12858.0000 - tn: 523180.0000 - fn: 26127.0000 - accuracy: 0.9403 - precision: 0.8755 - recall: 0.7758 - auc: 0.9693 - val_loss: 0.2804 - val_tp: 1063.0000 - val_fp: 487.0000 - val_tn: 4946.0000 - val_fn: 96.0000 - val_accuracy: 0.9116 - val_precision: 0.6858 - val_recall: 0.9172 - val_auc: 0.9759\n",
            "Epoch 123/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2302 - tp: 89973.0000 - fp: 12849.0000 - tn: 521716.0000 - fn: 26214.0000 - accuracy: 0.9400 - precision: 0.8750 - recall: 0.7744 - auc: 0.9694\n",
            "Epoch 00123: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2302 - tp: 90239.0000 - fp: 12876.0000 - tn: 523162.0000 - fn: 26293.0000 - accuracy: 0.9400 - precision: 0.8751 - recall: 0.7744 - auc: 0.9694 - val_loss: 0.2671 - val_tp: 1046.0000 - val_fp: 400.0000 - val_tn: 5033.0000 - val_fn: 113.0000 - val_accuracy: 0.9222 - val_precision: 0.7234 - val_recall: 0.9025 - val_auc: 0.9742\n",
            "Epoch 124/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2302 - tp: 90315.0000 - fp: 13012.0000 - tn: 522376.0000 - fn: 26073.0000 - accuracy: 0.9400 - precision: 0.8741 - recall: 0.7760 - auc: 0.9694\n",
            "Epoch 00124: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2302 - tp: 90427.0000 - fp: 13019.0000 - tn: 523019.0000 - fn: 26105.0000 - accuracy: 0.9400 - precision: 0.8741 - recall: 0.7760 - auc: 0.9694 - val_loss: 0.2407 - val_tp: 1012.0000 - val_fp: 261.0000 - val_tn: 5172.0000 - val_fn: 147.0000 - val_accuracy: 0.9381 - val_precision: 0.7950 - val_recall: 0.8732 - val_auc: 0.9768\n",
            "Epoch 125/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2294 - tp: 90428.0000 - fp: 12943.0000 - tn: 523095.0000 - fn: 26104.0000 - accuracy: 0.9402 - precision: 0.8748 - recall: 0.7760 - auc: 0.9698\n",
            "Epoch 00125: val_auc did not improve from 0.97817\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2294 - tp: 90428.0000 - fp: 12943.0000 - tn: 523095.0000 - fn: 26104.0000 - accuracy: 0.9402 - precision: 0.8748 - recall: 0.7760 - auc: 0.9698 - val_loss: 0.2347 - val_tp: 991.0000 - val_fp: 215.0000 - val_tn: 5218.0000 - val_fn: 168.0000 - val_accuracy: 0.9419 - val_precision: 0.8217 - val_recall: 0.8550 - val_auc: 0.9768\n",
            "Epoch 126/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2285 - tp: 90525.0000 - fp: 12769.0000 - tn: 522632.0000 - fn: 25850.0000 - accuracy: 0.9407 - precision: 0.8764 - recall: 0.7779 - auc: 0.9701\n",
            "Epoch 00126: val_auc improved from 0.97817 to 0.97911, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2285 - tp: 90642.0000 - fp: 12783.0000 - tn: 523255.0000 - fn: 25890.0000 - accuracy: 0.9407 - precision: 0.8764 - recall: 0.7778 - auc: 0.9701 - val_loss: 0.2212 - val_tp: 995.0000 - val_fp: 176.0000 - val_tn: 5257.0000 - val_fn: 164.0000 - val_accuracy: 0.9484 - val_precision: 0.8497 - val_recall: 0.8585 - val_auc: 0.9791\n",
            "Epoch 127/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2293 - tp: 90378.0000 - fp: 12700.0000 - tn: 522268.0000 - fn: 25918.0000 - accuracy: 0.9407 - precision: 0.8768 - recall: 0.7771 - auc: 0.9697\n",
            "Epoch 00127: val_auc did not improve from 0.97911\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2293 - tp: 90564.0000 - fp: 12726.0000 - tn: 523312.0000 - fn: 25968.0000 - accuracy: 0.9407 - precision: 0.8768 - recall: 0.7772 - auc: 0.9697 - val_loss: 0.2404 - val_tp: 1043.0000 - val_fp: 282.0000 - val_tn: 5151.0000 - val_fn: 116.0000 - val_accuracy: 0.9396 - val_precision: 0.7872 - val_recall: 0.8999 - val_auc: 0.9785\n",
            "Epoch 128/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2287 - tp: 90413.0000 - fp: 12817.0000 - tn: 522585.0000 - fn: 25961.0000 - accuracy: 0.9405 - precision: 0.8758 - recall: 0.7769 - auc: 0.9701\n",
            "Epoch 00128: val_auc did not improve from 0.97911\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2288 - tp: 90515.0000 - fp: 12834.0000 - tn: 523204.0000 - fn: 26017.0000 - accuracy: 0.9405 - precision: 0.8758 - recall: 0.7767 - auc: 0.9700 - val_loss: 0.2232 - val_tp: 969.0000 - val_fp: 162.0000 - val_tn: 5271.0000 - val_fn: 190.0000 - val_accuracy: 0.9466 - val_precision: 0.8568 - val_recall: 0.8361 - val_auc: 0.9777\n",
            "Epoch 129/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2281 - tp: 90605.0000 - fp: 13038.0000 - tn: 521480.0000 - fn: 25629.0000 - accuracy: 0.9406 - precision: 0.8742 - recall: 0.7795 - auc: 0.9704\n",
            "Epoch 00129: val_auc did not improve from 0.97911\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2281 - tp: 90846.0000 - fp: 13086.0000 - tn: 522952.0000 - fn: 25686.0000 - accuracy: 0.9406 - precision: 0.8741 - recall: 0.7796 - auc: 0.9704 - val_loss: 0.2414 - val_tp: 1018.0000 - val_fp: 251.0000 - val_tn: 5182.0000 - val_fn: 141.0000 - val_accuracy: 0.9405 - val_precision: 0.8022 - val_recall: 0.8783 - val_auc: 0.9788\n",
            "Epoch 130/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2274 - tp: 90656.0000 - fp: 12585.0000 - tn: 523453.0000 - fn: 25876.0000 - accuracy: 0.9411 - precision: 0.8781 - recall: 0.7779 - auc: 0.9706\n",
            "Epoch 00130: val_auc did not improve from 0.97911\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2274 - tp: 90656.0000 - fp: 12585.0000 - tn: 523453.0000 - fn: 25876.0000 - accuracy: 0.9411 - precision: 0.8781 - recall: 0.7779 - auc: 0.9706 - val_loss: 0.2525 - val_tp: 1054.0000 - val_fp: 326.0000 - val_tn: 5107.0000 - val_fn: 105.0000 - val_accuracy: 0.9346 - val_precision: 0.7638 - val_recall: 0.9094 - val_auc: 0.9788\n",
            "Epoch 131/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2277 - tp: 90714.0000 - fp: 13045.0000 - tn: 522358.0000 - fn: 25659.0000 - accuracy: 0.9406 - precision: 0.8743 - recall: 0.7795 - auc: 0.9705\n",
            "Epoch 00131: val_auc did not improve from 0.97911\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2277 - tp: 90834.0000 - fp: 13059.0000 - tn: 522979.0000 - fn: 25698.0000 - accuracy: 0.9406 - precision: 0.8743 - recall: 0.7795 - auc: 0.9705 - val_loss: 0.2234 - val_tp: 969.0000 - val_fp: 136.0000 - val_tn: 5297.0000 - val_fn: 190.0000 - val_accuracy: 0.9505 - val_precision: 0.8769 - val_recall: 0.8361 - val_auc: 0.9783\n",
            "Epoch 132/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2278 - tp: 90483.0000 - fp: 12876.0000 - tn: 522096.0000 - fn: 25809.0000 - accuracy: 0.9406 - precision: 0.8754 - recall: 0.7781 - auc: 0.9705\n",
            "Epoch 00132: val_auc improved from 0.97911 to 0.97946, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2278 - tp: 90667.0000 - fp: 12902.0000 - tn: 523136.0000 - fn: 25865.0000 - accuracy: 0.9406 - precision: 0.8754 - recall: 0.7780 - auc: 0.9705 - val_loss: 0.2264 - val_tp: 1001.0000 - val_fp: 188.0000 - val_tn: 5245.0000 - val_fn: 158.0000 - val_accuracy: 0.9475 - val_precision: 0.8419 - val_recall: 0.8637 - val_auc: 0.9795\n",
            "Epoch 133/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2270 - tp: 90861.0000 - fp: 13022.0000 - tn: 521497.0000 - fn: 25372.0000 - accuracy: 0.9410 - precision: 0.8746 - recall: 0.7817 - auc: 0.9707\n",
            "Epoch 00133: val_auc did not improve from 0.97946\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2270 - tp: 91105.0000 - fp: 13065.0000 - tn: 522973.0000 - fn: 25427.0000 - accuracy: 0.9410 - precision: 0.8746 - recall: 0.7818 - auc: 0.9707 - val_loss: 0.2395 - val_tp: 1039.0000 - val_fp: 268.0000 - val_tn: 5165.0000 - val_fn: 120.0000 - val_accuracy: 0.9411 - val_precision: 0.7950 - val_recall: 0.8965 - val_auc: 0.9786\n",
            "Epoch 134/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2268 - tp: 90868.0000 - fp: 12699.0000 - tn: 522685.0000 - fn: 25524.0000 - accuracy: 0.9414 - precision: 0.8774 - recall: 0.7807 - auc: 0.9708\n",
            "Epoch 00134: val_auc did not improve from 0.97946\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2268 - tp: 90976.0000 - fp: 12712.0000 - tn: 523326.0000 - fn: 25556.0000 - accuracy: 0.9414 - precision: 0.8774 - recall: 0.7807 - auc: 0.9708 - val_loss: 0.2420 - val_tp: 1017.0000 - val_fp: 238.0000 - val_tn: 5195.0000 - val_fn: 142.0000 - val_accuracy: 0.9424 - val_precision: 0.8104 - val_recall: 0.8775 - val_auc: 0.9790\n",
            "Epoch 135/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2261 - tp: 90818.0000 - fp: 12670.0000 - tn: 523368.0000 - fn: 25714.0000 - accuracy: 0.9412 - precision: 0.8776 - recall: 0.7793 - auc: 0.9711\n",
            "Epoch 00135: val_auc did not improve from 0.97946\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2261 - tp: 90818.0000 - fp: 12670.0000 - tn: 523368.0000 - fn: 25714.0000 - accuracy: 0.9412 - precision: 0.8776 - recall: 0.7793 - auc: 0.9711 - val_loss: 0.2411 - val_tp: 1028.0000 - val_fp: 263.0000 - val_tn: 5170.0000 - val_fn: 131.0000 - val_accuracy: 0.9402 - val_precision: 0.7963 - val_recall: 0.8870 - val_auc: 0.9787\n",
            "Epoch 136/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2258 - tp: 90823.0000 - fp: 12567.0000 - tn: 522824.0000 - fn: 25562.0000 - accuracy: 0.9415 - precision: 0.8785 - recall: 0.7804 - auc: 0.9713\n",
            "Epoch 00136: val_auc improved from 0.97946 to 0.98041, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2258 - tp: 90932.0000 - fp: 12593.0000 - tn: 523445.0000 - fn: 25600.0000 - accuracy: 0.9415 - precision: 0.8784 - recall: 0.7803 - auc: 0.9713 - val_loss: 0.2151 - val_tp: 970.0000 - val_fp: 153.0000 - val_tn: 5280.0000 - val_fn: 189.0000 - val_accuracy: 0.9481 - val_precision: 0.8638 - val_recall: 0.8369 - val_auc: 0.9804\n",
            "Epoch 137/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2255 - tp: 90989.0000 - fp: 12665.0000 - tn: 522707.0000 - fn: 25415.0000 - accuracy: 0.9416 - precision: 0.8778 - recall: 0.7817 - auc: 0.9713\n",
            "Epoch 00137: val_auc did not improve from 0.98041\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2255 - tp: 91089.0000 - fp: 12687.0000 - tn: 523351.0000 - fn: 25443.0000 - accuracy: 0.9416 - precision: 0.8777 - recall: 0.7817 - auc: 0.9713 - val_loss: 0.2481 - val_tp: 1063.0000 - val_fp: 337.0000 - val_tn: 5096.0000 - val_fn: 96.0000 - val_accuracy: 0.9343 - val_precision: 0.7593 - val_recall: 0.9172 - val_auc: 0.9801\n",
            "Epoch 138/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2252 - tp: 91218.0000 - fp: 12618.0000 - tn: 523195.0000 - fn: 25257.0000 - accuracy: 0.9419 - precision: 0.8785 - recall: 0.7832 - auc: 0.9713\n",
            "Epoch 00138: val_auc improved from 0.98041 to 0.98096, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2252 - tp: 91257.0000 - fp: 12623.0000 - tn: 523415.0000 - fn: 25275.0000 - accuracy: 0.9419 - precision: 0.8785 - recall: 0.7831 - auc: 0.9713 - val_loss: 0.2398 - val_tp: 1050.0000 - val_fp: 295.0000 - val_tn: 5138.0000 - val_fn: 109.0000 - val_accuracy: 0.9387 - val_precision: 0.7807 - val_recall: 0.9060 - val_auc: 0.9810\n",
            "Epoch 139/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2258 - tp: 90811.0000 - fp: 12625.0000 - tn: 522769.0000 - fn: 25571.0000 - accuracy: 0.9414 - precision: 0.8779 - recall: 0.7803 - auc: 0.9712\n",
            "Epoch 00139: val_auc improved from 0.98096 to 0.98265, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2258 - tp: 90928.0000 - fp: 12641.0000 - tn: 523397.0000 - fn: 25604.0000 - accuracy: 0.9414 - precision: 0.8779 - recall: 0.7803 - auc: 0.9712 - val_loss: 0.2120 - val_tp: 1002.0000 - val_fp: 135.0000 - val_tn: 5298.0000 - val_fn: 157.0000 - val_accuracy: 0.9557 - val_precision: 0.8813 - val_recall: 0.8645 - val_auc: 0.9826\n",
            "Epoch 140/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2242 - tp: 90890.0000 - fp: 12583.0000 - tn: 521963.0000 - fn: 25316.0000 - accuracy: 0.9418 - precision: 0.8784 - recall: 0.7821 - auc: 0.9718\n",
            "Epoch 00140: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2242 - tp: 91134.0000 - fp: 12607.0000 - tn: 523431.0000 - fn: 25398.0000 - accuracy: 0.9418 - precision: 0.8785 - recall: 0.7821 - auc: 0.9718 - val_loss: 0.2073 - val_tp: 967.0000 - val_fp: 127.0000 - val_tn: 5306.0000 - val_fn: 192.0000 - val_accuracy: 0.9516 - val_precision: 0.8839 - val_recall: 0.8343 - val_auc: 0.9820\n",
            "Epoch 141/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2241 - tp: 91074.0000 - fp: 12577.0000 - tn: 522805.0000 - fn: 25320.0000 - accuracy: 0.9419 - precision: 0.8787 - recall: 0.7825 - auc: 0.9718\n",
            "Epoch 00141: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2241 - tp: 91183.0000 - fp: 12592.0000 - tn: 523446.0000 - fn: 25349.0000 - accuracy: 0.9419 - precision: 0.8787 - recall: 0.7825 - auc: 0.9718 - val_loss: 0.2514 - val_tp: 1016.0000 - val_fp: 262.0000 - val_tn: 5171.0000 - val_fn: 143.0000 - val_accuracy: 0.9386 - val_precision: 0.7950 - val_recall: 0.8766 - val_auc: 0.9786\n",
            "Epoch 142/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2235 - tp: 91446.0000 - fp: 12605.0000 - tn: 523190.0000 - fn: 25047.0000 - accuracy: 0.9423 - precision: 0.8789 - recall: 0.7850 - auc: 0.9720\n",
            "Epoch 00142: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2235 - tp: 91477.0000 - fp: 12610.0000 - tn: 523428.0000 - fn: 25055.0000 - accuracy: 0.9423 - precision: 0.8789 - recall: 0.7850 - auc: 0.9720 - val_loss: 0.2268 - val_tp: 981.0000 - val_fp: 180.0000 - val_tn: 5253.0000 - val_fn: 178.0000 - val_accuracy: 0.9457 - val_precision: 0.8450 - val_recall: 0.8464 - val_auc: 0.9790\n",
            "Epoch 143/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2237 - tp: 91521.0000 - fp: 12829.0000 - tn: 523209.0000 - fn: 25011.0000 - accuracy: 0.9420 - precision: 0.8771 - recall: 0.7854 - auc: 0.9719\n",
            "Epoch 00143: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2237 - tp: 91521.0000 - fp: 12829.0000 - tn: 523209.0000 - fn: 25011.0000 - accuracy: 0.9420 - precision: 0.8771 - recall: 0.7854 - auc: 0.9719 - val_loss: 0.2262 - val_tp: 1030.0000 - val_fp: 227.0000 - val_tn: 5206.0000 - val_fn: 129.0000 - val_accuracy: 0.9460 - val_precision: 0.8194 - val_recall: 0.8887 - val_auc: 0.9818\n",
            "Epoch 144/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2227 - tp: 91478.0000 - fp: 12794.0000 - tn: 521745.0000 - fn: 24735.0000 - accuracy: 0.9423 - precision: 0.8773 - recall: 0.7872 - auc: 0.9723\n",
            "Epoch 00144: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2227 - tp: 91729.0000 - fp: 12834.0000 - tn: 523204.0000 - fn: 24803.0000 - accuracy: 0.9423 - precision: 0.8773 - recall: 0.7872 - auc: 0.9723 - val_loss: 0.2327 - val_tp: 1020.0000 - val_fp: 222.0000 - val_tn: 5211.0000 - val_fn: 139.0000 - val_accuracy: 0.9452 - val_precision: 0.8213 - val_recall: 0.8801 - val_auc: 0.9796\n",
            "Epoch 145/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2240 - tp: 91331.0000 - fp: 13024.0000 - tn: 521944.0000 - fn: 24965.0000 - accuracy: 0.9417 - precision: 0.8752 - recall: 0.7853 - auc: 0.9720\n",
            "Epoch 00145: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2241 - tp: 91502.0000 - fp: 13040.0000 - tn: 522998.0000 - fn: 25030.0000 - accuracy: 0.9417 - precision: 0.8753 - recall: 0.7852 - auc: 0.9720 - val_loss: 0.2237 - val_tp: 995.0000 - val_fp: 198.0000 - val_tn: 5235.0000 - val_fn: 164.0000 - val_accuracy: 0.9451 - val_precision: 0.8340 - val_recall: 0.8585 - val_auc: 0.9803\n",
            "Epoch 146/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2228 - tp: 91655.0000 - fp: 12767.0000 - tn: 523047.0000 - fn: 24819.0000 - accuracy: 0.9424 - precision: 0.8777 - recall: 0.7869 - auc: 0.9723\n",
            "Epoch 00146: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2228 - tp: 91699.0000 - fp: 12771.0000 - tn: 523267.0000 - fn: 24833.0000 - accuracy: 0.9424 - precision: 0.8778 - recall: 0.7869 - auc: 0.9723 - val_loss: 0.2205 - val_tp: 987.0000 - val_fp: 178.0000 - val_tn: 5255.0000 - val_fn: 172.0000 - val_accuracy: 0.9469 - val_precision: 0.8472 - val_recall: 0.8516 - val_auc: 0.9804\n",
            "Epoch 147/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2222 - tp: 91693.0000 - fp: 12787.0000 - tn: 523013.0000 - fn: 24795.0000 - accuracy: 0.9424 - precision: 0.8776 - recall: 0.7871 - auc: 0.9726\n",
            "Epoch 00147: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2222 - tp: 91728.0000 - fp: 12793.0000 - tn: 523245.0000 - fn: 24804.0000 - accuracy: 0.9424 - precision: 0.8776 - recall: 0.7871 - auc: 0.9726 - val_loss: 0.2358 - val_tp: 1065.0000 - val_fp: 326.0000 - val_tn: 5107.0000 - val_fn: 94.0000 - val_accuracy: 0.9363 - val_precision: 0.7656 - val_recall: 0.9189 - val_auc: 0.9825\n",
            "Epoch 148/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2225 - tp: 91589.0000 - fp: 12797.0000 - tn: 522592.0000 - fn: 24798.0000 - accuracy: 0.9423 - precision: 0.8774 - recall: 0.7869 - auc: 0.9724\n",
            "Epoch 00148: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2225 - tp: 91705.0000 - fp: 12812.0000 - tn: 523226.0000 - fn: 24827.0000 - accuracy: 0.9423 - precision: 0.8774 - recall: 0.7870 - auc: 0.9724 - val_loss: 0.2183 - val_tp: 978.0000 - val_fp: 161.0000 - val_tn: 5272.0000 - val_fn: 181.0000 - val_accuracy: 0.9481 - val_precision: 0.8586 - val_recall: 0.8438 - val_auc: 0.9804\n",
            "Epoch 149/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2223 - tp: 91662.0000 - fp: 12870.0000 - tn: 522528.0000 - fn: 24716.0000 - accuracy: 0.9423 - precision: 0.8769 - recall: 0.7876 - auc: 0.9725\n",
            "Epoch 00149: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2223 - tp: 91794.0000 - fp: 12877.0000 - tn: 523161.0000 - fn: 24738.0000 - accuracy: 0.9424 - precision: 0.8770 - recall: 0.7877 - auc: 0.9725 - val_loss: 0.2281 - val_tp: 1024.0000 - val_fp: 235.0000 - val_tn: 5198.0000 - val_fn: 135.0000 - val_accuracy: 0.9439 - val_precision: 0.8133 - val_recall: 0.8835 - val_auc: 0.9809\n",
            "Epoch 150/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2215 - tp: 91992.0000 - fp: 12943.0000 - tn: 522854.0000 - fn: 24499.0000 - accuracy: 0.9426 - precision: 0.8767 - recall: 0.7897 - auc: 0.9729\n",
            "Epoch 00150: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2215 - tp: 92025.0000 - fp: 12950.0000 - tn: 523088.0000 - fn: 24507.0000 - accuracy: 0.9426 - precision: 0.8766 - recall: 0.7897 - auc: 0.9729 - val_loss: 0.2410 - val_tp: 1051.0000 - val_fp: 278.0000 - val_tn: 5155.0000 - val_fn: 108.0000 - val_accuracy: 0.9414 - val_precision: 0.7908 - val_recall: 0.9068 - val_auc: 0.9816\n",
            "Epoch 151/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2214 - tp: 91859.0000 - fp: 12667.0000 - tn: 523143.0000 - fn: 24619.0000 - accuracy: 0.9428 - precision: 0.8788 - recall: 0.7886 - auc: 0.9727\n",
            "Epoch 00151: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2214 - tp: 91904.0000 - fp: 12670.0000 - tn: 523368.0000 - fn: 24628.0000 - accuracy: 0.9428 - precision: 0.8788 - recall: 0.7887 - auc: 0.9727 - val_loss: 0.2136 - val_tp: 1006.0000 - val_fp: 178.0000 - val_tn: 5255.0000 - val_fn: 153.0000 - val_accuracy: 0.9498 - val_precision: 0.8497 - val_recall: 0.8680 - val_auc: 0.9826\n",
            "Epoch 152/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2206 - tp: 91948.0000 - fp: 12676.0000 - tn: 523362.0000 - fn: 24584.0000 - accuracy: 0.9429 - precision: 0.8788 - recall: 0.7890 - auc: 0.9731\n",
            "Epoch 00152: val_auc did not improve from 0.98265\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2206 - tp: 91948.0000 - fp: 12676.0000 - tn: 523362.0000 - fn: 24584.0000 - accuracy: 0.9429 - precision: 0.8788 - recall: 0.7890 - auc: 0.9731 - val_loss: 0.2162 - val_tp: 974.0000 - val_fp: 140.0000 - val_tn: 5293.0000 - val_fn: 185.0000 - val_accuracy: 0.9507 - val_precision: 0.8743 - val_recall: 0.8404 - val_auc: 0.9814\n",
            "Epoch 153/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2210 - tp: 91934.0000 - fp: 12805.0000 - tn: 523233.0000 - fn: 24598.0000 - accuracy: 0.9427 - precision: 0.8777 - recall: 0.7889 - auc: 0.9729\n",
            "Epoch 00153: val_auc improved from 0.98265 to 0.98361, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2210 - tp: 91934.0000 - fp: 12805.0000 - tn: 523233.0000 - fn: 24598.0000 - accuracy: 0.9427 - precision: 0.8777 - recall: 0.7889 - auc: 0.9729 - val_loss: 0.2187 - val_tp: 1034.0000 - val_fp: 223.0000 - val_tn: 5210.0000 - val_fn: 125.0000 - val_accuracy: 0.9472 - val_precision: 0.8226 - val_recall: 0.8921 - val_auc: 0.9836\n",
            "Epoch 154/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2213 - tp: 91856.0000 - fp: 12725.0000 - tn: 523079.0000 - fn: 24628.0000 - accuracy: 0.9427 - precision: 0.8783 - recall: 0.7886 - auc: 0.9728\n",
            "Epoch 00154: val_auc did not improve from 0.98361\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2212 - tp: 91897.0000 - fp: 12731.0000 - tn: 523307.0000 - fn: 24635.0000 - accuracy: 0.9427 - precision: 0.8783 - recall: 0.7886 - auc: 0.9728 - val_loss: 0.2210 - val_tp: 1009.0000 - val_fp: 188.0000 - val_tn: 5245.0000 - val_fn: 150.0000 - val_accuracy: 0.9487 - val_precision: 0.8429 - val_recall: 0.8706 - val_auc: 0.9800\n",
            "Epoch 155/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2212 - tp: 91578.0000 - fp: 12654.0000 - tn: 521915.0000 - fn: 24605.0000 - accuracy: 0.9427 - precision: 0.8786 - recall: 0.7882 - auc: 0.9729\n",
            "Epoch 00155: val_auc did not improve from 0.98361\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2211 - tp: 91869.0000 - fp: 12688.0000 - tn: 523350.0000 - fn: 24663.0000 - accuracy: 0.9428 - precision: 0.8786 - recall: 0.7884 - auc: 0.9729 - val_loss: 0.2188 - val_tp: 1024.0000 - val_fp: 208.0000 - val_tn: 5225.0000 - val_fn: 135.0000 - val_accuracy: 0.9480 - val_precision: 0.8312 - val_recall: 0.8835 - val_auc: 0.9813\n",
            "Epoch 156/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2198 - tp: 92116.0000 - fp: 12592.0000 - tn: 522796.0000 - fn: 24272.0000 - accuracy: 0.9434 - precision: 0.8797 - recall: 0.7915 - auc: 0.9734\n",
            "Epoch 00156: val_auc improved from 0.98361 to 0.98515, saving model to ./best_model.h5\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2198 - tp: 92223.0000 - fp: 12610.0000 - tn: 523428.0000 - fn: 24309.0000 - accuracy: 0.9434 - precision: 0.8797 - recall: 0.7914 - auc: 0.9734 - val_loss: 0.2050 - val_tp: 1029.0000 - val_fp: 191.0000 - val_tn: 5242.0000 - val_fn: 130.0000 - val_accuracy: 0.9513 - val_precision: 0.8434 - val_recall: 0.8878 - val_auc: 0.9851\n",
            "Epoch 157/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2198 - tp: 92106.0000 - fp: 12570.0000 - tn: 523248.0000 - fn: 24364.0000 - accuracy: 0.9434 - precision: 0.8799 - recall: 0.7908 - auc: 0.9733\n",
            "Epoch 00157: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2198 - tp: 92148.0000 - fp: 12577.0000 - tn: 523461.0000 - fn: 24384.0000 - accuracy: 0.9434 - precision: 0.8799 - recall: 0.7908 - auc: 0.9733 - val_loss: 0.2011 - val_tp: 971.0000 - val_fp: 107.0000 - val_tn: 5326.0000 - val_fn: 188.0000 - val_accuracy: 0.9552 - val_precision: 0.9007 - val_recall: 0.8378 - val_auc: 0.9849\n",
            "Epoch 158/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2197 - tp: 92136.0000 - fp: 12680.0000 - tn: 523134.0000 - fn: 24338.0000 - accuracy: 0.9432 - precision: 0.8790 - recall: 0.7910 - auc: 0.9734\n",
            "Epoch 00158: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2197 - tp: 92177.0000 - fp: 12682.0000 - tn: 523356.0000 - fn: 24355.0000 - accuracy: 0.9432 - precision: 0.8791 - recall: 0.7910 - auc: 0.9734 - val_loss: 0.2168 - val_tp: 1016.0000 - val_fp: 211.0000 - val_tn: 5222.0000 - val_fn: 143.0000 - val_accuracy: 0.9463 - val_precision: 0.8280 - val_recall: 0.8766 - val_auc: 0.9822\n",
            "Epoch 159/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2202 - tp: 92008.0000 - fp: 12540.0000 - tn: 523265.0000 - fn: 24475.0000 - accuracy: 0.9433 - precision: 0.8801 - recall: 0.7899 - auc: 0.9732\n",
            "Epoch 00159: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2202 - tp: 92045.0000 - fp: 12544.0000 - tn: 523494.0000 - fn: 24487.0000 - accuracy: 0.9433 - precision: 0.8801 - recall: 0.7899 - auc: 0.9732 - val_loss: 0.2543 - val_tp: 1063.0000 - val_fp: 367.0000 - val_tn: 5066.0000 - val_fn: 96.0000 - val_accuracy: 0.9298 - val_precision: 0.7434 - val_recall: 0.9172 - val_auc: 0.9811\n",
            "Epoch 160/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2197 - tp: 91914.0000 - fp: 12644.0000 - tn: 522342.0000 - fn: 24364.0000 - accuracy: 0.9432 - precision: 0.8791 - recall: 0.7905 - auc: 0.9734\n",
            "Epoch 00160: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2197 - tp: 92109.0000 - fp: 12670.0000 - tn: 523368.0000 - fn: 24423.0000 - accuracy: 0.9432 - precision: 0.8791 - recall: 0.7904 - auc: 0.9734 - val_loss: 0.2141 - val_tp: 1008.0000 - val_fp: 171.0000 - val_tn: 5262.0000 - val_fn: 151.0000 - val_accuracy: 0.9512 - val_precision: 0.8550 - val_recall: 0.8697 - val_auc: 0.9825\n",
            "Epoch 161/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2192 - tp: 92115.0000 - fp: 12621.0000 - tn: 523188.0000 - fn: 24364.0000 - accuracy: 0.9433 - precision: 0.8795 - recall: 0.7908 - auc: 0.9736\n",
            "Epoch 00161: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2192 - tp: 92159.0000 - fp: 12628.0000 - tn: 523410.0000 - fn: 24373.0000 - accuracy: 0.9433 - precision: 0.8795 - recall: 0.7908 - auc: 0.9736 - val_loss: 0.2183 - val_tp: 992.0000 - val_fp: 169.0000 - val_tn: 5264.0000 - val_fn: 167.0000 - val_accuracy: 0.9490 - val_precision: 0.8544 - val_recall: 0.8559 - val_auc: 0.9769\n",
            "Epoch 162/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2192 - tp: 92011.0000 - fp: 12758.0000 - tn: 521776.0000 - fn: 24207.0000 - accuracy: 0.9432 - precision: 0.8782 - recall: 0.7917 - auc: 0.9737\n",
            "Epoch 00162: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2192 - tp: 92254.0000 - fp: 12784.0000 - tn: 523254.0000 - fn: 24278.0000 - accuracy: 0.9432 - precision: 0.8783 - recall: 0.7917 - auc: 0.9737 - val_loss: 0.2022 - val_tp: 1002.0000 - val_fp: 137.0000 - val_tn: 5296.0000 - val_fn: 157.0000 - val_accuracy: 0.9554 - val_precision: 0.8797 - val_recall: 0.8645 - val_auc: 0.9834\n",
            "Epoch 163/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2193 - tp: 92257.0000 - fp: 12495.0000 - tn: 523543.0000 - fn: 24275.0000 - accuracy: 0.9437 - precision: 0.8807 - recall: 0.7917 - auc: 0.9735\n",
            "Epoch 00163: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2193 - tp: 92257.0000 - fp: 12495.0000 - tn: 523543.0000 - fn: 24275.0000 - accuracy: 0.9437 - precision: 0.8807 - recall: 0.7917 - auc: 0.9735 - val_loss: 0.2168 - val_tp: 942.0000 - val_fp: 135.0000 - val_tn: 5298.0000 - val_fn: 217.0000 - val_accuracy: 0.9466 - val_precision: 0.8747 - val_recall: 0.8128 - val_auc: 0.9796\n",
            "Epoch 164/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2191 - tp: 92216.0000 - fp: 12778.0000 - tn: 522616.0000 - fn: 24166.0000 - accuracy: 0.9433 - precision: 0.8783 - recall: 0.7924 - auc: 0.9736\n",
            "Epoch 00164: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2191 - tp: 92340.0000 - fp: 12791.0000 - tn: 523247.0000 - fn: 24192.0000 - accuracy: 0.9433 - precision: 0.8783 - recall: 0.7924 - auc: 0.9736 - val_loss: 0.2169 - val_tp: 1022.0000 - val_fp: 178.0000 - val_tn: 5255.0000 - val_fn: 137.0000 - val_accuracy: 0.9522 - val_precision: 0.8517 - val_recall: 0.8818 - val_auc: 0.9843\n",
            "Epoch 165/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2189 - tp: 92370.0000 - fp: 12833.0000 - tn: 522970.0000 - fn: 24115.0000 - accuracy: 0.9434 - precision: 0.8780 - recall: 0.7930 - auc: 0.9738\n",
            "Epoch 00165: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2189 - tp: 92408.0000 - fp: 12840.0000 - tn: 523198.0000 - fn: 24124.0000 - accuracy: 0.9434 - precision: 0.8780 - recall: 0.7930 - auc: 0.9738 - val_loss: 0.2171 - val_tp: 983.0000 - val_fp: 151.0000 - val_tn: 5282.0000 - val_fn: 176.0000 - val_accuracy: 0.9504 - val_precision: 0.8668 - val_recall: 0.8481 - val_auc: 0.9827\n",
            "Epoch 166/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2173 - tp: 92419.0000 - fp: 12706.0000 - tn: 521835.0000 - fn: 23792.0000 - accuracy: 0.9439 - precision: 0.8791 - recall: 0.7953 - auc: 0.9744\n",
            "Epoch 00166: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2173 - tp: 92684.0000 - fp: 12742.0000 - tn: 523296.0000 - fn: 23848.0000 - accuracy: 0.9439 - precision: 0.8791 - recall: 0.7954 - auc: 0.9744 - val_loss: 0.2179 - val_tp: 1041.0000 - val_fp: 211.0000 - val_tn: 5222.0000 - val_fn: 118.0000 - val_accuracy: 0.9501 - val_precision: 0.8315 - val_recall: 0.8982 - val_auc: 0.9849\n",
            "Epoch 167/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2171 - tp: 92615.0000 - fp: 12725.0000 - tn: 521820.0000 - fn: 23592.0000 - accuracy: 0.9442 - precision: 0.8792 - recall: 0.7970 - auc: 0.9744\n",
            "Epoch 00167: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2171 - tp: 92877.0000 - fp: 12766.0000 - tn: 523272.0000 - fn: 23655.0000 - accuracy: 0.9442 - precision: 0.8792 - recall: 0.7970 - auc: 0.9744 - val_loss: 0.2038 - val_tp: 992.0000 - val_fp: 151.0000 - val_tn: 5282.0000 - val_fn: 167.0000 - val_accuracy: 0.9518 - val_precision: 0.8679 - val_recall: 0.8559 - val_auc: 0.9839\n",
            "Epoch 168/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2174 - tp: 92475.0000 - fp: 12725.0000 - tn: 522654.0000 - fn: 23922.0000 - accuracy: 0.9438 - precision: 0.8790 - recall: 0.7945 - auc: 0.9744\n",
            "Epoch 00168: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2175 - tp: 92583.0000 - fp: 12742.0000 - tn: 523296.0000 - fn: 23949.0000 - accuracy: 0.9438 - precision: 0.8790 - recall: 0.7945 - auc: 0.9744 - val_loss: 0.2074 - val_tp: 1010.0000 - val_fp: 189.0000 - val_tn: 5244.0000 - val_fn: 149.0000 - val_accuracy: 0.9487 - val_precision: 0.8424 - val_recall: 0.8714 - val_auc: 0.9839\n",
            "Epoch 169/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2180 - tp: 92233.0000 - fp: 12697.0000 - tn: 521835.0000 - fn: 23987.0000 - accuracy: 0.9436 - precision: 0.8790 - recall: 0.7936 - auc: 0.9741\n",
            "Epoch 00169: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2180 - tp: 92493.0000 - fp: 12749.0000 - tn: 523289.0000 - fn: 24039.0000 - accuracy: 0.9436 - precision: 0.8789 - recall: 0.7937 - auc: 0.9741 - val_loss: 0.2035 - val_tp: 1023.0000 - val_fp: 150.0000 - val_tn: 5283.0000 - val_fn: 136.0000 - val_accuracy: 0.9566 - val_precision: 0.8721 - val_recall: 0.8827 - val_auc: 0.9849\n",
            "Epoch 170/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2166 - tp: 92920.0000 - fp: 12533.0000 - tn: 523280.0000 - fn: 23555.0000 - accuracy: 0.9447 - precision: 0.8812 - recall: 0.7978 - auc: 0.9744\n",
            "Epoch 00170: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2166 - tp: 92965.0000 - fp: 12538.0000 - tn: 523500.0000 - fn: 23567.0000 - accuracy: 0.9447 - precision: 0.8812 - recall: 0.7978 - auc: 0.9744 - val_loss: 0.2302 - val_tp: 1061.0000 - val_fp: 320.0000 - val_tn: 5113.0000 - val_fn: 98.0000 - val_accuracy: 0.9366 - val_precision: 0.7683 - val_recall: 0.9154 - val_auc: 0.9826\n",
            "Epoch 171/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2167 - tp: 92763.0000 - fp: 12737.0000 - tn: 523077.0000 - fn: 23711.0000 - accuracy: 0.9441 - precision: 0.8793 - recall: 0.7964 - auc: 0.9745\n",
            "Epoch 00171: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2167 - tp: 92808.0000 - fp: 12738.0000 - tn: 523300.0000 - fn: 23724.0000 - accuracy: 0.9441 - precision: 0.8793 - recall: 0.7964 - auc: 0.9745 - val_loss: 0.2139 - val_tp: 1025.0000 - val_fp: 189.0000 - val_tn: 5244.0000 - val_fn: 134.0000 - val_accuracy: 0.9510 - val_precision: 0.8443 - val_recall: 0.8844 - val_auc: 0.9846\n",
            "Epoch 172/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2168 - tp: 92799.0000 - fp: 12804.0000 - tn: 522590.0000 - fn: 23583.0000 - accuracy: 0.9442 - precision: 0.8788 - recall: 0.7974 - auc: 0.9745\n",
            "Epoch 00172: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2168 - tp: 92924.0000 - fp: 12819.0000 - tn: 523219.0000 - fn: 23608.0000 - accuracy: 0.9442 - precision: 0.8788 - recall: 0.7974 - auc: 0.9745 - val_loss: 0.2166 - val_tp: 1037.0000 - val_fp: 190.0000 - val_tn: 5243.0000 - val_fn: 122.0000 - val_accuracy: 0.9527 - val_precision: 0.8452 - val_recall: 0.8947 - val_auc: 0.9833\n",
            "Epoch 173/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2167 - tp: 92699.0000 - fp: 12559.0000 - tn: 523269.0000 - fn: 23761.0000 - accuracy: 0.9443 - precision: 0.8807 - recall: 0.7960 - auc: 0.9744\n",
            "Epoch 00173: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2167 - tp: 92750.0000 - fp: 12561.0000 - tn: 523477.0000 - fn: 23782.0000 - accuracy: 0.9443 - precision: 0.8807 - recall: 0.7959 - auc: 0.9744 - val_loss: 0.2368 - val_tp: 1062.0000 - val_fp: 288.0000 - val_tn: 5145.0000 - val_fn: 97.0000 - val_accuracy: 0.9416 - val_precision: 0.7867 - val_recall: 0.9163 - val_auc: 0.9830\n",
            "Epoch 174/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2165 - tp: 92526.0000 - fp: 12591.0000 - tn: 522355.0000 - fn: 23792.0000 - accuracy: 0.9441 - precision: 0.8802 - recall: 0.7955 - auc: 0.9746\n",
            "Epoch 00174: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2164 - tp: 92701.0000 - fp: 12619.0000 - tn: 523419.0000 - fn: 23831.0000 - accuracy: 0.9441 - precision: 0.8802 - recall: 0.7955 - auc: 0.9746 - val_loss: 0.2208 - val_tp: 1019.0000 - val_fp: 199.0000 - val_tn: 5234.0000 - val_fn: 140.0000 - val_accuracy: 0.9486 - val_precision: 0.8366 - val_recall: 0.8792 - val_auc: 0.9836\n",
            "Epoch 175/2000\n",
            "1271/1275 [============================>.] - ETA: 0s - loss: 0.2160 - tp: 92672.0000 - fp: 12585.0000 - tn: 521972.0000 - fn: 23523.0000 - accuracy: 0.9445 - precision: 0.8804 - recall: 0.7976 - auc: 0.9747\n",
            "Epoch 00175: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 14s 11ms/step - loss: 0.2160 - tp: 92938.0000 - fp: 12612.0000 - tn: 523426.0000 - fn: 23594.0000 - accuracy: 0.9445 - precision: 0.8805 - recall: 0.7975 - auc: 0.9747 - val_loss: 0.2307 - val_tp: 1055.0000 - val_fp: 271.0000 - val_tn: 5162.0000 - val_fn: 104.0000 - val_accuracy: 0.9431 - val_precision: 0.7956 - val_recall: 0.9103 - val_auc: 0.9837\n",
            "Epoch 176/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2165 - tp: 92804.0000 - fp: 12570.0000 - tn: 523230.0000 - fn: 23684.0000 - accuracy: 0.9444 - precision: 0.8807 - recall: 0.7967 - auc: 0.9745\n",
            "Epoch 00176: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 11ms/step - loss: 0.2164 - tp: 92842.0000 - fp: 12576.0000 - tn: 523462.0000 - fn: 23690.0000 - accuracy: 0.9444 - precision: 0.8807 - recall: 0.7967 - auc: 0.9745 - val_loss: 0.2209 - val_tp: 1051.0000 - val_fp: 240.0000 - val_tn: 5193.0000 - val_fn: 108.0000 - val_accuracy: 0.9472 - val_precision: 0.8141 - val_recall: 0.9068 - val_auc: 0.9832\n",
            "Epoch 177/2000\n",
            "1272/1275 [============================>.] - ETA: 0s - loss: 0.2153 - tp: 92993.0000 - fp: 12695.0000 - tn: 522266.0000 - fn: 23310.0000 - accuracy: 0.9447 - precision: 0.8799 - recall: 0.7996 - auc: 0.9751\n",
            "Epoch 00177: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2153 - tp: 93187.0000 - fp: 12716.0000 - tn: 523322.0000 - fn: 23345.0000 - accuracy: 0.9447 - precision: 0.8799 - recall: 0.7997 - auc: 0.9751 - val_loss: 0.2297 - val_tp: 1064.0000 - val_fp: 290.0000 - val_tn: 5143.0000 - val_fn: 95.0000 - val_accuracy: 0.9416 - val_precision: 0.7858 - val_recall: 0.9180 - val_auc: 0.9834\n",
            "Epoch 178/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2156 - tp: 92895.0000 - fp: 12710.0000 - tn: 522679.0000 - fn: 23492.0000 - accuracy: 0.9445 - precision: 0.8796 - recall: 0.7982 - auc: 0.9750\n",
            "Epoch 00178: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2156 - tp: 93020.0000 - fp: 12725.0000 - tn: 523313.0000 - fn: 23512.0000 - accuracy: 0.9445 - precision: 0.8797 - recall: 0.7982 - auc: 0.9750 - val_loss: 0.2226 - val_tp: 1035.0000 - val_fp: 228.0000 - val_tn: 5205.0000 - val_fn: 124.0000 - val_accuracy: 0.9466 - val_precision: 0.8195 - val_recall: 0.8930 - val_auc: 0.9823\n",
            "Epoch 179/2000\n",
            "1274/1275 [============================>.] - ETA: 0s - loss: 0.2149 - tp: 92979.0000 - fp: 12635.0000 - tn: 523166.0000 - fn: 23508.0000 - accuracy: 0.9446 - precision: 0.8804 - recall: 0.7982 - auc: 0.9752\n",
            "Epoch 00179: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2149 - tp: 93014.0000 - fp: 12644.0000 - tn: 523394.0000 - fn: 23518.0000 - accuracy: 0.9446 - precision: 0.8803 - recall: 0.7982 - auc: 0.9752 - val_loss: 0.1997 - val_tp: 1022.0000 - val_fp: 176.0000 - val_tn: 5257.0000 - val_fn: 137.0000 - val_accuracy: 0.9525 - val_precision: 0.8531 - val_recall: 0.8818 - val_auc: 0.9842\n",
            "Epoch 180/2000\n",
            "1273/1275 [============================>.] - ETA: 0s - loss: 0.2155 - tp: 92928.0000 - fp: 12811.0000 - tn: 522577.0000 - fn: 23460.0000 - accuracy: 0.9444 - precision: 0.8788 - recall: 0.7984 - auc: 0.9749\n",
            "Epoch 00180: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2155 - tp: 93039.0000 - fp: 12821.0000 - tn: 523217.0000 - fn: 23493.0000 - accuracy: 0.9444 - precision: 0.8789 - recall: 0.7984 - auc: 0.9749 - val_loss: 0.2455 - val_tp: 1069.0000 - val_fp: 329.0000 - val_tn: 5104.0000 - val_fn: 90.0000 - val_accuracy: 0.9364 - val_precision: 0.7647 - val_recall: 0.9223 - val_auc: 0.9826\n",
            "Epoch 181/2000\n",
            "1275/1275 [==============================] - ETA: 0s - loss: 0.2150 - tp: 93055.0000 - fp: 12537.0000 - tn: 523501.0000 - fn: 23477.0000 - accuracy: 0.9448 - precision: 0.8813 - recall: 0.7985 - auc: 0.9750\n",
            "Epoch 00181: val_auc did not improve from 0.98515\n",
            "1275/1275 [==============================] - 13s 10ms/step - loss: 0.2150 - tp: 93055.0000 - fp: 12537.0000 - tn: 523501.0000 - fn: 23477.0000 - accuracy: 0.9448 - precision: 0.8813 - recall: 0.7985 - auc: 0.9750 - val_loss: 0.2187 - val_tp: 1059.0000 - val_fp: 255.0000 - val_tn: 5178.0000 - val_fn: 100.0000 - val_accuracy: 0.9461 - val_precision: 0.8059 - val_recall: 0.9137 - val_auc: 0.9849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e4fc1e6e-291c-4de7-a327-5d50cf55ce37"
      },
      "source": [
        "def plot_metrics(history):\n",
        "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch,  history.history[metric], color='b', label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color='b', linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "plot_metrics(history)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjbrzEe2ISI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "c2985b8d-5d6b-43e3-bc87-7579c254319d"
      },
      "source": [
        "# Save model weights to drive\n",
        "!cp -r best_model.h5 '/content/gdrive/My Drive/Kaggle/best_model_20200806_METRICS.h5'\n",
        "\n",
        "#new_model = tf.keras.models.load_model('./best_model.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model = tf.keras.models.load_model('./best_model.h5')\n",
        "#new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Kaggle/best_model_20200802_METRICS.h5', \n",
        "#                                        custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               228352    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 477,825\n",
            "Trainable params: 476,545\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAzqpDLIS0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty some RAM space\n",
        "indices_0_new = None\n",
        "data_backup = None\n",
        "dataset_transaction = None\n",
        "X_to_train = None\n",
        "Y_to_train = None\n",
        "Y_train = None\n",
        "X_train = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQtd0IsFspQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n",
        "  test_size = test_size\n",
        "  y_pred = (model.predict(X_test, batch_size=128)>threshold).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        " \n",
        "  precision = precision_cal(y_pred, y_ref)\n",
        "  recall = recall_cal(y_pred, y_ref)\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre = []\n",
        "re = []\n",
        "f1 = []\n",
        "\n",
        "pre_train = []\n",
        "re_train = []\n",
        "f1_train = []\n",
        "\n",
        "threshold_value = []\n",
        "indices = np.random.randint(0, len(X_to_train), size=(len(Y_test),))\n",
        "\n",
        "for i in range(90):\n",
        "  threshold_value.append(0.1+i*0.01)\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_test, Y_test, test_size=len(Y_test), threshold=threshold_value[-1])\n",
        "  \n",
        "  pre.append(temp_pre)\n",
        "  re.append(temp_re)\n",
        "  f1.append(temp_f1)\n",
        "\n",
        "  temp_pre, temp_re, temp_f1 = F1_score(new_model, X_to_train[indices], Y_to_train[indices], test_size=len(Y_to_train[indices]), threshold=threshold_value[-1])\n",
        "\n",
        "  pre_train.append(temp_pre)\n",
        "  re_train.append(temp_re)\n",
        "  f1_train.append(temp_f1)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CP4zuNU6WUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0835bade-5e77-4388-92d9-69a5105c8b4e"
      },
      "source": [
        "plt.plot(threshold_value, f1, 'b')\n",
        "plt.plot(threshold_value, pre, 'r')\n",
        "plt.plot(threshold_value, re, 'g')\n",
        "\n",
        "plt.plot(threshold_value, f1_train, '--b')\n",
        "plt.plot(threshold_value, pre_train, '--r')\n",
        "plt.plot(threshold_value, re_train, '--g')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f445eba9860>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUZfbHP296gIRQQg+EXqQTqUsTEBQEAVGwrGABXbHu2nZ/q4i6KruiLrgquq7oohRdFQXFRlGkhQ5SQif0HtLLnN8fJyGIEAJkMkk4n+e5z2Tm3rn3zM3M9773vKc4EcEwDMMo/vj52gDDMAyjYDBBNwzDKCGYoBuGYZQQTNANwzBKCCbohmEYJYQAXx24YsWKEh0d7avDG4ZhFEuWL19+WEQiz7bOZ4IeHR1NbGysrw5vGIZRLHHO7TzXOnO5GIZhlBBM0A3DMEoIJuiGYRglBBN0wzCMEsJ5Bd05965z7qBzbt051jvn3D+dc1ucc2ucc60L3kzDMAzjfORnhP4e0CeP9dcA9bOXkcAbl26WYRiGcaGcV9BFZAFwNI9NBgDvi7IYiHDOVS0oAw3DMIz8URBx6NWB3ac9j89+bd+ZGzrnRqKjeGrWrHlRBzuYdJBjKccoF1qOciHlCPQPvKj9GIZhFAoJCbBhAxw4APHx8MsvMHw4xMQU+KEKNbFIRCYBkwBiYmIuqhD75FWTeey7x049D/YPJjw4nIiQCCJCIigXWo4qZapQpXQVqoZVpX75+jSObEx0RDR+zuaADcPwEqmpsHMnrF0L69bBggWwahUcO/bbba+4osgK+h4g6rTnNbJf8wr9G/ZnytoprD6wGoC0rDQOJR8iNTOVuuXrciT5CIt2LyIpIwmPeE69L9g/mOrh1SkVWOrURaBqWFWqlK5C5TKVqVS6EpGlIoksHUm1sGpUD6uOc85bH8MwjOKCCCQnQ+nS+nzSJPjpJ9i8Gfbtg+PHdZvERH08HX9/qFsXrr8ehg6FqlXh6FFo1swrphaEoM8ERjvnpgLtgBMi8ht3S0HRsGJDVt2zisT0RPYn7mffyX3sOrELP+fHsGbDALhxxo0s27uMg0kHSc5IBqBKmSp0jOpIUnoSX8V9RWpWKv7OH0F+Jfw5lAspR6uqrWhRuQW1I2pTI7wG1cOrU6tsLSqVrmRibxglleXLdXS9aBGsWQO7dkFoKHTtCtu362ue0zQjNBRq1oQbb4T69SEuDsqWhY4ddRQeeIZbuHp1r5nuzteCzjn3EdANqAgcAJ4GAgFE5E2nyjYRjYRJBkaIyHmLtMTExIi3a7mICEdTjrLzxE4cjlZVWwEwafkk9ifuZ/vx7Ww9upUtR7fQLbobD7Z7kANJBxg4bSAe8RDsH0yGJ+M3gl86sDQNKzakYYWG1C1Xlzrl6lCnXB0aRzamUulKXv1MhmEUAIcPw48/wtdfw5Il6tu+8ko4dEj93cnJv94+MFDFOjoa6tVTl0mjRvpYoUKhmu6cWy4iZ/XXnFfQvUVhCPqFICI458jyZDFz00zWHlzL2oNrWXdwHfEJ8fy++e/pU68Pq/av4ql5TwHg7/zJkqxf7adqmaq0rNLyV0u98vXMf28YvmTvXli5EubNgxkz1Nd9OiEh0LgxVK6sI+7oaGjZUkW8Th2oVAmKyF25CXoB4BEPfs6PhLQEPtv4GTuP72TTkU3M3TGXvSf38kSnJ6hUuhI/7vqRpXuWsj9x/ymxLx1Ymo5RHelVpxc96/SkRZUWJvCGUZCkpqpo79kDO3aoW2TDBo0oiY+HjAzdLihIR9ZpadCpE/TvD717q6AXE/ISdJ+Vzy1u5AhweHA4v2/x+1OviwhxR+OoUqYK4cHhlAosxacbPz21PjQglCD/ILYd23YqOic8OJzGFRvTOLIxTSo2oU21NlxZ7UrCgsMK90MZRnEgNRU2blSxPtsSH3/2SJIcIiKgSRO44w4YNgxKlSo82wsZG6EXMDkCv3zvcuIT4olPiGdXwi6mDJrC8dTj3PPlPczfMZ+QgBBSMlM4mX4SAIejSWQTutbqyqDGg+ga3ZUAP7veGpcZIjqpuHix+raXLoXVq3NH2DmEheloOyMDTp7U9/XuDX/8I5Qvr+9t3Fh93JVK1ryWuVyKEDPWz2D6L9OJ3RvLjuM7AIgIieCBtg+wdO9S5m2fR2pWKuVDy3N9w+u5rcVtdKnVxVw0RsnD41HxXr1al5UrVYiPZiemly6tI+tq1cDPTx8ff1z93NWqQUCA+rnbtIFevaBDBwgO9u1nKgRM0Isoh5IOsWTPEvYn7ueu1ncBcMW/riApPYmIkAi2HN1CUkYS0RHR/L757xnadCiNKjaykEmjeJKUpIk2P/2kESY//QQnTug6Pz+N1+7cGdq3hw8/1NDBnPBAf38YOFAnNEFjvyMifPM5fIwJejEhy5PFv5b9i2nrp7Fw90IAapatSURwBGsPrkUQakfUpm/9vvRr0I+u0V0JCSg+kznGZUR6uo64Fy3SUffKlZqIk6M3DRtCq1bqOomPh9hYyMzUcEI/P3j9dTh4UN0mjRpBgwYl2vd9IZigF0N2n9jNx798zCcbPuHemHvpGt2V/6z8D++sfId9J/eR4cmgVGApetXpRd/6feldrzc1y15cfRzDuCSSkzXVfdUqdZ2sWgUrVuhkJkBUlGZGhoXBkCE6Cn/jDRgzRtdHRsLVV+sybNhvE3GMX2GCXkL4Ku4rhn4ylIS0BPycHzXCapCckczhlMMANKjQgF51etEtuhsx1WKoVbaWuWeMguXIERXr5ctzBXzz5lzXSFgYtGihS+nSmg6/fDksW6bb/PADdO+u+4iN1UzKli11VG7kCxP0EkSmJ5OV+1by+abP+WDNB+xJ2MP3v/+elftX8r8N/yN2bywpmSkAlA8tT0y1GPrU7cOARgOoU66Oj603ihVpaSrYixZp1MnixRrjnUN0tAp3nTqajJOaqjVLOnfWbTt00NF2TAz07KkTl+3aaXSKcdGYoJdQPOJh3cF1NK/cHIAe7/fgh+0/0KBCA6LCo/B3/uw8oQlQAE0rNaV/g/70bdCXdtXb4e/n70vzjaLE4cNaJXDtWvV3r1wJ69erXxvUbdKunabHt2mjPu1Ro3SkfeBA7n5eeAGeeEJ96MuW6bbFKGmnOGCCfpmwev9qZsfN5ttt3/Lz7p9Jy0qjT70+TLxmIjM3zWTi0onsOL4DDx4qhFbg6rpXE1MthpZVWtKicgsqlCrcmhRGIZGZqcK7c2duMs6BA1q35NAhnZQ8XZQrVYLWrXUEHhamS2IibNumr/397zq52bWrjs5bt9YJzhYtIDzcV5/yssEE/TIkJSOFRfGLyPRkcnXdq0nPSif8hXDSstIoHViaKmWqcCz1GEdTcptRNavUjAENBzCg0QDaVG1j/vfijMejrpKpU2H6dI0YySEoCKpU0cnIyEj9OypKo0giIzWj0jkV6DVrct9TowYMHgzjxvnmMxmACbqRTVpmGt9t+45p66fx2cbPOJl+kr/3+jstKrfg++3f8+nGT4k7EocgVC1Tle61u9M9Wpc65eqYwBdlUlI0tjvH3710qSbohIRAv35www0a/letGlSsqIL9xhvwwQcaoXJSM5YJC9P3BQRonHhIiJaGrVjRJi6LCCboxm9IzUzlp10/0SSyCdXCqjFt3TSGfjKUkIAQmlVqRpB/EJuPbOZQ8iEAKpWuRLvq7WhXvR0dozrSvkZ7QgNDffwpLmNOnlQf9+LFWgJ2/nydlHRO093btYNu3eC661Sg586FOXO02mBcnLpGxo2DWbOgaVPNyGzcWJcqVYpMZUHjt5igG+dFRFgcv5gP137ItPXTTgn5z3f8zKr9q/hi8xesO7iO3QnaPjbYP5gOUR3oHt2dTlGdaFu9rRUX8xYej7o+5s1T4V616tfRJo0bax2T3r3Vnx0eriPrb76Bu+6C3dktf6tV022efdarTRYM72KCblwQGVkZLN+3nPUH13Nn6zsBuH7q9Xy+6XMA6pWvR62ytTicfJg1B9YgCH7OjxaVW9ClVhd61elFl1pdTOAvhtTU3MzKDRt0WbtWU91BJyHbttVEnUaN1LcdH6+x3rGx6j6ZPBluvllH8GPG6Ei9a1cdudvIu9hjgm5cMoeTD7N873KW7lnKxxs+Zs2BNXSK6sSXN3/JkvglzNkyh1UHVrEofhGpmakE+AXQvkZ7utbqSpdaXehQo4MJ/JlkZsLWrSq8q1fntj1LS9P15cvnVgzs3Fnrd2/YoCPw3/1OR941s7ODK1TQEME2beCmm3RC0yiRmKAbBYqIsGzvMpIzkukW3Y2TaSeJeCmCqmWq0qJyC0oHleZk2kniE+LZcHgDWZKFv/OnQ1QH+tbvS9/6fWlaqenlN8manq6j7+++g++/1xF1jnj7+WnoX9euOqJu104jTkBF/L334P33Nbxw2DAtXgXw+ecq3rVq2ej7MsEE3fAqJ9NO8v7q91m4eyHrDq4j7mgcqZmpTLhmAsNbDufLTV8ycdlEDiYdJO5oHAAVQisQHRFNrYha1I6oTdvqbelcszNVw6r6+NMUMPHx8NVXMHu2Cnlioop3TIyOups10xF448Y6Yl+yROPF775b39+3r743IED/vuMOrXliyTqXLZcs6M65PsBrgD/wjoi8eMb6WsC7QCRwFLhVROLz2qcJesnFIx7iE+IpG1yWsiFlmbV5FgOmDiBLsigXUo6GFRsS7B9MgF8Ae07uYcfxHaRmaiGneuXr0bVWVw2XrN2damHVfPxpLgIRjSh5+WUVcVDXyLXX6qRkt265pV9nz9ZY8RUrtF2aiIr1iRPqH586VbvxDB5c4ho1GBfHJQm6c84f2Az0AuKBZcAwEfnltG1mAF+KyGTn3FXACBG5La/9mqBfXpxIPcE3W7/hi81fMDtuNkdSjhD/cDzVw6szftF4vor7iqjwKA4mH2Th7oUcT9VJwPrl69MhqgNtq7WlXY12tKzSsuh2ctq2TYX8X//Syclq1eDee7WOd2CghgguXqwp8T//rOGBL74IEyao26R9e61/0q6dZVwa5+RSBb0DMEZEemc/fxJARF44bZv1QB8R2e3UMXpCRPL8RpqgX75kebLYemwr9crXw8/58e7Kd3n8u8c5nHyY8qHl6V23N9ER0VQIrcD8nfNZsmcJB5M00zEiJILedXvTr0E/+tTrQ8VSFX33QQ4e1OqBOT7xnFDCZs3gT3+CoUM1+uSuu3T0DerrvvJKeOkljVjJytLmDYaRTy5V0G9Axfqu7Oe3Ae1EZPRp23wILBGR15xzg4BPgIoicuSMfY0ERgLUrFmzzc6dOy/hYxkliYysDL7d9i0frv2QeTvmUSuiFgvv0CYfwz8bTkZWBmHBYRxNOcr8nfM5mHQQh6N55ean3DPta7SnUmkvuiVENJxwxgx1leSkxZctqyVhu3bV9PiVK3Wk3a+f+sPvuAMGDNAO89HR3rPPuCwoDEGvBkwEagMLgMFAUxE5fq792gjdyIuk9CRKB5VGRBgwdQALdi7gRJq2K2tdtTX9G/THz/kxd8fcU4XIAKLCo2hTrQ1tq7WlY1RHrqx+JaUCL6HTjYiGFE6frsvWrTqi7tJFy8F27AhTpuhIfO1abVrs5wd//rMm8BhGAeN1l8sZ25cBNopIjbz2a4JuXAge8bDh0AZmxc3is42fcdMVN/Fg+wc5nHyYh75+iOiIaByOrce2Ers39lQ0TYBfAK2rtuaaetcwoOEAWlZpef5wyfR0FehZs1TE4+JUxHv0gBtv1JrfKSk6GhfR6JOMjNw48C5dtJGxYXiBSxX0AHRStAewB50UvVlE1p+2TUXgqIh4nHPPA1ki8lRe+zVBNy4FEcE5x6Ldi7juo+s4kqLevajwKDrV7MQDbR/gaMpRft79M/N2zmPR7kUIQlR4FP0b9mdAwwF0je5KkH+QivK6dRrTPXeuJvekpOhI+6qrtG1akybamWfZMg0t3LhRJ0GrVPHxmTAuNwoibPFa4FU0bPFdEXneOTcWiBWRmdlumRcAQV0u94lIWl77NEE3CgqPeFi1fxU/7fqJhbsXsnDXQubePpcqQfV5+dspfBw3mbJZDUhJS+Ng1hb2Bywmy6USklWajvtqcOfy4wxZfYAAceyv3JJtNbqwq0ILfqnUnb2B0XRY+xZ3LbsHgES/cNYExTA35Bo+rXwPlClDaKiGifv762N6uoabJyZqo/v0dF0yMjT3x99fl1KlNBIxZ6lQQZfy5fU6Ub263gRUqmSFDo1cLLHIKNaIaCvLHTv08eTJ3CUhQUO2ExJynyecFHbvhh3bHTT/ADqMh8hfICAdgJD0AF7/PICFdVP5XyM4XgpCksoQunoQvdZW4veHV9A7Yz73l/4PX0TcRp2AXXTL/I64Sp1IrFqf8Ag//P11EH/ihC4iuUtwsEYdhoXl9jsOCNAlp3F9VpYK/qFDGixz8KB+tpSU335+57TDW85SrRrUr69LrVp6nNKloUwZKFdOl/LlLfeopGKCbhRZjhzROcc9e2D/fl0OH1aRPH48V8gTE8+9j9BQFdAcEQ0L0xFuThJmtazdZH3+MRt+msXq0onsrObPP8t1ouZNHeiy9y0WHVhIs/hQro87Rv8tmZQ/VJFPWgWz6qZostIf5Md/98NfQnDO4e+vo+XZszXq8JVX4JFHfmvTrl3aM+K55+Cvf/31uurVtWBixYrq5VmyRANlypbVz+Kcenj27IFNm2DfPhX61FRITtaLwLZtWsolr59vQIBeUIKCVPDbtdNpgKuu0rpeVimgeJKXoBfRDA2jpHHsmLqdt27NrUcVGwvbt/96u9BQLWESEaECl1NcsEwZFabQUBWnK6/UXJzkZC1zcviwin/O4223Qd+YA/xw5xTazXoEeDh7AbfOwx/HLaVmYhxdI0ax5IdojlZ7n6d7ZvJ0Tyi9vwV9Gndlc+AM1m4YD6FZBAQK5UMqUC6kIuVCKrDzaCB16lSjXz+oWlVLsiQm5vaJyMkLuvZadZnkhJrv3aufuXx5ff7TT/Dqq7mtO0E/Z1qaVgcYPlyLJ55OWJhe8NLS4OmnNcQ9MFAFPCJC9122rNqSkaHL0aNa++vTT3UfVarklo3p3BkaNtT3G8UbG6EbXiErS2tKffGFLosX/3o0GR2tgtWypY54/f11fXKyil6vXhpQsmuXuhXO5JVX4KGH9CLRuLEKaMWK6oOuWC6LR2pMp+f0kexPKcunnccTeUNX6nriiFr6CRHff0zAvnhVvv37VQ1F2J90gK/ivqJMUBmGXDGElIwUyr5YlvKh5SkbXJa0rDT2Je4jPUtdNzHVYhjUaBCDGg+iQYUGF11sLOdz57iPkpI0WAY0b2nDhtxz5JyWRx81StePGpVbIiZH5Js21QhK0LuHlBSoW1fFPjlZR/y7d+v87969ul1QkJ7Hpk31riZnqV3b/PdFDXO5GF4jK0vdArGxumzapCPQHTt0ZAhaRDAmRkfeAQEqSFdcoYmUKSk6mszZFnRE+8gj8PjjOnJ95x0thRIVpSP0jIzcSUSPR20I3LtTwwy/+kqzNlNStFvPuHHqXxg7VoezQUFaT2XIEF2fU1PlLJxMO8mbsW+yeM9iFscvZu9JVb+H2z9MlTJVmLF+BrH79Dtcq2wtetXpRc86PelVtxflQ8t765SfE49H85iOHs29IPTrBwsX5pZTB/3YM2fqhaRePT2fpUvreTx6VO9wcggL04oEv/udVu+NidH/l+E7TNCNAiEjQ10lK1ZoMuSKFer/TkrS9aVL53YwK1tW/bW9e2vuzZlJwQMHwv/+p3/Hxv7aVZDvTPj4eI0TnzZNe2iC+mh+9zsddc+fr3VVevXSK82KFRozfpF1UuIT4lkcv5hWVVpRt3xd/rfhfwyePphSgaWICIngWMoxUjJTCPALoHt0dwY1HkT/hv19XmBMJHuyOEFH8kFBOmIX0Qvn5s3qtsmpXHD33TBihP5vx4zR7U7vMd2ggbq82rZVkW/Rwtw1hYkJunHBiOht+aJFuixdqiKeqkURKVNG3SWtWmloXVaWei9++EFDuqOiVMSdg//+V0fW0dG5zeaDgi7SsBMn4OOPtbnx/Pn6WuvWWo1QRNuuLVigr3frBs88o4k+XuBoylG+2foN32z9hm+3fUt8ghYYHdl6JHN3zD2V3FSrbC3a1dB+rJ1rdqZV1VZFssBYUpK6d8LC1Kd+7JjOASxZoqe2QgWd0C1XDrZs0Ulb0Mid9u31At6zp94dWHka72GCbpyX3btVtFev1mX58twfbGio/kibN9cfb1qaRlp88IGOzP7wB20gHxqqP+xrrtGlwDqeiagTfsIEndVLTdWYvVtu0ZCNzp31ihIVpaPv227TdYVYN0VE2Hh4I/N3zmdUG3VwD5w2kC82f0G5kHJkeDJISEsAIDw4/FSrvv4N+xMdUXh2XgwHD+aWdF+wQC/QPXrodXX8eP2/792r8xmgd1sdOqh7JiZGo2sscbbgMEE3zsqxY1pn6oMPNNoCdAKsYUO9je7YUZf4eB3orl6tflrndLLsxx81JnrrVvV116tXwCOzlBT1y7z2mmZoli2rM6XNmuntwKef6oh95069suzdqyEnRSQeb3H8Yr7c/CU/bP+BpXuWkiVZ1C1Xl6uir2LuzrlsOboFgOaVmzOg4QBuvOJGmlZq6mOr8yYn1t7PTz1dDz+sk6ygLrOoKP33rF6td2o58tKunVZMGDhQv1/GxWOCbgAqurGxOmf4/fc6WZaerj7RDh10cJuSovq4fr0K/VVXqRvl2Wc1zK1rV/Wdli7tJSNTU7Wm+PTpOnOXmKgGPvCAxiQ+/7w68/391eg77tDR+EX7cAqHk2kn+Xbbt6RmpnJzs5vJ9GRyx+d34BEP245tY8meJXjEQ9NKTRnWdBhDmw6lTrk6vjb7vIjoyHzePP1urV+v3yt/f7jzTh3NR0bmuuRABwu33KKd9GrkWfHJOBsm6Jc5e/bApEm65PyomjVTT8Udd6jQt2+vr1eooPOKDRvCgw/qLbPXySlL++67Wrnw+HE1pHdvVYZHH1WDly3TW4ru3XXiM6z4Np1evnc5PT/oyfHU46fKAFcLq8bBpIMs37ccgDZV2zCkyRBuaHIDdcvX9bHFF87XX2tI5ZIl6jHLzFRxr1NHXwMdMNx6q06BWE+P/GGCfhkiom6UCRPUa+HxqA7WqqX+8oUL4fe/hzff1G2//VZH3nlE8RU8mZnaYu3llzV1MiQEBg3SIdzixTpC93j0Q9x3XyEaVjhkebKI3RvLt9u+5estX7MofhHzbp9HzbI1eWXRK3y26TN2ntDwoHrl650Ki+xdtzelg7x1i+QdEhJ0DvvkSbj5Zo2siYnRO8S0NL3BGjRIv5O9elnUTF6YoF9GpKWpRr72mg56IyL01nf/fn09K0tD1q65Rn9A3bv7yMj33tOuPdu36+zpfffBTTflhlWUL6+G33OPDukuAw4nHyYiJIIAvwCemfcMY+aPASDIP4hyIeU4nnqctKw0woLCuLX5rYxqM4oWVVr41uiLJC1Nv6OffqrXbtCbsawsjYS69VZdmjcvMlMiRQYT9MuATZvg7bdVJ48c0Xjwq69W33dYmMYTnzihLS4bNPCRkSkpmiX00kvqB2rbVoX80CENiHYO/vEPFfNhwzR84jJFRNh1YhdL9ixh/o75fLH5C1IyU5g6eCrvr3mfqeumkp6VTpuqbRjYaCADGg3gisgrLjpb1Zfs3q13kVOnatjj2rXw5Zcq7vXq6Vdh2DD9Thsm6CWaFSvgscd0kjMgQN3OVaroROb27erD7NHDx0ZmZWmCz9/+prcKnTtr3v7SpepOSUvTkIhGjXxsaNFFRNh7ci/Vw6uT6cmk1qu1SExLpFRQKfYn6sRInXJ1uK7BdfSt35cutboQHBDsY6svnmee0UGIc7mRMoMG6QClSROfmuZz8hJ0q9JQTDl8WOt4xMToiObJJ1W458yBf/9bU+WnTlXt9Cn79ulV5oEHdIg1a5amF44YoWn511+vaYom5nninKN6eHUA/Jwfb1/3Nl2iu3Ao6dCp1wL8Angz9k2u/u/VVPx7RQZPH8zkVZM5knwkr10XSZ56SnMh7r8/t5DZzJlaa+b223OzWo1fYyP0Ysbx4zqROW6culBuvVV9kWXKqAejVy+t0Fckbk/nzNFZroQEraXypz/pLFh0tIr6009r9Ipx0SSmJ7Jo9yLm75xPzzo9aVu9LR+s/oD3Vr/HjuM72J+4Hz/nR5daXXi80+P0rtu72Lll0tO1IuXevVrRYeJEHbU/+KC2br3casuYy6UEsHevVhh86y2NFGjVSsU9NVXTsEtdQh/kAmfJEjV22jTNQIqIUGO3bNGMlKQkLwayG39f+Hce++4xyoWUo3/D/pQLKcdnmz5jx/EddI/uzos9X6Rt9ba+NvOi+eQTHRvs2KEVNp99FkaOvHyqQprLpZjz0UcaFz5+vE50du+uESzBwTpaLxJzhyIastCxowa1z5ql6fnbt6vb5fHHc52hJuZe5dFOj7L4zsVcVfsqPljzAa8ve51+Dfrxzz7/ZN3BdbR7px0Dpw1k5b6Vvjb1ojh0SAc4YWHqjrn3Xi3bExfna8uKACLik6VNmzZi5E1Skshdd2mydceOIj//LFKrlkhgoMjf/y6SkeFrC7P54QeRtm3V0Nq1RR57TCQ4WCQsTOS550QSE31t4WXL1qNb5YHZD8joWaNFROREyglp9WYrCX0uVBiD9P+ov8TuifWxlRfOhg0iXbroV65BA/2qhYSIvPyySGamr63zLmgv57Pqar7EF+gDbAK2AE+cZX1NYC6wElgDXHu+fZqg583atSJXXKH/oSefFElPF/F4RB5+WGTpUl9bl83KlSJ9+qiR1auLjB+vVxmPR+SZZ0T27vW1hcYZ7D+5X2q9UksYgwSMDZDAsYHCGOR3//6dTF83XTKyisoo4fxkZYm8/bZIhQoizz4rct11+lXs3Vvk6FFfW+c9LknQAX9gK1AHCAJWA03O2GYScG/2302AHefbrwn62cnKEnn1VR3gVqokMmGCSKdOInFxvrbsNOLiRIYO1a9PuXIif/iDSKFKV3kAACAASURBVPfuIuHhJuLFAI/HI4t2L5LRs0ZLxXEVhTFI5LhIYQxSY3wN+duCv8mhpEO+NjPfHD2aO+C5804Rf3+RunVF1q3ztWXeIS9Bz48PvS2wRUS2iUg6MBUYcKbnBsipxFAW2HuBnh8DdTVfe62GaHfrpnG3Dz6o1Qzj431tHZoBMmqUhtDMnAl33aW1yP/1L63KNGZMIdcOMC4G5xzta7RnwrUT2PvIXj4f+jk7HtzB50M/JyQghD//8GeqvlyV2z+9nTUH1vja3PNSrpxGvzin/vWcQmDt2mmJ38uKcym95I6+bwDeOe35bcDEM7apCqwF4oFjQJvz7ddG6Ll4PCLvvCMSESESGioyYoRI1ao6AB41SuTYMR8buG+fyAMPiAQFqQN/9Gj1+wQEiJQvLzJunPnJSwjjfx5/atSes9w440bZcWyHr03LF6mpIsOH5948gsiwYSKHis8Nx3nhEl0u+RH0R4A/Zv/dAfgF8DvLvkYCsUBszZo1C+8MFGE2bRLp2lX/E126iGzcqNp55ZUiixb52LikJPWFlyql97F33CEyY0bu+v/8R+TwYZ+ZZ3iHjKwM+WLTF9JvSj9hDOL3jJ8EPxssj37zaLFwxXg8Iq+8IuLnp27LgAB9/PhjX1tWMFyqoHcA5pz2/EngyTO2WQ9EnfZ8G1Apr/1e7iP0tDSRsWPVVx4eLtK5s8jcubouOVl96T4jK0vkv/8VqVFDvyI33CAyZYpIq1b6K9m40YfGGYXJsj3L5Nut38rwz4YLY5CQZ0Pkj3P+KHsTiv5cyTffaMDV6tUirVvrV/lPf/Lxb6sAuFRBD8gW6NrkTopeccY2XwHDs/9ujPrQXV77vZwF/ccfRRo31rPfurW6WgICRF57zceGeTwic+aItGmTa9yUKSKDB+vzmjVFpk7V7YzLjmEfDzvlhgkaGySjZ42WPQl7fG1WvoiNFYmJ0a/xoEF681lcuSRB1/dzLbAZjXb5S/ZrY4H+2X83ARZmi/0q4Orz7fNyFPR9+9Q/DiKVK4vUqaN/9+ghsn69j41bulQjVXKE+733RI4fFylTRqR0ab2dSE72sZGGLzmRekIe/vph8X/GX4KfDRa/Z/wkaGyQPPTVQ7Lv5D5fm5cnTz2VO0YBdWnuK9omn5NLFnRvLJeToKekiLzwgmpjYKDeBr70kurmjBk+HvBmZoo8/bS6UiIj1Wf+0ku5Rn32mcj+/T400ChqrD2wVrr+p6swBmn+RnPxf8ZfQp8Llad+eEpOpp30tXlnxePR32COqIeG6l1ycZwCMkH3ER6PyOef547EO3YUefNNXZeeXgRu+3bvzk23u/56kdtu0yuOv7+m4hnGOfB4PDJj/QyJPxEvcUfi5OoPrhbGIFX/UVXeXfGuZGYVzXTNSZN07NK0qQZttWsncrJoXoPOSV6CbrVcvERcHPTtCwMGaHut3/8eFi2CyZO1pElgoA8Laolo4ayWLbWz7003wezZ2ph55EgtomXlbI08cM5xQ5MbqB5enXrl61EmqAwASRlJ3DHzDpr+qykfrP6ATE+mjy39NXffrV/9mjW1gfWyZXDDDVrRsURwLqX39lJSR+hpaerBCArS+hJjxoj07KmD4NtuKwLh2tu3i1xzjRrUpo36zqtX1xG6ZXkaF8nxlOMy7qdxUvUfVTUa5rkQYQxS+9Xa8vbyt4vciD3Ho/jPf+pPYejQ4lMDBnO5FA5Ll4o0ayankhlWrRKpWFH9dZMm+dhX7vFoHYFSpXSS8667cic59+61yBWjQEjNSJV3V7wrzf7VTG755Ba5ctKVwhjkyklXyur9q31t3q/weNTjmJPEN3x48QhpNEH3MmlpIk88ob65atV0HjGHsWOLgDs6M1Pk/vv1392zZ24Vozfe8LFhRknF4/FIWmaaeDweuefLe8TvGT/xe8ZPHv/2cUlOLzrRUt98o1UaK1fWn8QddxR9UTdB9yJxcbnxrSNGiLz/vk6C+jwMMYfkZA28zUkQql5dg96ff7743GMaxZqFuxZKo4mNTsWw1/9nfVl7YK2vzTrFV1+JOJf7O7777qIt6nkJuk2KXgJTpmjnoC1btD7VgQM6+VmmTBGZZDl8WHvSffop3HyztlYPC9OOQn/+M/j7+9pC4zKgY1RH1tyzhpd6vkSwfzBxR+No9WYrXl/6uo4qfUyfPvDww7nxAW+/rU2qiyXnUnpvL8V5hH7ypMjtt+vVvHNndbeEhGiceU5JcJ+zerVIdLTWFpg+XR36Q4cWvxgto0Sx6/guGTh1oLR6s5UwBuk3pZ/sP+n7PIeUFL3DjotTX7pzuaU4ihqYy6XgWLlSO6Q4p9lnGRmaKHTjjRrWXST49FOd+KxcWeQvf/G1NYbxG7I8WfLKoldOZZ1OXjnZ1yadIiFBf+PVqxfNxKO8BN1cLvlERLuNt2+vTZr79YOuXSEgAF54QWNba9TwsZEej3bMHTgQ6tXThosvvACrV/vYMMP4NX7Oj4faP8TotqNJy0rj9s9vp8f7PTiWcsyndiUkwK23quv00CG4447cVrjFARP0fHD4sCYI3X+/Fs2PiIAvvlCfGxSRbuMJCdoR46mntJP0nj3aSXf2bGjRwtfWGcZZebXPqywcsZAKoRX4YfsP1Hy1JnO2zPGZPaVK6e/9pZfg0Ue1j8vrr/vMnAumKEhRkWbePNXDOXP0yh0bq1fur76Cxx7ztXXZbNyoV5ovv4Rhw+CHH6BCBVi6FHr39rV1hpEnHWt2ZNfDuxjSZAiJ6Yn0mdKHJ797kvSswo8sCAjQYAc/P/j+e50wffRRzfwuDpignwMRGD8eevTQqJVXX9VU4ZgY9WD06eNrC7NZuVL9QEeOwHffaR5z794aydKgga+tM4x8USqwFNOHTGfdveu4q9VdvLjwRRpNbMSi3YsK3ZboaHjrLVi8GBo2hOBg7bbo8RS6KRfOuZzr3l6K8qTo6W2sBg3SwBCPR+Tdd7WoVpEhLk5bsVStqp2lc7CsT6OYM2HJhFNx67d8coskpRd+JbsRI3RidOJEKVJ5eOQxKerERx7/mJgYic1xQhchDhxQV/TPP+tVOTYWPvkE6tTxtWVnsG8fdOoER49CRoZOgG7eDKGhvrbMMAqEGetnMOLzESRlJBEREsGC4QtoVrlZoR0/MRFSUqBiRU3nWLpUe6FHRRWaCWfFObdcRGLOts5cLqfx44+aKLRihc5uT54Mx49rVEuR4tgxdavs36++oapVtZSjiblRghhyxRDiH4mnd93eHE89TszbMXy37btCO36ZMhAZCVlZWoQ0KwvuuadoR72YoKO+sXHjoHt3neXu0gXefVf95CtWFLEgkd271cANG1TAy5RR37nPYyYNo+CJCIngq1u+YkTLEVQqXYlrplzDOyveKVQbxo6FW27RKLfZs2HGjEI9/AVx2Qt6QoKGbT/+uLpa+vbV2e2XXoLPPoNy5Xxt4WmsXq0ToDt3wp13gnPw7bc6i2MYJRTnHO8OeJf1f1hPzzo9ufuLu/nz938utLIB99+vFTMWL4bmzTXqJSWlUA59wVzWgr5li+rjrFkaxTJtGrz4IsyfryGJRSK+PIdvvoHOnVXEf/oJ3nwT1q2DJk18bZlhFArhweGMajOK0IBQXvjpBUZ8PoKMrAyvHzcyUvPz5s/XhMJdu+Af//D6YS+KoiRZhcr330PbtjoJ+tJL8PHHOloPDdW5xiLFmjWa2VS7toYlpqbq61Wq+NYuwyhkWlZpSXhwOOFB4UxePZkBUweQmJ7o9ePedZfqxb//raL+4ouau1fUyJegO+f6OOc2Oee2OOeeOMv6V5xzq7KXzc654wVvasHxxhs6p1i9urpannhC5xkTvf+9uHASEmDIEPX9DB8Or72mviDDuAyJjojmm9u+wd/Pn8hSkXy95Wt6vN+DE6knvHpcf3+tqFqtGtx3H2Rmqm4UOc4Vz5izAP7AVqAOEASsBprksf39wLvn268v4tAzM0UeeURjSq+9VuSPf5RTPR+OHSt0c86PxyNy003aOeOtt7RyYo8eRaSco2H4jqXxSyXsb2FS4+UaEvBMgHR4p4MkpCZ4/bg5KR5PPKHasWiR1w/5G7jE4lxtgS0isk1E0oGpwIA8th8GfHTRVxgvkZQEgwdr9ucDD+jkxssva9PY2bO1PkuR41//Usf+Y4/Bc89BpUrw0Uean2wYlzFXVr+SL2/+klub38q0G6axdM9S+n3Uj6T0JK8e1zm9m+/cWT2eRab8Rw7nUnrJHXHfALxz2vPbgInn2LYWsA/wP8f6kUAsEFuzZs3CuZyJlrVt3VoHuq+9lvvahAlFOKly8WKRwECRfv30tqJMGZHly31tlWEUScb/PF7cGCc9Jvfweou7oUNFIiJEXnhBR+k//ujVw/0GCrF87lDgYxHJOsfFY5KIxIhITGRkZAEf+uwsWQJXXqlJlO+8A/HxmiBQowaMHq1X3CLH7t1w/fWakjZ5ss7a/vwztG7ta8sMo8iRnpXOhKUTqBFeg++3f8/dX9zt1ZDGRx/VhMPkZM0ifeEFrx3qgsmPoO8BTk92rZH92tkYShFyt0yZojXLQ0O1sNZTT2m03+bNvrYsD5KSNKIlMVErg6WlqYulWeGlPBtGcSLIP4i3+r3FoeRDVC5dmSlrp/Dyope9drzWreG667Q/wqhR6rItKi0H8iPoy4D6zrnazrkgVLRnnrmRc64RUA4o/PJoZ2HcOC132769jszvvFNnphcsgMaNfW3dOfB44PbbYdUqjZ18+234/HNfW2UYRZ5edXvx5bAvOZ56nMhSkTz27WN8veVrrx3vqafUl+6cJmu/+KLXDnVBnFfQRSQTGA3MATYA00VkvXNurHOu/2mbDgWmijfvdfKBiPY/fvxxbfj6/PM6GVq6tObjtGzpS+vOw//9n1YC69BBC7D/5S9aPMIwjPPSo04PJl03SUfqZSoz9OOhxB3xTiHzmBjNKt+9G/7wB5g+XRMVfc65nOveXrwRtpiVJXLffTpRcffdGqb4008izZuL7NhR4IcrWJ57Tg1v2lQfH320CM/YGkbR5f1V78v6g+ulwksVpN4/68nehL1eOU5qqj7u26cRxSNHeuUwv4HLoadoZiaMGKHtov70J3jySU0G6NRJe0DUquVrC/Ng3Dgdnd90k7pdHnxQJ0KL5IytYRRtbmtxG00im/DRDR+xJ2EPPd7vwcGkgwV+nOBgfUxKUvfue+9pNzNfUiIEPSNDT+j778Mzz+jtUKNGMHWqri9SNVnO5JVX1D904406i/vzz/qaiblhXBLjF40nPDic7ce20+uDXhxNOVrgx9i6VbsahYVBerqWXPIlRVnq8kVammrhtGk60I2M1Laa7doVoTZx5+Lll+GRR6BePb0qiUDZsibmhlEAPNf9OU6knaB+hfpsPLSRqz+4mpNpBdvcoG5dLbv94YfaY8YE/RJIS9OSt599piVOkpN1gqJfP51TLJLZnzk8/7z6hurW1dmU6Gj1ERmGUSC0qdaGf/f/N2sPrqVHnR6s3L+SkV+OLPAY9aeegoMHoWZNrWbt07CQcznXvb1c6qRoWprIddfp/OFbb+nkJ2gv0CJd6sTjEfm//1Nj69SxCVDD8DKPffOYMAa5/qPrhTHIG8sKvjlo9+4iZcvqz3nt2gLf/a8gj0nRYlkUJDMTbr4ZvvhCy52MHKmv//ADdOtWhD0WHo+Oyl95BerXh7g4ePZZDU8sskYbRvHmbz3+RqYnk/vb3U9KZgoPff0Q7aq3o1XVVgV2jD/9SfMBQUfpTZsW2K4viGLXJDorSydAp07VQJAFC7SM5e9+5wUjC5LMTC2qPHmytkC56Satc37vvb62zDAuGw4mHqTVW60oFVSK5SOXEx4cXiD7zchQl2/79uo9/eqrAtntWSlRTaLHjlUxHzsWvvxST9yuXb626jykpGh20+TJMHSoOvw7dTIxN4xCRES4Z9Y9NIpsxLaj27hv9n0Ftu/AQI1n6NVLOxulpRXYri+IYifo99+vevjNNxrh9+GH6n4pshw/DtdcAzNnasuTqVM1MN4wjELFOceV1a7kh+0/0LdBX/675r98FVdwQ+nVq9Xtm5ICCxcW2G4viGIn6CEhmma7aJGWBr/pJl9blAd79kCXLnrlGTIEli6FMWOsaqJh+IjHOj3GVbWv4vvt31Mnog6jvhxVYKGMFSrA+vWa9+Kr8MViKeh162rc+ZAhvrYmDzZs0JosO3Zovc0ZM9T5/9RTvrbMMC5b/P38+WDgB5QKLEWgfyC7E3bzlx/+UiD7rlEDWrTQJKNvvy2QXV4wxU7QAwLUFT14sK8tyYPFi9VHnpGh3afHj9eR+jvvWDSLYfiYamHVmNRvEglpCdzS7BYmLp3Iot0FUyS2b184eRJWrPBNGYBiJ+hFnnnzoGdPvf/6+We4+mp19H/2WW7xB8MwfMrAxgOJuz+ON/u9SVTZKO6ceScpGSmXvN9rr9XoZIDvv7/k3V0wJugFyZw5OgFaq5bGl+fU0xw4EMqV861thmH8itJBpQn2D+ba+tey4fAGHv/u8UveZ/v20L+/luueO7cAjLxATNALipkz9T/ZqJH6yUeMgL/+1cd5wIZh5MXSPUt5M/ZNWlZuyYSlE5i1edYl7c/fX3vStG8Py5cXkJEXgAl6QfDFF3DDDdo9Y/RouO02LcH25ZfmMzeMIkynmp0YfeVoVh1YRYPyDRj++XD2ndx3yftt0kTzBtPTC8DIC8AE/VL5+utcMb/pJrj7br08z5unHWQNwyjSjO0+lvDgcGqWrUlSehLDPx+ORzwXvb+TJ+GNNzQmYt26AjQ0H5igXwrffgvXX6+FG77+GjZtKialHg3DyKFcaDkeaf8I323/jofbP8w3W7/hzdg3L3p/YWFaqgkK3+1ign6xzJ2rPvMGDTSOsnx5rRT2yScQGupr6wzDuAAeav8Q/Rr0Y3DjwXSL7sbT854mIS3hovfXsaN6Wy+iXNUlkS9Bd871cc5tcs5tcc49cY5tbnTO/eKcW++c+7BgzSxiLFigI/HatbUIcr9+ep/l769FHQzDKFaUDSnLF8O+oHW11ozrOY7DyYf5x8//uOj9XXmlxkMsKpjw9nxzXkF3zvkDrwPXAE2AYc65JmdsUx94EugkIlcAD3nB1qLBwoUabFq9OoSHw+zZ2nUoLMzXlhmGcYnsT9zPsr3LuPGKG3l50cvsT9x/UfuJya6FuGGD+tILi/yM0NsCW0Rkm4ikA1OBAWdsczfwuogcAxCRgu/IWhRYvFjjzKtUgTJlNB1s6lR44AFfW2YYRgHw3zX/5b7Z9zGw0UDSs9J5Zt4zF7Wfpk21WnZmptZ3KSzyI+jVgd2nPY/Pfu10GgANnHMLnXOLnXNFvZvnhbNwoWZ9VqqkDrK1a7U+y403+toywzAKiD9c+QeqlKnCxKUTGdl6JG+veJvNRzZf8H6Cg+Gxx/TvwpwYLahJ0QCgPtANGAa87Zz7TZiHc26kcy7WORd7yBeFDi6WefOgd2+oWlWLHU+YoIXYB5x5o2IYRnGmVGApxnQdw8LdC2lbvS2hgaH8+fs/X9S+QkI0PqIwJ0bzI+h7gKjTntfIfu104oGZIpIhItuBzajA/woRmSQiMSISExkZebE2Fy7ffac+86goFfVy5bSSfc+evrbMMAwvcGfrO2lYoSEvLnyRh9o9xCcbPmHV/lUXvJ85c7Q2+s8/e8HIc5AfQV8G1HfO1XbOBQFDgZlnbPMZOjrHOVcRdcFsK0A7fcOCBXDddVqvt2pVmDgRfvrJ11YZhuFFAvwCeKnnS7Sq0oq7Wt9F2eCyPDP/wn3pOROjv/xSeBOj5xV0EckERgNzgA3AdBFZ75wb65zrn73ZHOCIc+4XYC7wqIgc8ZbRhcLatRpnXquWLnPnwttvqx/dMIwSzYBGA/hw8IfUiqjFw+0f5rONn7Fy34V1GrviCo1izsxUUS8M8uVDF5HZItJAROqKyPPZrz0lIjOz/xYReUREmohIMxGZ6k2jvc6uXRrNUqqU/ldmzYJXX4U77/S1ZYZhFCJrD6wlqmwUZYPLMnbB2At6b2Cg1nSBwpsYtUzRMzlyRH3liYnw3nvw449aCvfBB31tmWEYhcyY+WN46OuHuDfm3osapXfqpI+FNTFqgn46R45o2+5t27QhxdVXa3WdvxRMiyrDMIoXf+3yV06mn8Tfz5+IkIgL9qU/8YTW6luxwksGnoEJeg4HD0L37poF0Lu3Ft4S0bhzK4FrGJclLau0pG/9vrwZ+yb3xdzH55s+v6BRelSUthZevTq3k5E3MUEH2LcPunXTDkMDB2p987Q0X1tlGEYR4C+d/8KRlCOEBoYSFhTGuJ/HXdD79+6F1FQdM3obE/T9+1XMd+2CO+6AadPg3nvh73+3kblhGHSI6sCAhgMI8g9iVJtRzFg/gx3Hd+T7/WvW6OPu3XlvVxBc3oJ+6BD06AF79sCf/gSvvw6DBmkmqIm5YRjZfHrTpzza6VEebP8gfs6PVxa9ku/3tmqljzt2eMe207l8Bf3YMZ303LZNW8U1bKjZn1OmaBlcwzCMbJxziAgbDm1gaNOhvLPyHY4k5y/VpmVLfSyMWPTLU9ATEnTi85df1MXSrRsMGwbffKMFGAzDMM7g++3fc/V/r6ZZpWYkZyTzRuwb+Xpfw4b6uHGjF43L5vIT9KQkrc2yciW88or6y2dmVzIwN4thGOfgqtpXUadcHWbFzeLa+tcyYekEUjJSzvu+6Gh93LnTu/bB5SboqanaA3TRInjtNXjhBcjKyk3nMgzDOAd+zo9RbUYxf+d8hjQZwsGkg7y/+v3zvq9pU/XuZmYWgo3eP0QRIT0dhgzR6omvvqqCfvKklkSrV8/X1hmGUQwY0XIEQf5BLN+3nJhqMfxz6T8RkTzf4+en5aB27fK+fZeHoGdmwq236uTnK6/Af/6jMUSzZkGLFr62zjCMYkJk6UhuaHID32/7npGtR/LLoV9YHL/4vO/btg0OHFAngTcp+YKelQW3367dhV5+WWuyXH01fPJJbqEFwzCMfPJan9dYfc9qhjUbRpmgMry94u3zvufwYX2Mj/eubSVb0D0ebez34YdaYGvIEJ34fPFFraZoGIZxgVQsVZFA/0BCA0IZ1nQY09ZPIyEtIc/35EyMeju5qOQKugj84Q9aMfHppzWtv21bjT83DMO4BJbtWUatV2vRKaoTyRnJfLT2ozy3zwld3LTJu3aVXEH/85/hrbfgySd1VmLyZA1RLFfO15YZhlHMaVChAcdSj7Fg1wKaV25+XrdLs2b6uG6dd+0qmYI+caK6VUaN0pDEp59WP/pf/+prywzDKAGUDSnLrc1u5aO1H3Fz05tZvm95nlUYGzTQcaW3I11KnqB/8gk88IC2j7vtNu0y1LUrTJpkiUOGYRQYo9uOJiUzhdTMVEICQnhnxTvn3LZtW2jTRqOnvUnJEvQff4RbbtGK8h99pCGJd98N//sfBAX52jrDMEoQzSo3o1t0N95d9S6DGg9iytopJGckn3P7mjW9P0IP8O7uC5H163VUHh2tvvOsLAgLU/eLYRiGF3iu+3MkZSSR6cnkw7UfsnDXQnrV7XXWbTdt0tgMEe85C/I1QnfO9XHObXLObXHOPXGW9cOdc4ecc6uyl7sK3tQ82L0b+vSB0FCtlnjDDTB4cKGaYBjG5Uenmp24uu7VdK7ZGX/nz4KdC865bVoaZGR4N9DuvCN055w/8DrQC4gHljnnZorImcUgp4nIaC/YmDfHjmlMeUKCZoLedZcK/LvvFrophmFcfhxOPsxLP71Eo4qNWLDr3IJeowbExak8lS/vHVvyM0JvC2wRkW0ikg5MBQZ4x5wLJC1Ni21t3qzJQ08+qa6XTz+1LFDDMAoFh2Pison4O3+WxC8hLfPs7Svr1tXHrVu9Z0t+BL06cHp+U3z2a2cy2Dm3xjn3sXMu6mw7cs6NdM7FOudiDx06dBHmnoaItoxbsEBjzKdP1yqKH36otc4NwzAKgQqlKnBLs1vYeGQjaVlpLNu77KzbXXGFPq5d6z1bCirK5QsgWkSaA98Ck8+2kYhMEpEYEYmJjIy8tCM+84yK9/PPa3OKMWNg6lT1nxuGYRQiw1sOJz1LYxLP5UePidHJUG9GuuRH0PcAp4+4a2S/dgoROSIiOfcZ7wBtCsa8c/DBByrot9+ujimPB2rX1lothmEYhUy76u0oE1SG8qHlzynov/sd1KkDKefviXHR5EfQlwH1nXO1nXNBwFBg5ukbOOeqnva0P7Ch4Ew8gwULNFmoWzcID1dRnz3ba4czDMM4H4H+gQxuPJio8CgW7l5Ipufs3Sy8HYt+XkEXkUxgNDAHFerpIrLeOTfWOdc/e7MHnHPrnXOrgQeA4d4ymN27oVEj6N4dJkzQcrh9+3rtcIZhGPnhvevf48nfPUlieiKr9q866zYbN8Lq1d6zIV+JRSIyG5h9xmtPnfb3k8CTBWvaObjlltwa5zfeCOPHW0q/YRhFgs61OgPqR4+pFvOb9X5+kJioPXcCvJDWWfxS/48cgfvuU5fL++/rGTIMwygCDP14KGUCy5zTj1412zm9b593jl/81LBCBe0D+umnEBzsa2sMwzBOUSuiFpmSyYKdC/CI5zfrcxpd7NjhneMXP0EH6NgRIiJ8bYVhGMav6Fm7J6mZqRxLPcYvh85MptfpP/BeXfTiKeiGYRhFkB51epz6+8edP/5mfZcuOhb1lqfYBN0wDKOAqBFeg0YVGxHkH8Si+EW/Wd+rl5afGjXKO8c3QTcMwyhA/q/z/3FF5BWs3H/uDkbewgTdMAyjALml+S1cW/9aNhzaQEqGF9NCz4IJumEYRgFTqVQlsiSLdQe93BX6DEzQDcMwCpg3l78JUOhuFxN0wzCMAqZ9jfY4HCv2rijU45qgG4ZhFDAx1WIQhCV7lhTqqPydSAAACIFJREFUcU3QDcMwCpg2VbWC+C+HfiHLk1VoxzVBNwzDKGCaV26On/Mj3ZPOpiObCu24JuiGYRgFTGhgKO/210b1K/cV3sSoCbphGIYXuLnZzQT7BxdqpIsJumEYhhfYe3IvFUtVLNSJUS+UWDcMwzD2ntzLnpN7OJ56HBHBFUIjHhuhG4ZheIEWVVrgcCRlJLHrhBcbiZ6GCbphGIYXKBVYitoRtYHCyxjNl6A75/o45zY557Y4557IY7vBzjlxzv22mZ5hGMZlRvuo9kDhRbqcV9Cdc/7A68A1QBNgmHOuyVm2CwMeBAo3NcowDKOI0q56OxyOxXsWF8rx8jNCbwtsEZFtIpIOTAUGnGW7Z4GXgNQCtM8wDKPYclfruxjadOhZ29F5g/wIenVg92nP47NfO4VzrjUQJSKz8tqRc26kcy7WORd76NChCzbWMAyjOFEqsBTNKzcnPiGexPRErx/vkidFnXN+wHjgj+fbVkQmiUiMiMRERkZe6qENwzCKPGsOrAFgx/EdXj9WfgR9DxB12vMa2a/lEAY0BeY553YA7YGZNjFqGIYBx1KOAUVH0JcB9Z1ztZ1zQcBQYGbOShE5ISIVRSRaRKKBxUB/EYn1isWGYRjFiJhqOrbdcGiD1491XkEXkUxgNDAH2ABMF5H1zrmxzrn+3jbQMAyjONO8cnMg1/XiTfKV+i8is4HZZ7z21Dm27XbpZhmGYZQM6pavC0Dc0TivH8syRQ3DMLxInXJ1CA0I5UDSAa8fywTdMAzDi0SERDCi5QhOpJ7w+rFM0A3DMLxMdEQ0x1KPeV3UTdANwzC8TE5xLm+HLpqgG4ZheJlAv0AAth3b5tXjmKAbhmF4mcIKXTRBNwzD8DLNKjUDYN3BdV49jgm6YRiGl8mJRd96bKtXj2OCbhiG4WVqlq1J+dDyp+q6eAsTdMMwDC8T6B/ILc1u4WjqUUTEa8cxQTcMwygEoiOiSUhL4Hjqca8dwwTdMAyjEFgUvwiA7ce3e+0YJuiGYRiFQKVSlQDYdHiT145hgm4YhlEItKrSCsjNGvUGJuiGYRiFQNNKTQG82jDaBN0wDKMQyIlFNx+6YRhGMadiqYpEhUeRlJ7ktWOYoBuGYRQCzjkGNR7E4eTDXotFN0E3DMMoJKLLRpOUkcSRlCNe2b8JumEYRiHx0+6fANh21DtldPMl6M65Ps65Tc65Lc65J86y/h7n3Frn3Crn3E/OuSYFb6phGEbxpn6F+gCsPrDaK/s/r6A75/yB14FrgCbAsLMI9oci0kxEWgLjgPEFbqlhGEYxp03VNgDsTtjtlf3nZ4TeFtgiIttEJB2YCgw4fQMRSTjtaWnAe9VnDMMwiilNKzWlbHBZ2lVv55X9B+Rjm+rA6ZeTeOA31jjn7gMeAYKAq862I+fcSGAkQM2aNS/UVsMwjGJNwwoNOf5EMSjOJSKvi0hd4HHg/86xzSQRiRGRmMjIyII6tGEYRrHAOefV/edH0PcAUac9r5H92rmYClx/KUYZhmEYF05+BH0ZUN85V9s5FwQMBWaevoFzrv5pT/sCcQVnomEYhpEfzutDF5FM59xoYA7gD7wrIuudc2OBWBGZCYx2zvUEMoBjwO3eNNowDMP4LfmZFEVEZgOzz3jtqdP+frCA7TIMwzAuEMsUNQzDKCGYoBuGYZQQTNANwzBKCCbohmEYJQTnrbq85z2wc4eAnT45eMFRETjsayOKGHZOfo2dj99i5+TXXOj5qCUiZ83M9JmglwScc7EiEuNrO4oSdk5+jZ2P32Ln5NcU5Pkwl4thGEYJwQTdMAyjhGCCfmlM8rUBRRA7J7/GzsdvsXPyawrsfJgP3TAMo4RgI3TDMIwSggm6YRhGCcEEPR/ko0n2I865X5xza5xz3zvnavnCzsLkfOfktO0GO+fEOVeiw9Tycz6cczdmf0/WO+c+LGwbC5N8/GZqOufmOudWZv9urvWFnYWFc+5d59xB59y6c6x3zrl/Zp+vNc651hd1IBGxJY8FLRm8FaiDttdbDTQ5Y5vuQKnsv+8Fpvnabl+fk+ztwoAFwGIgxtd2+/g7Uh9YCZTLfl7J13b7+HxMAu7N/rsJsMPXdnv5nHQBWgPrzrH+WuArwAHtgSUXcxwboZ+f/DTJnisiydlPF6NdnUoy5z0n2TwLvASkFqZxPiA/5+Nu4HUROQYgIgcL2cbCJD/nQ4Dw7L/LAnsL0b5CR0QWAEfz2GQA8L4oi4EI51zVCz2OCfr5OVuT7Op5bH8neqUtyZz3nGTfMkaJyKzCNMxH5Oc70gBo4Jxb6Jxb7JzrU2jWFT75OR9jgFudc/For4X7C8e0IsuF6sxZyVeDCyN/OOduBWKArr62xZc45/yA8cBwH5tSlAhA3S7d0Du4Bc65ZiLivRbwRZthwHsi8rJzrgPwgXOuqYh4fG1YccZG6OcnX02ys1vw/QXoLyJphWSbrzjfOQkDmgLznHM7UJ/gzBI8MZqf70g8MFNEMkRkO7AZFfiSSH7Ox53AdAARWQSEoEWqLlfypTPnwwT9/OSnSXYr4C1UzEuybzSHPM+JiJwQkYoiEi0i0ei8Qn8RifWNuV7nvN8R4DN0dI5zriLqgtlWmEYWIvk5H7v+v707xE0oCOIw/o3AIblEb4AnQVRwBFKOQBo0B+ACJD0AAofrRVBVNRUEUVPXDOI9z6MJhQzf7wC7kxH/TXY2WWAEEBFPNIF++Ncq78sOmLavXYbAd2Z+XbqIVy5nZLdPsldAH9hGBMBnZk5uVvSVdezJw+jYj3dgHBF74BdYZObxdlVfT8d+vAJvETGnGZC+ZPvco6KI2NAc6IN2brAEegCZuaaZIzwDH8APMPvTPoV7KEkPxSsXSSrCQJekIgx0SSrCQJekIgx0SSrCQJekIgx0SSriBEyA0ZFeAAnyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2H3PvUGb8KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "6fbc4d6b-80af-458b-b35d-2bd9bc64c878"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "baseline_results = new_model.evaluate(X_test, Y_test,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(new_model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "predictions = new_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "plot_cm(Y_test, predictions, p=0.5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss :  0.19972075521945953\n",
            "tp :  5548.0\n",
            "fp :  852.0\n",
            "tn :  27628.0\n",
            "fn :  665.0\n",
            "accuracy :  0.95627361536026\n",
            "precision :  0.8668749928474426\n",
            "recall :  0.8929663896560669\n",
            "auc :  0.9869619607925415\n",
            "\n",
            "Legitimate Transactions Detected (True Negatives):  27628\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  852\n",
            "Fraudulent Transactions Missed (False Negatives):  665\n",
            "Fraudulent Transactions Detected (True Positives):  5548\n",
            "Total Fraudulent Transactions:  6213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUuAUMBb_9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "269cf296-0eba-4b8a-d3c6-532c0efce213"
      },
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,30])\n",
        "  plt.ylim([70,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "indices = np.random.randint(0, len(X_to_train), size=(len(Y_test),))\n",
        "\n",
        "train_prediction = new_model.predict(X_to_train[indices], batch_size=BATCH_SIZE, verbose=0)\n",
        "plot_roc(\"Train Baseline\", Y_train[indices], train_prediction, color='b')\n",
        "plot_roc(\"Test Baseline\", Y_test, predictions, color='b', linestyle='--')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-554f139278a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_to_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Baseline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Baseline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43d7bce2-6a9f-403f-fa51-312ba9f6fb96"
      },
      "source": [
        "prediction = np.squeeze(predictions, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist((prediction>0.9).astype('int'), bins=[0,1,2])\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.85).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.08% 3.32%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "e9358bab-c7a7-4e6e-fea4-e275cfa7a51c"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content\")\n",
        "test_transaction = pd.read_csv('test_transaction.csv')\n",
        "test_identity = pd.read_csv('test_identity.csv', names=saved_columns, header=0)\n",
        "test_identity.head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Android 6.0.1</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x720</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>LGLS676 Build/MXB48T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ie 11.0 for tablet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Trident/7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>MYA-L13 Build/HUAWEIMYA-L13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 67.0 for android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SM-G9650 Build/R16NW</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  ...  id_38  DeviceType                   DeviceInfo\n",
              "0        3663586  -45.0  280290.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "1        3663588    0.0    3579.0  ...      T      mobile         LGLS676 Build/MXB48T\n",
              "2        3663597   -5.0  185210.0  ...      F     desktop                  Trident/7.0\n",
              "3        3663601  -45.0  252944.0  ...      F      mobile  MYA-L13 Build/HUAWEIMYA-L13\n",
              "4        3663602  -95.0  328680.0  ...      F      mobile         SM-G9650 Build/R16NW\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8CCDZ5bl8V6p",
        "colab": {}
      },
      "source": [
        "dataset_transaction = None\n",
        "to_remove_id = ['DeviceInfo', 'id_30', 'id_31', 'id_33']\n",
        "for column in to_remove_id:\n",
        "  a = test_identity.pop(column)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVs1d5a_wMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e9b28869-ab36-412d-cb51-a80f52a71219"
      },
      "source": [
        "test_identity.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663586</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>280290.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3579.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>24.0</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663597</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>185210.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-360.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>271.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>desktop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663601</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>252944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>427.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663602</td>\n",
              "      <td>-95.0</td>\n",
              "      <td>328680.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-33.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>mobile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01     id_02  id_03  ...  id_36  id_37  id_38  DeviceType\n",
              "0        3663586  -45.0  280290.0    NaN  ...      F      T      F      mobile\n",
              "1        3663588    0.0    3579.0    0.0  ...      F      T      T      mobile\n",
              "2        3663597   -5.0  185210.0    NaN  ...      T      T      F     desktop\n",
              "3        3663601  -45.0  252944.0    0.0  ...      F      T      F      mobile\n",
              "4        3663602  -95.0  328680.0    NaN  ...      F      T      F      mobile\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "du0_nSm48V63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18814da9-cbe0-46d7-d49f-5caa85da742d"
      },
      "source": [
        "merged_data = pd.merge(left=test_transaction, right=test_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "TransactionID = merged_data.pop('TransactionID')\n",
        "test_transaction = None\n",
        "merged_data.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506691, 428)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoViKsx6cZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d86e588-11f4-4ec0-ce57-acc8c6ad8dd0"
      },
      "source": [
        "test_transaction = copy.copy(merged_data)\n",
        "merged_data = None\n",
        "float_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = []\n",
        "for column in skip_int_columns:\n",
        "  int_columns_test.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "print(len(float_columns_test), len(int_columns_test), len(obj_columns_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "399 2 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzQZ6nR6wOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_normalization(X, indices, cache_min, cache_max, cache_mean):\n",
        "  X_out = copy.copy(X)\n",
        "  X_out[indices] = (X_out[indices] - cache_mean)/(cache_max - cache_min)\n",
        "  X_out[np.where(np.isnan(X_out))[0]] = 0.0\n",
        "  return X_out.astype('float16')  \n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM75lh_6lhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in float_columns_test:\n",
        "  # Set to float 16\n",
        "  test_transaction[column].astype('float32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjog0oM7p4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns_test:\n",
        "  # Set to int 32\n",
        "  test_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMTT8KB74NL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c792003-57c9-44f8-ceb5-f84b89926efa"
      },
      "source": [
        "encoded_column = 0\n",
        "for column in obj_columns_test:\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(cache[column])\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.transform(test_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(cache[column])))])\n",
        "  test_transaction = pd.concat([test_transaction, pd_encoded], axis=1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "\n",
        "\n",
        "for column in obj_columns_test:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "for column in to_remove:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_OOqFi8HrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "270947df-f462-4986-c58f-bae559138d6d"
      },
      "source": [
        "# Check if we have the same shape with the X_train\n",
        "#print(test_transaction.shape, X_train.shape)\n",
        "print(test_transaction.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506691, 891)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9vDvpDZdpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the prediction and submit the output\n",
        "result = (new_model.predict(test_transaction)>0.1).astype('int8')\n",
        "result_pd = pd.DataFrame(result, columns=['isFraud'])\n",
        "data_to_file = pd.concat([TransactionID, result_pd], axis=1)\n",
        "data_to_file.head(5)\n",
        "data_to_file.to_csv(\"./submission.csv\", index=False)\n",
        "data_to_file.to_csv('/content/gdrive/My Drive/Kaggle/submission.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9099XTi4s2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "392f82ad-a730-4b54-c78b-c188b216a70e"
      },
      "source": [
        "!kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"New submission with model_20200806 with threshold 0.1\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 4.83M/4.83M [00:02<00:00, 1.93MB/s]\n",
            "Successfully submitted to IEEE-CIS Fraud Detection"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICp4sPm6brq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3e2nvzrHir4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fe4ae314-d9e2-483a-9baa-e8b9302f14bf"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohc = OneHotEncoder()\n",
        "a = {'a': ['Null', 'A', 'B', 'C', 'D']}\n",
        "df = pd.DataFrame(a)\n",
        "df\n",
        "encoded = ohc.fit_transform(df['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsaGKlzMUlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69213f3a-42cb-4edc-cbc8-5bc6c0bb5a7e"
      },
      "source": [
        "b = {'a': ['Null', 'A', 'B', 'C', 'E']}\n",
        "df_b = pd.DataFrame(b)\n",
        "ohc_b = OneHotEncoder(handle_unknown='ignore')\n",
        "ohc_b.fit(df['a'].values.reshape(-1,1))\n",
        "encoded_b = ohc_b.transform(df_b['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded_b = pd.DataFrame(encoded_b.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvykuaRPMpZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "20c01d2a-8a3b-41b6-a7ae-b6d7a36f327f"
      },
      "source": [
        "for column in obj_columns:\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(dataset_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 5\n",
            "P_emaildomain 60\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_RMIz3NTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2e5b2844-8bdd-45ee-f5ec-83cd54c9cba7"
      },
      "source": [
        "for column in obj_columns_test:\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(test_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 4\n",
            "P_emaildomain 61\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}