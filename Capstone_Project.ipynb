{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bd67c24b-bc95-49ab-d844-7cf075284a8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c9eba325-2579-4f0d-f94f-d58884309009"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 78.7MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 105MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 82% 48.0M/58.3M [00:00<00:00, 32.7MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 63.3MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 218MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 63% 33.0M/52.2M [00:00<00:00, 35.1MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 71.3MB/s]\n",
            "Archive:  train_transaction.csv.zip\n",
            "  inflating: train_transaction.csv   \n",
            "\n",
            "Archive:  test_identity.csv.zip\n",
            "  inflating: test_identity.csv       \n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "\n",
            "Archive:  test_transaction.csv.zip\n",
            "  inflating: test_transaction.csv    \n",
            "\n",
            "Archive:  train_identity.csv.zip\n",
            "  inflating: train_identity.csv      \n",
            "\n",
            "5 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "453e2e8d-1fa5-4240-a1ef-8547eb53fb5a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "a106d9b4-cb31-4e35-92b8-bd036262b6fe"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "dc45811d-ba82-4fd8-a765-fa28b5c33bae"
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>Android 7.0</td>\n",
              "      <td>samsung browser 6.2</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2220x1080</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987008</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>98945.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>49.0</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>621.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>iOS 11.1.2</td>\n",
              "      <td>mobile safari 11.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1334x750</td>\n",
              "      <td>match_status:1</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>iOS Device</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987010</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>191631.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>121.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>410.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Windows</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987011</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>221832.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>176.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>529.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Mac OS X 10_11_6</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x800</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>MacOS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01  ...  DeviceType                     DeviceInfo\n",
              "0        2987004    0.0  ...      mobile  SAMSUNG SM-G892A Build/NRD90M\n",
              "1        2987008   -5.0  ...      mobile                     iOS Device\n",
              "2        2987010   -5.0  ...     desktop                        Windows\n",
              "3        2987011   -5.0  ...     desktop                            NaN\n",
              "4        2987016    0.0  ...     desktop                          MacOS\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b655002b-7809-4851-bfc5-8acffda1d6a4"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e0fabc4b-5f6a-4b6a-cb6e-bb0677d7a8cc"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "b836fada-4cd9-4803-bb06-2f822fcc05a4"
      },
      "source": [
        "data_backup = copy.copy(dataset_transaction)\n",
        "data_backup.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "\n",
        "cache = dict()\n",
        "dataset_transaction = copy.copy(data_backup)\n",
        "\n",
        "#dataset_transaction.pop()\n",
        "\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "b7e93b25-9c59-42d1-b0d1-ef4e2bfd7011"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "cd16e2e8-674d-4186-e511-44942aa516f1"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "3dc63567-016c-4245-9944-d798a0b500a7"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 905 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  M9_0  M9_1  M9_2\n",
              "0        2987000        0      -0.463379  ...     0     1     0\n",
              "1        2987001        0      -0.463379  ...     0     1     0\n",
              "2        2987002        0      -0.463379  ...     1     0     0\n",
              "3        2987003        0      -0.463379  ...     0     1     0\n",
              "4        2987004        0      -0.463379  ...     0     1     0\n",
              "\n",
              "[5 rows x 905 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b0b7c58-7a89-488a-ef8e-e53c5f32c156"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colums_to_analyze = ['isFraud', 'TransactionDT', 'TransactionAmt', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2']\n",
        "analyzing_data = dataset_transaction[colums_to_analyze]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "715b0ffe-c22d-4bc3-9b23-ea77c1d3988f"
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4a56b317b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "0e05e8ac-9d0b-46e0-c9e7-0e2423400cbb"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 904 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  TransactionAmt     card1  ...  M8_2  M9_0  M9_1  M9_2\n",
              "0        0      -0.463379       -0.002083  0.231445  ...     0     0     1     0\n",
              "1        0      -0.463379       -0.003321 -0.410645  ...     0     0     1     0\n",
              "2        0      -0.463379       -0.002380 -0.301025  ...     0     1     0     0\n",
              "3        0      -0.463379       -0.002663  0.473389  ...     0     0     1     0\n",
              "4        0      -0.463379       -0.002663 -0.310547  ...     0     0     1     0\n",
              "\n",
              "[5 rows x 904 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3f33667-c1fa-4df1-c987-d2c9b0dd9afa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "print(X_train.shape, Y_train.shape, X_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(472432, 903) (472432,) (118108, 903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d13dd7ff-0671-40c7-d968-a782f8a22778"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.52%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b2455434-28cf-4228-fb56-bd38b8583e22"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 10\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_to_train = np.array(X_train)[indices_0_new]\n",
        "Y_to_train = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "\n",
        "X_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_to_train, axis=1)\n",
        "print(X_to_train.shape, Y_to_train.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(455805, 1)\n",
            "(638702, 1)\n",
            "(638702, 903) (638702,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "543f24d2-312a-4652-f29d-ef0c20ea7b1e"
      },
      "source": [
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 28.64%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TMevC7JyZ_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  out_model.add(Dense(dense1, activation=\"relu\", input_shape=(X_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "\n",
        "  out_model.add(Dense(dense2, activation=\"relu\", \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[tfa.metrics.F1Score(num_classes=2, average=\"micro\", threshold=0.9)])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1ab8d846-626d-444c-b7f7-9ea94cd77e86"
      },
      "source": [
        "my_model = create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0, l2_rate=0, init_std=0.05, lr=0.0001)\n",
        "my_model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 128)               115712    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 144,705\n",
            "Trainable params: 144,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5869ecd-7f57-4ac8-c2b6-9a5878322edb"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "NB_EPOCH = 1000\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_f1_score', patience=50, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/best_model', monitor='val_f1_score', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.2, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.4931 - f1_score: 0.0000e+00\n",
            "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.4922 - f1_score: 0.0000e+00 - val_loss: 1.1764 - val_f1_score: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.3929 - f1_score: 0.0062\n",
            "Epoch 00002: val_f1_score improved from 0.00000 to 0.03091, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3926 - f1_score: 0.0065 - val_loss: 0.9702 - val_f1_score: 0.0309\n",
            "Epoch 3/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3705 - f1_score: 0.1222\n",
            "Epoch 00003: val_f1_score improved from 0.03091 to 0.18373, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3704 - f1_score: 0.1227 - val_loss: 0.8607 - val_f1_score: 0.1837\n",
            "Epoch 4/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.3601 - f1_score: 0.2204\n",
            "Epoch 00004: val_f1_score improved from 0.18373 to 0.26232, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 8ms/step - loss: 0.3598 - f1_score: 0.2203 - val_loss: 0.8989 - val_f1_score: 0.2623\n",
            "Epoch 5/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.3527 - f1_score: 0.2823\n",
            "Epoch 00005: val_f1_score improved from 0.26232 to 0.28561, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3526 - f1_score: 0.2820 - val_loss: 0.9895 - val_f1_score: 0.2856\n",
            "Epoch 6/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.3482 - f1_score: 0.3048\n",
            "Epoch 00006: val_f1_score did not improve from 0.28561\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3482 - f1_score: 0.3049 - val_loss: 0.8932 - val_f1_score: 0.2852\n",
            "Epoch 7/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3457 - f1_score: 0.3060\n",
            "Epoch 00007: val_f1_score improved from 0.28561 to 0.33169, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3457 - f1_score: 0.3054 - val_loss: 0.8889 - val_f1_score: 0.3317\n",
            "Epoch 8/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.3429 - f1_score: 0.3275\n",
            "Epoch 00008: val_f1_score improved from 0.33169 to 0.34547, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 8ms/step - loss: 0.3427 - f1_score: 0.3256 - val_loss: 0.8988 - val_f1_score: 0.3455\n",
            "Epoch 9/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.3409 - f1_score: 0.3319\n",
            "Epoch 00009: val_f1_score improved from 0.34547 to 0.36836, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3407 - f1_score: 0.3326 - val_loss: 0.7943 - val_f1_score: 0.3684\n",
            "Epoch 10/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.3388 - f1_score: 0.3491\n",
            "Epoch 00010: val_f1_score did not improve from 0.36836\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3392 - f1_score: 0.3493 - val_loss: 0.8399 - val_f1_score: 0.3624\n",
            "Epoch 11/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.3353 - f1_score: 0.3548\n",
            "Epoch 00011: val_f1_score improved from 0.36836 to 0.41379, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3357 - f1_score: 0.3533 - val_loss: 0.7256 - val_f1_score: 0.4138\n",
            "Epoch 12/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3332 - f1_score: 0.3626\n",
            "Epoch 00012: val_f1_score did not improve from 0.41379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3334 - f1_score: 0.3626 - val_loss: 0.8423 - val_f1_score: 0.3898\n",
            "Epoch 13/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3320 - f1_score: 0.3645\n",
            "Epoch 00013: val_f1_score did not improve from 0.41379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3320 - f1_score: 0.3640 - val_loss: 0.8315 - val_f1_score: 0.3616\n",
            "Epoch 14/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.3295 - f1_score: 0.3773\n",
            "Epoch 00014: val_f1_score did not improve from 0.41379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3297 - f1_score: 0.3769 - val_loss: 0.8258 - val_f1_score: 0.3804\n",
            "Epoch 15/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.3270 - f1_score: 0.3809\n",
            "Epoch 00015: val_f1_score did not improve from 0.41379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3271 - f1_score: 0.3803 - val_loss: 0.8142 - val_f1_score: 0.4048\n",
            "Epoch 16/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.3270 - f1_score: 0.3880\n",
            "Epoch 00016: val_f1_score improved from 0.41379 to 0.43440, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.3266 - f1_score: 0.3874 - val_loss: 0.8183 - val_f1_score: 0.4344\n",
            "Epoch 17/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.3248 - f1_score: 0.3937\n",
            "Epoch 00017: val_f1_score did not improve from 0.43440\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.3248 - f1_score: 0.3939 - val_loss: 0.7463 - val_f1_score: 0.4154\n",
            "Epoch 18/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.3241 - f1_score: 0.3999\n",
            "Epoch 00018: val_f1_score did not improve from 0.43440\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.3244 - f1_score: 0.3993 - val_loss: 0.8100 - val_f1_score: 0.4229\n",
            "Epoch 19/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.3212 - f1_score: 0.4038\n",
            "Epoch 00019: val_f1_score did not improve from 0.43440\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.3212 - f1_score: 0.4038 - val_loss: 0.8398 - val_f1_score: 0.3992\n",
            "Epoch 20/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.3203 - f1_score: 0.4120\n",
            "Epoch 00020: val_f1_score improved from 0.43440 to 0.44475, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3202 - f1_score: 0.4118 - val_loss: 0.8186 - val_f1_score: 0.4447\n",
            "Epoch 21/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.3186 - f1_score: 0.4169\n",
            "Epoch 00021: val_f1_score did not improve from 0.44475\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3189 - f1_score: 0.4162 - val_loss: 0.9084 - val_f1_score: 0.3910\n",
            "Epoch 22/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.3189 - f1_score: 0.4179\n",
            "Epoch 00022: val_f1_score improved from 0.44475 to 0.45992, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3187 - f1_score: 0.4180 - val_loss: 0.8230 - val_f1_score: 0.4599\n",
            "Epoch 23/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3153 - f1_score: 0.4256\n",
            "Epoch 00023: val_f1_score improved from 0.45992 to 0.49211, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3152 - f1_score: 0.4255 - val_loss: 0.6786 - val_f1_score: 0.4921\n",
            "Epoch 24/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.3146 - f1_score: 0.4262\n",
            "Epoch 00024: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3145 - f1_score: 0.4264 - val_loss: 0.7569 - val_f1_score: 0.4660\n",
            "Epoch 25/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.3122 - f1_score: 0.4378\n",
            "Epoch 00025: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3124 - f1_score: 0.4373 - val_loss: 0.7432 - val_f1_score: 0.4774\n",
            "Epoch 26/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.3130 - f1_score: 0.4354\n",
            "Epoch 00026: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3131 - f1_score: 0.4367 - val_loss: 0.8461 - val_f1_score: 0.4393\n",
            "Epoch 27/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3102 - f1_score: 0.4446\n",
            "Epoch 00027: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3101 - f1_score: 0.4444 - val_loss: 0.8028 - val_f1_score: 0.4303\n",
            "Epoch 28/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.3074 - f1_score: 0.4463\n",
            "Epoch 00028: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3075 - f1_score: 0.4458 - val_loss: 0.7687 - val_f1_score: 0.4477\n",
            "Epoch 29/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.3078 - f1_score: 0.4463\n",
            "Epoch 00029: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3076 - f1_score: 0.4466 - val_loss: 0.7414 - val_f1_score: 0.4921\n",
            "Epoch 30/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.3050 - f1_score: 0.4506\n",
            "Epoch 00030: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3050 - f1_score: 0.4504 - val_loss: 0.7865 - val_f1_score: 0.4576\n",
            "Epoch 31/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.3040 - f1_score: 0.4549\n",
            "Epoch 00031: val_f1_score did not improve from 0.49211\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.3037 - f1_score: 0.4560 - val_loss: 0.8508 - val_f1_score: 0.4727\n",
            "Epoch 32/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.3016 - f1_score: 0.4557\n",
            "Epoch 00032: val_f1_score improved from 0.49211 to 0.49659, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.3024 - f1_score: 0.4549 - val_loss: 0.7062 - val_f1_score: 0.4966\n",
            "Epoch 33/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.3008 - f1_score: 0.4633\n",
            "Epoch 00033: val_f1_score improved from 0.49659 to 0.50557, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.3008 - f1_score: 0.4633 - val_loss: 0.8023 - val_f1_score: 0.5056\n",
            "Epoch 34/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.3001 - f1_score: 0.4717\n",
            "Epoch 00034: val_f1_score did not improve from 0.50557\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2999 - f1_score: 0.4719 - val_loss: 0.8488 - val_f1_score: 0.4964\n",
            "Epoch 35/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2999 - f1_score: 0.4676\n",
            "Epoch 00035: val_f1_score improved from 0.50557 to 0.53882, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.2998 - f1_score: 0.4677 - val_loss: 0.6504 - val_f1_score: 0.5388\n",
            "Epoch 36/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2963 - f1_score: 0.4758\n",
            "Epoch 00036: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2960 - f1_score: 0.4762 - val_loss: 0.8691 - val_f1_score: 0.4558\n",
            "Epoch 37/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2954 - f1_score: 0.4774\n",
            "Epoch 00037: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2953 - f1_score: 0.4774 - val_loss: 0.6569 - val_f1_score: 0.4815\n",
            "Epoch 38/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.2928 - f1_score: 0.4877\n",
            "Epoch 00038: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2932 - f1_score: 0.4865 - val_loss: 0.6969 - val_f1_score: 0.4867\n",
            "Epoch 39/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2917 - f1_score: 0.4833\n",
            "Epoch 00039: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2918 - f1_score: 0.4832 - val_loss: 0.6818 - val_f1_score: 0.5121\n",
            "Epoch 40/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2916 - f1_score: 0.4878\n",
            "Epoch 00040: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2917 - f1_score: 0.4878 - val_loss: 0.8240 - val_f1_score: 0.4799\n",
            "Epoch 41/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.2883 - f1_score: 0.4908\n",
            "Epoch 00041: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2882 - f1_score: 0.4909 - val_loss: 0.8101 - val_f1_score: 0.5110\n",
            "Epoch 42/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2877 - f1_score: 0.4968\n",
            "Epoch 00042: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2873 - f1_score: 0.4966 - val_loss: 0.7605 - val_f1_score: 0.4726\n",
            "Epoch 43/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2869 - f1_score: 0.4985\n",
            "Epoch 00043: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2872 - f1_score: 0.4977 - val_loss: 0.7579 - val_f1_score: 0.4888\n",
            "Epoch 44/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2832 - f1_score: 0.5035\n",
            "Epoch 00044: val_f1_score did not improve from 0.53882\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2833 - f1_score: 0.5030 - val_loss: 0.6880 - val_f1_score: 0.5309\n",
            "Epoch 45/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2841 - f1_score: 0.5051\n",
            "Epoch 00045: val_f1_score improved from 0.53882 to 0.57477, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.2835 - f1_score: 0.5051 - val_loss: 0.7194 - val_f1_score: 0.5748\n",
            "Epoch 46/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2822 - f1_score: 0.5107\n",
            "Epoch 00046: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2822 - f1_score: 0.5109 - val_loss: 0.7256 - val_f1_score: 0.5322\n",
            "Epoch 47/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2810 - f1_score: 0.5139\n",
            "Epoch 00047: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2812 - f1_score: 0.5141 - val_loss: 0.6279 - val_f1_score: 0.5440\n",
            "Epoch 48/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2794 - f1_score: 0.5188\n",
            "Epoch 00048: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2796 - f1_score: 0.5186 - val_loss: 0.7163 - val_f1_score: 0.5173\n",
            "Epoch 49/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2790 - f1_score: 0.5152\n",
            "Epoch 00049: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2787 - f1_score: 0.5156 - val_loss: 0.6618 - val_f1_score: 0.5565\n",
            "Epoch 50/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.2757 - f1_score: 0.5273\n",
            "Epoch 00050: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2755 - f1_score: 0.5266 - val_loss: 0.7059 - val_f1_score: 0.5388\n",
            "Epoch 51/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2758 - f1_score: 0.5259\n",
            "Epoch 00051: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2755 - f1_score: 0.5267 - val_loss: 0.6714 - val_f1_score: 0.5492\n",
            "Epoch 52/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2745 - f1_score: 0.5283\n",
            "Epoch 00052: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2744 - f1_score: 0.5287 - val_loss: 0.7334 - val_f1_score: 0.5595\n",
            "Epoch 53/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2727 - f1_score: 0.5347\n",
            "Epoch 00053: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2728 - f1_score: 0.5342 - val_loss: 0.6703 - val_f1_score: 0.5369\n",
            "Epoch 54/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2718 - f1_score: 0.5384\n",
            "Epoch 00054: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2718 - f1_score: 0.5370 - val_loss: 0.6479 - val_f1_score: 0.5514\n",
            "Epoch 55/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2704 - f1_score: 0.5414\n",
            "Epoch 00055: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2705 - f1_score: 0.5412 - val_loss: 0.7376 - val_f1_score: 0.5476\n",
            "Epoch 56/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2690 - f1_score: 0.5429\n",
            "Epoch 00056: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2696 - f1_score: 0.5424 - val_loss: 0.7375 - val_f1_score: 0.5210\n",
            "Epoch 57/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2673 - f1_score: 0.5479\n",
            "Epoch 00057: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2674 - f1_score: 0.5475 - val_loss: 0.6748 - val_f1_score: 0.5720\n",
            "Epoch 58/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.2668 - f1_score: 0.5538\n",
            "Epoch 00058: val_f1_score did not improve from 0.57477\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2668 - f1_score: 0.5538 - val_loss: 0.7544 - val_f1_score: 0.5423\n",
            "Epoch 59/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2658 - f1_score: 0.5524\n",
            "Epoch 00059: val_f1_score improved from 0.57477 to 0.61068, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 8ms/step - loss: 0.2659 - f1_score: 0.5530 - val_loss: 0.5884 - val_f1_score: 0.6107\n",
            "Epoch 60/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.2642 - f1_score: 0.5619\n",
            "Epoch 00060: val_f1_score did not improve from 0.61068\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2642 - f1_score: 0.5606 - val_loss: 0.6111 - val_f1_score: 0.5950\n",
            "Epoch 61/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2631 - f1_score: 0.5563\n",
            "Epoch 00061: val_f1_score did not improve from 0.61068\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2629 - f1_score: 0.5564 - val_loss: 0.6020 - val_f1_score: 0.6024\n",
            "Epoch 62/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2631 - f1_score: 0.5602\n",
            "Epoch 00062: val_f1_score improved from 0.61068 to 0.61567, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.2630 - f1_score: 0.5602 - val_loss: 0.6606 - val_f1_score: 0.6157\n",
            "Epoch 63/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2608 - f1_score: 0.5693\n",
            "Epoch 00063: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2608 - f1_score: 0.5689 - val_loss: 0.6185 - val_f1_score: 0.5938\n",
            "Epoch 64/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.2596 - f1_score: 0.5693\n",
            "Epoch 00064: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2603 - f1_score: 0.5681 - val_loss: 0.6905 - val_f1_score: 0.5539\n",
            "Epoch 65/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2592 - f1_score: 0.5698\n",
            "Epoch 00065: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2597 - f1_score: 0.5701 - val_loss: 0.8117 - val_f1_score: 0.5151\n",
            "Epoch 66/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.2595 - f1_score: 0.5730\n",
            "Epoch 00066: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2591 - f1_score: 0.5733 - val_loss: 0.6861 - val_f1_score: 0.5948\n",
            "Epoch 67/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2558 - f1_score: 0.5773\n",
            "Epoch 00067: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2558 - f1_score: 0.5771 - val_loss: 0.6437 - val_f1_score: 0.5936\n",
            "Epoch 68/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2555 - f1_score: 0.5785\n",
            "Epoch 00068: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2555 - f1_score: 0.5791 - val_loss: 0.6480 - val_f1_score: 0.5593\n",
            "Epoch 69/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2533 - f1_score: 0.5833\n",
            "Epoch 00069: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2535 - f1_score: 0.5828 - val_loss: 0.7289 - val_f1_score: 0.5927\n",
            "Epoch 70/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2544 - f1_score: 0.5871\n",
            "Epoch 00070: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2544 - f1_score: 0.5881 - val_loss: 0.7354 - val_f1_score: 0.5658\n",
            "Epoch 71/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2530 - f1_score: 0.5834\n",
            "Epoch 00071: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2532 - f1_score: 0.5827 - val_loss: 0.6347 - val_f1_score: 0.6111\n",
            "Epoch 72/1000\n",
            "295/308 [===========================>..] - ETA: 0s - loss: 0.2516 - f1_score: 0.5923\n",
            "Epoch 00072: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2517 - f1_score: 0.5915 - val_loss: 0.6570 - val_f1_score: 0.5856\n",
            "Epoch 73/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2509 - f1_score: 0.5889\n",
            "Epoch 00073: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2517 - f1_score: 0.5887 - val_loss: 0.6709 - val_f1_score: 0.5837\n",
            "Epoch 74/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2529 - f1_score: 0.5864\n",
            "Epoch 00074: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2522 - f1_score: 0.5880 - val_loss: 0.6227 - val_f1_score: 0.6059\n",
            "Epoch 75/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2489 - f1_score: 0.5974\n",
            "Epoch 00075: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2486 - f1_score: 0.5964 - val_loss: 0.6308 - val_f1_score: 0.5784\n",
            "Epoch 76/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.2473 - f1_score: 0.6024\n",
            "Epoch 00076: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2481 - f1_score: 0.6009 - val_loss: 0.6961 - val_f1_score: 0.5952\n",
            "Epoch 77/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2472 - f1_score: 0.6010\n",
            "Epoch 00077: val_f1_score did not improve from 0.61567\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2472 - f1_score: 0.6010 - val_loss: 0.6396 - val_f1_score: 0.6136\n",
            "Epoch 78/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2469 - f1_score: 0.6072\n",
            "Epoch 00078: val_f1_score improved from 0.61567 to 0.63572, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.2469 - f1_score: 0.6069 - val_loss: 0.5458 - val_f1_score: 0.6357\n",
            "Epoch 79/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.2442 - f1_score: 0.6011\n",
            "Epoch 00079: val_f1_score did not improve from 0.63572\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2442 - f1_score: 0.6029 - val_loss: 0.7328 - val_f1_score: 0.5500\n",
            "Epoch 80/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2439 - f1_score: 0.6074\n",
            "Epoch 00080: val_f1_score improved from 0.63572 to 0.65112, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.2439 - f1_score: 0.6082 - val_loss: 0.5871 - val_f1_score: 0.6511\n",
            "Epoch 81/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2421 - f1_score: 0.6086\n",
            "Epoch 00081: val_f1_score did not improve from 0.65112\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2424 - f1_score: 0.6081 - val_loss: 0.6081 - val_f1_score: 0.6109\n",
            "Epoch 82/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2417 - f1_score: 0.6105\n",
            "Epoch 00082: val_f1_score did not improve from 0.65112\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2419 - f1_score: 0.6106 - val_loss: 0.5768 - val_f1_score: 0.6411\n",
            "Epoch 83/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2409 - f1_score: 0.6140\n",
            "Epoch 00083: val_f1_score did not improve from 0.65112\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2409 - f1_score: 0.6139 - val_loss: 0.7319 - val_f1_score: 0.5948\n",
            "Epoch 84/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2398 - f1_score: 0.6143\n",
            "Epoch 00084: val_f1_score improved from 0.65112 to 0.67964, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.2398 - f1_score: 0.6146 - val_loss: 0.5402 - val_f1_score: 0.6796\n",
            "Epoch 85/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2377 - f1_score: 0.6174\n",
            "Epoch 00085: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2376 - f1_score: 0.6169 - val_loss: 0.5795 - val_f1_score: 0.6395\n",
            "Epoch 86/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2399 - f1_score: 0.6179\n",
            "Epoch 00086: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2401 - f1_score: 0.6177 - val_loss: 0.6683 - val_f1_score: 0.5955\n",
            "Epoch 87/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2366 - f1_score: 0.6223\n",
            "Epoch 00087: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2361 - f1_score: 0.6224 - val_loss: 0.6448 - val_f1_score: 0.6146\n",
            "Epoch 88/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2362 - f1_score: 0.6269\n",
            "Epoch 00088: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2367 - f1_score: 0.6270 - val_loss: 0.5467 - val_f1_score: 0.6430\n",
            "Epoch 89/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2361 - f1_score: 0.6255\n",
            "Epoch 00089: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2358 - f1_score: 0.6257 - val_loss: 0.6419 - val_f1_score: 0.6297\n",
            "Epoch 90/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2353 - f1_score: 0.6289\n",
            "Epoch 00090: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2357 - f1_score: 0.6282 - val_loss: 0.5572 - val_f1_score: 0.6038\n",
            "Epoch 91/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2340 - f1_score: 0.6282\n",
            "Epoch 00091: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2336 - f1_score: 0.6300 - val_loss: 0.6255 - val_f1_score: 0.6262\n",
            "Epoch 92/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2324 - f1_score: 0.6339\n",
            "Epoch 00092: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2325 - f1_score: 0.6335 - val_loss: 0.5756 - val_f1_score: 0.6315\n",
            "Epoch 93/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2312 - f1_score: 0.6349\n",
            "Epoch 00093: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2310 - f1_score: 0.6350 - val_loss: 0.6412 - val_f1_score: 0.6544\n",
            "Epoch 94/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2306 - f1_score: 0.6372\n",
            "Epoch 00094: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2305 - f1_score: 0.6371 - val_loss: 0.5921 - val_f1_score: 0.6752\n",
            "Epoch 95/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2312 - f1_score: 0.6390\n",
            "Epoch 00095: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2313 - f1_score: 0.6387 - val_loss: 0.5942 - val_f1_score: 0.6552\n",
            "Epoch 96/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2293 - f1_score: 0.6396\n",
            "Epoch 00096: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2292 - f1_score: 0.6397 - val_loss: 0.6106 - val_f1_score: 0.6456\n",
            "Epoch 97/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2260 - f1_score: 0.6442\n",
            "Epoch 00097: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2259 - f1_score: 0.6441 - val_loss: 0.5536 - val_f1_score: 0.6527\n",
            "Epoch 98/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2272 - f1_score: 0.6466\n",
            "Epoch 00098: val_f1_score did not improve from 0.67964\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2270 - f1_score: 0.6469 - val_loss: 0.5431 - val_f1_score: 0.6584\n",
            "Epoch 99/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2272 - f1_score: 0.6488\n",
            "Epoch 00099: val_f1_score improved from 0.67964 to 0.70965, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.2275 - f1_score: 0.6479 - val_loss: 0.4911 - val_f1_score: 0.7096\n",
            "Epoch 100/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2265 - f1_score: 0.6421\n",
            "Epoch 00100: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2272 - f1_score: 0.6415 - val_loss: 0.4586 - val_f1_score: 0.6582\n",
            "Epoch 101/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.2251 - f1_score: 0.6508\n",
            "Epoch 00101: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2251 - f1_score: 0.6508 - val_loss: 0.6641 - val_f1_score: 0.6480\n",
            "Epoch 102/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2249 - f1_score: 0.6503\n",
            "Epoch 00102: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2243 - f1_score: 0.6509 - val_loss: 0.6168 - val_f1_score: 0.6590\n",
            "Epoch 103/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2256 - f1_score: 0.6493\n",
            "Epoch 00103: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2254 - f1_score: 0.6499 - val_loss: 0.5808 - val_f1_score: 0.6745\n",
            "Epoch 104/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2219 - f1_score: 0.6575\n",
            "Epoch 00104: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2221 - f1_score: 0.6568 - val_loss: 0.6220 - val_f1_score: 0.6082\n",
            "Epoch 105/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2227 - f1_score: 0.6524\n",
            "Epoch 00105: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2227 - f1_score: 0.6524 - val_loss: 0.6234 - val_f1_score: 0.6518\n",
            "Epoch 106/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.2224 - f1_score: 0.6577\n",
            "Epoch 00106: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2222 - f1_score: 0.6579 - val_loss: 0.6230 - val_f1_score: 0.6532\n",
            "Epoch 107/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2219 - f1_score: 0.6577\n",
            "Epoch 00107: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2218 - f1_score: 0.6575 - val_loss: 0.5768 - val_f1_score: 0.6767\n",
            "Epoch 108/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2205 - f1_score: 0.6625\n",
            "Epoch 00108: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2204 - f1_score: 0.6627 - val_loss: 0.5662 - val_f1_score: 0.6705\n",
            "Epoch 109/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2197 - f1_score: 0.6589\n",
            "Epoch 00109: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2195 - f1_score: 0.6599 - val_loss: 0.5731 - val_f1_score: 0.7018\n",
            "Epoch 110/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2197 - f1_score: 0.6629\n",
            "Epoch 00110: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2194 - f1_score: 0.6635 - val_loss: 0.5403 - val_f1_score: 0.6912\n",
            "Epoch 111/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2175 - f1_score: 0.6630\n",
            "Epoch 00111: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2171 - f1_score: 0.6646 - val_loss: 0.6311 - val_f1_score: 0.6834\n",
            "Epoch 112/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.2138 - f1_score: 0.6689\n",
            "Epoch 00112: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2143 - f1_score: 0.6682 - val_loss: 0.4685 - val_f1_score: 0.6724\n",
            "Epoch 113/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2158 - f1_score: 0.6663\n",
            "Epoch 00113: val_f1_score did not improve from 0.70965\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2160 - f1_score: 0.6670 - val_loss: 0.4929 - val_f1_score: 0.6999\n",
            "Epoch 114/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2168 - f1_score: 0.6650\n",
            "Epoch 00114: val_f1_score improved from 0.70965 to 0.71924, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.2164 - f1_score: 0.6649 - val_loss: 0.4473 - val_f1_score: 0.7192\n",
            "Epoch 115/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.2161 - f1_score: 0.6685\n",
            "Epoch 00115: val_f1_score did not improve from 0.71924\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2158 - f1_score: 0.6676 - val_loss: 0.5685 - val_f1_score: 0.6880\n",
            "Epoch 116/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2126 - f1_score: 0.6743\n",
            "Epoch 00116: val_f1_score did not improve from 0.71924\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2127 - f1_score: 0.6745 - val_loss: 0.5241 - val_f1_score: 0.6753\n",
            "Epoch 117/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2118 - f1_score: 0.6738\n",
            "Epoch 00117: val_f1_score did not improve from 0.71924\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2117 - f1_score: 0.6742 - val_loss: 0.5319 - val_f1_score: 0.7036\n",
            "Epoch 118/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.2144 - f1_score: 0.6734\n",
            "Epoch 00118: val_f1_score improved from 0.71924 to 0.74828, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.2136 - f1_score: 0.6748 - val_loss: 0.4425 - val_f1_score: 0.7483\n",
            "Epoch 119/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2111 - f1_score: 0.6758\n",
            "Epoch 00119: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2109 - f1_score: 0.6764 - val_loss: 0.5491 - val_f1_score: 0.7082\n",
            "Epoch 120/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2092 - f1_score: 0.6799\n",
            "Epoch 00120: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2092 - f1_score: 0.6801 - val_loss: 0.4745 - val_f1_score: 0.7243\n",
            "Epoch 121/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.2104 - f1_score: 0.6790\n",
            "Epoch 00121: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2102 - f1_score: 0.6798 - val_loss: 0.4810 - val_f1_score: 0.7135\n",
            "Epoch 122/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2096 - f1_score: 0.6813\n",
            "Epoch 00122: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2100 - f1_score: 0.6816 - val_loss: 0.5554 - val_f1_score: 0.6625\n",
            "Epoch 123/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.2092 - f1_score: 0.6806\n",
            "Epoch 00123: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2088 - f1_score: 0.6804 - val_loss: 0.5315 - val_f1_score: 0.7008\n",
            "Epoch 124/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2069 - f1_score: 0.6855\n",
            "Epoch 00124: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2069 - f1_score: 0.6853 - val_loss: 0.5048 - val_f1_score: 0.7099\n",
            "Epoch 125/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2076 - f1_score: 0.6836\n",
            "Epoch 00125: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2074 - f1_score: 0.6836 - val_loss: 0.5248 - val_f1_score: 0.6784\n",
            "Epoch 126/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2069 - f1_score: 0.6854\n",
            "Epoch 00126: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2068 - f1_score: 0.6851 - val_loss: 0.5003 - val_f1_score: 0.7324\n",
            "Epoch 127/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2067 - f1_score: 0.6864\n",
            "Epoch 00127: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2068 - f1_score: 0.6864 - val_loss: 0.4905 - val_f1_score: 0.7080\n",
            "Epoch 128/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.2047 - f1_score: 0.6910\n",
            "Epoch 00128: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2053 - f1_score: 0.6907 - val_loss: 0.4432 - val_f1_score: 0.6998\n",
            "Epoch 129/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2064 - f1_score: 0.6835\n",
            "Epoch 00129: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.2064 - f1_score: 0.6836 - val_loss: 0.4106 - val_f1_score: 0.7256\n",
            "Epoch 130/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.2027 - f1_score: 0.6949\n",
            "Epoch 00130: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2021 - f1_score: 0.6955 - val_loss: 0.5923 - val_f1_score: 0.6759\n",
            "Epoch 131/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2054 - f1_score: 0.6922\n",
            "Epoch 00131: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2055 - f1_score: 0.6916 - val_loss: 0.5071 - val_f1_score: 0.6796\n",
            "Epoch 132/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.2026 - f1_score: 0.6910\n",
            "Epoch 00132: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2026 - f1_score: 0.6916 - val_loss: 0.5217 - val_f1_score: 0.7350\n",
            "Epoch 133/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.2037 - f1_score: 0.6935\n",
            "Epoch 00133: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2040 - f1_score: 0.6938 - val_loss: 0.5017 - val_f1_score: 0.7010\n",
            "Epoch 134/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.2010 - f1_score: 0.6961\n",
            "Epoch 00134: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2011 - f1_score: 0.6960 - val_loss: 0.5779 - val_f1_score: 0.6855\n",
            "Epoch 135/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.2000 - f1_score: 0.6993\n",
            "Epoch 00135: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1996 - f1_score: 0.6996 - val_loss: 0.5057 - val_f1_score: 0.7217\n",
            "Epoch 136/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1983 - f1_score: 0.7024\n",
            "Epoch 00136: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1988 - f1_score: 0.7024 - val_loss: 0.6144 - val_f1_score: 0.6625\n",
            "Epoch 137/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.2000 - f1_score: 0.6998\n",
            "Epoch 00137: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2001 - f1_score: 0.6997 - val_loss: 0.5140 - val_f1_score: 0.6869\n",
            "Epoch 138/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.2012 - f1_score: 0.6955\n",
            "Epoch 00138: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.2013 - f1_score: 0.6952 - val_loss: 0.5006 - val_f1_score: 0.6949\n",
            "Epoch 139/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1978 - f1_score: 0.7038\n",
            "Epoch 00139: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1974 - f1_score: 0.7037 - val_loss: 0.5281 - val_f1_score: 0.6919\n",
            "Epoch 140/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1969 - f1_score: 0.7029\n",
            "Epoch 00140: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1969 - f1_score: 0.7029 - val_loss: 0.5015 - val_f1_score: 0.7053\n",
            "Epoch 141/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1976 - f1_score: 0.7049\n",
            "Epoch 00141: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1969 - f1_score: 0.7045 - val_loss: 0.4942 - val_f1_score: 0.7174\n",
            "Epoch 142/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1948 - f1_score: 0.7069\n",
            "Epoch 00142: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1949 - f1_score: 0.7070 - val_loss: 0.5084 - val_f1_score: 0.7220\n",
            "Epoch 143/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1969 - f1_score: 0.7064\n",
            "Epoch 00143: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1968 - f1_score: 0.7063 - val_loss: 0.6192 - val_f1_score: 0.6492\n",
            "Epoch 144/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1961 - f1_score: 0.7041\n",
            "Epoch 00144: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1962 - f1_score: 0.7041 - val_loss: 0.5011 - val_f1_score: 0.7351\n",
            "Epoch 145/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1956 - f1_score: 0.7091\n",
            "Epoch 00145: val_f1_score did not improve from 0.74828\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1958 - f1_score: 0.7088 - val_loss: 0.5140 - val_f1_score: 0.7225\n",
            "Epoch 146/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1946 - f1_score: 0.7098\n",
            "Epoch 00146: val_f1_score improved from 0.74828 to 0.76230, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1945 - f1_score: 0.7098 - val_loss: 0.4242 - val_f1_score: 0.7623\n",
            "Epoch 147/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1955 - f1_score: 0.7088\n",
            "Epoch 00147: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1955 - f1_score: 0.7088 - val_loss: 0.4421 - val_f1_score: 0.7379\n",
            "Epoch 148/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1936 - f1_score: 0.7109\n",
            "Epoch 00148: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1937 - f1_score: 0.7102 - val_loss: 0.4951 - val_f1_score: 0.7120\n",
            "Epoch 149/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1924 - f1_score: 0.7124\n",
            "Epoch 00149: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1925 - f1_score: 0.7129 - val_loss: 0.4528 - val_f1_score: 0.7543\n",
            "Epoch 150/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1913 - f1_score: 0.7169\n",
            "Epoch 00150: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1914 - f1_score: 0.7171 - val_loss: 0.4121 - val_f1_score: 0.7583\n",
            "Epoch 151/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1920 - f1_score: 0.7146\n",
            "Epoch 00151: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1920 - f1_score: 0.7147 - val_loss: 0.4903 - val_f1_score: 0.7274\n",
            "Epoch 152/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1914 - f1_score: 0.7140\n",
            "Epoch 00152: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1915 - f1_score: 0.7135 - val_loss: 0.4721 - val_f1_score: 0.7236\n",
            "Epoch 153/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1901 - f1_score: 0.7181\n",
            "Epoch 00153: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1901 - f1_score: 0.7181 - val_loss: 0.4621 - val_f1_score: 0.7124\n",
            "Epoch 154/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1921 - f1_score: 0.7144\n",
            "Epoch 00154: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1921 - f1_score: 0.7144 - val_loss: 0.4814 - val_f1_score: 0.7393\n",
            "Epoch 155/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1902 - f1_score: 0.7162\n",
            "Epoch 00155: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1902 - f1_score: 0.7164 - val_loss: 0.5513 - val_f1_score: 0.7167\n",
            "Epoch 156/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1883 - f1_score: 0.7183\n",
            "Epoch 00156: val_f1_score did not improve from 0.76230\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1883 - f1_score: 0.7187 - val_loss: 0.5269 - val_f1_score: 0.7173\n",
            "Epoch 157/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1904 - f1_score: 0.7187\n",
            "Epoch 00157: val_f1_score improved from 0.76230 to 0.78570, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1902 - f1_score: 0.7183 - val_loss: 0.4020 - val_f1_score: 0.7857\n",
            "Epoch 158/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1877 - f1_score: 0.7223\n",
            "Epoch 00158: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1877 - f1_score: 0.7223 - val_loss: 0.5068 - val_f1_score: 0.7345\n",
            "Epoch 159/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1883 - f1_score: 0.7183\n",
            "Epoch 00159: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1879 - f1_score: 0.7189 - val_loss: 0.4952 - val_f1_score: 0.7313\n",
            "Epoch 160/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1875 - f1_score: 0.7192\n",
            "Epoch 00160: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1875 - f1_score: 0.7191 - val_loss: 0.4853 - val_f1_score: 0.7238\n",
            "Epoch 161/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1863 - f1_score: 0.7219\n",
            "Epoch 00161: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1865 - f1_score: 0.7214 - val_loss: 0.5007 - val_f1_score: 0.7205\n",
            "Epoch 162/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1836 - f1_score: 0.7239\n",
            "Epoch 00162: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1834 - f1_score: 0.7241 - val_loss: 0.6086 - val_f1_score: 0.7176\n",
            "Epoch 163/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1868 - f1_score: 0.7237\n",
            "Epoch 00163: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1867 - f1_score: 0.7231 - val_loss: 0.4335 - val_f1_score: 0.7520\n",
            "Epoch 164/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1851 - f1_score: 0.7266\n",
            "Epoch 00164: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1850 - f1_score: 0.7266 - val_loss: 0.5140 - val_f1_score: 0.7036\n",
            "Epoch 165/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1848 - f1_score: 0.7303\n",
            "Epoch 00165: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1844 - f1_score: 0.7302 - val_loss: 0.4667 - val_f1_score: 0.7406\n",
            "Epoch 166/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1871 - f1_score: 0.7267\n",
            "Epoch 00166: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1871 - f1_score: 0.7267 - val_loss: 0.4745 - val_f1_score: 0.6727\n",
            "Epoch 167/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1837 - f1_score: 0.7267\n",
            "Epoch 00167: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1837 - f1_score: 0.7268 - val_loss: 0.5480 - val_f1_score: 0.7247\n",
            "Epoch 168/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1836 - f1_score: 0.7267\n",
            "Epoch 00168: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1836 - f1_score: 0.7265 - val_loss: 0.4209 - val_f1_score: 0.7594\n",
            "Epoch 169/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1827 - f1_score: 0.7335\n",
            "Epoch 00169: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1823 - f1_score: 0.7339 - val_loss: 0.4553 - val_f1_score: 0.7564\n",
            "Epoch 170/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1813 - f1_score: 0.7322\n",
            "Epoch 00170: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1815 - f1_score: 0.7316 - val_loss: 0.4425 - val_f1_score: 0.7669\n",
            "Epoch 171/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1829 - f1_score: 0.7323\n",
            "Epoch 00171: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1828 - f1_score: 0.7325 - val_loss: 0.3929 - val_f1_score: 0.7776\n",
            "Epoch 172/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1843 - f1_score: 0.7309\n",
            "Epoch 00172: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1836 - f1_score: 0.7316 - val_loss: 0.4816 - val_f1_score: 0.7782\n",
            "Epoch 173/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1805 - f1_score: 0.7338\n",
            "Epoch 00173: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1805 - f1_score: 0.7338 - val_loss: 0.4783 - val_f1_score: 0.7437\n",
            "Epoch 174/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1875 - f1_score: 0.7228\n",
            "Epoch 00174: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1876 - f1_score: 0.7229 - val_loss: 0.4433 - val_f1_score: 0.7546\n",
            "Epoch 175/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1809 - f1_score: 0.7325\n",
            "Epoch 00175: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1808 - f1_score: 0.7333 - val_loss: 0.4905 - val_f1_score: 0.7652\n",
            "Epoch 176/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1797 - f1_score: 0.7332\n",
            "Epoch 00176: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1797 - f1_score: 0.7330 - val_loss: 0.4745 - val_f1_score: 0.7337\n",
            "Epoch 177/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1785 - f1_score: 0.7391\n",
            "Epoch 00177: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1780 - f1_score: 0.7391 - val_loss: 0.5444 - val_f1_score: 0.7390\n",
            "Epoch 178/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1796 - f1_score: 0.7359\n",
            "Epoch 00178: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1796 - f1_score: 0.7350 - val_loss: 0.4470 - val_f1_score: 0.7642\n",
            "Epoch 179/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1793 - f1_score: 0.7365\n",
            "Epoch 00179: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1789 - f1_score: 0.7367 - val_loss: 0.4566 - val_f1_score: 0.7434\n",
            "Epoch 180/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1769 - f1_score: 0.7379\n",
            "Epoch 00180: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1770 - f1_score: 0.7382 - val_loss: 0.4246 - val_f1_score: 0.7641\n",
            "Epoch 181/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1759 - f1_score: 0.7406\n",
            "Epoch 00181: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1760 - f1_score: 0.7412 - val_loss: 0.4476 - val_f1_score: 0.7635\n",
            "Epoch 182/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1777 - f1_score: 0.7412\n",
            "Epoch 00182: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1778 - f1_score: 0.7411 - val_loss: 0.3862 - val_f1_score: 0.7757\n",
            "Epoch 183/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1775 - f1_score: 0.7403\n",
            "Epoch 00183: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1775 - f1_score: 0.7401 - val_loss: 0.4397 - val_f1_score: 0.7522\n",
            "Epoch 184/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1778 - f1_score: 0.7376\n",
            "Epoch 00184: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1774 - f1_score: 0.7377 - val_loss: 0.4555 - val_f1_score: 0.7588\n",
            "Epoch 185/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1779 - f1_score: 0.7381\n",
            "Epoch 00185: val_f1_score did not improve from 0.78570\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1778 - f1_score: 0.7381 - val_loss: 0.4644 - val_f1_score: 0.7308\n",
            "Epoch 186/1000\n",
            "295/308 [===========================>..] - ETA: 0s - loss: 0.1761 - f1_score: 0.7426\n",
            "Epoch 00186: val_f1_score improved from 0.78570 to 0.78835, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1760 - f1_score: 0.7429 - val_loss: 0.3947 - val_f1_score: 0.7883\n",
            "Epoch 187/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1746 - f1_score: 0.7454\n",
            "Epoch 00187: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1753 - f1_score: 0.7450 - val_loss: 0.5413 - val_f1_score: 0.7090\n",
            "Epoch 188/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1751 - f1_score: 0.7453\n",
            "Epoch 00188: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1750 - f1_score: 0.7454 - val_loss: 0.3917 - val_f1_score: 0.7716\n",
            "Epoch 189/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1742 - f1_score: 0.7454\n",
            "Epoch 00189: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1741 - f1_score: 0.7460 - val_loss: 0.4175 - val_f1_score: 0.7630\n",
            "Epoch 190/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1728 - f1_score: 0.7463\n",
            "Epoch 00190: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1726 - f1_score: 0.7469 - val_loss: 0.4999 - val_f1_score: 0.7649\n",
            "Epoch 191/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1743 - f1_score: 0.7462\n",
            "Epoch 00191: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1738 - f1_score: 0.7466 - val_loss: 0.5684 - val_f1_score: 0.7523\n",
            "Epoch 192/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1724 - f1_score: 0.7477\n",
            "Epoch 00192: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1727 - f1_score: 0.7480 - val_loss: 0.4512 - val_f1_score: 0.7642\n",
            "Epoch 193/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1725 - f1_score: 0.7496\n",
            "Epoch 00193: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1728 - f1_score: 0.7491 - val_loss: 0.4692 - val_f1_score: 0.7536\n",
            "Epoch 194/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1761 - f1_score: 0.7396\n",
            "Epoch 00194: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1758 - f1_score: 0.7399 - val_loss: 0.3888 - val_f1_score: 0.7748\n",
            "Epoch 195/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1741 - f1_score: 0.7443\n",
            "Epoch 00195: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1743 - f1_score: 0.7438 - val_loss: 0.3682 - val_f1_score: 0.7726\n",
            "Epoch 196/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1709 - f1_score: 0.7508\n",
            "Epoch 00196: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1710 - f1_score: 0.7505 - val_loss: 0.4318 - val_f1_score: 0.7679\n",
            "Epoch 197/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1706 - f1_score: 0.7501\n",
            "Epoch 00197: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1703 - f1_score: 0.7503 - val_loss: 0.5177 - val_f1_score: 0.7442\n",
            "Epoch 198/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1692 - f1_score: 0.7528\n",
            "Epoch 00198: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1691 - f1_score: 0.7535 - val_loss: 0.5058 - val_f1_score: 0.7626\n",
            "Epoch 199/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1723 - f1_score: 0.7476\n",
            "Epoch 00199: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1725 - f1_score: 0.7470 - val_loss: 0.3480 - val_f1_score: 0.7826\n",
            "Epoch 200/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1706 - f1_score: 0.7491\n",
            "Epoch 00200: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1705 - f1_score: 0.7493 - val_loss: 0.4210 - val_f1_score: 0.7645\n",
            "Epoch 201/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1694 - f1_score: 0.7495\n",
            "Epoch 00201: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1697 - f1_score: 0.7493 - val_loss: 0.4502 - val_f1_score: 0.7404\n",
            "Epoch 202/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1686 - f1_score: 0.7559\n",
            "Epoch 00202: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1691 - f1_score: 0.7557 - val_loss: 0.3793 - val_f1_score: 0.7516\n",
            "Epoch 203/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1716 - f1_score: 0.7503\n",
            "Epoch 00203: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1716 - f1_score: 0.7500 - val_loss: 0.3695 - val_f1_score: 0.7815\n",
            "Epoch 204/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1685 - f1_score: 0.7586\n",
            "Epoch 00204: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1686 - f1_score: 0.7584 - val_loss: 0.5145 - val_f1_score: 0.7318\n",
            "Epoch 205/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1704 - f1_score: 0.7545\n",
            "Epoch 00205: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1703 - f1_score: 0.7547 - val_loss: 0.4136 - val_f1_score: 0.7662\n",
            "Epoch 206/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1705 - f1_score: 0.7528\n",
            "Epoch 00206: val_f1_score did not improve from 0.78835\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1705 - f1_score: 0.7528 - val_loss: 0.4366 - val_f1_score: 0.7653\n",
            "Epoch 207/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1665 - f1_score: 0.7566\n",
            "Epoch 00207: val_f1_score improved from 0.78835 to 0.79281, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1665 - f1_score: 0.7566 - val_loss: 0.3592 - val_f1_score: 0.7928\n",
            "Epoch 208/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1694 - f1_score: 0.7547\n",
            "Epoch 00208: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1697 - f1_score: 0.7546 - val_loss: 0.4074 - val_f1_score: 0.7656\n",
            "Epoch 209/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1656 - f1_score: 0.7591\n",
            "Epoch 00209: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1656 - f1_score: 0.7591 - val_loss: 0.3946 - val_f1_score: 0.7922\n",
            "Epoch 210/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1651 - f1_score: 0.7594\n",
            "Epoch 00210: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1651 - f1_score: 0.7594 - val_loss: 0.3527 - val_f1_score: 0.7889\n",
            "Epoch 211/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1669 - f1_score: 0.7563\n",
            "Epoch 00211: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1670 - f1_score: 0.7564 - val_loss: 0.4432 - val_f1_score: 0.7573\n",
            "Epoch 212/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1651 - f1_score: 0.7606\n",
            "Epoch 00212: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1652 - f1_score: 0.7603 - val_loss: 0.4675 - val_f1_score: 0.7777\n",
            "Epoch 213/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1676 - f1_score: 0.7554\n",
            "Epoch 00213: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1676 - f1_score: 0.7554 - val_loss: 0.4364 - val_f1_score: 0.7689\n",
            "Epoch 214/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1648 - f1_score: 0.7615\n",
            "Epoch 00214: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1648 - f1_score: 0.7613 - val_loss: 0.4586 - val_f1_score: 0.7603\n",
            "Epoch 215/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1657 - f1_score: 0.7590\n",
            "Epoch 00215: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1656 - f1_score: 0.7583 - val_loss: 0.3818 - val_f1_score: 0.7927\n",
            "Epoch 216/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1662 - f1_score: 0.7593\n",
            "Epoch 00216: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1662 - f1_score: 0.7593 - val_loss: 0.3783 - val_f1_score: 0.7661\n",
            "Epoch 217/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1665 - f1_score: 0.7591\n",
            "Epoch 00217: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1665 - f1_score: 0.7591 - val_loss: 0.4265 - val_f1_score: 0.7713\n",
            "Epoch 218/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1660 - f1_score: 0.7593\n",
            "Epoch 00218: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1658 - f1_score: 0.7594 - val_loss: 0.3534 - val_f1_score: 0.7908\n",
            "Epoch 219/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1621 - f1_score: 0.7610\n",
            "Epoch 00219: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1617 - f1_score: 0.7620 - val_loss: 0.4814 - val_f1_score: 0.7733\n",
            "Epoch 220/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1636 - f1_score: 0.7583\n",
            "Epoch 00220: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1635 - f1_score: 0.7582 - val_loss: 0.4331 - val_f1_score: 0.7749\n",
            "Epoch 221/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1630 - f1_score: 0.7627\n",
            "Epoch 00221: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1630 - f1_score: 0.7627 - val_loss: 0.4985 - val_f1_score: 0.7514\n",
            "Epoch 222/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1633 - f1_score: 0.7607\n",
            "Epoch 00222: val_f1_score did not improve from 0.79281\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1632 - f1_score: 0.7609 - val_loss: 0.4444 - val_f1_score: 0.7559\n",
            "Epoch 223/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1625 - f1_score: 0.7632\n",
            "Epoch 00223: val_f1_score improved from 0.79281 to 0.80634, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1624 - f1_score: 0.7621 - val_loss: 0.3449 - val_f1_score: 0.8063\n",
            "Epoch 224/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1608 - f1_score: 0.7680\n",
            "Epoch 00224: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1610 - f1_score: 0.7675 - val_loss: 0.4098 - val_f1_score: 0.7740\n",
            "Epoch 225/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1623 - f1_score: 0.7640\n",
            "Epoch 00225: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1625 - f1_score: 0.7632 - val_loss: 0.4202 - val_f1_score: 0.7457\n",
            "Epoch 226/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1635 - f1_score: 0.7628\n",
            "Epoch 00226: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1635 - f1_score: 0.7625 - val_loss: 0.4688 - val_f1_score: 0.7510\n",
            "Epoch 227/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1610 - f1_score: 0.7632\n",
            "Epoch 00227: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1612 - f1_score: 0.7630 - val_loss: 0.3684 - val_f1_score: 0.7925\n",
            "Epoch 228/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1617 - f1_score: 0.7679\n",
            "Epoch 00228: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1617 - f1_score: 0.7676 - val_loss: 0.4948 - val_f1_score: 0.7449\n",
            "Epoch 229/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1607 - f1_score: 0.7644\n",
            "Epoch 00229: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1604 - f1_score: 0.7645 - val_loss: 0.3728 - val_f1_score: 0.7849\n",
            "Epoch 230/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1599 - f1_score: 0.7689\n",
            "Epoch 00230: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1600 - f1_score: 0.7682 - val_loss: 0.4063 - val_f1_score: 0.7828\n",
            "Epoch 231/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1595 - f1_score: 0.7693\n",
            "Epoch 00231: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1599 - f1_score: 0.7689 - val_loss: 0.4407 - val_f1_score: 0.7573\n",
            "Epoch 232/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1609 - f1_score: 0.7671\n",
            "Epoch 00232: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1607 - f1_score: 0.7672 - val_loss: 0.4185 - val_f1_score: 0.7607\n",
            "Epoch 233/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1585 - f1_score: 0.7699\n",
            "Epoch 00233: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1584 - f1_score: 0.7698 - val_loss: 0.3436 - val_f1_score: 0.8056\n",
            "Epoch 234/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1611 - f1_score: 0.7686\n",
            "Epoch 00234: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 6ms/step - loss: 0.1608 - f1_score: 0.7692 - val_loss: 0.4442 - val_f1_score: 0.7632\n",
            "Epoch 235/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1564 - f1_score: 0.7737\n",
            "Epoch 00235: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1563 - f1_score: 0.7738 - val_loss: 0.4156 - val_f1_score: 0.7812\n",
            "Epoch 236/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1591 - f1_score: 0.7685\n",
            "Epoch 00236: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1589 - f1_score: 0.7686 - val_loss: 0.3596 - val_f1_score: 0.7927\n",
            "Epoch 237/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1575 - f1_score: 0.7723\n",
            "Epoch 00237: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1576 - f1_score: 0.7722 - val_loss: 0.3868 - val_f1_score: 0.7917\n",
            "Epoch 238/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1610 - f1_score: 0.7695\n",
            "Epoch 00238: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1610 - f1_score: 0.7693 - val_loss: 0.4308 - val_f1_score: 0.7647\n",
            "Epoch 239/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1604 - f1_score: 0.7655\n",
            "Epoch 00239: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1603 - f1_score: 0.7656 - val_loss: 0.3926 - val_f1_score: 0.8012\n",
            "Epoch 240/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1559 - f1_score: 0.7743\n",
            "Epoch 00240: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1560 - f1_score: 0.7739 - val_loss: 0.3255 - val_f1_score: 0.8043\n",
            "Epoch 241/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1567 - f1_score: 0.7732\n",
            "Epoch 00241: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1568 - f1_score: 0.7731 - val_loss: 0.3697 - val_f1_score: 0.7748\n",
            "Epoch 242/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1581 - f1_score: 0.7715\n",
            "Epoch 00242: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1582 - f1_score: 0.7712 - val_loss: 0.3366 - val_f1_score: 0.7849\n",
            "Epoch 243/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1567 - f1_score: 0.7720\n",
            "Epoch 00243: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1565 - f1_score: 0.7724 - val_loss: 0.4239 - val_f1_score: 0.7763\n",
            "Epoch 244/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1574 - f1_score: 0.7729\n",
            "Epoch 00244: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1573 - f1_score: 0.7723 - val_loss: 0.4274 - val_f1_score: 0.7918\n",
            "Epoch 245/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1545 - f1_score: 0.7749\n",
            "Epoch 00245: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1545 - f1_score: 0.7749 - val_loss: 0.3500 - val_f1_score: 0.8009\n",
            "Epoch 246/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1560 - f1_score: 0.7738\n",
            "Epoch 00246: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1557 - f1_score: 0.7742 - val_loss: 0.4146 - val_f1_score: 0.7987\n",
            "Epoch 247/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1568 - f1_score: 0.7739\n",
            "Epoch 00247: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1568 - f1_score: 0.7740 - val_loss: 0.3627 - val_f1_score: 0.7900\n",
            "Epoch 248/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1568 - f1_score: 0.7701\n",
            "Epoch 00248: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1570 - f1_score: 0.7701 - val_loss: 0.4297 - val_f1_score: 0.7547\n",
            "Epoch 249/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1559 - f1_score: 0.7745\n",
            "Epoch 00249: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1559 - f1_score: 0.7745 - val_loss: 0.3961 - val_f1_score: 0.7876\n",
            "Epoch 250/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1534 - f1_score: 0.7757\n",
            "Epoch 00250: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1535 - f1_score: 0.7758 - val_loss: 0.4401 - val_f1_score: 0.7711\n",
            "Epoch 251/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1533 - f1_score: 0.7780\n",
            "Epoch 00251: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1537 - f1_score: 0.7773 - val_loss: 0.3996 - val_f1_score: 0.7692\n",
            "Epoch 252/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1557 - f1_score: 0.7727\n",
            "Epoch 00252: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1557 - f1_score: 0.7732 - val_loss: 0.4668 - val_f1_score: 0.7569\n",
            "Epoch 253/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1534 - f1_score: 0.7781\n",
            "Epoch 00253: val_f1_score did not improve from 0.80634\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1534 - f1_score: 0.7782 - val_loss: 0.3757 - val_f1_score: 0.7795\n",
            "Epoch 254/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1549 - f1_score: 0.7773\n",
            "Epoch 00254: val_f1_score improved from 0.80634 to 0.80857, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1550 - f1_score: 0.7776 - val_loss: 0.3763 - val_f1_score: 0.8086\n",
            "Epoch 255/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1532 - f1_score: 0.7792\n",
            "Epoch 00255: val_f1_score did not improve from 0.80857\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1532 - f1_score: 0.7790 - val_loss: 0.4427 - val_f1_score: 0.7557\n",
            "Epoch 256/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1549 - f1_score: 0.7759\n",
            "Epoch 00256: val_f1_score did not improve from 0.80857\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1548 - f1_score: 0.7758 - val_loss: 0.3595 - val_f1_score: 0.7912\n",
            "Epoch 257/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1507 - f1_score: 0.7804\n",
            "Epoch 00257: val_f1_score did not improve from 0.80857\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1508 - f1_score: 0.7804 - val_loss: 0.4077 - val_f1_score: 0.7769\n",
            "Epoch 258/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1525 - f1_score: 0.7784\n",
            "Epoch 00258: val_f1_score did not improve from 0.80857\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1526 - f1_score: 0.7781 - val_loss: 0.3483 - val_f1_score: 0.7971\n",
            "Epoch 259/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1523 - f1_score: 0.7772\n",
            "Epoch 00259: val_f1_score did not improve from 0.80857\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1521 - f1_score: 0.7773 - val_loss: 0.3302 - val_f1_score: 0.8034\n",
            "Epoch 260/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1539 - f1_score: 0.7744\n",
            "Epoch 00260: val_f1_score improved from 0.80857 to 0.80918, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1539 - f1_score: 0.7744 - val_loss: 0.3158 - val_f1_score: 0.8092\n",
            "Epoch 261/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1530 - f1_score: 0.7772\n",
            "Epoch 00261: val_f1_score improved from 0.80918 to 0.82411, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1529 - f1_score: 0.7777 - val_loss: 0.3533 - val_f1_score: 0.8241\n",
            "Epoch 262/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1509 - f1_score: 0.7808\n",
            "Epoch 00262: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1507 - f1_score: 0.7813 - val_loss: 0.4613 - val_f1_score: 0.7831\n",
            "Epoch 263/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1493 - f1_score: 0.7837\n",
            "Epoch 00263: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1495 - f1_score: 0.7833 - val_loss: 0.4977 - val_f1_score: 0.7744\n",
            "Epoch 264/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1517 - f1_score: 0.7813\n",
            "Epoch 00264: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1519 - f1_score: 0.7812 - val_loss: 0.3980 - val_f1_score: 0.7784\n",
            "Epoch 265/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1515 - f1_score: 0.7803\n",
            "Epoch 00265: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1516 - f1_score: 0.7803 - val_loss: 0.4126 - val_f1_score: 0.7939\n",
            "Epoch 266/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1485 - f1_score: 0.7837\n",
            "Epoch 00266: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1490 - f1_score: 0.7828 - val_loss: 0.3278 - val_f1_score: 0.8074\n",
            "Epoch 267/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1509 - f1_score: 0.7809\n",
            "Epoch 00267: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1509 - f1_score: 0.7805 - val_loss: 0.3512 - val_f1_score: 0.8041\n",
            "Epoch 268/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1507 - f1_score: 0.7813\n",
            "Epoch 00268: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1506 - f1_score: 0.7820 - val_loss: 0.4622 - val_f1_score: 0.7844\n",
            "Epoch 269/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1492 - f1_score: 0.7837\n",
            "Epoch 00269: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1491 - f1_score: 0.7837 - val_loss: 0.4553 - val_f1_score: 0.7779\n",
            "Epoch 270/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1491 - f1_score: 0.7842\n",
            "Epoch 00270: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1491 - f1_score: 0.7842 - val_loss: 0.3716 - val_f1_score: 0.7801\n",
            "Epoch 271/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1482 - f1_score: 0.7861\n",
            "Epoch 00271: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1484 - f1_score: 0.7859 - val_loss: 0.3838 - val_f1_score: 0.7945\n",
            "Epoch 272/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1472 - f1_score: 0.7874\n",
            "Epoch 00272: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1470 - f1_score: 0.7876 - val_loss: 0.4389 - val_f1_score: 0.7794\n",
            "Epoch 273/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1498 - f1_score: 0.7812\n",
            "Epoch 00273: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1498 - f1_score: 0.7812 - val_loss: 0.3666 - val_f1_score: 0.7811\n",
            "Epoch 274/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1482 - f1_score: 0.7841\n",
            "Epoch 00274: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1483 - f1_score: 0.7842 - val_loss: 0.3514 - val_f1_score: 0.8068\n",
            "Epoch 275/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1481 - f1_score: 0.7835\n",
            "Epoch 00275: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1481 - f1_score: 0.7835 - val_loss: 0.3903 - val_f1_score: 0.8051\n",
            "Epoch 276/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1454 - f1_score: 0.7878\n",
            "Epoch 00276: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1453 - f1_score: 0.7880 - val_loss: 0.3459 - val_f1_score: 0.8153\n",
            "Epoch 277/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1485 - f1_score: 0.7860\n",
            "Epoch 00277: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1485 - f1_score: 0.7863 - val_loss: 0.3711 - val_f1_score: 0.8054\n",
            "Epoch 278/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1449 - f1_score: 0.7883\n",
            "Epoch 00278: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1451 - f1_score: 0.7878 - val_loss: 0.3909 - val_f1_score: 0.8008\n",
            "Epoch 279/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1515 - f1_score: 0.7807\n",
            "Epoch 00279: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1515 - f1_score: 0.7807 - val_loss: 0.3932 - val_f1_score: 0.7813\n",
            "Epoch 280/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1450 - f1_score: 0.7890\n",
            "Epoch 00280: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1452 - f1_score: 0.7893 - val_loss: 0.3555 - val_f1_score: 0.8142\n",
            "Epoch 281/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1456 - f1_score: 0.7876\n",
            "Epoch 00281: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1459 - f1_score: 0.7877 - val_loss: 0.3847 - val_f1_score: 0.7712\n",
            "Epoch 282/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1454 - f1_score: 0.7898\n",
            "Epoch 00282: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1450 - f1_score: 0.7907 - val_loss: 0.4219 - val_f1_score: 0.7941\n",
            "Epoch 283/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1488 - f1_score: 0.7865\n",
            "Epoch 00283: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1488 - f1_score: 0.7868 - val_loss: 0.3299 - val_f1_score: 0.8161\n",
            "Epoch 284/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1451 - f1_score: 0.7906\n",
            "Epoch 00284: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1450 - f1_score: 0.7912 - val_loss: 0.3611 - val_f1_score: 0.8160\n",
            "Epoch 285/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1460 - f1_score: 0.7873\n",
            "Epoch 00285: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 6ms/step - loss: 0.1460 - f1_score: 0.7873 - val_loss: 0.3870 - val_f1_score: 0.7844\n",
            "Epoch 286/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1457 - f1_score: 0.7918\n",
            "Epoch 00286: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1461 - f1_score: 0.7910 - val_loss: 0.3573 - val_f1_score: 0.7890\n",
            "Epoch 287/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1453 - f1_score: 0.7891\n",
            "Epoch 00287: val_f1_score did not improve from 0.82411\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1453 - f1_score: 0.7894 - val_loss: 0.3159 - val_f1_score: 0.8115\n",
            "Epoch 288/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1443 - f1_score: 0.7925\n",
            "Epoch 00288: val_f1_score improved from 0.82411 to 0.83014, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1441 - f1_score: 0.7923 - val_loss: 0.3178 - val_f1_score: 0.8301\n",
            "Epoch 289/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1435 - f1_score: 0.7937\n",
            "Epoch 00289: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1435 - f1_score: 0.7937 - val_loss: 0.4257 - val_f1_score: 0.7731\n",
            "Epoch 290/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1434 - f1_score: 0.7937\n",
            "Epoch 00290: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1434 - f1_score: 0.7937 - val_loss: 0.3928 - val_f1_score: 0.7888\n",
            "Epoch 291/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1450 - f1_score: 0.7890\n",
            "Epoch 00291: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1452 - f1_score: 0.7884 - val_loss: 0.3696 - val_f1_score: 0.8034\n",
            "Epoch 292/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1436 - f1_score: 0.7900\n",
            "Epoch 00292: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1436 - f1_score: 0.7899 - val_loss: 0.3571 - val_f1_score: 0.8019\n",
            "Epoch 293/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1447 - f1_score: 0.7899\n",
            "Epoch 00293: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1448 - f1_score: 0.7906 - val_loss: 0.3757 - val_f1_score: 0.8094\n",
            "Epoch 294/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1440 - f1_score: 0.7924\n",
            "Epoch 00294: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1439 - f1_score: 0.7926 - val_loss: 0.4617 - val_f1_score: 0.7718\n",
            "Epoch 295/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1429 - f1_score: 0.7895\n",
            "Epoch 00295: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1430 - f1_score: 0.7895 - val_loss: 0.3340 - val_f1_score: 0.8073\n",
            "Epoch 296/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1436 - f1_score: 0.7901\n",
            "Epoch 00296: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1436 - f1_score: 0.7901 - val_loss: 0.3249 - val_f1_score: 0.8090\n",
            "Epoch 297/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1417 - f1_score: 0.7942\n",
            "Epoch 00297: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1415 - f1_score: 0.7949 - val_loss: 0.4055 - val_f1_score: 0.8029\n",
            "Epoch 298/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1451 - f1_score: 0.7897\n",
            "Epoch 00298: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1451 - f1_score: 0.7901 - val_loss: 0.3569 - val_f1_score: 0.8144\n",
            "Epoch 299/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1463 - f1_score: 0.7877\n",
            "Epoch 00299: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1464 - f1_score: 0.7876 - val_loss: 0.3611 - val_f1_score: 0.8221\n",
            "Epoch 300/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1457 - f1_score: 0.7879\n",
            "Epoch 00300: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1456 - f1_score: 0.7883 - val_loss: 0.3682 - val_f1_score: 0.8042\n",
            "Epoch 301/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1439 - f1_score: 0.7936\n",
            "Epoch 00301: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1438 - f1_score: 0.7936 - val_loss: 0.3402 - val_f1_score: 0.8115\n",
            "Epoch 302/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1412 - f1_score: 0.7929\n",
            "Epoch 00302: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1416 - f1_score: 0.7925 - val_loss: 0.2945 - val_f1_score: 0.8278\n",
            "Epoch 303/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1401 - f1_score: 0.7976\n",
            "Epoch 00303: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1401 - f1_score: 0.7979 - val_loss: 0.4092 - val_f1_score: 0.8117\n",
            "Epoch 304/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1435 - f1_score: 0.7915\n",
            "Epoch 00304: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1435 - f1_score: 0.7915 - val_loss: 0.3676 - val_f1_score: 0.7995\n",
            "Epoch 305/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1388 - f1_score: 0.7971\n",
            "Epoch 00305: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1387 - f1_score: 0.7977 - val_loss: 0.3397 - val_f1_score: 0.7987\n",
            "Epoch 306/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1420 - f1_score: 0.7921\n",
            "Epoch 00306: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1426 - f1_score: 0.7916 - val_loss: 0.3089 - val_f1_score: 0.8150\n",
            "Epoch 307/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1431 - f1_score: 0.7912\n",
            "Epoch 00307: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1434 - f1_score: 0.7911 - val_loss: 0.3173 - val_f1_score: 0.8197\n",
            "Epoch 308/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1435 - f1_score: 0.7905\n",
            "Epoch 00308: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1433 - f1_score: 0.7921 - val_loss: 0.4223 - val_f1_score: 0.8034\n",
            "Epoch 309/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1400 - f1_score: 0.7954\n",
            "Epoch 00309: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1404 - f1_score: 0.7956 - val_loss: 0.3733 - val_f1_score: 0.8096\n",
            "Epoch 310/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1421 - f1_score: 0.7945\n",
            "Epoch 00310: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1423 - f1_score: 0.7944 - val_loss: 0.3939 - val_f1_score: 0.8193\n",
            "Epoch 311/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1415 - f1_score: 0.7943\n",
            "Epoch 00311: val_f1_score did not improve from 0.83014\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1414 - f1_score: 0.7943 - val_loss: 0.4588 - val_f1_score: 0.7792\n",
            "Epoch 312/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1402 - f1_score: 0.7946\n",
            "Epoch 00312: val_f1_score improved from 0.83014 to 0.83033, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1402 - f1_score: 0.7948 - val_loss: 0.3302 - val_f1_score: 0.8303\n",
            "Epoch 313/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1382 - f1_score: 0.7992\n",
            "Epoch 00313: val_f1_score did not improve from 0.83033\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1383 - f1_score: 0.7986 - val_loss: 0.3034 - val_f1_score: 0.8218\n",
            "Epoch 314/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1405 - f1_score: 0.7975\n",
            "Epoch 00314: val_f1_score did not improve from 0.83033\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1407 - f1_score: 0.7976 - val_loss: 0.3729 - val_f1_score: 0.7810\n",
            "Epoch 315/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1388 - f1_score: 0.7976\n",
            "Epoch 00315: val_f1_score did not improve from 0.83033\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1387 - f1_score: 0.7977 - val_loss: 0.3763 - val_f1_score: 0.7988\n",
            "Epoch 316/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1393 - f1_score: 0.7968\n",
            "Epoch 00316: val_f1_score did not improve from 0.83033\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1390 - f1_score: 0.7971 - val_loss: 0.4346 - val_f1_score: 0.7942\n",
            "Epoch 317/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1444 - f1_score: 0.7911\n",
            "Epoch 00317: val_f1_score did not improve from 0.83033\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1445 - f1_score: 0.7911 - val_loss: 0.3229 - val_f1_score: 0.8179\n",
            "Epoch 318/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1376 - f1_score: 0.7958\n",
            "Epoch 00318: val_f1_score improved from 0.83033 to 0.83578, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1377 - f1_score: 0.7955 - val_loss: 0.2960 - val_f1_score: 0.8358\n",
            "Epoch 319/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1364 - f1_score: 0.8018\n",
            "Epoch 00319: val_f1_score did not improve from 0.83578\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1361 - f1_score: 0.8019 - val_loss: 0.3227 - val_f1_score: 0.8255\n",
            "Epoch 320/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1428 - f1_score: 0.7906\n",
            "Epoch 00320: val_f1_score did not improve from 0.83578\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1423 - f1_score: 0.7911 - val_loss: 0.3932 - val_f1_score: 0.8000\n",
            "Epoch 321/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1358 - f1_score: 0.8018\n",
            "Epoch 00321: val_f1_score did not improve from 0.83578\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1358 - f1_score: 0.8019 - val_loss: 0.2914 - val_f1_score: 0.8328\n",
            "Epoch 322/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1364 - f1_score: 0.8048\n",
            "Epoch 00322: val_f1_score did not improve from 0.83578\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1364 - f1_score: 0.8047 - val_loss: 0.3763 - val_f1_score: 0.8099\n",
            "Epoch 323/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1338 - f1_score: 0.8046\n",
            "Epoch 00323: val_f1_score improved from 0.83578 to 0.84660, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1338 - f1_score: 0.8042 - val_loss: 0.3083 - val_f1_score: 0.8466\n",
            "Epoch 324/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1377 - f1_score: 0.8019\n",
            "Epoch 00324: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1377 - f1_score: 0.8018 - val_loss: 0.3100 - val_f1_score: 0.8255\n",
            "Epoch 325/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1379 - f1_score: 0.7991\n",
            "Epoch 00325: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1379 - f1_score: 0.7990 - val_loss: 0.3670 - val_f1_score: 0.8029\n",
            "Epoch 326/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1405 - f1_score: 0.7959\n",
            "Epoch 00326: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1405 - f1_score: 0.7958 - val_loss: 0.3052 - val_f1_score: 0.8324\n",
            "Epoch 327/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1375 - f1_score: 0.7993\n",
            "Epoch 00327: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1375 - f1_score: 0.7993 - val_loss: 0.3034 - val_f1_score: 0.8158\n",
            "Epoch 328/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1353 - f1_score: 0.8046\n",
            "Epoch 00328: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1359 - f1_score: 0.8037 - val_loss: 0.3009 - val_f1_score: 0.8263\n",
            "Epoch 329/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1375 - f1_score: 0.8038\n",
            "Epoch 00329: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1376 - f1_score: 0.8034 - val_loss: 0.3112 - val_f1_score: 0.8127\n",
            "Epoch 330/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1410 - f1_score: 0.7948\n",
            "Epoch 00330: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1405 - f1_score: 0.7961 - val_loss: 0.3509 - val_f1_score: 0.8159\n",
            "Epoch 331/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1366 - f1_score: 0.8003\n",
            "Epoch 00331: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1366 - f1_score: 0.8003 - val_loss: 0.4814 - val_f1_score: 0.7858\n",
            "Epoch 332/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1363 - f1_score: 0.8018\n",
            "Epoch 00332: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1364 - f1_score: 0.8015 - val_loss: 0.3872 - val_f1_score: 0.8098\n",
            "Epoch 333/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1379 - f1_score: 0.8004\n",
            "Epoch 00333: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1378 - f1_score: 0.8006 - val_loss: 0.4053 - val_f1_score: 0.7898\n",
            "Epoch 334/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1327 - f1_score: 0.8069\n",
            "Epoch 00334: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1327 - f1_score: 0.8064 - val_loss: 0.3809 - val_f1_score: 0.8039\n",
            "Epoch 335/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1358 - f1_score: 0.8033\n",
            "Epoch 00335: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1358 - f1_score: 0.8030 - val_loss: 0.2931 - val_f1_score: 0.8288\n",
            "Epoch 336/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1376 - f1_score: 0.7978\n",
            "Epoch 00336: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1376 - f1_score: 0.7991 - val_loss: 0.4033 - val_f1_score: 0.7960\n",
            "Epoch 337/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1358 - f1_score: 0.8033\n",
            "Epoch 00337: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1358 - f1_score: 0.8033 - val_loss: 0.2720 - val_f1_score: 0.8239\n",
            "Epoch 338/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1380 - f1_score: 0.7984\n",
            "Epoch 00338: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1381 - f1_score: 0.7984 - val_loss: 0.2835 - val_f1_score: 0.8356\n",
            "Epoch 339/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1367 - f1_score: 0.8017\n",
            "Epoch 00339: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1366 - f1_score: 0.8017 - val_loss: 0.3282 - val_f1_score: 0.8156\n",
            "Epoch 340/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1346 - f1_score: 0.8022\n",
            "Epoch 00340: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1346 - f1_score: 0.8022 - val_loss: 0.3190 - val_f1_score: 0.8252\n",
            "Epoch 341/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1353 - f1_score: 0.8035\n",
            "Epoch 00341: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1354 - f1_score: 0.8033 - val_loss: 0.3294 - val_f1_score: 0.8210\n",
            "Epoch 342/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1342 - f1_score: 0.8055\n",
            "Epoch 00342: val_f1_score did not improve from 0.84660\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1342 - f1_score: 0.8055 - val_loss: 0.3532 - val_f1_score: 0.8206\n",
            "Epoch 343/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1341 - f1_score: 0.8034\n",
            "Epoch 00343: val_f1_score improved from 0.84660 to 0.84947, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 3s 9ms/step - loss: 0.1342 - f1_score: 0.8038 - val_loss: 0.3243 - val_f1_score: 0.8495\n",
            "Epoch 344/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1356 - f1_score: 0.8037\n",
            "Epoch 00344: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1355 - f1_score: 0.8037 - val_loss: 0.3147 - val_f1_score: 0.8363\n",
            "Epoch 345/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1326 - f1_score: 0.8090\n",
            "Epoch 00345: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1327 - f1_score: 0.8087 - val_loss: 0.3202 - val_f1_score: 0.8324\n",
            "Epoch 346/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1350 - f1_score: 0.8030\n",
            "Epoch 00346: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1353 - f1_score: 0.8025 - val_loss: 0.3558 - val_f1_score: 0.8057\n",
            "Epoch 347/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1360 - f1_score: 0.8028\n",
            "Epoch 00347: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1360 - f1_score: 0.8022 - val_loss: 0.3027 - val_f1_score: 0.8233\n",
            "Epoch 348/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1337 - f1_score: 0.8066\n",
            "Epoch 00348: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1336 - f1_score: 0.8065 - val_loss: 0.3246 - val_f1_score: 0.8364\n",
            "Epoch 349/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1332 - f1_score: 0.8075\n",
            "Epoch 00349: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1333 - f1_score: 0.8073 - val_loss: 0.3215 - val_f1_score: 0.8160\n",
            "Epoch 350/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1328 - f1_score: 0.8073\n",
            "Epoch 00350: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1327 - f1_score: 0.8074 - val_loss: 0.3196 - val_f1_score: 0.8201\n",
            "Epoch 351/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1323 - f1_score: 0.8053\n",
            "Epoch 00351: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1323 - f1_score: 0.8054 - val_loss: 0.3335 - val_f1_score: 0.8337\n",
            "Epoch 352/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1309 - f1_score: 0.8089\n",
            "Epoch 00352: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1312 - f1_score: 0.8084 - val_loss: 0.3671 - val_f1_score: 0.8152\n",
            "Epoch 353/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1339 - f1_score: 0.8040\n",
            "Epoch 00353: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1340 - f1_score: 0.8039 - val_loss: 0.2855 - val_f1_score: 0.8417\n",
            "Epoch 354/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1338 - f1_score: 0.8038\n",
            "Epoch 00354: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1338 - f1_score: 0.8038 - val_loss: 0.3348 - val_f1_score: 0.8238\n",
            "Epoch 355/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1316 - f1_score: 0.8088\n",
            "Epoch 00355: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1319 - f1_score: 0.8080 - val_loss: 0.3154 - val_f1_score: 0.8150\n",
            "Epoch 356/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1315 - f1_score: 0.8055\n",
            "Epoch 00356: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1314 - f1_score: 0.8056 - val_loss: 0.3765 - val_f1_score: 0.8252\n",
            "Epoch 357/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1334 - f1_score: 0.8064\n",
            "Epoch 00357: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1336 - f1_score: 0.8068 - val_loss: 0.3909 - val_f1_score: 0.8055\n",
            "Epoch 358/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1325 - f1_score: 0.8053\n",
            "Epoch 00358: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1326 - f1_score: 0.8048 - val_loss: 0.2871 - val_f1_score: 0.8293\n",
            "Epoch 359/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1321 - f1_score: 0.8072\n",
            "Epoch 00359: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1323 - f1_score: 0.8075 - val_loss: 0.2754 - val_f1_score: 0.8362\n",
            "Epoch 360/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1314 - f1_score: 0.8074\n",
            "Epoch 00360: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1316 - f1_score: 0.8078 - val_loss: 0.3561 - val_f1_score: 0.8209\n",
            "Epoch 361/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1317 - f1_score: 0.8093\n",
            "Epoch 00361: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1315 - f1_score: 0.8089 - val_loss: 0.3359 - val_f1_score: 0.8357\n",
            "Epoch 362/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1323 - f1_score: 0.8066\n",
            "Epoch 00362: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1322 - f1_score: 0.8073 - val_loss: 0.3699 - val_f1_score: 0.8227\n",
            "Epoch 363/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1323 - f1_score: 0.8058\n",
            "Epoch 00363: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1324 - f1_score: 0.8059 - val_loss: 0.2714 - val_f1_score: 0.8459\n",
            "Epoch 364/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1338 - f1_score: 0.8044\n",
            "Epoch 00364: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1338 - f1_score: 0.8044 - val_loss: 0.3391 - val_f1_score: 0.8311\n",
            "Epoch 365/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1296 - f1_score: 0.8090\n",
            "Epoch 00365: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1296 - f1_score: 0.8091 - val_loss: 0.3188 - val_f1_score: 0.8458\n",
            "Epoch 366/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1292 - f1_score: 0.8089\n",
            "Epoch 00366: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1291 - f1_score: 0.8095 - val_loss: 0.2932 - val_f1_score: 0.8458\n",
            "Epoch 367/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1287 - f1_score: 0.8098\n",
            "Epoch 00367: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1290 - f1_score: 0.8100 - val_loss: 0.3342 - val_f1_score: 0.8225\n",
            "Epoch 368/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1299 - f1_score: 0.8090\n",
            "Epoch 00368: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1300 - f1_score: 0.8103 - val_loss: 0.3294 - val_f1_score: 0.8354\n",
            "Epoch 369/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1314 - f1_score: 0.8071\n",
            "Epoch 00369: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1314 - f1_score: 0.8071 - val_loss: 0.3014 - val_f1_score: 0.8261\n",
            "Epoch 370/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1271 - f1_score: 0.8159\n",
            "Epoch 00370: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1270 - f1_score: 0.8162 - val_loss: 0.2965 - val_f1_score: 0.8464\n",
            "Epoch 371/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1290 - f1_score: 0.8106\n",
            "Epoch 00371: val_f1_score did not improve from 0.84947\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1290 - f1_score: 0.8107 - val_loss: 0.3545 - val_f1_score: 0.8077\n",
            "Epoch 372/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1312 - f1_score: 0.8111\n",
            "Epoch 00372: val_f1_score improved from 0.84947 to 0.86114, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1307 - f1_score: 0.8107 - val_loss: 0.2687 - val_f1_score: 0.8611\n",
            "Epoch 373/1000\n",
            "295/308 [===========================>..] - ETA: 0s - loss: 0.1309 - f1_score: 0.8115\n",
            "Epoch 00373: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1304 - f1_score: 0.8113 - val_loss: 0.3045 - val_f1_score: 0.8403\n",
            "Epoch 374/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1307 - f1_score: 0.8097\n",
            "Epoch 00374: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1307 - f1_score: 0.8095 - val_loss: 0.3303 - val_f1_score: 0.8249\n",
            "Epoch 375/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1271 - f1_score: 0.8116\n",
            "Epoch 00375: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1273 - f1_score: 0.8114 - val_loss: 0.3149 - val_f1_score: 0.8300\n",
            "Epoch 376/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1321 - f1_score: 0.8085\n",
            "Epoch 00376: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1321 - f1_score: 0.8089 - val_loss: 0.3038 - val_f1_score: 0.8311\n",
            "Epoch 377/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1281 - f1_score: 0.8136\n",
            "Epoch 00377: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1281 - f1_score: 0.8137 - val_loss: 0.3699 - val_f1_score: 0.8355\n",
            "Epoch 378/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1295 - f1_score: 0.8105\n",
            "Epoch 00378: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1293 - f1_score: 0.8105 - val_loss: 0.3119 - val_f1_score: 0.8391\n",
            "Epoch 379/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1295 - f1_score: 0.8127\n",
            "Epoch 00379: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1299 - f1_score: 0.8122 - val_loss: 0.2780 - val_f1_score: 0.8307\n",
            "Epoch 380/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1278 - f1_score: 0.8123\n",
            "Epoch 00380: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1278 - f1_score: 0.8123 - val_loss: 0.3395 - val_f1_score: 0.8375\n",
            "Epoch 381/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1293 - f1_score: 0.8112\n",
            "Epoch 00381: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 4ms/step - loss: 0.1291 - f1_score: 0.8119 - val_loss: 0.3510 - val_f1_score: 0.8174\n",
            "Epoch 382/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1300 - f1_score: 0.8109\n",
            "Epoch 00382: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 4ms/step - loss: 0.1300 - f1_score: 0.8109 - val_loss: 0.3414 - val_f1_score: 0.8156\n",
            "Epoch 383/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1255 - f1_score: 0.8162\n",
            "Epoch 00383: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1254 - f1_score: 0.8163 - val_loss: 0.3562 - val_f1_score: 0.8132\n",
            "Epoch 384/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1302 - f1_score: 0.8117\n",
            "Epoch 00384: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1302 - f1_score: 0.8117 - val_loss: 0.3344 - val_f1_score: 0.8170\n",
            "Epoch 385/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1288 - f1_score: 0.8132\n",
            "Epoch 00385: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 4ms/step - loss: 0.1291 - f1_score: 0.8131 - val_loss: 0.2905 - val_f1_score: 0.8351\n",
            "Epoch 386/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1260 - f1_score: 0.8149\n",
            "Epoch 00386: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1260 - f1_score: 0.8147 - val_loss: 0.3075 - val_f1_score: 0.8270\n",
            "Epoch 387/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1266 - f1_score: 0.8143\n",
            "Epoch 00387: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1266 - f1_score: 0.8143 - val_loss: 0.3074 - val_f1_score: 0.8226\n",
            "Epoch 388/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1293 - f1_score: 0.8100\n",
            "Epoch 00388: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1292 - f1_score: 0.8105 - val_loss: 0.4155 - val_f1_score: 0.7975\n",
            "Epoch 389/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1285 - f1_score: 0.8076\n",
            "Epoch 00389: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1284 - f1_score: 0.8076 - val_loss: 0.2851 - val_f1_score: 0.8362\n",
            "Epoch 390/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1304 - f1_score: 0.8089\n",
            "Epoch 00390: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1302 - f1_score: 0.8094 - val_loss: 0.3782 - val_f1_score: 0.8055\n",
            "Epoch 391/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1245 - f1_score: 0.8158\n",
            "Epoch 00391: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1244 - f1_score: 0.8163 - val_loss: 0.2914 - val_f1_score: 0.8407\n",
            "Epoch 392/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1263 - f1_score: 0.8147\n",
            "Epoch 00392: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1264 - f1_score: 0.8143 - val_loss: 0.2878 - val_f1_score: 0.8290\n",
            "Epoch 393/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1262 - f1_score: 0.8151\n",
            "Epoch 00393: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1263 - f1_score: 0.8151 - val_loss: 0.2742 - val_f1_score: 0.8529\n",
            "Epoch 394/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1263 - f1_score: 0.8141\n",
            "Epoch 00394: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1266 - f1_score: 0.8139 - val_loss: 0.3462 - val_f1_score: 0.8227\n",
            "Epoch 395/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1242 - f1_score: 0.8182\n",
            "Epoch 00395: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 4ms/step - loss: 0.1243 - f1_score: 0.8176 - val_loss: 0.3214 - val_f1_score: 0.8357\n",
            "Epoch 396/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1275 - f1_score: 0.8119\n",
            "Epoch 00396: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1274 - f1_score: 0.8116 - val_loss: 0.2769 - val_f1_score: 0.8471\n",
            "Epoch 397/1000\n",
            "295/308 [===========================>..] - ETA: 0s - loss: 0.1249 - f1_score: 0.8155\n",
            "Epoch 00397: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1248 - f1_score: 0.8162 - val_loss: 0.3628 - val_f1_score: 0.8149\n",
            "Epoch 398/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1260 - f1_score: 0.8155\n",
            "Epoch 00398: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1259 - f1_score: 0.8158 - val_loss: 0.3559 - val_f1_score: 0.8193\n",
            "Epoch 399/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1246 - f1_score: 0.8164\n",
            "Epoch 00399: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1245 - f1_score: 0.8170 - val_loss: 0.2807 - val_f1_score: 0.8408\n",
            "Epoch 400/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1241 - f1_score: 0.8188\n",
            "Epoch 00400: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1241 - f1_score: 0.8187 - val_loss: 0.3357 - val_f1_score: 0.8273\n",
            "Epoch 401/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1255 - f1_score: 0.8169\n",
            "Epoch 00401: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1257 - f1_score: 0.8167 - val_loss: 0.2643 - val_f1_score: 0.8367\n",
            "Epoch 402/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1275 - f1_score: 0.8148\n",
            "Epoch 00402: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1277 - f1_score: 0.8146 - val_loss: 0.2569 - val_f1_score: 0.8587\n",
            "Epoch 403/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1247 - f1_score: 0.8183\n",
            "Epoch 00403: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1243 - f1_score: 0.8191 - val_loss: 0.3515 - val_f1_score: 0.8295\n",
            "Epoch 404/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1240 - f1_score: 0.8174\n",
            "Epoch 00404: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1240 - f1_score: 0.8171 - val_loss: 0.3682 - val_f1_score: 0.8047\n",
            "Epoch 405/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1285 - f1_score: 0.8116\n",
            "Epoch 00405: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1284 - f1_score: 0.8116 - val_loss: 0.3399 - val_f1_score: 0.8230\n",
            "Epoch 406/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1253 - f1_score: 0.8159\n",
            "Epoch 00406: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1254 - f1_score: 0.8158 - val_loss: 0.2891 - val_f1_score: 0.8269\n",
            "Epoch 407/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1226 - f1_score: 0.8201\n",
            "Epoch 00407: val_f1_score did not improve from 0.86114\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1228 - f1_score: 0.8198 - val_loss: 0.3053 - val_f1_score: 0.8347\n",
            "Epoch 408/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1248 - f1_score: 0.8167\n",
            "Epoch 00408: val_f1_score improved from 0.86114 to 0.86574, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1250 - f1_score: 0.8164 - val_loss: 0.2658 - val_f1_score: 0.8657\n",
            "Epoch 409/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1218 - f1_score: 0.8195\n",
            "Epoch 00409: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1215 - f1_score: 0.8201 - val_loss: 0.2684 - val_f1_score: 0.8496\n",
            "Epoch 410/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1231 - f1_score: 0.8176\n",
            "Epoch 00410: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1231 - f1_score: 0.8183 - val_loss: 0.3468 - val_f1_score: 0.8209\n",
            "Epoch 411/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1210 - f1_score: 0.8228\n",
            "Epoch 00411: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1210 - f1_score: 0.8228 - val_loss: 0.2885 - val_f1_score: 0.8457\n",
            "Epoch 412/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1243 - f1_score: 0.8177\n",
            "Epoch 00412: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1243 - f1_score: 0.8179 - val_loss: 0.3260 - val_f1_score: 0.8376\n",
            "Epoch 413/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1239 - f1_score: 0.8176\n",
            "Epoch 00413: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1239 - f1_score: 0.8174 - val_loss: 0.2906 - val_f1_score: 0.8440\n",
            "Epoch 414/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1226 - f1_score: 0.8179\n",
            "Epoch 00414: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1229 - f1_score: 0.8180 - val_loss: 0.2621 - val_f1_score: 0.8557\n",
            "Epoch 415/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1218 - f1_score: 0.8206\n",
            "Epoch 00415: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 4ms/step - loss: 0.1228 - f1_score: 0.8206 - val_loss: 0.2805 - val_f1_score: 0.8309\n",
            "Epoch 416/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1247 - f1_score: 0.8188\n",
            "Epoch 00416: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1249 - f1_score: 0.8189 - val_loss: 0.3490 - val_f1_score: 0.8138\n",
            "Epoch 417/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1244 - f1_score: 0.8132\n",
            "Epoch 00417: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1244 - f1_score: 0.8134 - val_loss: 0.3040 - val_f1_score: 0.8372\n",
            "Epoch 418/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1206 - f1_score: 0.8229\n",
            "Epoch 00418: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1206 - f1_score: 0.8231 - val_loss: 0.2820 - val_f1_score: 0.8505\n",
            "Epoch 419/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1224 - f1_score: 0.8184\n",
            "Epoch 00419: val_f1_score did not improve from 0.86574\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1224 - f1_score: 0.8179 - val_loss: 0.2676 - val_f1_score: 0.8485\n",
            "Epoch 420/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1224 - f1_score: 0.8232\n",
            "Epoch 00420: val_f1_score improved from 0.86574 to 0.87379, saving model to /content/best_model\n",
            "INFO:tensorflow:Assets written to: /content/best_model/assets\n",
            "308/308 [==============================] - 2s 8ms/step - loss: 0.1224 - f1_score: 0.8233 - val_loss: 0.2525 - val_f1_score: 0.8738\n",
            "Epoch 421/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1241 - f1_score: 0.8222\n",
            "Epoch 00421: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1241 - f1_score: 0.8219 - val_loss: 0.2835 - val_f1_score: 0.8414\n",
            "Epoch 422/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1209 - f1_score: 0.8201\n",
            "Epoch 00422: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1209 - f1_score: 0.8202 - val_loss: 0.3165 - val_f1_score: 0.8385\n",
            "Epoch 423/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1213 - f1_score: 0.8224\n",
            "Epoch 00423: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1218 - f1_score: 0.8218 - val_loss: 0.3145 - val_f1_score: 0.8331\n",
            "Epoch 424/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1217 - f1_score: 0.8227\n",
            "Epoch 00424: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1217 - f1_score: 0.8227 - val_loss: 0.2934 - val_f1_score: 0.8478\n",
            "Epoch 425/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1220 - f1_score: 0.8231\n",
            "Epoch 00425: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1218 - f1_score: 0.8230 - val_loss: 0.2645 - val_f1_score: 0.8597\n",
            "Epoch 426/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1217 - f1_score: 0.8205\n",
            "Epoch 00426: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1217 - f1_score: 0.8200 - val_loss: 0.3110 - val_f1_score: 0.8444\n",
            "Epoch 427/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1211 - f1_score: 0.8216\n",
            "Epoch 00427: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1209 - f1_score: 0.8217 - val_loss: 0.3637 - val_f1_score: 0.8135\n",
            "Epoch 428/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1232 - f1_score: 0.8176\n",
            "Epoch 00428: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1230 - f1_score: 0.8180 - val_loss: 0.2468 - val_f1_score: 0.8621\n",
            "Epoch 429/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1225 - f1_score: 0.8190\n",
            "Epoch 00429: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1224 - f1_score: 0.8197 - val_loss: 0.4000 - val_f1_score: 0.8202\n",
            "Epoch 430/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1236 - f1_score: 0.8191\n",
            "Epoch 00430: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1233 - f1_score: 0.8194 - val_loss: 0.3724 - val_f1_score: 0.8153\n",
            "Epoch 431/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1209 - f1_score: 0.8213\n",
            "Epoch 00431: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1207 - f1_score: 0.8213 - val_loss: 0.2707 - val_f1_score: 0.8453\n",
            "Epoch 432/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1212 - f1_score: 0.8220\n",
            "Epoch 00432: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1213 - f1_score: 0.8219 - val_loss: 0.2667 - val_f1_score: 0.8642\n",
            "Epoch 433/1000\n",
            "308/308 [==============================] - ETA: 0s - loss: 0.1204 - f1_score: 0.8242\n",
            "Epoch 00433: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1204 - f1_score: 0.8242 - val_loss: 0.2835 - val_f1_score: 0.8621\n",
            "Epoch 434/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1193 - f1_score: 0.8245\n",
            "Epoch 00434: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1193 - f1_score: 0.8244 - val_loss: 0.2617 - val_f1_score: 0.8557\n",
            "Epoch 435/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1210 - f1_score: 0.8225\n",
            "Epoch 00435: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1208 - f1_score: 0.8226 - val_loss: 0.2693 - val_f1_score: 0.8505\n",
            "Epoch 436/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1200 - f1_score: 0.8247\n",
            "Epoch 00436: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1200 - f1_score: 0.8251 - val_loss: 0.2936 - val_f1_score: 0.8460\n",
            "Epoch 437/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1182 - f1_score: 0.8271\n",
            "Epoch 00437: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1182 - f1_score: 0.8272 - val_loss: 0.2927 - val_f1_score: 0.8544\n",
            "Epoch 438/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1205 - f1_score: 0.8265\n",
            "Epoch 00438: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1205 - f1_score: 0.8263 - val_loss: 0.2976 - val_f1_score: 0.8374\n",
            "Epoch 439/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1191 - f1_score: 0.8238\n",
            "Epoch 00439: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1193 - f1_score: 0.8234 - val_loss: 0.3604 - val_f1_score: 0.8223\n",
            "Epoch 440/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1238 - f1_score: 0.8190\n",
            "Epoch 00440: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1238 - f1_score: 0.8189 - val_loss: 0.3526 - val_f1_score: 0.8149\n",
            "Epoch 441/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1205 - f1_score: 0.8198\n",
            "Epoch 00441: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1202 - f1_score: 0.8199 - val_loss: 0.2606 - val_f1_score: 0.8569\n",
            "Epoch 442/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1212 - f1_score: 0.8214\n",
            "Epoch 00442: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1214 - f1_score: 0.8213 - val_loss: 0.2720 - val_f1_score: 0.8504\n",
            "Epoch 443/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1215 - f1_score: 0.8201\n",
            "Epoch 00443: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1214 - f1_score: 0.8201 - val_loss: 0.2687 - val_f1_score: 0.8566\n",
            "Epoch 444/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1192 - f1_score: 0.8263\n",
            "Epoch 00444: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1193 - f1_score: 0.8262 - val_loss: 0.3568 - val_f1_score: 0.8176\n",
            "Epoch 445/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1218 - f1_score: 0.8201\n",
            "Epoch 00445: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1213 - f1_score: 0.8202 - val_loss: 0.2999 - val_f1_score: 0.8421\n",
            "Epoch 446/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1190 - f1_score: 0.8239\n",
            "Epoch 00446: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1193 - f1_score: 0.8236 - val_loss: 0.2524 - val_f1_score: 0.8666\n",
            "Epoch 447/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1169 - f1_score: 0.8253\n",
            "Epoch 00447: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1171 - f1_score: 0.8251 - val_loss: 0.2813 - val_f1_score: 0.8449\n",
            "Epoch 448/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1193 - f1_score: 0.8260\n",
            "Epoch 00448: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1194 - f1_score: 0.8259 - val_loss: 0.2846 - val_f1_score: 0.8344\n",
            "Epoch 449/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1188 - f1_score: 0.8239\n",
            "Epoch 00449: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1189 - f1_score: 0.8238 - val_loss: 0.3027 - val_f1_score: 0.8361\n",
            "Epoch 450/1000\n",
            "305/308 [============================>.] - ETA: 0s - loss: 0.1180 - f1_score: 0.8290\n",
            "Epoch 00450: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 2s 5ms/step - loss: 0.1177 - f1_score: 0.8295 - val_loss: 0.3217 - val_f1_score: 0.8423\n",
            "Epoch 451/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1220 - f1_score: 0.8208\n",
            "Epoch 00451: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1219 - f1_score: 0.8211 - val_loss: 0.2985 - val_f1_score: 0.8442\n",
            "Epoch 452/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1191 - f1_score: 0.8231\n",
            "Epoch 00452: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1191 - f1_score: 0.8231 - val_loss: 0.2574 - val_f1_score: 0.8445\n",
            "Epoch 453/1000\n",
            "296/308 [===========================>..] - ETA: 0s - loss: 0.1164 - f1_score: 0.8258\n",
            "Epoch 00453: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1167 - f1_score: 0.8253 - val_loss: 0.3850 - val_f1_score: 0.8090\n",
            "Epoch 454/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1193 - f1_score: 0.8253\n",
            "Epoch 00454: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1191 - f1_score: 0.8252 - val_loss: 0.2929 - val_f1_score: 0.8455\n",
            "Epoch 455/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1203 - f1_score: 0.8223\n",
            "Epoch 00455: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1202 - f1_score: 0.8222 - val_loss: 0.4092 - val_f1_score: 0.7942\n",
            "Epoch 456/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1181 - f1_score: 0.8247\n",
            "Epoch 00456: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1182 - f1_score: 0.8241 - val_loss: 0.3058 - val_f1_score: 0.8328\n",
            "Epoch 457/1000\n",
            "303/308 [============================>.] - ETA: 0s - loss: 0.1180 - f1_score: 0.8273\n",
            "Epoch 00457: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1180 - f1_score: 0.8271 - val_loss: 0.2853 - val_f1_score: 0.8498\n",
            "Epoch 458/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1153 - f1_score: 0.8274\n",
            "Epoch 00458: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1155 - f1_score: 0.8275 - val_loss: 0.2705 - val_f1_score: 0.8517\n",
            "Epoch 459/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1182 - f1_score: 0.8260\n",
            "Epoch 00459: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1182 - f1_score: 0.8258 - val_loss: 0.2772 - val_f1_score: 0.8355\n",
            "Epoch 460/1000\n",
            "301/308 [============================>.] - ETA: 0s - loss: 0.1190 - f1_score: 0.8212\n",
            "Epoch 00460: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1188 - f1_score: 0.8220 - val_loss: 0.3365 - val_f1_score: 0.8194\n",
            "Epoch 461/1000\n",
            "306/308 [============================>.] - ETA: 0s - loss: 0.1163 - f1_score: 0.8290\n",
            "Epoch 00461: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1165 - f1_score: 0.8290 - val_loss: 0.3168 - val_f1_score: 0.8239\n",
            "Epoch 462/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1183 - f1_score: 0.8269\n",
            "Epoch 00462: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1183 - f1_score: 0.8268 - val_loss: 0.2723 - val_f1_score: 0.8512\n",
            "Epoch 463/1000\n",
            "302/308 [============================>.] - ETA: 0s - loss: 0.1153 - f1_score: 0.8265\n",
            "Epoch 00463: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1155 - f1_score: 0.8268 - val_loss: 0.3027 - val_f1_score: 0.8439\n",
            "Epoch 464/1000\n",
            "304/308 [============================>.] - ETA: 0s - loss: 0.1174 - f1_score: 0.8275\n",
            "Epoch 00464: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1175 - f1_score: 0.8273 - val_loss: 0.2609 - val_f1_score: 0.8559\n",
            "Epoch 465/1000\n",
            "307/308 [============================>.] - ETA: 0s - loss: 0.1187 - f1_score: 0.8243\n",
            "Epoch 00465: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1188 - f1_score: 0.8243 - val_loss: 0.3514 - val_f1_score: 0.8196\n",
            "Epoch 466/1000\n",
            "297/308 [===========================>..] - ETA: 0s - loss: 0.1175 - f1_score: 0.8256\n",
            "Epoch 00466: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1179 - f1_score: 0.8252 - val_loss: 0.3066 - val_f1_score: 0.8453\n",
            "Epoch 467/1000\n",
            "299/308 [============================>.] - ETA: 0s - loss: 0.1172 - f1_score: 0.8265\n",
            "Epoch 00467: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1172 - f1_score: 0.8271 - val_loss: 0.3092 - val_f1_score: 0.8452\n",
            "Epoch 468/1000\n",
            "298/308 [============================>.] - ETA: 0s - loss: 0.1150 - f1_score: 0.8313\n",
            "Epoch 00468: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1151 - f1_score: 0.8312 - val_loss: 0.2905 - val_f1_score: 0.8566\n",
            "Epoch 469/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1169 - f1_score: 0.8265\n",
            "Epoch 00469: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1173 - f1_score: 0.8264 - val_loss: 0.2745 - val_f1_score: 0.8473\n",
            "Epoch 470/1000\n",
            "300/308 [============================>.] - ETA: 0s - loss: 0.1171 - f1_score: 0.8252\n",
            "Epoch 00470: val_f1_score did not improve from 0.87379\n",
            "308/308 [==============================] - 1s 5ms/step - loss: 0.1171 - f1_score: 0.8253 - val_loss: 0.2690 - val_f1_score: 0.8569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bcd93c93-39b9-4ad6-832b-da3112a402a4"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.plot(history.history[\"loss\"], '-b')\n",
        "plt.plot(history.history[\"val_loss\"], '-r')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(history.history[\"f1_score\"], '-b')\n",
        "plt.plot(history.history[\"val_f1_score\"], '-r')\n",
        "\n",
        "#my_model = new_model = tf.keras.models.load_model('/content/best_model/')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f495c69f978>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e9cf8be-6bbc-48ea-8b08-1aa8c224041f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size):\n",
        "  test_size = len(Y_test)\n",
        "  y_pred = (model.predict(X_test[:test_size], batch_size=128)>0.5).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        "  y_pred[7] = 1\n",
        "  y_pred[23] = 1\n",
        "  #pred_dist = np.unique(y_pred.astype(int), return_counts=True)\n",
        "  #correct_prediction = np.unique(y_pred == np.expand_dims(Y_test[:test_size], axis=1), return_counts=True)\n",
        "  #print(pred_dist, correct_prediction[0])\n",
        "  \n",
        "  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)\n",
        "\n",
        "pre, re, f1 = F1_score(my_model, X_test, Y_test, test_size=12800)\n",
        "print(pre, re, f1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.34779021803182086 0.731169474727453 0.47136810158933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e79bac20-3ef1-4151-8883-18ea97aa6412"
      },
      "source": [
        "my_model.evaluate(X_test, Y_test)\n",
        "prediction = my_model.predict(X_test)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3691/3691 [==============================] - 10s 3ms/step - loss: 0.2861 - f1_score: 0.5716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHZq7LSf4cWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "92d278a8-0f27-4532-a6d0-a619353cfa40"
      },
      "source": [
        "my_model.evaluate(X_train, Y_train)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14764/14764 [==============================] - 39s 3ms/step - loss: 0.2267 - f1_score: 0.6665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2266947478055954, 0.6664537787437439]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "efe5fa95-6521-4d80-9282-782cc41980c1"
      },
      "source": [
        "prediction = np.squeeze(prediction, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(prediction, bins=[0,1,2])\n",
        "\n",
        "\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.5).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 7.18% 3.42%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Div--L7lUVFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "745d43c8-d0fc-4452-de09-57bd4a302cb1"
      },
      "source": [
        "fraud_count"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([65115, 33254]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}