{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c31f65a7-4940-4bf7-bafc-b61eb5ccc627"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "721405e7-adb4-439a-ab86-105f36858dd7"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 79% 46.0M/58.3M [00:00<00:00, 202MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 231MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 161MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 223MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 90% 47.0M/52.2M [00:00<00:00, 197MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 206MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 217MB/s]\n",
            "Archive:  train_transaction.csv.zip\n",
            "replace train_transaction.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: train_transaction.csv   \n",
            "\n",
            "Archive:  test_transaction.csv.zip\n",
            "  inflating: test_transaction.csv    \n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "\n",
            "Archive:  test_identity.csv.zip\n",
            "  inflating: test_identity.csv       \n",
            "\n",
            "Archive:  train_identity.csv.zip\n",
            "  inflating: train_identity.csv      \n",
            "\n",
            "5 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ec67c7cc-27f4-4fed-97f3-fd44c31634d7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "4c589c15-fd8a-4236-b2cd-cd5d121515a2"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "fa3e2cc3-7372-41e8-b679-d509c0fe1c85"
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>Android 7.0</td>\n",
              "      <td>samsung browser 6.2</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2220x1080</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987008</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>98945.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>49.0</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>621.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>iOS 11.1.2</td>\n",
              "      <td>mobile safari 11.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1334x750</td>\n",
              "      <td>match_status:1</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>iOS Device</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987010</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>191631.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>121.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>410.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Windows</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987011</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>221832.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>176.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>529.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Mac OS X 10_11_6</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x800</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>MacOS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01  ...  DeviceType                     DeviceInfo\n",
              "0        2987004    0.0  ...      mobile  SAMSUNG SM-G892A Build/NRD90M\n",
              "1        2987008   -5.0  ...      mobile                     iOS Device\n",
              "2        2987010   -5.0  ...     desktop                        Windows\n",
              "3        2987011   -5.0  ...     desktop                            NaN\n",
              "4        2987016    0.0  ...     desktop                          MacOS\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0aee3e59-9823-4e5d-c29b-796b1caed357"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3ec086d1-e421-4cc6-bc0a-328b045805ac"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "cache = dict()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "cdc6cb9d-c395-4d64-bcfe-c211db0e546e"
      },
      "source": [
        "data_backup = copy.copy(dataset_transaction)\n",
        "data_backup.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "\n",
        "dataset_transaction = copy.copy(data_backup)\n",
        "\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN column for every features\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7c1ccdc5-fd31-4cf8-ed52-3339b84516e9"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 770 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 770 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  #if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "  #  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "bde68aac-0e38-4fda-b5c6-562e6e34946a"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 770 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 770 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79939235-82e0-4d09-a1b9-1fca0ef5ceeb"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoded_column = 0\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "  cache[column] = dataset_transaction[column].values.reshape(-1,1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuvQmMmLRnM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "6f5780a0-534c-4faa-db94-a6bc70c08091"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 920 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  M9_0  M9_1  M9_2\n",
              "0        2987000        0      -0.463379  ...     0     1     0\n",
              "1        2987001        0      -0.463379  ...     0     1     0\n",
              "2        2987002        0      -0.463379  ...     1     0     0\n",
              "3        2987003        0      -0.463379  ...     0     1     0\n",
              "4        2987004        0      -0.463379  ...     0     1     0\n",
              "\n",
              "[5 rows x 920 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c7af907-70ee-467b-8871-64b1b2cda944"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colums_to_analyze = ['isFraud', 'TransactionDT', 'TransactionAmt', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2']\n",
        "analyzing_data = dataset_transaction[colums_to_analyze]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "96b975e3-10ff-4079-aef2-6bbf962e6db7"
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1300f1f6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f28ad952-1789-4c20-a1ce-603c247efcca"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 919 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  TransactionAmt     card1  ...  M8_2  M9_0  M9_1  M9_2\n",
              "0        0      -0.463379       -0.002083  0.231445  ...     0     0     1     0\n",
              "1        0      -0.463379       -0.003321 -0.410645  ...     0     0     1     0\n",
              "2        0      -0.463379       -0.002380 -0.301025  ...     0     1     0     0\n",
              "3        0      -0.463379       -0.002663  0.473389  ...     0     0     1     0\n",
              "4        0      -0.463379       -0.002663 -0.310547  ...     0     0     1     0\n",
              "\n",
              "[5 rows x 919 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b46cdd1b-b6a0-4ee6-8d75-50178647aeb2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "print(X_train.shape, Y_train.shape, X_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(472432, 918) (472432,) (118108, 918)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "1f854578-5f09-4353-cad0-576f49ef9636"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "06667918-7210-45ab-e349-47f6ff3fed60"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 10\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_to_train = np.array(X_train)[indices_0_new]\n",
        "Y_to_train = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "\n",
        "X_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_to_train, axis=1)\n",
        "print(X_to_train.shape, Y_to_train.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(455901, 1)\n",
            "(637742, 1)\n",
            "(637742, 918) (637742,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2005d8df-2a01-4424-9982-1c8ca0a30f5c"
      },
      "source": [
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 28.51%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARiUlEQVR4nO3df6xkZX3H8fenrIA/+bm1hKUuxE3MYqriBvFHWoVWFqwuTdVAbF3tVmrFRmPTijWprZYU/ymWVG2IEKExAkVbqELpFjCmNQtcFEGgyHXVshuUdRdBYsRCv/1jnsXhdp575y47c6/s+5VM7jnf85x5vvfc2fncmXPubKoKSZJG+YWlbkCStHwZEpKkLkNCktRlSEiSugwJSVLXiqVuYG87/PDDa/Xq1UvdhiT9XLnlllt+UFUr59afciGxevVqZmZmlroNSfq5kuS7o+q+3SRJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSep6yv3F9ZOx+uwvLnULegr7zrmvW+oWpEXzlYQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSusYOiST7Jflaki+09aOT3JhkNsllSfZv9QPa+mzbvnroPj7Q6ncnOXmovr7VZpOcPVQfOYckaToW80riPcBdQ+sfBc6rqucDDwCbWn0T8ECrn9fGkWQtcDpwLLAe+EQLnv2AjwOnAGuBM9rY+eaQJE3BWCGRZBXwOuBTbT3AicAVbcjFwGlteUNbp20/qY3fAFxaVY9U1beBWeD4dputqq1V9VPgUmDDAnNIkqZg3FcSHwP+FPjftn4Y8MOqerStbwOObMtHAvcCtO0PtvGP1+fs06vPN8cTJDkzyUySmR07doz5LUmSFrJgSCT5TeD+qrplCv3skaq6oKrWVdW6lStXLnU7kvSUsWKMMa8E3pDkVOBA4DnA3wIHJ1nRftNfBWxv47cDRwHbkqwADgJ2DtV3G95nVH3nPHNIkqZgwVcSVfWBqlpVVasZnHi+vqreAtwAvLEN2whc2Zavauu07ddXVbX66e3qp6OBNcBNwM3AmnYl0/5tjqvaPr05JElT8GT+TuL9wPuSzDI4f3Bhq18IHNbq7wPOBqiqO4DLgTuBfwXOqqrH2quEdwPXMrh66vI2dr45JElTMM7bTY+rqi8BX2rLWxlcmTR3zE+AN3X2Pwc4Z0T9auDqEfWRc0iSpsO/uJYkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSuhYMiSQHJrkpydeT3JHkL1v96CQ3JplNclmS/Vv9gLY+27avHrqvD7T63UlOHqqvb7XZJGcP1UfOIUmajnFeSTwCnFhVLwJeDKxPcgLwUeC8qno+8ACwqY3fBDzQ6ue1cSRZC5wOHAusBz6RZL8k+wEfB04B1gJntLHMM4ckaQoWDIkaeLitPq3dCjgRuKLVLwZOa8sb2jpt+0lJ0uqXVtUjVfVtYBY4vt1mq2prVf0UuBTY0PbpzSFJmoKxzkm03/hvBe4HNgPfAn5YVY+2IduAI9vykcC9AG37g8Bhw/U5+/Tqh80zhyRpCsYKiap6rKpeDKxi8Jv/Cyba1SIlOTPJTJKZHTt2LHU7kvSUsairm6rqh8ANwMuBg5OsaJtWAdvb8nbgKIC2/SBg53B9zj69+s555pjb1wVVta6q1q1cuXIx35IkaR7jXN20MsnBbfnpwG8AdzEIize2YRuBK9vyVW2dtv36qqpWP71d/XQ0sAa4CbgZWNOuZNqfwcntq9o+vTkkSVOwYuEhHAFc3K5C+gXg8qr6QpI7gUuT/BXwNeDCNv5C4B+SzAK7GDzpU1V3JLkcuBN4FDirqh4DSPJu4FpgP+Ciqrqj3df7O3NIkqZgwZCoqtuAl4yob2VwfmJu/SfAmzr3dQ5wzoj61cDV484hSZoO/+JaktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXQuGRJKjktyQ5M4kdyR5T6sfmmRzknva10NaPUnOTzKb5LYkxw3d18Y2/p4kG4fqL01ye9vn/CSZbw5J0nSM80riUeCPq2otcAJwVpK1wNnAdVW1BriurQOcAqxptzOBT8LgCR/4EPAy4HjgQ0NP+p8E3jG03/pW780hSZqCBUOiqu6rqq+25R8BdwFHAhuAi9uwi4HT2vIG4JIa2AIcnOQI4GRgc1XtqqoHgM3A+rbtOVW1paoKuGTOfY2aQ5I0BYs6J5FkNfAS4EbguVV1X9v0PeC5bflI4N6h3ba12nz1bSPqzDPH3L7OTDKTZGbHjh2L+ZYkSfMYOySSPAv4HPDeqnpoeFt7BVB7ubcnmG+OqrqgqtZV1bqVK1dOsg1J2qeMFRJJnsYgID5TVZ9v5e+3t4poX+9v9e3AUUO7r2q1+eqrRtTnm0OSNAXjXN0U4ELgrqr6m6FNVwG7r1DaCFw5VH9ru8rpBODB9pbRtcBrkxzSTli/Fri2bXsoyQltrrfOua9Rc0iSpmDFGGNeCfwucHuSW1vtz4BzgcuTbAK+C7y5bbsaOBWYBX4MvB2gqnYl+Qhwcxv34ara1ZbfBXwaeDpwTbsxzxySpClYMCSq6j+AdDafNGJ8AWd17usi4KIR9RnghSPqO0fNIUmaDv/iWpLUZUhIkrrGOSchaS9YffYXl7oFPYV959zXTeR+fSUhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqWjAkklyU5P4k3xiqHZpkc5J72tdDWj1Jzk8ym+S2JMcN7bOxjb8nycah+kuT3N72OT9J5ptDkjQ947yS+DSwfk7tbOC6qloDXNfWAU4B1rTbmcAnYfCED3wIeBlwPPChoSf9TwLvGNpv/QJzSJKmZMGQqKovA7vmlDcAF7fli4HThuqX1MAW4OAkRwAnA5uraldVPQBsBta3bc+pqi1VVcAlc+5r1BySpCnZ03MSz62q+9ry94DntuUjgXuHxm1rtfnq20bU55vj/0lyZpKZJDM7duzYg29HkjTKkz5x3V4B1F7oZY/nqKoLqmpdVa1buXLlJFuRpH3KnobE99tbRbSv97f6duCooXGrWm2++qoR9fnmkCRNyZ6GxFXA7iuUNgJXDtXf2q5yOgF4sL1ldC3w2iSHtBPWrwWubdseSnJCu6rprXPua9QckqQpWbHQgCSfBV4NHJ5kG4OrlM4FLk+yCfgu8OY2/GrgVGAW+DHwdoCq2pXkI8DNbdyHq2r3yfB3MbiC6unANe3GPHNIkqZkwZCoqjM6m04aMbaAszr3cxFw0Yj6DPDCEfWdo+aQJE2Pf3EtSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkrmUfEknWJ7k7yWySs5e6H0nalyzrkEiyH/Bx4BRgLXBGkrVL25Uk7TuWdUgAxwOzVbW1qn4KXApsWOKeJGmfsWKpG1jAkcC9Q+vbgJfNHZTkTODMtvpwkrv3cL7DgR/s4b6TZF+LY1+LY1+Lsyz7ykefdF/PG1Vc7iExlqq6ALjgyd5PkpmqWrcXWtqr7Gtx7Gtx7Gtx9rW+lvvbTduBo4bWV7WaJGkKlntI3AysSXJ0kv2B04GrlrgnSdpnLOu3m6rq0STvBq4F9gMuqqo7Jjjlk37LakLsa3Hsa3Hsa3H2qb5SVZO4X0nSU8Byf7tJkrSEDAlJUtc+ExILfbxHkgOSXNa235hk9dC2D7T63UlOnnJf70tyZ5LbklyX5HlD2x5Lcmu77dUT+mP09bYkO4bm//2hbRuT3NNuG6fc13lDPX0zyQ+Htk3keCW5KMn9Sb7R2Z4k57eeb0ty3NC2SR6rhfp6S+vn9iRfSfKioW3fafVbk8xMua9XJ3lw6Gf150PbJvYxPWP09SdDPX2jPZ4ObdsmebyOSnJDex64I8l7RoyZ3GOsqp7yNwYnvb8FHAPsD3wdWDtnzLuAv2/LpwOXteW1bfwBwNHtfvabYl+vAZ7Rlv9wd19t/eElPF5vA/5uxL6HAlvb10Pa8iHT6mvO+D9icLHDpI/XrwLHAd/obD8VuAYIcAJw46SP1Zh9vWL3fAw++ubGoW3fAQ5fouP1auALT/bnv7f7mjP29cD1UzpeRwDHteVnA98c8e9xYo+xfeWVxDgf77EBuLgtXwGclCStfmlVPVJV3wZm2/1Npa+quqGqftxWtzD4W5FJezIfh3IysLmqdlXVA8BmYP0S9XUG8Nm9NHdXVX0Z2DXPkA3AJTWwBTg4yRFM9lgt2FdVfaXNC9N7bI1zvHom+jE9i+xrKo8tgKq6r6q+2pZ/BNzF4NMohk3sMbavhMSoj/eYe5AfH1NVjwIPAoeNue8k+xq2icFvC7sdmGQmyZYkp+2lnhbT12+3l7ZXJNn9R4/L4ni1t+WOBq4fKk/qeC2k1/ckj9VizX1sFfBvSW7J4GNvpu3lSb6e5Jokx7basjheSZ7B4In2c0PlqRyvDN4Gfwlw45xNE3uMLeu/k9DPJPkdYB3wa0Pl51XV9iTHANcnub2qvjWllv4F+GxVPZLkDxi8CjtxSnOP43Tgiqp6bKi2lMdr2UryGgYh8aqh8qvasfpFYHOS/2q/aU/DVxn8rB5Ocirwz8CaKc09jtcD/1lVw686Jn68kjyLQTC9t6oe2pv3PZ995ZXEOB/v8fiYJCuAg4CdY+47yb5I8uvAB4E3VNUju+tVtb193Qp8icFvGFPpq6p2DvXyKeCl4+47yb6GnM6ctwMmeLwW0ut7yT92JsmvMPj5baiqnbvrQ8fqfuCf2HtvsS6oqh6qqofb8tXA05IczjI4Xs18j62JHK8kT2MQEJ+pqs+PGDK5x9gkTrQstxuDV0xbGbz9sPuE17FzxpzFE09cX96Wj+WJJ663svdOXI/T10sYnKxbM6d+CHBAWz4cuIe9dBJvzL6OGFr+LWBL/exE2bdbf4e05UOn1Vcb9wIGJxIzjePV7nM1/ROxr+OJJxVvmvSxGrOvX2Zwju0Vc+rPBJ49tPwVYP0U+/ql3T87Bk+2/92O3Vg//0n11bYfxOC8xTOndbza934J8LF5xkzsMbbXDu5yvzE4+/9NBk+4H2y1DzP47RzgQOAf2z+am4Bjhvb9YNvvbuCUKff178D3gVvb7apWfwVwe/uHcjuwacp9/TVwR5v/BuAFQ/v+XjuOs8Dbp9lXW/8L4Nw5+03seDH4rfI+4H8YvOe7CXgn8M62PQz+86xvtbnXTelYLdTXp4AHhh5bM61+TDtOX28/4w9Oua93Dz22tjAUYqN+/tPqq415G4MLWYb3m/TxehWDcx63Df2sTp3WY8yP5ZAkde0r5yQkSXvAkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnq+j+/AopvSVLkXgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  out_model.add(Dense(dense1, activation=\"relu\", input_shape=(X_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(dense2, activation=\"relu\", \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[tfa.metrics.F1Score(num_classes=2, average=\"micro\", threshold=0.9)])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "51140613-6f1f-4cae-9954-b128212df157"
      },
      "source": [
        "my_model = create_model(dense1=256, dense2=128, dropout_rate=0.4, l1_rate=1e-4, l2_rate=5e-4, init_std=0.05, lr=0.00001)\n",
        "my_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               235264    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 352,129\n",
            "Trainable params: 351,361\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00153a9d-2f31-4c60-df0a-69c60f4a3f1e"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "NB_EPOCH = 2000\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_f1_score', patience=200, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/best_model', monitor='val_f1_score', verbose=1, save_best_only=True,\n",
        "    save_weights_only=True, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.2, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 1.3917 - f1_score: 0.3593\n",
            "Epoch 00001: val_f1_score improved from -inf to 0.39749, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 15s 7ms/step - loss: 1.3917 - f1_score: 0.3593 - val_loss: 1.1701 - val_f1_score: 0.3975\n",
            "Epoch 2/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 1.1224 - f1_score: 0.4257\n",
            "Epoch 00002: val_f1_score improved from 0.39749 to 0.41427, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 14s 7ms/step - loss: 1.1222 - f1_score: 0.4257 - val_loss: 1.0316 - val_f1_score: 0.4143\n",
            "Epoch 3/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 1.0049 - f1_score: 0.4322\n",
            "Epoch 00003: val_f1_score did not improve from 0.41427\n",
            "1993/1993 [==============================] - 14s 7ms/step - loss: 1.0049 - f1_score: 0.4322 - val_loss: 0.9496 - val_f1_score: 0.4107\n",
            "Epoch 4/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.9374 - f1_score: 0.4409\n",
            "Epoch 00004: val_f1_score did not improve from 0.41427\n",
            "1993/1993 [==============================] - 14s 7ms/step - loss: 0.9373 - f1_score: 0.4410 - val_loss: 0.9005 - val_f1_score: 0.3852\n",
            "Epoch 5/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.8897 - f1_score: 0.4487\n",
            "Epoch 00005: val_f1_score improved from 0.41427 to 0.42697, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.8896 - f1_score: 0.4487 - val_loss: 0.8551 - val_f1_score: 0.4270\n",
            "Epoch 6/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.8525 - f1_score: 0.4545\n",
            "Epoch 00006: val_f1_score improved from 0.42697 to 0.45191, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 14s 7ms/step - loss: 0.8524 - f1_score: 0.4547 - val_loss: 0.8215 - val_f1_score: 0.4519\n",
            "Epoch 7/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.8223 - f1_score: 0.4623\n",
            "Epoch 00007: val_f1_score improved from 0.45191 to 0.45608, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.8222 - f1_score: 0.4623 - val_loss: 0.7951 - val_f1_score: 0.4561\n",
            "Epoch 8/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.7970 - f1_score: 0.4636\n",
            "Epoch 00008: val_f1_score did not improve from 0.45608\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.7970 - f1_score: 0.4635 - val_loss: 0.7727 - val_f1_score: 0.4398\n",
            "Epoch 9/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.7749 - f1_score: 0.4658\n",
            "Epoch 00009: val_f1_score did not improve from 0.45608\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.7749 - f1_score: 0.4658 - val_loss: 0.7515 - val_f1_score: 0.4275\n",
            "Epoch 10/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.7554 - f1_score: 0.4684\n",
            "Epoch 00010: val_f1_score did not improve from 0.45608\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.7554 - f1_score: 0.4684 - val_loss: 0.7313 - val_f1_score: 0.4525\n",
            "Epoch 11/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.7378 - f1_score: 0.4748\n",
            "Epoch 00011: val_f1_score improved from 0.45608 to 0.46217, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.7378 - f1_score: 0.4748 - val_loss: 0.7139 - val_f1_score: 0.4622\n",
            "Epoch 12/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.7217 - f1_score: 0.4774\n",
            "Epoch 00012: val_f1_score improved from 0.46217 to 0.46491, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.7217 - f1_score: 0.4774 - val_loss: 0.6982 - val_f1_score: 0.4649\n",
            "Epoch 13/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.7075 - f1_score: 0.4821\n",
            "Epoch 00013: val_f1_score did not improve from 0.46491\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.7074 - f1_score: 0.4821 - val_loss: 0.6868 - val_f1_score: 0.4599\n",
            "Epoch 14/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.6943 - f1_score: 0.4873\n",
            "Epoch 00014: val_f1_score did not improve from 0.46491\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.6943 - f1_score: 0.4873 - val_loss: 0.6721 - val_f1_score: 0.4579\n",
            "Epoch 15/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.6825 - f1_score: 0.4915\n",
            "Epoch 00015: val_f1_score improved from 0.46491 to 0.46990, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.6825 - f1_score: 0.4915 - val_loss: 0.6615 - val_f1_score: 0.4699\n",
            "Epoch 16/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.6714 - f1_score: 0.4963\n",
            "Epoch 00016: val_f1_score did not improve from 0.46990\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.6713 - f1_score: 0.4963 - val_loss: 0.6512 - val_f1_score: 0.4599\n",
            "Epoch 17/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.6613 - f1_score: 0.4993\n",
            "Epoch 00017: val_f1_score improved from 0.46990 to 0.48813, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.6613 - f1_score: 0.4992 - val_loss: 0.6381 - val_f1_score: 0.4881\n",
            "Epoch 18/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.6514 - f1_score: 0.5048\n",
            "Epoch 00018: val_f1_score improved from 0.48813 to 0.49046, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.6515 - f1_score: 0.5047 - val_loss: 0.6306 - val_f1_score: 0.4905\n",
            "Epoch 19/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.6426 - f1_score: 0.5097\n",
            "Epoch 00019: val_f1_score did not improve from 0.49046\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.6426 - f1_score: 0.5098 - val_loss: 0.6220 - val_f1_score: 0.4867\n",
            "Epoch 20/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.6336 - f1_score: 0.5154\n",
            "Epoch 00020: val_f1_score improved from 0.49046 to 0.50529, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.6336 - f1_score: 0.5155 - val_loss: 0.6140 - val_f1_score: 0.5053\n",
            "Epoch 21/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.6261 - f1_score: 0.5164\n",
            "Epoch 00021: val_f1_score did not improve from 0.50529\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.6261 - f1_score: 0.5164 - val_loss: 0.6083 - val_f1_score: 0.4941\n",
            "Epoch 22/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.6189 - f1_score: 0.5206\n",
            "Epoch 00022: val_f1_score improved from 0.50529 to 0.51075, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.6189 - f1_score: 0.5205 - val_loss: 0.5960 - val_f1_score: 0.5107\n",
            "Epoch 23/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.6113 - f1_score: 0.5255\n",
            "Epoch 00023: val_f1_score did not improve from 0.51075\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.6114 - f1_score: 0.5255 - val_loss: 0.5944 - val_f1_score: 0.4743\n",
            "Epoch 24/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.6052 - f1_score: 0.5279\n",
            "Epoch 00024: val_f1_score did not improve from 0.51075\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.6051 - f1_score: 0.5278 - val_loss: 0.5842 - val_f1_score: 0.5047\n",
            "Epoch 25/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.5988 - f1_score: 0.5325\n",
            "Epoch 00025: val_f1_score improved from 0.51075 to 0.51231, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.5988 - f1_score: 0.5325 - val_loss: 0.5781 - val_f1_score: 0.5123\n",
            "Epoch 26/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.5922 - f1_score: 0.5345\n",
            "Epoch 00026: val_f1_score improved from 0.51231 to 0.52544, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.5923 - f1_score: 0.5345 - val_loss: 0.5733 - val_f1_score: 0.5254\n",
            "Epoch 27/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.5871 - f1_score: 0.5370\n",
            "Epoch 00027: val_f1_score did not improve from 0.52544\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5871 - f1_score: 0.5369 - val_loss: 0.5633 - val_f1_score: 0.5206\n",
            "Epoch 28/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.5813 - f1_score: 0.5408\n",
            "Epoch 00028: val_f1_score did not improve from 0.52544\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5813 - f1_score: 0.5407 - val_loss: 0.5616 - val_f1_score: 0.5033\n",
            "Epoch 29/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.5770 - f1_score: 0.5428\n",
            "Epoch 00029: val_f1_score did not improve from 0.52544\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5769 - f1_score: 0.5428 - val_loss: 0.5540 - val_f1_score: 0.5155\n",
            "Epoch 30/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.5717 - f1_score: 0.5473\n",
            "Epoch 00030: val_f1_score improved from 0.52544 to 0.55990, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5717 - f1_score: 0.5472 - val_loss: 0.5488 - val_f1_score: 0.5599\n",
            "Epoch 31/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.5674 - f1_score: 0.5489\n",
            "Epoch 00031: val_f1_score did not improve from 0.55990\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5674 - f1_score: 0.5489 - val_loss: 0.5478 - val_f1_score: 0.5096\n",
            "Epoch 32/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.5626 - f1_score: 0.5528\n",
            "Epoch 00032: val_f1_score did not improve from 0.55990\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5626 - f1_score: 0.5528 - val_loss: 0.5384 - val_f1_score: 0.5568\n",
            "Epoch 33/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.5578 - f1_score: 0.5554\n",
            "Epoch 00033: val_f1_score did not improve from 0.55990\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5578 - f1_score: 0.5554 - val_loss: 0.5356 - val_f1_score: 0.5400\n",
            "Epoch 34/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.5547 - f1_score: 0.5581\n",
            "Epoch 00034: val_f1_score did not improve from 0.55990\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5546 - f1_score: 0.5580 - val_loss: 0.5302 - val_f1_score: 0.5516\n",
            "Epoch 35/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.5506 - f1_score: 0.5604\n",
            "Epoch 00035: val_f1_score improved from 0.55990 to 0.57517, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5505 - f1_score: 0.5605 - val_loss: 0.5272 - val_f1_score: 0.5752\n",
            "Epoch 36/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.5469 - f1_score: 0.5629\n",
            "Epoch 00036: val_f1_score did not improve from 0.57517\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5469 - f1_score: 0.5629 - val_loss: 0.5222 - val_f1_score: 0.5655\n",
            "Epoch 37/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.5425 - f1_score: 0.5672\n",
            "Epoch 00037: val_f1_score did not improve from 0.57517\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5425 - f1_score: 0.5672 - val_loss: 0.5187 - val_f1_score: 0.5486\n",
            "Epoch 38/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.5396 - f1_score: 0.5680\n",
            "Epoch 00038: val_f1_score did not improve from 0.57517\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5396 - f1_score: 0.5680 - val_loss: 0.5149 - val_f1_score: 0.5677\n",
            "Epoch 39/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.5358 - f1_score: 0.5721\n",
            "Epoch 00039: val_f1_score did not improve from 0.57517\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5357 - f1_score: 0.5722 - val_loss: 0.5102 - val_f1_score: 0.5582\n",
            "Epoch 40/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.5327 - f1_score: 0.5751\n",
            "Epoch 00040: val_f1_score did not improve from 0.57517\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5327 - f1_score: 0.5752 - val_loss: 0.5104 - val_f1_score: 0.5532\n",
            "Epoch 41/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.5294 - f1_score: 0.5775\n",
            "Epoch 00041: val_f1_score did not improve from 0.57517\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5294 - f1_score: 0.5774 - val_loss: 0.5069 - val_f1_score: 0.5281\n",
            "Epoch 42/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.5262 - f1_score: 0.5786\n",
            "Epoch 00042: val_f1_score improved from 0.57517 to 0.58134, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5263 - f1_score: 0.5787 - val_loss: 0.5038 - val_f1_score: 0.5813\n",
            "Epoch 43/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.5231 - f1_score: 0.5832\n",
            "Epoch 00043: val_f1_score improved from 0.58134 to 0.59279, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5231 - f1_score: 0.5831 - val_loss: 0.4977 - val_f1_score: 0.5928\n",
            "Epoch 44/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.5206 - f1_score: 0.5834\n",
            "Epoch 00044: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5205 - f1_score: 0.5835 - val_loss: 0.4972 - val_f1_score: 0.5621\n",
            "Epoch 45/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.5170 - f1_score: 0.5877\n",
            "Epoch 00045: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.5171 - f1_score: 0.5877 - val_loss: 0.4911 - val_f1_score: 0.5621\n",
            "Epoch 46/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.5145 - f1_score: 0.5909\n",
            "Epoch 00046: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.5145 - f1_score: 0.5909 - val_loss: 0.4908 - val_f1_score: 0.5658\n",
            "Epoch 47/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.5122 - f1_score: 0.5907\n",
            "Epoch 00047: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.5122 - f1_score: 0.5906 - val_loss: 0.4854 - val_f1_score: 0.5776\n",
            "Epoch 48/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.5084 - f1_score: 0.5966\n",
            "Epoch 00048: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5085 - f1_score: 0.5965 - val_loss: 0.4845 - val_f1_score: 0.5615\n",
            "Epoch 49/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.5057 - f1_score: 0.5997\n",
            "Epoch 00049: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.5057 - f1_score: 0.5997 - val_loss: 0.4840 - val_f1_score: 0.5835\n",
            "Epoch 50/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.5040 - f1_score: 0.5992\n",
            "Epoch 00050: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.5039 - f1_score: 0.5993 - val_loss: 0.4786 - val_f1_score: 0.5801\n",
            "Epoch 51/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.5014 - f1_score: 0.6003\n",
            "Epoch 00051: val_f1_score did not improve from 0.59279\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.5013 - f1_score: 0.6004 - val_loss: 0.4749 - val_f1_score: 0.5838\n",
            "Epoch 52/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4987 - f1_score: 0.6049\n",
            "Epoch 00052: val_f1_score improved from 0.59279 to 0.60294, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4986 - f1_score: 0.6049 - val_loss: 0.4713 - val_f1_score: 0.6029\n",
            "Epoch 53/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.4967 - f1_score: 0.6068\n",
            "Epoch 00053: val_f1_score did not improve from 0.60294\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4968 - f1_score: 0.6067 - val_loss: 0.4707 - val_f1_score: 0.5908\n",
            "Epoch 54/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4938 - f1_score: 0.6098\n",
            "Epoch 00054: val_f1_score improved from 0.60294 to 0.60924, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4938 - f1_score: 0.6098 - val_loss: 0.4670 - val_f1_score: 0.6092\n",
            "Epoch 55/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4917 - f1_score: 0.6102\n",
            "Epoch 00055: val_f1_score did not improve from 0.60924\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4917 - f1_score: 0.6102 - val_loss: 0.4766 - val_f1_score: 0.5647\n",
            "Epoch 56/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4898 - f1_score: 0.6147\n",
            "Epoch 00056: val_f1_score did not improve from 0.60924\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4897 - f1_score: 0.6148 - val_loss: 0.4611 - val_f1_score: 0.5987\n",
            "Epoch 57/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4869 - f1_score: 0.6157\n",
            "Epoch 00057: val_f1_score improved from 0.60924 to 0.62861, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4868 - f1_score: 0.6158 - val_loss: 0.4614 - val_f1_score: 0.6286\n",
            "Epoch 58/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4857 - f1_score: 0.6157\n",
            "Epoch 00058: val_f1_score did not improve from 0.62861\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4857 - f1_score: 0.6157 - val_loss: 0.4606 - val_f1_score: 0.6083\n",
            "Epoch 59/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.4836 - f1_score: 0.6178\n",
            "Epoch 00059: val_f1_score did not improve from 0.62861\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4836 - f1_score: 0.6177 - val_loss: 0.4585 - val_f1_score: 0.5960\n",
            "Epoch 60/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4806 - f1_score: 0.6218\n",
            "Epoch 00060: val_f1_score did not improve from 0.62861\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4806 - f1_score: 0.6219 - val_loss: 0.4576 - val_f1_score: 0.5993\n",
            "Epoch 61/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.4795 - f1_score: 0.6253\n",
            "Epoch 00061: val_f1_score improved from 0.62861 to 0.64752, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4794 - f1_score: 0.6255 - val_loss: 0.4513 - val_f1_score: 0.6475\n",
            "Epoch 62/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4777 - f1_score: 0.6257\n",
            "Epoch 00062: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4777 - f1_score: 0.6257 - val_loss: 0.4506 - val_f1_score: 0.6148\n",
            "Epoch 63/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4747 - f1_score: 0.6272\n",
            "Epoch 00063: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4747 - f1_score: 0.6273 - val_loss: 0.4471 - val_f1_score: 0.6049\n",
            "Epoch 64/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4733 - f1_score: 0.6290\n",
            "Epoch 00064: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4733 - f1_score: 0.6291 - val_loss: 0.4431 - val_f1_score: 0.6273\n",
            "Epoch 65/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4716 - f1_score: 0.6314\n",
            "Epoch 00065: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4716 - f1_score: 0.6314 - val_loss: 0.4524 - val_f1_score: 0.5940\n",
            "Epoch 66/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4685 - f1_score: 0.6350\n",
            "Epoch 00066: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4685 - f1_score: 0.6350 - val_loss: 0.4482 - val_f1_score: 0.6040\n",
            "Epoch 67/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.4682 - f1_score: 0.6355\n",
            "Epoch 00067: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4682 - f1_score: 0.6354 - val_loss: 0.4424 - val_f1_score: 0.6420\n",
            "Epoch 68/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4657 - f1_score: 0.6383\n",
            "Epoch 00068: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4657 - f1_score: 0.6383 - val_loss: 0.4468 - val_f1_score: 0.6076\n",
            "Epoch 69/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4642 - f1_score: 0.6395\n",
            "Epoch 00069: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4642 - f1_score: 0.6395 - val_loss: 0.4345 - val_f1_score: 0.6449\n",
            "Epoch 70/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4628 - f1_score: 0.6424\n",
            "Epoch 00070: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4628 - f1_score: 0.6424 - val_loss: 0.4432 - val_f1_score: 0.6197\n",
            "Epoch 71/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4606 - f1_score: 0.6437\n",
            "Epoch 00071: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4606 - f1_score: 0.6436 - val_loss: 0.4394 - val_f1_score: 0.5902\n",
            "Epoch 72/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4593 - f1_score: 0.6448\n",
            "Epoch 00072: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4593 - f1_score: 0.6448 - val_loss: 0.4323 - val_f1_score: 0.6283\n",
            "Epoch 73/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.4577 - f1_score: 0.6477\n",
            "Epoch 00073: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4577 - f1_score: 0.6477 - val_loss: 0.4349 - val_f1_score: 0.6233\n",
            "Epoch 74/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.4563 - f1_score: 0.6495\n",
            "Epoch 00074: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4563 - f1_score: 0.6495 - val_loss: 0.4304 - val_f1_score: 0.6070\n",
            "Epoch 75/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4548 - f1_score: 0.6492\n",
            "Epoch 00075: val_f1_score did not improve from 0.64752\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4548 - f1_score: 0.6492 - val_loss: 0.4265 - val_f1_score: 0.6407\n",
            "Epoch 76/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.4535 - f1_score: 0.6507\n",
            "Epoch 00076: val_f1_score improved from 0.64752 to 0.65151, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4534 - f1_score: 0.6507 - val_loss: 0.4234 - val_f1_score: 0.6515\n",
            "Epoch 77/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.4519 - f1_score: 0.6548\n",
            "Epoch 00077: val_f1_score did not improve from 0.65151\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4519 - f1_score: 0.6547 - val_loss: 0.4246 - val_f1_score: 0.6170\n",
            "Epoch 78/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4500 - f1_score: 0.6553\n",
            "Epoch 00078: val_f1_score did not improve from 0.65151\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4501 - f1_score: 0.6552 - val_loss: 0.4228 - val_f1_score: 0.6250\n",
            "Epoch 79/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4484 - f1_score: 0.6581\n",
            "Epoch 00079: val_f1_score did not improve from 0.65151\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4484 - f1_score: 0.6582 - val_loss: 0.4275 - val_f1_score: 0.6346\n",
            "Epoch 80/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4476 - f1_score: 0.6600\n",
            "Epoch 00080: val_f1_score improved from 0.65151 to 0.67031, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4476 - f1_score: 0.6600 - val_loss: 0.4185 - val_f1_score: 0.6703\n",
            "Epoch 81/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4449 - f1_score: 0.6624\n",
            "Epoch 00081: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4449 - f1_score: 0.6624 - val_loss: 0.4202 - val_f1_score: 0.6487\n",
            "Epoch 82/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.4435 - f1_score: 0.6645\n",
            "Epoch 00082: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4436 - f1_score: 0.6645 - val_loss: 0.4157 - val_f1_score: 0.6582\n",
            "Epoch 83/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4425 - f1_score: 0.6641\n",
            "Epoch 00083: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4425 - f1_score: 0.6641 - val_loss: 0.4168 - val_f1_score: 0.6617\n",
            "Epoch 84/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4405 - f1_score: 0.6662\n",
            "Epoch 00084: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4405 - f1_score: 0.6663 - val_loss: 0.4103 - val_f1_score: 0.6625\n",
            "Epoch 85/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4393 - f1_score: 0.6697\n",
            "Epoch 00085: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4393 - f1_score: 0.6697 - val_loss: 0.4148 - val_f1_score: 0.6231\n",
            "Epoch 86/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4384 - f1_score: 0.6703\n",
            "Epoch 00086: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4384 - f1_score: 0.6703 - val_loss: 0.4125 - val_f1_score: 0.6586\n",
            "Epoch 87/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.4373 - f1_score: 0.6718\n",
            "Epoch 00087: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4373 - f1_score: 0.6717 - val_loss: 0.4155 - val_f1_score: 0.6400\n",
            "Epoch 88/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4354 - f1_score: 0.6738\n",
            "Epoch 00088: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4353 - f1_score: 0.6739 - val_loss: 0.4143 - val_f1_score: 0.6330\n",
            "Epoch 89/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4337 - f1_score: 0.6766\n",
            "Epoch 00089: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4338 - f1_score: 0.6766 - val_loss: 0.4079 - val_f1_score: 0.6481\n",
            "Epoch 90/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4328 - f1_score: 0.6762\n",
            "Epoch 00090: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4328 - f1_score: 0.6762 - val_loss: 0.4146 - val_f1_score: 0.6228\n",
            "Epoch 91/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4317 - f1_score: 0.6787\n",
            "Epoch 00091: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4317 - f1_score: 0.6787 - val_loss: 0.4127 - val_f1_score: 0.6233\n",
            "Epoch 92/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4301 - f1_score: 0.6796\n",
            "Epoch 00092: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4301 - f1_score: 0.6796 - val_loss: 0.4293 - val_f1_score: 0.6086\n",
            "Epoch 93/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.4294 - f1_score: 0.6824\n",
            "Epoch 00093: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4294 - f1_score: 0.6825 - val_loss: 0.4040 - val_f1_score: 0.6666\n",
            "Epoch 94/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.4283 - f1_score: 0.6813\n",
            "Epoch 00094: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4283 - f1_score: 0.6812 - val_loss: 0.4018 - val_f1_score: 0.6552\n",
            "Epoch 95/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.4273 - f1_score: 0.6834\n",
            "Epoch 00095: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4273 - f1_score: 0.6833 - val_loss: 0.3971 - val_f1_score: 0.6699\n",
            "Epoch 96/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.4248 - f1_score: 0.6864\n",
            "Epoch 00096: val_f1_score did not improve from 0.67031\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4248 - f1_score: 0.6863 - val_loss: 0.4091 - val_f1_score: 0.6254\n",
            "Epoch 97/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4244 - f1_score: 0.6852\n",
            "Epoch 00097: val_f1_score improved from 0.67031 to 0.67552, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4245 - f1_score: 0.6852 - val_loss: 0.4031 - val_f1_score: 0.6755\n",
            "Epoch 98/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4235 - f1_score: 0.6885\n",
            "Epoch 00098: val_f1_score improved from 0.67552 to 0.68160, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4235 - f1_score: 0.6886 - val_loss: 0.3938 - val_f1_score: 0.6816\n",
            "Epoch 99/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.4219 - f1_score: 0.6891\n",
            "Epoch 00099: val_f1_score did not improve from 0.68160\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.4219 - f1_score: 0.6891 - val_loss: 0.3986 - val_f1_score: 0.6746\n",
            "Epoch 100/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.4206 - f1_score: 0.6909\n",
            "Epoch 00100: val_f1_score did not improve from 0.68160\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4206 - f1_score: 0.6909 - val_loss: 0.4020 - val_f1_score: 0.6596\n",
            "Epoch 101/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.4198 - f1_score: 0.6930\n",
            "Epoch 00101: val_f1_score improved from 0.68160 to 0.69934, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4198 - f1_score: 0.6930 - val_loss: 0.3891 - val_f1_score: 0.6993\n",
            "Epoch 102/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4186 - f1_score: 0.6934\n",
            "Epoch 00102: val_f1_score did not improve from 0.69934\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4186 - f1_score: 0.6934 - val_loss: 0.3862 - val_f1_score: 0.6927\n",
            "Epoch 103/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4172 - f1_score: 0.6942\n",
            "Epoch 00103: val_f1_score improved from 0.69934 to 0.70735, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4172 - f1_score: 0.6942 - val_loss: 0.3859 - val_f1_score: 0.7073\n",
            "Epoch 104/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4163 - f1_score: 0.6960\n",
            "Epoch 00104: val_f1_score did not improve from 0.70735\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4162 - f1_score: 0.6959 - val_loss: 0.3855 - val_f1_score: 0.6797\n",
            "Epoch 105/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.4148 - f1_score: 0.6989\n",
            "Epoch 00105: val_f1_score did not improve from 0.70735\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4148 - f1_score: 0.6988 - val_loss: 0.3922 - val_f1_score: 0.6552\n",
            "Epoch 106/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4140 - f1_score: 0.6997\n",
            "Epoch 00106: val_f1_score did not improve from 0.70735\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4140 - f1_score: 0.6997 - val_loss: 0.3893 - val_f1_score: 0.6649\n",
            "Epoch 107/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4130 - f1_score: 0.6998\n",
            "Epoch 00107: val_f1_score did not improve from 0.70735\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4130 - f1_score: 0.6998 - val_loss: 0.3956 - val_f1_score: 0.6417\n",
            "Epoch 108/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4119 - f1_score: 0.7016\n",
            "Epoch 00108: val_f1_score did not improve from 0.70735\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4120 - f1_score: 0.7015 - val_loss: 0.3839 - val_f1_score: 0.6713\n",
            "Epoch 109/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4113 - f1_score: 0.7027\n",
            "Epoch 00109: val_f1_score did not improve from 0.70735\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4114 - f1_score: 0.7027 - val_loss: 0.3854 - val_f1_score: 0.6703\n",
            "Epoch 110/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4099 - f1_score: 0.7029\n",
            "Epoch 00110: val_f1_score improved from 0.70735 to 0.71710, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4099 - f1_score: 0.7030 - val_loss: 0.3814 - val_f1_score: 0.7171\n",
            "Epoch 111/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.4091 - f1_score: 0.7044\n",
            "Epoch 00111: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4090 - f1_score: 0.7044 - val_loss: 0.3872 - val_f1_score: 0.6824\n",
            "Epoch 112/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4077 - f1_score: 0.7076\n",
            "Epoch 00112: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4076 - f1_score: 0.7076 - val_loss: 0.3828 - val_f1_score: 0.6976\n",
            "Epoch 113/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.4071 - f1_score: 0.7070\n",
            "Epoch 00113: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4071 - f1_score: 0.7071 - val_loss: 0.3802 - val_f1_score: 0.7016\n",
            "Epoch 114/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4058 - f1_score: 0.7091\n",
            "Epoch 00114: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4058 - f1_score: 0.7091 - val_loss: 0.3826 - val_f1_score: 0.6827\n",
            "Epoch 115/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.4053 - f1_score: 0.7101\n",
            "Epoch 00115: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4053 - f1_score: 0.7101 - val_loss: 0.3824 - val_f1_score: 0.6438\n",
            "Epoch 116/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4041 - f1_score: 0.7101\n",
            "Epoch 00116: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4040 - f1_score: 0.7101 - val_loss: 0.3767 - val_f1_score: 0.6984\n",
            "Epoch 117/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4031 - f1_score: 0.7129\n",
            "Epoch 00117: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4031 - f1_score: 0.7129 - val_loss: 0.3787 - val_f1_score: 0.6762\n",
            "Epoch 118/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.4019 - f1_score: 0.7137\n",
            "Epoch 00118: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4019 - f1_score: 0.7137 - val_loss: 0.3716 - val_f1_score: 0.7097\n",
            "Epoch 119/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.4022 - f1_score: 0.7131\n",
            "Epoch 00119: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4022 - f1_score: 0.7130 - val_loss: 0.3823 - val_f1_score: 0.6637\n",
            "Epoch 120/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.4006 - f1_score: 0.7158\n",
            "Epoch 00120: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.4006 - f1_score: 0.7158 - val_loss: 0.3772 - val_f1_score: 0.6719\n",
            "Epoch 121/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3990 - f1_score: 0.7160\n",
            "Epoch 00121: val_f1_score did not improve from 0.71710\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3990 - f1_score: 0.7159 - val_loss: 0.3754 - val_f1_score: 0.6884\n",
            "Epoch 122/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3984 - f1_score: 0.7215\n",
            "Epoch 00122: val_f1_score improved from 0.71710 to 0.71858, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3984 - f1_score: 0.7215 - val_loss: 0.3731 - val_f1_score: 0.7186\n",
            "Epoch 123/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3976 - f1_score: 0.7199\n",
            "Epoch 00123: val_f1_score improved from 0.71858 to 0.72770, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3976 - f1_score: 0.7199 - val_loss: 0.3680 - val_f1_score: 0.7277\n",
            "Epoch 124/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3962 - f1_score: 0.7234\n",
            "Epoch 00124: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3962 - f1_score: 0.7234 - val_loss: 0.3642 - val_f1_score: 0.7127\n",
            "Epoch 125/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3962 - f1_score: 0.7222\n",
            "Epoch 00125: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3963 - f1_score: 0.7222 - val_loss: 0.3714 - val_f1_score: 0.7052\n",
            "Epoch 126/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3953 - f1_score: 0.7224\n",
            "Epoch 00126: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3953 - f1_score: 0.7224 - val_loss: 0.3686 - val_f1_score: 0.6971\n",
            "Epoch 127/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3941 - f1_score: 0.7231\n",
            "Epoch 00127: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3940 - f1_score: 0.7231 - val_loss: 0.3703 - val_f1_score: 0.6788\n",
            "Epoch 128/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3932 - f1_score: 0.7254\n",
            "Epoch 00128: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3932 - f1_score: 0.7254 - val_loss: 0.3735 - val_f1_score: 0.6828\n",
            "Epoch 129/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3923 - f1_score: 0.7269\n",
            "Epoch 00129: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3923 - f1_score: 0.7269 - val_loss: 0.3776 - val_f1_score: 0.6624\n",
            "Epoch 130/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3914 - f1_score: 0.7273\n",
            "Epoch 00130: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3914 - f1_score: 0.7273 - val_loss: 0.3648 - val_f1_score: 0.7216\n",
            "Epoch 131/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3914 - f1_score: 0.7290\n",
            "Epoch 00131: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3914 - f1_score: 0.7289 - val_loss: 0.3833 - val_f1_score: 0.6503\n",
            "Epoch 132/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3893 - f1_score: 0.7301\n",
            "Epoch 00132: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3894 - f1_score: 0.7300 - val_loss: 0.3703 - val_f1_score: 0.6955\n",
            "Epoch 133/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3886 - f1_score: 0.7311\n",
            "Epoch 00133: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3886 - f1_score: 0.7310 - val_loss: 0.3626 - val_f1_score: 0.6983\n",
            "Epoch 134/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3882 - f1_score: 0.7316\n",
            "Epoch 00134: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3881 - f1_score: 0.7315 - val_loss: 0.3615 - val_f1_score: 0.7035\n",
            "Epoch 135/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3879 - f1_score: 0.7319\n",
            "Epoch 00135: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3879 - f1_score: 0.7320 - val_loss: 0.3752 - val_f1_score: 0.6818\n",
            "Epoch 136/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3873 - f1_score: 0.7330\n",
            "Epoch 00136: val_f1_score did not improve from 0.72770\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3873 - f1_score: 0.7330 - val_loss: 0.3661 - val_f1_score: 0.7025\n",
            "Epoch 137/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3855 - f1_score: 0.7351\n",
            "Epoch 00137: val_f1_score improved from 0.72770 to 0.72880, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3855 - f1_score: 0.7351 - val_loss: 0.3600 - val_f1_score: 0.7288\n",
            "Epoch 138/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3857 - f1_score: 0.7323\n",
            "Epoch 00138: val_f1_score did not improve from 0.72880\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3857 - f1_score: 0.7323 - val_loss: 0.3604 - val_f1_score: 0.7073\n",
            "Epoch 139/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3842 - f1_score: 0.7379\n",
            "Epoch 00139: val_f1_score improved from 0.72880 to 0.75069, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3842 - f1_score: 0.7379 - val_loss: 0.3559 - val_f1_score: 0.7507\n",
            "Epoch 140/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3837 - f1_score: 0.7353\n",
            "Epoch 00140: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3837 - f1_score: 0.7353 - val_loss: 0.3532 - val_f1_score: 0.7262\n",
            "Epoch 141/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3839 - f1_score: 0.7359\n",
            "Epoch 00141: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3839 - f1_score: 0.7359 - val_loss: 0.3527 - val_f1_score: 0.7459\n",
            "Epoch 142/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3823 - f1_score: 0.7374\n",
            "Epoch 00142: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3823 - f1_score: 0.7373 - val_loss: 0.3656 - val_f1_score: 0.6949\n",
            "Epoch 143/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3815 - f1_score: 0.7400\n",
            "Epoch 00143: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3815 - f1_score: 0.7400 - val_loss: 0.3710 - val_f1_score: 0.6781\n",
            "Epoch 144/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.3805 - f1_score: 0.7422\n",
            "Epoch 00144: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3805 - f1_score: 0.7421 - val_loss: 0.3548 - val_f1_score: 0.7239\n",
            "Epoch 145/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3791 - f1_score: 0.7430\n",
            "Epoch 00145: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3792 - f1_score: 0.7429 - val_loss: 0.3585 - val_f1_score: 0.7084\n",
            "Epoch 146/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3799 - f1_score: 0.7427\n",
            "Epoch 00146: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3799 - f1_score: 0.7427 - val_loss: 0.3598 - val_f1_score: 0.7080\n",
            "Epoch 147/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3781 - f1_score: 0.7439\n",
            "Epoch 00147: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3781 - f1_score: 0.7439 - val_loss: 0.3632 - val_f1_score: 0.7129\n",
            "Epoch 148/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3772 - f1_score: 0.7436\n",
            "Epoch 00148: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3771 - f1_score: 0.7436 - val_loss: 0.3547 - val_f1_score: 0.7235\n",
            "Epoch 149/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3777 - f1_score: 0.7461\n",
            "Epoch 00149: val_f1_score did not improve from 0.75069\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3777 - f1_score: 0.7462 - val_loss: 0.3562 - val_f1_score: 0.7452\n",
            "Epoch 150/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3763 - f1_score: 0.7463\n",
            "Epoch 00150: val_f1_score improved from 0.75069 to 0.76688, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3763 - f1_score: 0.7463 - val_loss: 0.3525 - val_f1_score: 0.7669\n",
            "Epoch 151/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3759 - f1_score: 0.7477\n",
            "Epoch 00151: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3758 - f1_score: 0.7477 - val_loss: 0.3492 - val_f1_score: 0.7283\n",
            "Epoch 152/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3743 - f1_score: 0.7481\n",
            "Epoch 00152: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3743 - f1_score: 0.7481 - val_loss: 0.3472 - val_f1_score: 0.7489\n",
            "Epoch 153/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3743 - f1_score: 0.7483\n",
            "Epoch 00153: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3744 - f1_score: 0.7484 - val_loss: 0.3501 - val_f1_score: 0.7386\n",
            "Epoch 154/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3723 - f1_score: 0.7513\n",
            "Epoch 00154: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3723 - f1_score: 0.7513 - val_loss: 0.3453 - val_f1_score: 0.7337\n",
            "Epoch 155/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3725 - f1_score: 0.7521\n",
            "Epoch 00155: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3726 - f1_score: 0.7521 - val_loss: 0.3641 - val_f1_score: 0.6958\n",
            "Epoch 156/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3718 - f1_score: 0.7511\n",
            "Epoch 00156: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3718 - f1_score: 0.7511 - val_loss: 0.3466 - val_f1_score: 0.7260\n",
            "Epoch 157/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3719 - f1_score: 0.7514\n",
            "Epoch 00157: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3719 - f1_score: 0.7514 - val_loss: 0.3523 - val_f1_score: 0.7093\n",
            "Epoch 158/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3707 - f1_score: 0.7531\n",
            "Epoch 00158: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3706 - f1_score: 0.7531 - val_loss: 0.3518 - val_f1_score: 0.7429\n",
            "Epoch 159/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3705 - f1_score: 0.7546\n",
            "Epoch 00159: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3705 - f1_score: 0.7546 - val_loss: 0.3503 - val_f1_score: 0.7095\n",
            "Epoch 160/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3695 - f1_score: 0.7553\n",
            "Epoch 00160: val_f1_score did not improve from 0.76688\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3695 - f1_score: 0.7553 - val_loss: 0.3443 - val_f1_score: 0.7504\n",
            "Epoch 161/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3687 - f1_score: 0.7554\n",
            "Epoch 00161: val_f1_score improved from 0.76688 to 0.76794, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3688 - f1_score: 0.7553 - val_loss: 0.3398 - val_f1_score: 0.7679\n",
            "Epoch 162/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3686 - f1_score: 0.7541\n",
            "Epoch 00162: val_f1_score did not improve from 0.76794\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3686 - f1_score: 0.7541 - val_loss: 0.3420 - val_f1_score: 0.7641\n",
            "Epoch 163/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3678 - f1_score: 0.7554\n",
            "Epoch 00163: val_f1_score did not improve from 0.76794\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3678 - f1_score: 0.7554 - val_loss: 0.3557 - val_f1_score: 0.6996\n",
            "Epoch 164/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.3668 - f1_score: 0.7581\n",
            "Epoch 00164: val_f1_score improved from 0.76794 to 0.77689, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3668 - f1_score: 0.7584 - val_loss: 0.3410 - val_f1_score: 0.7769\n",
            "Epoch 165/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3669 - f1_score: 0.7576\n",
            "Epoch 00165: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3669 - f1_score: 0.7576 - val_loss: 0.3446 - val_f1_score: 0.7353\n",
            "Epoch 166/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3658 - f1_score: 0.7590\n",
            "Epoch 00166: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3658 - f1_score: 0.7590 - val_loss: 0.3418 - val_f1_score: 0.7411\n",
            "Epoch 167/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3662 - f1_score: 0.7559\n",
            "Epoch 00167: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3662 - f1_score: 0.7559 - val_loss: 0.3518 - val_f1_score: 0.7047\n",
            "Epoch 168/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3647 - f1_score: 0.7594\n",
            "Epoch 00168: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3647 - f1_score: 0.7595 - val_loss: 0.3508 - val_f1_score: 0.7056\n",
            "Epoch 169/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3640 - f1_score: 0.7615\n",
            "Epoch 00169: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3640 - f1_score: 0.7615 - val_loss: 0.3445 - val_f1_score: 0.7156\n",
            "Epoch 170/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3638 - f1_score: 0.7606\n",
            "Epoch 00170: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3638 - f1_score: 0.7606 - val_loss: 0.3666 - val_f1_score: 0.7009\n",
            "Epoch 171/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3633 - f1_score: 0.7614\n",
            "Epoch 00171: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3633 - f1_score: 0.7614 - val_loss: 0.3425 - val_f1_score: 0.7304\n",
            "Epoch 172/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3630 - f1_score: 0.7617\n",
            "Epoch 00172: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3630 - f1_score: 0.7617 - val_loss: 0.3455 - val_f1_score: 0.7079\n",
            "Epoch 173/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3626 - f1_score: 0.7624\n",
            "Epoch 00173: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3626 - f1_score: 0.7624 - val_loss: 0.3364 - val_f1_score: 0.7549\n",
            "Epoch 174/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3616 - f1_score: 0.7636\n",
            "Epoch 00174: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3616 - f1_score: 0.7636 - val_loss: 0.3321 - val_f1_score: 0.7524\n",
            "Epoch 175/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3604 - f1_score: 0.7649\n",
            "Epoch 00175: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3604 - f1_score: 0.7648 - val_loss: 0.3386 - val_f1_score: 0.7518\n",
            "Epoch 176/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3605 - f1_score: 0.7646\n",
            "Epoch 00176: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3605 - f1_score: 0.7646 - val_loss: 0.3428 - val_f1_score: 0.7239\n",
            "Epoch 177/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3593 - f1_score: 0.7661\n",
            "Epoch 00177: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3593 - f1_score: 0.7661 - val_loss: 0.3446 - val_f1_score: 0.7283\n",
            "Epoch 178/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3593 - f1_score: 0.7666\n",
            "Epoch 00178: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3593 - f1_score: 0.7665 - val_loss: 0.3471 - val_f1_score: 0.7078\n",
            "Epoch 179/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3578 - f1_score: 0.7679\n",
            "Epoch 00179: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3578 - f1_score: 0.7679 - val_loss: 0.3500 - val_f1_score: 0.7234\n",
            "Epoch 180/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3582 - f1_score: 0.7680\n",
            "Epoch 00180: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3582 - f1_score: 0.7680 - val_loss: 0.3502 - val_f1_score: 0.6980\n",
            "Epoch 181/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3575 - f1_score: 0.7695\n",
            "Epoch 00181: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3575 - f1_score: 0.7695 - val_loss: 0.3404 - val_f1_score: 0.7508\n",
            "Epoch 182/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3564 - f1_score: 0.7702\n",
            "Epoch 00182: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3564 - f1_score: 0.7702 - val_loss: 0.3311 - val_f1_score: 0.7734\n",
            "Epoch 183/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3566 - f1_score: 0.7706\n",
            "Epoch 00183: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3566 - f1_score: 0.7705 - val_loss: 0.3311 - val_f1_score: 0.7420\n",
            "Epoch 184/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3543 - f1_score: 0.7739\n",
            "Epoch 00184: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3543 - f1_score: 0.7739 - val_loss: 0.3435 - val_f1_score: 0.7113\n",
            "Epoch 185/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3544 - f1_score: 0.7733\n",
            "Epoch 00185: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3544 - f1_score: 0.7733 - val_loss: 0.3368 - val_f1_score: 0.7599\n",
            "Epoch 186/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3538 - f1_score: 0.7739\n",
            "Epoch 00186: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3538 - f1_score: 0.7739 - val_loss: 0.3276 - val_f1_score: 0.7742\n",
            "Epoch 187/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3532 - f1_score: 0.7746\n",
            "Epoch 00187: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3533 - f1_score: 0.7745 - val_loss: 0.3357 - val_f1_score: 0.7397\n",
            "Epoch 188/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3531 - f1_score: 0.7742\n",
            "Epoch 00188: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3531 - f1_score: 0.7742 - val_loss: 0.3255 - val_f1_score: 0.7646\n",
            "Epoch 189/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3531 - f1_score: 0.7727\n",
            "Epoch 00189: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3531 - f1_score: 0.7727 - val_loss: 0.3306 - val_f1_score: 0.7398\n",
            "Epoch 190/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3521 - f1_score: 0.7746\n",
            "Epoch 00190: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3522 - f1_score: 0.7746 - val_loss: 0.3283 - val_f1_score: 0.7645\n",
            "Epoch 191/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3513 - f1_score: 0.7754\n",
            "Epoch 00191: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3513 - f1_score: 0.7754 - val_loss: 0.3230 - val_f1_score: 0.7743\n",
            "Epoch 192/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3519 - f1_score: 0.7746\n",
            "Epoch 00192: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3518 - f1_score: 0.7747 - val_loss: 0.3264 - val_f1_score: 0.7572\n",
            "Epoch 193/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3510 - f1_score: 0.7759\n",
            "Epoch 00193: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3510 - f1_score: 0.7759 - val_loss: 0.3266 - val_f1_score: 0.7699\n",
            "Epoch 194/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3494 - f1_score: 0.7788\n",
            "Epoch 00194: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3494 - f1_score: 0.7788 - val_loss: 0.3278 - val_f1_score: 0.7516\n",
            "Epoch 195/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3495 - f1_score: 0.7768\n",
            "Epoch 00195: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3495 - f1_score: 0.7768 - val_loss: 0.3314 - val_f1_score: 0.7340\n",
            "Epoch 196/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3497 - f1_score: 0.7773\n",
            "Epoch 00196: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3497 - f1_score: 0.7771 - val_loss: 0.3443 - val_f1_score: 0.7091\n",
            "Epoch 197/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3497 - f1_score: 0.7782\n",
            "Epoch 00197: val_f1_score did not improve from 0.77689\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3497 - f1_score: 0.7781 - val_loss: 0.3363 - val_f1_score: 0.7190\n",
            "Epoch 198/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3482 - f1_score: 0.7779\n",
            "Epoch 00198: val_f1_score improved from 0.77689 to 0.77894, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3482 - f1_score: 0.7780 - val_loss: 0.3245 - val_f1_score: 0.7789\n",
            "Epoch 199/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3481 - f1_score: 0.7783\n",
            "Epoch 00199: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3482 - f1_score: 0.7783 - val_loss: 0.3310 - val_f1_score: 0.7374\n",
            "Epoch 200/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3472 - f1_score: 0.7799\n",
            "Epoch 00200: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3472 - f1_score: 0.7799 - val_loss: 0.3255 - val_f1_score: 0.7578\n",
            "Epoch 201/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3469 - f1_score: 0.7806\n",
            "Epoch 00201: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3469 - f1_score: 0.7807 - val_loss: 0.3259 - val_f1_score: 0.7543\n",
            "Epoch 202/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3461 - f1_score: 0.7824\n",
            "Epoch 00202: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3461 - f1_score: 0.7824 - val_loss: 0.3308 - val_f1_score: 0.7200\n",
            "Epoch 203/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3467 - f1_score: 0.7804\n",
            "Epoch 00203: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3467 - f1_score: 0.7804 - val_loss: 0.3267 - val_f1_score: 0.7480\n",
            "Epoch 204/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.3452 - f1_score: 0.7824\n",
            "Epoch 00204: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3452 - f1_score: 0.7823 - val_loss: 0.3187 - val_f1_score: 0.7714\n",
            "Epoch 205/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3442 - f1_score: 0.7830\n",
            "Epoch 00205: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3442 - f1_score: 0.7829 - val_loss: 0.3241 - val_f1_score: 0.7431\n",
            "Epoch 206/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3442 - f1_score: 0.7836\n",
            "Epoch 00206: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3442 - f1_score: 0.7836 - val_loss: 0.3309 - val_f1_score: 0.7284\n",
            "Epoch 207/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3435 - f1_score: 0.7842\n",
            "Epoch 00207: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3435 - f1_score: 0.7842 - val_loss: 0.3168 - val_f1_score: 0.7761\n",
            "Epoch 208/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3437 - f1_score: 0.7847\n",
            "Epoch 00208: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3437 - f1_score: 0.7846 - val_loss: 0.3389 - val_f1_score: 0.7115\n",
            "Epoch 209/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3428 - f1_score: 0.7837\n",
            "Epoch 00209: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3428 - f1_score: 0.7837 - val_loss: 0.3217 - val_f1_score: 0.7603\n",
            "Epoch 210/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3418 - f1_score: 0.7846\n",
            "Epoch 00210: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3418 - f1_score: 0.7846 - val_loss: 0.3166 - val_f1_score: 0.7635\n",
            "Epoch 211/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3419 - f1_score: 0.7847\n",
            "Epoch 00211: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3419 - f1_score: 0.7846 - val_loss: 0.3312 - val_f1_score: 0.7378\n",
            "Epoch 212/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3414 - f1_score: 0.7860\n",
            "Epoch 00212: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3413 - f1_score: 0.7861 - val_loss: 0.3334 - val_f1_score: 0.7173\n",
            "Epoch 213/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3416 - f1_score: 0.7843\n",
            "Epoch 00213: val_f1_score did not improve from 0.77894\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3416 - f1_score: 0.7843 - val_loss: 0.3253 - val_f1_score: 0.7403\n",
            "Epoch 214/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3401 - f1_score: 0.7884\n",
            "Epoch 00214: val_f1_score improved from 0.77894 to 0.80058, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3400 - f1_score: 0.7885 - val_loss: 0.3132 - val_f1_score: 0.8006\n",
            "Epoch 215/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3395 - f1_score: 0.7884\n",
            "Epoch 00215: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3395 - f1_score: 0.7883 - val_loss: 0.3333 - val_f1_score: 0.7205\n",
            "Epoch 216/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3397 - f1_score: 0.7880\n",
            "Epoch 00216: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3397 - f1_score: 0.7880 - val_loss: 0.3266 - val_f1_score: 0.7276\n",
            "Epoch 217/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3392 - f1_score: 0.7888\n",
            "Epoch 00217: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3391 - f1_score: 0.7889 - val_loss: 0.3131 - val_f1_score: 0.7723\n",
            "Epoch 218/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3383 - f1_score: 0.7897\n",
            "Epoch 00218: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3383 - f1_score: 0.7897 - val_loss: 0.3180 - val_f1_score: 0.7498\n",
            "Epoch 219/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3390 - f1_score: 0.7890\n",
            "Epoch 00219: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3390 - f1_score: 0.7890 - val_loss: 0.3202 - val_f1_score: 0.7654\n",
            "Epoch 220/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3381 - f1_score: 0.7891\n",
            "Epoch 00220: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3381 - f1_score: 0.7891 - val_loss: 0.3215 - val_f1_score: 0.7374\n",
            "Epoch 221/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3383 - f1_score: 0.7896\n",
            "Epoch 00221: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3383 - f1_score: 0.7896 - val_loss: 0.3173 - val_f1_score: 0.7803\n",
            "Epoch 222/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3365 - f1_score: 0.7919\n",
            "Epoch 00222: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3365 - f1_score: 0.7919 - val_loss: 0.3140 - val_f1_score: 0.7683\n",
            "Epoch 223/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3359 - f1_score: 0.7933\n",
            "Epoch 00223: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3359 - f1_score: 0.7933 - val_loss: 0.3172 - val_f1_score: 0.7768\n",
            "Epoch 224/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3359 - f1_score: 0.7929\n",
            "Epoch 00224: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3359 - f1_score: 0.7929 - val_loss: 0.3182 - val_f1_score: 0.7519\n",
            "Epoch 225/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3360 - f1_score: 0.7913\n",
            "Epoch 00225: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3360 - f1_score: 0.7913 - val_loss: 0.3328 - val_f1_score: 0.7283\n",
            "Epoch 226/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3352 - f1_score: 0.7932\n",
            "Epoch 00226: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3352 - f1_score: 0.7932 - val_loss: 0.3211 - val_f1_score: 0.7316\n",
            "Epoch 227/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3350 - f1_score: 0.7933\n",
            "Epoch 00227: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3350 - f1_score: 0.7934 - val_loss: 0.3164 - val_f1_score: 0.7684\n",
            "Epoch 228/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3340 - f1_score: 0.7936\n",
            "Epoch 00228: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3340 - f1_score: 0.7936 - val_loss: 0.3112 - val_f1_score: 0.7725\n",
            "Epoch 229/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3329 - f1_score: 0.7949\n",
            "Epoch 00229: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3330 - f1_score: 0.7949 - val_loss: 0.3205 - val_f1_score: 0.7376\n",
            "Epoch 230/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3336 - f1_score: 0.7950\n",
            "Epoch 00230: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3336 - f1_score: 0.7950 - val_loss: 0.3104 - val_f1_score: 0.7760\n",
            "Epoch 231/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3325 - f1_score: 0.7970\n",
            "Epoch 00231: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3325 - f1_score: 0.7969 - val_loss: 0.3080 - val_f1_score: 0.7865\n",
            "Epoch 232/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3325 - f1_score: 0.7956\n",
            "Epoch 00232: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3325 - f1_score: 0.7956 - val_loss: 0.3170 - val_f1_score: 0.7644\n",
            "Epoch 233/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3324 - f1_score: 0.7953\n",
            "Epoch 00233: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3324 - f1_score: 0.7953 - val_loss: 0.3128 - val_f1_score: 0.7572\n",
            "Epoch 234/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3318 - f1_score: 0.7956\n",
            "Epoch 00234: val_f1_score did not improve from 0.80058\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3318 - f1_score: 0.7956 - val_loss: 0.3274 - val_f1_score: 0.7422\n",
            "Epoch 235/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3316 - f1_score: 0.7965\n",
            "Epoch 00235: val_f1_score improved from 0.80058 to 0.80489, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3316 - f1_score: 0.7965 - val_loss: 0.3102 - val_f1_score: 0.8049\n",
            "Epoch 236/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3313 - f1_score: 0.7970\n",
            "Epoch 00236: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3313 - f1_score: 0.7970 - val_loss: 0.3225 - val_f1_score: 0.7377\n",
            "Epoch 237/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3302 - f1_score: 0.7980\n",
            "Epoch 00237: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3302 - f1_score: 0.7980 - val_loss: 0.3137 - val_f1_score: 0.7670\n",
            "Epoch 238/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3297 - f1_score: 0.7987\n",
            "Epoch 00238: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3298 - f1_score: 0.7987 - val_loss: 0.3147 - val_f1_score: 0.7636\n",
            "Epoch 239/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3299 - f1_score: 0.7982\n",
            "Epoch 00239: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3300 - f1_score: 0.7982 - val_loss: 0.3195 - val_f1_score: 0.7428\n",
            "Epoch 240/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3291 - f1_score: 0.7997\n",
            "Epoch 00240: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3291 - f1_score: 0.7996 - val_loss: 0.3101 - val_f1_score: 0.7594\n",
            "Epoch 241/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3288 - f1_score: 0.8010\n",
            "Epoch 00241: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3287 - f1_score: 0.8011 - val_loss: 0.3143 - val_f1_score: 0.7431\n",
            "Epoch 242/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3297 - f1_score: 0.7979\n",
            "Epoch 00242: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3297 - f1_score: 0.7979 - val_loss: 0.3115 - val_f1_score: 0.7716\n",
            "Epoch 243/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3286 - f1_score: 0.7997\n",
            "Epoch 00243: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3286 - f1_score: 0.7997 - val_loss: 0.3146 - val_f1_score: 0.7548\n",
            "Epoch 244/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3280 - f1_score: 0.8013\n",
            "Epoch 00244: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3280 - f1_score: 0.8013 - val_loss: 0.3082 - val_f1_score: 0.7715\n",
            "Epoch 245/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3274 - f1_score: 0.8011\n",
            "Epoch 00245: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3275 - f1_score: 0.8010 - val_loss: 0.3162 - val_f1_score: 0.7498\n",
            "Epoch 246/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3269 - f1_score: 0.8015\n",
            "Epoch 00246: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3269 - f1_score: 0.8014 - val_loss: 0.3100 - val_f1_score: 0.7498\n",
            "Epoch 247/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3271 - f1_score: 0.8006\n",
            "Epoch 00247: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3270 - f1_score: 0.8006 - val_loss: 0.3006 - val_f1_score: 0.7668\n",
            "Epoch 248/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3265 - f1_score: 0.8015\n",
            "Epoch 00248: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3264 - f1_score: 0.8015 - val_loss: 0.3019 - val_f1_score: 0.7763\n",
            "Epoch 249/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3268 - f1_score: 0.8015\n",
            "Epoch 00249: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3267 - f1_score: 0.8015 - val_loss: 0.3062 - val_f1_score: 0.7759\n",
            "Epoch 250/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3255 - f1_score: 0.8034\n",
            "Epoch 00250: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3255 - f1_score: 0.8034 - val_loss: 0.3148 - val_f1_score: 0.7510\n",
            "Epoch 251/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3254 - f1_score: 0.8034\n",
            "Epoch 00251: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3254 - f1_score: 0.8035 - val_loss: 0.3176 - val_f1_score: 0.7550\n",
            "Epoch 252/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3247 - f1_score: 0.8034\n",
            "Epoch 00252: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3247 - f1_score: 0.8033 - val_loss: 0.3160 - val_f1_score: 0.7361\n",
            "Epoch 253/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3240 - f1_score: 0.8042\n",
            "Epoch 00253: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3240 - f1_score: 0.8042 - val_loss: 0.2991 - val_f1_score: 0.7990\n",
            "Epoch 254/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3239 - f1_score: 0.8038\n",
            "Epoch 00254: val_f1_score did not improve from 0.80489\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3239 - f1_score: 0.8039 - val_loss: 0.3053 - val_f1_score: 0.7773\n",
            "Epoch 255/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.3231 - f1_score: 0.8049\n",
            "Epoch 00255: val_f1_score improved from 0.80489 to 0.80805, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3231 - f1_score: 0.8048 - val_loss: 0.2978 - val_f1_score: 0.8080\n",
            "Epoch 256/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3234 - f1_score: 0.8049\n",
            "Epoch 00256: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3234 - f1_score: 0.8049 - val_loss: 0.3029 - val_f1_score: 0.7673\n",
            "Epoch 257/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3233 - f1_score: 0.8037\n",
            "Epoch 00257: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3233 - f1_score: 0.8037 - val_loss: 0.3101 - val_f1_score: 0.7800\n",
            "Epoch 258/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3220 - f1_score: 0.8059\n",
            "Epoch 00258: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3220 - f1_score: 0.8060 - val_loss: 0.3043 - val_f1_score: 0.7690\n",
            "Epoch 259/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3227 - f1_score: 0.8052\n",
            "Epoch 00259: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3227 - f1_score: 0.8052 - val_loss: 0.3047 - val_f1_score: 0.7572\n",
            "Epoch 260/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3218 - f1_score: 0.8079\n",
            "Epoch 00260: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3218 - f1_score: 0.8079 - val_loss: 0.3078 - val_f1_score: 0.7738\n",
            "Epoch 261/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3209 - f1_score: 0.8072\n",
            "Epoch 00261: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3209 - f1_score: 0.8072 - val_loss: 0.3052 - val_f1_score: 0.7706\n",
            "Epoch 262/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3221 - f1_score: 0.8050\n",
            "Epoch 00262: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3220 - f1_score: 0.8050 - val_loss: 0.2993 - val_f1_score: 0.7914\n",
            "Epoch 263/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3208 - f1_score: 0.8080\n",
            "Epoch 00263: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3208 - f1_score: 0.8080 - val_loss: 0.2973 - val_f1_score: 0.8016\n",
            "Epoch 264/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3208 - f1_score: 0.8064\n",
            "Epoch 00264: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3208 - f1_score: 0.8064 - val_loss: 0.2984 - val_f1_score: 0.7793\n",
            "Epoch 265/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3212 - f1_score: 0.8076\n",
            "Epoch 00265: val_f1_score did not improve from 0.80805\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3212 - f1_score: 0.8077 - val_loss: 0.2956 - val_f1_score: 0.7975\n",
            "Epoch 266/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3203 - f1_score: 0.8062\n",
            "Epoch 00266: val_f1_score improved from 0.80805 to 0.82390, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3203 - f1_score: 0.8062 - val_loss: 0.2994 - val_f1_score: 0.8239\n",
            "Epoch 267/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3193 - f1_score: 0.8087\n",
            "Epoch 00267: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3193 - f1_score: 0.8087 - val_loss: 0.2998 - val_f1_score: 0.7986\n",
            "Epoch 268/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3193 - f1_score: 0.8081\n",
            "Epoch 00268: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3193 - f1_score: 0.8082 - val_loss: 0.3024 - val_f1_score: 0.7709\n",
            "Epoch 269/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3183 - f1_score: 0.8093\n",
            "Epoch 00269: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3184 - f1_score: 0.8093 - val_loss: 0.2980 - val_f1_score: 0.7818\n",
            "Epoch 270/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3186 - f1_score: 0.8089\n",
            "Epoch 00270: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3186 - f1_score: 0.8089 - val_loss: 0.3132 - val_f1_score: 0.7448\n",
            "Epoch 271/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3179 - f1_score: 0.8099\n",
            "Epoch 00271: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3179 - f1_score: 0.8098 - val_loss: 0.2942 - val_f1_score: 0.8078\n",
            "Epoch 272/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3177 - f1_score: 0.8100\n",
            "Epoch 00272: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3177 - f1_score: 0.8100 - val_loss: 0.3085 - val_f1_score: 0.7480\n",
            "Epoch 273/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3177 - f1_score: 0.8092\n",
            "Epoch 00273: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3177 - f1_score: 0.8092 - val_loss: 0.3016 - val_f1_score: 0.7775\n",
            "Epoch 274/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3172 - f1_score: 0.8106\n",
            "Epoch 00274: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.3172 - f1_score: 0.8106 - val_loss: 0.3214 - val_f1_score: 0.7321\n",
            "Epoch 275/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3166 - f1_score: 0.8125\n",
            "Epoch 00275: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3166 - f1_score: 0.8124 - val_loss: 0.3001 - val_f1_score: 0.7846\n",
            "Epoch 276/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3166 - f1_score: 0.8105\n",
            "Epoch 00276: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3166 - f1_score: 0.8105 - val_loss: 0.3015 - val_f1_score: 0.7669\n",
            "Epoch 277/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3157 - f1_score: 0.8118\n",
            "Epoch 00277: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3157 - f1_score: 0.8117 - val_loss: 0.3075 - val_f1_score: 0.7489\n",
            "Epoch 278/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3146 - f1_score: 0.8129\n",
            "Epoch 00278: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3146 - f1_score: 0.8129 - val_loss: 0.2958 - val_f1_score: 0.7834\n",
            "Epoch 279/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3157 - f1_score: 0.8121\n",
            "Epoch 00279: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3157 - f1_score: 0.8119 - val_loss: 0.2993 - val_f1_score: 0.7698\n",
            "Epoch 280/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3144 - f1_score: 0.8140\n",
            "Epoch 00280: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3144 - f1_score: 0.8141 - val_loss: 0.3083 - val_f1_score: 0.7419\n",
            "Epoch 281/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3142 - f1_score: 0.8130\n",
            "Epoch 00281: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3142 - f1_score: 0.8130 - val_loss: 0.3061 - val_f1_score: 0.7533\n",
            "Epoch 282/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3143 - f1_score: 0.8136\n",
            "Epoch 00282: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3143 - f1_score: 0.8136 - val_loss: 0.3024 - val_f1_score: 0.7601\n",
            "Epoch 283/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3135 - f1_score: 0.8148\n",
            "Epoch 00283: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3135 - f1_score: 0.8149 - val_loss: 0.2957 - val_f1_score: 0.7828\n",
            "Epoch 284/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3125 - f1_score: 0.8164\n",
            "Epoch 00284: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3125 - f1_score: 0.8164 - val_loss: 0.2984 - val_f1_score: 0.7787\n",
            "Epoch 285/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3133 - f1_score: 0.8139\n",
            "Epoch 00285: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3133 - f1_score: 0.8139 - val_loss: 0.2936 - val_f1_score: 0.7869\n",
            "Epoch 286/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3134 - f1_score: 0.8147\n",
            "Epoch 00286: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3134 - f1_score: 0.8147 - val_loss: 0.2915 - val_f1_score: 0.7922\n",
            "Epoch 287/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3130 - f1_score: 0.8135\n",
            "Epoch 00287: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3130 - f1_score: 0.8135 - val_loss: 0.3070 - val_f1_score: 0.7497\n",
            "Epoch 288/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3122 - f1_score: 0.8154\n",
            "Epoch 00288: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3123 - f1_score: 0.8154 - val_loss: 0.3009 - val_f1_score: 0.7549\n",
            "Epoch 289/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3116 - f1_score: 0.8166\n",
            "Epoch 00289: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3116 - f1_score: 0.8165 - val_loss: 0.3078 - val_f1_score: 0.7412\n",
            "Epoch 290/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3120 - f1_score: 0.8143\n",
            "Epoch 00290: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3120 - f1_score: 0.8143 - val_loss: 0.2914 - val_f1_score: 0.7860\n",
            "Epoch 291/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3123 - f1_score: 0.8145\n",
            "Epoch 00291: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3122 - f1_score: 0.8146 - val_loss: 0.2954 - val_f1_score: 0.7877\n",
            "Epoch 292/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3113 - f1_score: 0.8157\n",
            "Epoch 00292: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3113 - f1_score: 0.8157 - val_loss: 0.2994 - val_f1_score: 0.7776\n",
            "Epoch 293/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3104 - f1_score: 0.8161\n",
            "Epoch 00293: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3104 - f1_score: 0.8161 - val_loss: 0.3103 - val_f1_score: 0.7392\n",
            "Epoch 294/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3116 - f1_score: 0.8155\n",
            "Epoch 00294: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3116 - f1_score: 0.8156 - val_loss: 0.2870 - val_f1_score: 0.8168\n",
            "Epoch 295/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3099 - f1_score: 0.8161\n",
            "Epoch 00295: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3099 - f1_score: 0.8161 - val_loss: 0.2986 - val_f1_score: 0.7636\n",
            "Epoch 296/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3098 - f1_score: 0.8172\n",
            "Epoch 00296: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3098 - f1_score: 0.8172 - val_loss: 0.2878 - val_f1_score: 0.7855\n",
            "Epoch 297/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3092 - f1_score: 0.8183\n",
            "Epoch 00297: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3092 - f1_score: 0.8184 - val_loss: 0.2852 - val_f1_score: 0.7995\n",
            "Epoch 298/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3090 - f1_score: 0.8185\n",
            "Epoch 00298: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3090 - f1_score: 0.8185 - val_loss: 0.2890 - val_f1_score: 0.7932\n",
            "Epoch 299/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3087 - f1_score: 0.8195\n",
            "Epoch 00299: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3088 - f1_score: 0.8194 - val_loss: 0.3192 - val_f1_score: 0.7053\n",
            "Epoch 300/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3087 - f1_score: 0.8177\n",
            "Epoch 00300: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3087 - f1_score: 0.8177 - val_loss: 0.2873 - val_f1_score: 0.7914\n",
            "Epoch 301/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3082 - f1_score: 0.8177\n",
            "Epoch 00301: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3082 - f1_score: 0.8175 - val_loss: 0.2958 - val_f1_score: 0.7608\n",
            "Epoch 302/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3078 - f1_score: 0.8207\n",
            "Epoch 00302: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3078 - f1_score: 0.8207 - val_loss: 0.2928 - val_f1_score: 0.7674\n",
            "Epoch 303/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3078 - f1_score: 0.8187\n",
            "Epoch 00303: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3078 - f1_score: 0.8187 - val_loss: 0.2954 - val_f1_score: 0.7647\n",
            "Epoch 304/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3075 - f1_score: 0.8189\n",
            "Epoch 00304: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3075 - f1_score: 0.8188 - val_loss: 0.2905 - val_f1_score: 0.8007\n",
            "Epoch 305/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3074 - f1_score: 0.8192\n",
            "Epoch 00305: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3075 - f1_score: 0.8193 - val_loss: 0.2999 - val_f1_score: 0.7768\n",
            "Epoch 306/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3073 - f1_score: 0.8189\n",
            "Epoch 00306: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3073 - f1_score: 0.8189 - val_loss: 0.3100 - val_f1_score: 0.7356\n",
            "Epoch 307/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3076 - f1_score: 0.8182\n",
            "Epoch 00307: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3076 - f1_score: 0.8182 - val_loss: 0.2924 - val_f1_score: 0.7728\n",
            "Epoch 308/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3065 - f1_score: 0.8205\n",
            "Epoch 00308: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3065 - f1_score: 0.8205 - val_loss: 0.3006 - val_f1_score: 0.7545\n",
            "Epoch 309/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3060 - f1_score: 0.8203\n",
            "Epoch 00309: val_f1_score did not improve from 0.82390\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3060 - f1_score: 0.8203 - val_loss: 0.2966 - val_f1_score: 0.7678\n",
            "Epoch 310/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.3061 - f1_score: 0.8189\n",
            "Epoch 00310: val_f1_score improved from 0.82390 to 0.83645, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3061 - f1_score: 0.8190 - val_loss: 0.2826 - val_f1_score: 0.8364\n",
            "Epoch 311/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3055 - f1_score: 0.8199\n",
            "Epoch 00311: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3055 - f1_score: 0.8198 - val_loss: 0.2917 - val_f1_score: 0.7817\n",
            "Epoch 312/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.3052 - f1_score: 0.8199\n",
            "Epoch 00312: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3052 - f1_score: 0.8199 - val_loss: 0.2944 - val_f1_score: 0.7792\n",
            "Epoch 313/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3045 - f1_score: 0.8225\n",
            "Epoch 00313: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3045 - f1_score: 0.8225 - val_loss: 0.2849 - val_f1_score: 0.8091\n",
            "Epoch 314/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3050 - f1_score: 0.8217\n",
            "Epoch 00314: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 7ms/step - loss: 0.3050 - f1_score: 0.8217 - val_loss: 0.2836 - val_f1_score: 0.7980\n",
            "Epoch 315/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3044 - f1_score: 0.8218\n",
            "Epoch 00315: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3044 - f1_score: 0.8218 - val_loss: 0.2843 - val_f1_score: 0.8094\n",
            "Epoch 316/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.3034 - f1_score: 0.8234\n",
            "Epoch 00316: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3034 - f1_score: 0.8234 - val_loss: 0.2857 - val_f1_score: 0.8154\n",
            "Epoch 317/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3032 - f1_score: 0.8239\n",
            "Epoch 00317: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3032 - f1_score: 0.8239 - val_loss: 0.3175 - val_f1_score: 0.7368\n",
            "Epoch 318/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3034 - f1_score: 0.8236\n",
            "Epoch 00318: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3034 - f1_score: 0.8235 - val_loss: 0.2954 - val_f1_score: 0.7644\n",
            "Epoch 319/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3035 - f1_score: 0.8228\n",
            "Epoch 00319: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3035 - f1_score: 0.8229 - val_loss: 0.2894 - val_f1_score: 0.8142\n",
            "Epoch 320/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.3034 - f1_score: 0.8217\n",
            "Epoch 00320: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3033 - f1_score: 0.8218 - val_loss: 0.2876 - val_f1_score: 0.7794\n",
            "Epoch 321/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3030 - f1_score: 0.8235\n",
            "Epoch 00321: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3030 - f1_score: 0.8235 - val_loss: 0.2984 - val_f1_score: 0.7636\n",
            "Epoch 322/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3016 - f1_score: 0.8237\n",
            "Epoch 00322: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3016 - f1_score: 0.8237 - val_loss: 0.2875 - val_f1_score: 0.7847\n",
            "Epoch 323/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.3008 - f1_score: 0.8263\n",
            "Epoch 00323: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3008 - f1_score: 0.8262 - val_loss: 0.2993 - val_f1_score: 0.7739\n",
            "Epoch 324/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3011 - f1_score: 0.8248\n",
            "Epoch 00324: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3010 - f1_score: 0.8248 - val_loss: 0.2959 - val_f1_score: 0.7459\n",
            "Epoch 325/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.3018 - f1_score: 0.8237\n",
            "Epoch 00325: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3018 - f1_score: 0.8237 - val_loss: 0.2967 - val_f1_score: 0.7526\n",
            "Epoch 326/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3006 - f1_score: 0.8264\n",
            "Epoch 00326: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3006 - f1_score: 0.8264 - val_loss: 0.2998 - val_f1_score: 0.7501\n",
            "Epoch 327/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3008 - f1_score: 0.8245\n",
            "Epoch 00327: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3009 - f1_score: 0.8246 - val_loss: 0.2826 - val_f1_score: 0.8149\n",
            "Epoch 328/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.3006 - f1_score: 0.8248\n",
            "Epoch 00328: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3007 - f1_score: 0.8247 - val_loss: 0.2759 - val_f1_score: 0.8243\n",
            "Epoch 329/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.3004 - f1_score: 0.8252\n",
            "Epoch 00329: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.3004 - f1_score: 0.8253 - val_loss: 0.2805 - val_f1_score: 0.8160\n",
            "Epoch 330/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.3000 - f1_score: 0.8248\n",
            "Epoch 00330: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3000 - f1_score: 0.8248 - val_loss: 0.2969 - val_f1_score: 0.7618\n",
            "Epoch 331/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.3001 - f1_score: 0.8253\n",
            "Epoch 00331: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.3001 - f1_score: 0.8252 - val_loss: 0.2792 - val_f1_score: 0.8074\n",
            "Epoch 332/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2988 - f1_score: 0.8275\n",
            "Epoch 00332: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2988 - f1_score: 0.8276 - val_loss: 0.2854 - val_f1_score: 0.7973\n",
            "Epoch 333/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2986 - f1_score: 0.8263\n",
            "Epoch 00333: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2986 - f1_score: 0.8263 - val_loss: 0.2798 - val_f1_score: 0.7964\n",
            "Epoch 334/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2988 - f1_score: 0.8268\n",
            "Epoch 00334: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2987 - f1_score: 0.8268 - val_loss: 0.2904 - val_f1_score: 0.8011\n",
            "Epoch 335/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2983 - f1_score: 0.8271\n",
            "Epoch 00335: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2983 - f1_score: 0.8271 - val_loss: 0.2913 - val_f1_score: 0.7616\n",
            "Epoch 336/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2986 - f1_score: 0.8263\n",
            "Epoch 00336: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2985 - f1_score: 0.8263 - val_loss: 0.2823 - val_f1_score: 0.8010\n",
            "Epoch 337/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2980 - f1_score: 0.8279\n",
            "Epoch 00337: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2980 - f1_score: 0.8279 - val_loss: 0.2801 - val_f1_score: 0.7957\n",
            "Epoch 338/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2978 - f1_score: 0.8274\n",
            "Epoch 00338: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2978 - f1_score: 0.8275 - val_loss: 0.2797 - val_f1_score: 0.8094\n",
            "Epoch 339/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2968 - f1_score: 0.8299\n",
            "Epoch 00339: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2968 - f1_score: 0.8299 - val_loss: 0.2876 - val_f1_score: 0.7751\n",
            "Epoch 340/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2987 - f1_score: 0.8275\n",
            "Epoch 00340: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2988 - f1_score: 0.8275 - val_loss: 0.2949 - val_f1_score: 0.7603\n",
            "Epoch 341/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2969 - f1_score: 0.8282\n",
            "Epoch 00341: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2969 - f1_score: 0.8282 - val_loss: 0.2821 - val_f1_score: 0.7788\n",
            "Epoch 342/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2964 - f1_score: 0.8282\n",
            "Epoch 00342: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2965 - f1_score: 0.8281 - val_loss: 0.2756 - val_f1_score: 0.8148\n",
            "Epoch 343/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2962 - f1_score: 0.8288\n",
            "Epoch 00343: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2962 - f1_score: 0.8288 - val_loss: 0.2932 - val_f1_score: 0.7593\n",
            "Epoch 344/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2957 - f1_score: 0.8304\n",
            "Epoch 00344: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2957 - f1_score: 0.8304 - val_loss: 0.2758 - val_f1_score: 0.8094\n",
            "Epoch 345/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2951 - f1_score: 0.8297\n",
            "Epoch 00345: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2951 - f1_score: 0.8297 - val_loss: 0.2786 - val_f1_score: 0.7955\n",
            "Epoch 346/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.2951 - f1_score: 0.8311\n",
            "Epoch 00346: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2952 - f1_score: 0.8309 - val_loss: 0.2950 - val_f1_score: 0.7509\n",
            "Epoch 347/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2957 - f1_score: 0.8294\n",
            "Epoch 00347: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2956 - f1_score: 0.8294 - val_loss: 0.2753 - val_f1_score: 0.8097\n",
            "Epoch 348/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2945 - f1_score: 0.8296\n",
            "Epoch 00348: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2945 - f1_score: 0.8295 - val_loss: 0.2864 - val_f1_score: 0.7769\n",
            "Epoch 349/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2944 - f1_score: 0.8307\n",
            "Epoch 00349: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2944 - f1_score: 0.8307 - val_loss: 0.2737 - val_f1_score: 0.8082\n",
            "Epoch 350/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2952 - f1_score: 0.8294\n",
            "Epoch 00350: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2952 - f1_score: 0.8295 - val_loss: 0.2800 - val_f1_score: 0.8041\n",
            "Epoch 351/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2944 - f1_score: 0.8306\n",
            "Epoch 00351: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2944 - f1_score: 0.8306 - val_loss: 0.2774 - val_f1_score: 0.8034\n",
            "Epoch 352/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2940 - f1_score: 0.8312\n",
            "Epoch 00352: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2940 - f1_score: 0.8312 - val_loss: 0.2802 - val_f1_score: 0.7785\n",
            "Epoch 353/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2932 - f1_score: 0.8325\n",
            "Epoch 00353: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2932 - f1_score: 0.8324 - val_loss: 0.2877 - val_f1_score: 0.7594\n",
            "Epoch 354/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2934 - f1_score: 0.8312\n",
            "Epoch 00354: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2934 - f1_score: 0.8312 - val_loss: 0.2832 - val_f1_score: 0.7791\n",
            "Epoch 355/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2940 - f1_score: 0.8313\n",
            "Epoch 00355: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2939 - f1_score: 0.8313 - val_loss: 0.2739 - val_f1_score: 0.8017\n",
            "Epoch 356/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.2929 - f1_score: 0.8314\n",
            "Epoch 00356: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2929 - f1_score: 0.8315 - val_loss: 0.2938 - val_f1_score: 0.7694\n",
            "Epoch 357/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2929 - f1_score: 0.8315\n",
            "Epoch 00357: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2929 - f1_score: 0.8315 - val_loss: 0.2845 - val_f1_score: 0.8081\n",
            "Epoch 358/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2928 - f1_score: 0.8311\n",
            "Epoch 00358: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2928 - f1_score: 0.8310 - val_loss: 0.2930 - val_f1_score: 0.7724\n",
            "Epoch 359/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2917 - f1_score: 0.8322\n",
            "Epoch 00359: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2918 - f1_score: 0.8322 - val_loss: 0.2941 - val_f1_score: 0.7534\n",
            "Epoch 360/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2922 - f1_score: 0.8324\n",
            "Epoch 00360: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2922 - f1_score: 0.8324 - val_loss: 0.2811 - val_f1_score: 0.7867\n",
            "Epoch 361/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.2922 - f1_score: 0.8309\n",
            "Epoch 00361: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2922 - f1_score: 0.8309 - val_loss: 0.2730 - val_f1_score: 0.8223\n",
            "Epoch 362/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2918 - f1_score: 0.8319\n",
            "Epoch 00362: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2918 - f1_score: 0.8319 - val_loss: 0.2802 - val_f1_score: 0.7968\n",
            "Epoch 363/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2912 - f1_score: 0.8336\n",
            "Epoch 00363: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2912 - f1_score: 0.8336 - val_loss: 0.2836 - val_f1_score: 0.7713\n",
            "Epoch 364/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2914 - f1_score: 0.8327\n",
            "Epoch 00364: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2914 - f1_score: 0.8327 - val_loss: 0.2783 - val_f1_score: 0.7911\n",
            "Epoch 365/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2915 - f1_score: 0.8320\n",
            "Epoch 00365: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2915 - f1_score: 0.8320 - val_loss: 0.2777 - val_f1_score: 0.7937\n",
            "Epoch 366/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2908 - f1_score: 0.8322\n",
            "Epoch 00366: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2908 - f1_score: 0.8321 - val_loss: 0.2726 - val_f1_score: 0.8094\n",
            "Epoch 367/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2910 - f1_score: 0.8335\n",
            "Epoch 00367: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2910 - f1_score: 0.8334 - val_loss: 0.2917 - val_f1_score: 0.7682\n",
            "Epoch 368/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2904 - f1_score: 0.8329\n",
            "Epoch 00368: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2904 - f1_score: 0.8329 - val_loss: 0.2854 - val_f1_score: 0.7606\n",
            "Epoch 369/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2900 - f1_score: 0.8330\n",
            "Epoch 00369: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2901 - f1_score: 0.8329 - val_loss: 0.2811 - val_f1_score: 0.7705\n",
            "Epoch 370/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2893 - f1_score: 0.8343\n",
            "Epoch 00370: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2893 - f1_score: 0.8343 - val_loss: 0.2867 - val_f1_score: 0.7593\n",
            "Epoch 371/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2892 - f1_score: 0.8346\n",
            "Epoch 00371: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2892 - f1_score: 0.8346 - val_loss: 0.2767 - val_f1_score: 0.7673\n",
            "Epoch 372/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2901 - f1_score: 0.8331\n",
            "Epoch 00372: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2901 - f1_score: 0.8331 - val_loss: 0.2796 - val_f1_score: 0.8065\n",
            "Epoch 373/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2892 - f1_score: 0.8343\n",
            "Epoch 00373: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2892 - f1_score: 0.8344 - val_loss: 0.2712 - val_f1_score: 0.8174\n",
            "Epoch 374/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2890 - f1_score: 0.8342\n",
            "Epoch 00374: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2890 - f1_score: 0.8341 - val_loss: 0.2856 - val_f1_score: 0.7738\n",
            "Epoch 375/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2894 - f1_score: 0.8335\n",
            "Epoch 00375: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2894 - f1_score: 0.8334 - val_loss: 0.2835 - val_f1_score: 0.7777\n",
            "Epoch 376/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.2879 - f1_score: 0.8365\n",
            "Epoch 00376: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2878 - f1_score: 0.8365 - val_loss: 0.2689 - val_f1_score: 0.8132\n",
            "Epoch 377/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2884 - f1_score: 0.8343\n",
            "Epoch 00377: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2884 - f1_score: 0.8344 - val_loss: 0.2738 - val_f1_score: 0.7877\n",
            "Epoch 378/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2882 - f1_score: 0.8356\n",
            "Epoch 00378: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2882 - f1_score: 0.8356 - val_loss: 0.2706 - val_f1_score: 0.8040\n",
            "Epoch 379/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2882 - f1_score: 0.8334\n",
            "Epoch 00379: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2882 - f1_score: 0.8334 - val_loss: 0.2866 - val_f1_score: 0.7553\n",
            "Epoch 380/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2874 - f1_score: 0.8367\n",
            "Epoch 00380: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2874 - f1_score: 0.8367 - val_loss: 0.2837 - val_f1_score: 0.7763\n",
            "Epoch 381/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2878 - f1_score: 0.8341\n",
            "Epoch 00381: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2878 - f1_score: 0.8341 - val_loss: 0.2781 - val_f1_score: 0.7893\n",
            "Epoch 382/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2865 - f1_score: 0.8370\n",
            "Epoch 00382: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2865 - f1_score: 0.8369 - val_loss: 0.2818 - val_f1_score: 0.7620\n",
            "Epoch 383/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2867 - f1_score: 0.8355\n",
            "Epoch 00383: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2867 - f1_score: 0.8355 - val_loss: 0.2734 - val_f1_score: 0.8135\n",
            "Epoch 384/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2867 - f1_score: 0.8364\n",
            "Epoch 00384: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2867 - f1_score: 0.8364 - val_loss: 0.2713 - val_f1_score: 0.8053\n",
            "Epoch 385/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2873 - f1_score: 0.8355\n",
            "Epoch 00385: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2873 - f1_score: 0.8355 - val_loss: 0.2852 - val_f1_score: 0.7809\n",
            "Epoch 386/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2855 - f1_score: 0.8378\n",
            "Epoch 00386: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2855 - f1_score: 0.8377 - val_loss: 0.2787 - val_f1_score: 0.7687\n",
            "Epoch 387/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2861 - f1_score: 0.8363\n",
            "Epoch 00387: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2861 - f1_score: 0.8363 - val_loss: 0.2783 - val_f1_score: 0.7723\n",
            "Epoch 388/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2853 - f1_score: 0.8369\n",
            "Epoch 00388: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2853 - f1_score: 0.8369 - val_loss: 0.2778 - val_f1_score: 0.7877\n",
            "Epoch 389/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2845 - f1_score: 0.8380\n",
            "Epoch 00389: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2845 - f1_score: 0.8380 - val_loss: 0.2748 - val_f1_score: 0.7890\n",
            "Epoch 390/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2856 - f1_score: 0.8360\n",
            "Epoch 00390: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2856 - f1_score: 0.8360 - val_loss: 0.2798 - val_f1_score: 0.7823\n",
            "Epoch 391/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2839 - f1_score: 0.8393\n",
            "Epoch 00391: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2839 - f1_score: 0.8393 - val_loss: 0.2642 - val_f1_score: 0.8193\n",
            "Epoch 392/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2848 - f1_score: 0.8376\n",
            "Epoch 00392: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2848 - f1_score: 0.8377 - val_loss: 0.2658 - val_f1_score: 0.8325\n",
            "Epoch 393/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2842 - f1_score: 0.8376\n",
            "Epoch 00393: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2842 - f1_score: 0.8376 - val_loss: 0.2735 - val_f1_score: 0.7886\n",
            "Epoch 394/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2848 - f1_score: 0.8365\n",
            "Epoch 00394: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2848 - f1_score: 0.8365 - val_loss: 0.2766 - val_f1_score: 0.7826\n",
            "Epoch 395/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2841 - f1_score: 0.8375\n",
            "Epoch 00395: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2841 - f1_score: 0.8376 - val_loss: 0.2739 - val_f1_score: 0.7802\n",
            "Epoch 396/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2836 - f1_score: 0.8388\n",
            "Epoch 00396: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2835 - f1_score: 0.8388 - val_loss: 0.2681 - val_f1_score: 0.8006\n",
            "Epoch 397/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2840 - f1_score: 0.8373\n",
            "Epoch 00397: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2840 - f1_score: 0.8373 - val_loss: 0.2672 - val_f1_score: 0.8217\n",
            "Epoch 398/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2832 - f1_score: 0.8385\n",
            "Epoch 00398: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2831 - f1_score: 0.8386 - val_loss: 0.2711 - val_f1_score: 0.7817\n",
            "Epoch 399/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2833 - f1_score: 0.8385\n",
            "Epoch 00399: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2833 - f1_score: 0.8385 - val_loss: 0.2710 - val_f1_score: 0.8050\n",
            "Epoch 400/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2828 - f1_score: 0.8396\n",
            "Epoch 00400: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2828 - f1_score: 0.8396 - val_loss: 0.2712 - val_f1_score: 0.7929\n",
            "Epoch 401/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2832 - f1_score: 0.8390\n",
            "Epoch 00401: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2832 - f1_score: 0.8390 - val_loss: 0.2712 - val_f1_score: 0.8003\n",
            "Epoch 402/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2837 - f1_score: 0.8372\n",
            "Epoch 00402: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2837 - f1_score: 0.8371 - val_loss: 0.2726 - val_f1_score: 0.7819\n",
            "Epoch 403/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2827 - f1_score: 0.8394\n",
            "Epoch 00403: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2827 - f1_score: 0.8394 - val_loss: 0.2637 - val_f1_score: 0.8077\n",
            "Epoch 404/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2818 - f1_score: 0.8396\n",
            "Epoch 00404: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2818 - f1_score: 0.8396 - val_loss: 0.2687 - val_f1_score: 0.7926\n",
            "Epoch 405/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2819 - f1_score: 0.8401\n",
            "Epoch 00405: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2819 - f1_score: 0.8401 - val_loss: 0.2747 - val_f1_score: 0.7876\n",
            "Epoch 406/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2810 - f1_score: 0.8422\n",
            "Epoch 00406: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2810 - f1_score: 0.8422 - val_loss: 0.2851 - val_f1_score: 0.7625\n",
            "Epoch 407/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2813 - f1_score: 0.8404\n",
            "Epoch 00407: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2813 - f1_score: 0.8404 - val_loss: 0.2812 - val_f1_score: 0.7809\n",
            "Epoch 408/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2817 - f1_score: 0.8402\n",
            "Epoch 00408: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2817 - f1_score: 0.8401 - val_loss: 0.2675 - val_f1_score: 0.8123\n",
            "Epoch 409/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2813 - f1_score: 0.8391\n",
            "Epoch 00409: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2813 - f1_score: 0.8390 - val_loss: 0.2777 - val_f1_score: 0.7794\n",
            "Epoch 410/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2803 - f1_score: 0.8422\n",
            "Epoch 00410: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2803 - f1_score: 0.8421 - val_loss: 0.2736 - val_f1_score: 0.7913\n",
            "Epoch 411/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2800 - f1_score: 0.8421\n",
            "Epoch 00411: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2800 - f1_score: 0.8421 - val_loss: 0.2681 - val_f1_score: 0.8008\n",
            "Epoch 412/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2805 - f1_score: 0.8404\n",
            "Epoch 00412: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2805 - f1_score: 0.8404 - val_loss: 0.2856 - val_f1_score: 0.7823\n",
            "Epoch 413/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.2799 - f1_score: 0.8415\n",
            "Epoch 00413: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2799 - f1_score: 0.8413 - val_loss: 0.2719 - val_f1_score: 0.7678\n",
            "Epoch 414/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2803 - f1_score: 0.8400\n",
            "Epoch 00414: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2803 - f1_score: 0.8400 - val_loss: 0.2597 - val_f1_score: 0.8141\n",
            "Epoch 415/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2806 - f1_score: 0.8405\n",
            "Epoch 00415: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2806 - f1_score: 0.8405 - val_loss: 0.2611 - val_f1_score: 0.8258\n",
            "Epoch 416/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2799 - f1_score: 0.8423\n",
            "Epoch 00416: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2799 - f1_score: 0.8423 - val_loss: 0.2617 - val_f1_score: 0.8025\n",
            "Epoch 417/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2789 - f1_score: 0.8426\n",
            "Epoch 00417: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2789 - f1_score: 0.8426 - val_loss: 0.2666 - val_f1_score: 0.8073\n",
            "Epoch 418/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2789 - f1_score: 0.8430\n",
            "Epoch 00418: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2790 - f1_score: 0.8430 - val_loss: 0.2630 - val_f1_score: 0.8290\n",
            "Epoch 419/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2791 - f1_score: 0.8419\n",
            "Epoch 00419: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2790 - f1_score: 0.8419 - val_loss: 0.2680 - val_f1_score: 0.7930\n",
            "Epoch 420/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2796 - f1_score: 0.8416\n",
            "Epoch 00420: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2796 - f1_score: 0.8415 - val_loss: 0.2712 - val_f1_score: 0.7764\n",
            "Epoch 421/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2781 - f1_score: 0.8429\n",
            "Epoch 00421: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2781 - f1_score: 0.8430 - val_loss: 0.2725 - val_f1_score: 0.7899\n",
            "Epoch 422/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2787 - f1_score: 0.8422\n",
            "Epoch 00422: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2787 - f1_score: 0.8422 - val_loss: 0.2690 - val_f1_score: 0.8048\n",
            "Epoch 423/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2785 - f1_score: 0.8427\n",
            "Epoch 00423: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2785 - f1_score: 0.8427 - val_loss: 0.2622 - val_f1_score: 0.8231\n",
            "Epoch 424/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2784 - f1_score: 0.8418\n",
            "Epoch 00424: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2784 - f1_score: 0.8418 - val_loss: 0.2724 - val_f1_score: 0.7902\n",
            "Epoch 425/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2777 - f1_score: 0.8439\n",
            "Epoch 00425: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2777 - f1_score: 0.8439 - val_loss: 0.2719 - val_f1_score: 0.8124\n",
            "Epoch 426/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2775 - f1_score: 0.8420\n",
            "Epoch 00426: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2775 - f1_score: 0.8420 - val_loss: 0.2718 - val_f1_score: 0.7955\n",
            "Epoch 427/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2776 - f1_score: 0.8431\n",
            "Epoch 00427: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2776 - f1_score: 0.8432 - val_loss: 0.2609 - val_f1_score: 0.8275\n",
            "Epoch 428/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2775 - f1_score: 0.8424\n",
            "Epoch 00428: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2776 - f1_score: 0.8424 - val_loss: 0.2697 - val_f1_score: 0.7883\n",
            "Epoch 429/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2773 - f1_score: 0.8441\n",
            "Epoch 00429: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2773 - f1_score: 0.8441 - val_loss: 0.2673 - val_f1_score: 0.8104\n",
            "Epoch 430/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2768 - f1_score: 0.8434\n",
            "Epoch 00430: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2768 - f1_score: 0.8434 - val_loss: 0.2804 - val_f1_score: 0.7683\n",
            "Epoch 431/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2767 - f1_score: 0.8439\n",
            "Epoch 00431: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2766 - f1_score: 0.8439 - val_loss: 0.2746 - val_f1_score: 0.7791\n",
            "Epoch 432/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2761 - f1_score: 0.8443\n",
            "Epoch 00432: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2761 - f1_score: 0.8443 - val_loss: 0.2630 - val_f1_score: 0.7971\n",
            "Epoch 433/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2764 - f1_score: 0.8431\n",
            "Epoch 00433: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2764 - f1_score: 0.8431 - val_loss: 0.2886 - val_f1_score: 0.7592\n",
            "Epoch 434/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2767 - f1_score: 0.8429\n",
            "Epoch 00434: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2767 - f1_score: 0.8429 - val_loss: 0.2644 - val_f1_score: 0.7903\n",
            "Epoch 435/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2756 - f1_score: 0.8447\n",
            "Epoch 00435: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2756 - f1_score: 0.8447 - val_loss: 0.2820 - val_f1_score: 0.7701\n",
            "Epoch 436/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2761 - f1_score: 0.8434\n",
            "Epoch 00436: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2761 - f1_score: 0.8434 - val_loss: 0.2637 - val_f1_score: 0.8035\n",
            "Epoch 437/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2755 - f1_score: 0.8439\n",
            "Epoch 00437: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2755 - f1_score: 0.8440 - val_loss: 0.2639 - val_f1_score: 0.8185\n",
            "Epoch 438/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2743 - f1_score: 0.8460\n",
            "Epoch 00438: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2743 - f1_score: 0.8461 - val_loss: 0.2620 - val_f1_score: 0.8223\n",
            "Epoch 439/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2750 - f1_score: 0.8447\n",
            "Epoch 00439: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2749 - f1_score: 0.8447 - val_loss: 0.2582 - val_f1_score: 0.8267\n",
            "Epoch 440/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2753 - f1_score: 0.8430\n",
            "Epoch 00440: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2753 - f1_score: 0.8430 - val_loss: 0.2784 - val_f1_score: 0.7920\n",
            "Epoch 441/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2744 - f1_score: 0.8449\n",
            "Epoch 00441: val_f1_score did not improve from 0.83645\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2744 - f1_score: 0.8449 - val_loss: 0.2642 - val_f1_score: 0.7991\n",
            "Epoch 442/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2757 - f1_score: 0.8435\n",
            "Epoch 00442: val_f1_score improved from 0.83645 to 0.83938, saving model to /content/best_model\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2757 - f1_score: 0.8435 - val_loss: 0.2544 - val_f1_score: 0.8394\n",
            "Epoch 443/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2753 - f1_score: 0.8434\n",
            "Epoch 00443: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2753 - f1_score: 0.8434 - val_loss: 0.2660 - val_f1_score: 0.7911\n",
            "Epoch 444/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2740 - f1_score: 0.8450\n",
            "Epoch 00444: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2740 - f1_score: 0.8450 - val_loss: 0.2705 - val_f1_score: 0.7746\n",
            "Epoch 445/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2736 - f1_score: 0.8463\n",
            "Epoch 00445: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2736 - f1_score: 0.8463 - val_loss: 0.2576 - val_f1_score: 0.8155\n",
            "Epoch 446/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2744 - f1_score: 0.8454\n",
            "Epoch 00446: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2744 - f1_score: 0.8454 - val_loss: 0.2664 - val_f1_score: 0.8077\n",
            "Epoch 447/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2738 - f1_score: 0.8454\n",
            "Epoch 00447: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2738 - f1_score: 0.8454 - val_loss: 0.2545 - val_f1_score: 0.8370\n",
            "Epoch 448/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2730 - f1_score: 0.8463\n",
            "Epoch 00448: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2730 - f1_score: 0.8463 - val_loss: 0.2739 - val_f1_score: 0.7771\n",
            "Epoch 449/2000\n",
            "1989/1993 [============================>.] - ETA: 0s - loss: 0.2735 - f1_score: 0.8447\n",
            "Epoch 00449: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2735 - f1_score: 0.8448 - val_loss: 0.2666 - val_f1_score: 0.8002\n",
            "Epoch 450/2000\n",
            "1984/1993 [============================>.] - ETA: 0s - loss: 0.2734 - f1_score: 0.8445\n",
            "Epoch 00450: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2734 - f1_score: 0.8446 - val_loss: 0.2764 - val_f1_score: 0.7782\n",
            "Epoch 451/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2725 - f1_score: 0.8456\n",
            "Epoch 00451: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2725 - f1_score: 0.8457 - val_loss: 0.2602 - val_f1_score: 0.8192\n",
            "Epoch 452/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2725 - f1_score: 0.8462\n",
            "Epoch 00452: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2725 - f1_score: 0.8462 - val_loss: 0.2609 - val_f1_score: 0.8053\n",
            "Epoch 453/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2727 - f1_score: 0.8462\n",
            "Epoch 00453: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2727 - f1_score: 0.8461 - val_loss: 0.2718 - val_f1_score: 0.7640\n",
            "Epoch 454/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2724 - f1_score: 0.8462\n",
            "Epoch 00454: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2724 - f1_score: 0.8462 - val_loss: 0.2893 - val_f1_score: 0.7459\n",
            "Epoch 455/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2722 - f1_score: 0.8460\n",
            "Epoch 00455: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2722 - f1_score: 0.8460 - val_loss: 0.2591 - val_f1_score: 0.8204\n",
            "Epoch 456/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2722 - f1_score: 0.8465\n",
            "Epoch 00456: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2722 - f1_score: 0.8464 - val_loss: 0.2690 - val_f1_score: 0.8067\n",
            "Epoch 457/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2714 - f1_score: 0.8472\n",
            "Epoch 00457: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2714 - f1_score: 0.8472 - val_loss: 0.2590 - val_f1_score: 0.8220\n",
            "Epoch 458/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2715 - f1_score: 0.8470\n",
            "Epoch 00458: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2715 - f1_score: 0.8470 - val_loss: 0.2682 - val_f1_score: 0.7856\n",
            "Epoch 459/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2708 - f1_score: 0.8474\n",
            "Epoch 00459: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2707 - f1_score: 0.8475 - val_loss: 0.2522 - val_f1_score: 0.8392\n",
            "Epoch 460/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2710 - f1_score: 0.8464\n",
            "Epoch 00460: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2710 - f1_score: 0.8464 - val_loss: 0.2611 - val_f1_score: 0.8322\n",
            "Epoch 461/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2707 - f1_score: 0.8492\n",
            "Epoch 00461: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2707 - f1_score: 0.8491 - val_loss: 0.2686 - val_f1_score: 0.7733\n",
            "Epoch 462/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2709 - f1_score: 0.8470\n",
            "Epoch 00462: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2708 - f1_score: 0.8470 - val_loss: 0.2593 - val_f1_score: 0.7863\n",
            "Epoch 463/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2697 - f1_score: 0.8491\n",
            "Epoch 00463: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2697 - f1_score: 0.8491 - val_loss: 0.2574 - val_f1_score: 0.8050\n",
            "Epoch 464/2000\n",
            "1987/1993 [============================>.] - ETA: 0s - loss: 0.2707 - f1_score: 0.8471\n",
            "Epoch 00464: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2708 - f1_score: 0.8472 - val_loss: 0.2579 - val_f1_score: 0.8065\n",
            "Epoch 465/2000\n",
            "1993/1993 [==============================] - ETA: 0s - loss: 0.2700 - f1_score: 0.8476\n",
            "Epoch 00465: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2700 - f1_score: 0.8476 - val_loss: 0.2637 - val_f1_score: 0.8057\n",
            "Epoch 466/2000\n",
            "1985/1993 [============================>.] - ETA: 0s - loss: 0.2699 - f1_score: 0.8482\n",
            "Epoch 00466: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2700 - f1_score: 0.8483 - val_loss: 0.2575 - val_f1_score: 0.8214\n",
            "Epoch 467/2000\n",
            "1988/1993 [============================>.] - ETA: 0s - loss: 0.2698 - f1_score: 0.8486\n",
            "Epoch 00467: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2698 - f1_score: 0.8487 - val_loss: 0.2622 - val_f1_score: 0.7987\n",
            "Epoch 468/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2701 - f1_score: 0.8485\n",
            "Epoch 00468: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2700 - f1_score: 0.8485 - val_loss: 0.2662 - val_f1_score: 0.8013\n",
            "Epoch 469/2000\n",
            "1986/1993 [============================>.] - ETA: 0s - loss: 0.2695 - f1_score: 0.8475\n",
            "Epoch 00469: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2695 - f1_score: 0.8475 - val_loss: 0.2670 - val_f1_score: 0.7820\n",
            "Epoch 470/2000\n",
            "1991/1993 [============================>.] - ETA: 0s - loss: 0.2693 - f1_score: 0.8478\n",
            "Epoch 00470: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2693 - f1_score: 0.8479 - val_loss: 0.2583 - val_f1_score: 0.8268\n",
            "Epoch 471/2000\n",
            "1990/1993 [============================>.] - ETA: 0s - loss: 0.2685 - f1_score: 0.8486\n",
            "Epoch 00471: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 13s 6ms/step - loss: 0.2685 - f1_score: 0.8486 - val_loss: 0.2631 - val_f1_score: 0.8001\n",
            "Epoch 472/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2696 - f1_score: 0.8477\n",
            "Epoch 00472: val_f1_score did not improve from 0.83938\n",
            "1993/1993 [==============================] - 12s 6ms/step - loss: 0.2696 - f1_score: 0.8477 - val_loss: 0.2719 - val_f1_score: 0.7795\n",
            "Epoch 473/2000\n",
            "1992/1993 [============================>.] - ETA: 0s - loss: 0.2695 - f1_score: 0.8481"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a5906e4c-4b61-4599-bf59-1b8fe8f7a7a6"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.plot(history.history[\"loss\"], '-b')\n",
        "plt.plot(history.history[\"val_loss\"], '-r')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(history.history[\"f1_score\"], '-b')\n",
        "plt.plot(history.history[\"val_f1_score\"], '-r')\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc8d25a8e48>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjbrzEe2ISI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1c002955-3392-4524-ee45-5f050a26270b"
      },
      "source": [
        "# Save model weights to drive\n",
        "#!cp best_model* '/content/gdrive/My Drive/Kaggle'\n",
        "\n",
        "new_model = create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0, l2_rate=0, init_std=0.05, lr=0.00005)\n",
        "new_model.load_weights('/content/gdrive/My Drive/Kaggle/best_model')\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               115712    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 145,473\n",
            "Trainable params: 145,089\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc6b1f76-5bb9-4e4f-839e-9631d4ea8b3f"
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n",
        "  test_size = len(Y_test)\n",
        "  y_pred = (model.predict(X_test[:test_size], batch_size=128)>threshold).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        "  y_pred[7] = 1\n",
        "  y_pred[23] = 1\n",
        "  #pred_dist = np.unique(y_pred.astype(int), return_counts=True)\n",
        "  #correct_prediction = np.unique(y_pred == np.expand_dims(Y_test[:test_size], axis=1), return_counts=True)\n",
        "  #print(pred_dist, correct_prediction[0])\n",
        "  \n",
        "  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)\n",
        "\n",
        "pre, re, f1 = F1_score(new_model, X_test, Y_test, test_size=len(Y_test), threshold=0.9)\n",
        "print(pre, re, f1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8491379310344828 0.7181044957472661 0.7781435154707045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3e7d59b-70a8-4120-e9dd-17b795e098ab"
      },
      "source": [
        "new_model.evaluate(X_test, Y_test)\n",
        "prediction = new_model.predict(X_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3691/3691 [==============================] - 15s 4ms/step - loss: 0.0712 - f1_score: 0.7901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHZq7LSf4cWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea920f33-a12d-4d88-9d63-449031a59980"
      },
      "source": [
        "try:\n",
        "  new_model.evaluate(X_train, Y_train, batch_size=BATCH_SIZE)\n",
        "except NameError:\n",
        "  BATCH_SIZE = 256\n",
        "  new_model.evaluate(X_train, Y_train, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0697 - f1_score: 0.7809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "27f02cf3-feff-4db4-d45c-d9754eddfa9c"
      },
      "source": [
        "prediction = np.squeeze(prediction, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist((prediction>0.9).astype('int'), bins=[0,1,2])\n",
        "\n",
        "\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.9).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.08% 3.54%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Div--L7lUVFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "745d43c8-d0fc-4452-de09-57bd4a302cb1"
      },
      "source": [
        "fraud_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([65115, 33254]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "fb19c8c3-ad9f-47e7-d062-a7f6d7a632d1"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "test_transaction = pd.read_csv('test_transaction.csv')\n",
        "test_transaction.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663549</td>\n",
              "      <td>18403224</td>\n",
              "      <td>31.95</td>\n",
              "      <td>W</td>\n",
              "      <td>10409</td>\n",
              "      <td>111.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>226.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>170.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>398.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>418.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.950001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.950001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663550</td>\n",
              "      <td>18403263</td>\n",
              "      <td>49.00</td>\n",
              "      <td>W</td>\n",
              "      <td>4272</td>\n",
              "      <td>111.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>226.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>299.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>aol.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>634.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>231.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>280.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>280.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663551</td>\n",
              "      <td>18403310</td>\n",
              "      <td>171.00</td>\n",
              "      <td>W</td>\n",
              "      <td>4476</td>\n",
              "      <td>574.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>226.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>472.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>2635.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>hotmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>136.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1321.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1058.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663552</td>\n",
              "      <td>18403310</td>\n",
              "      <td>284.95</td>\n",
              "      <td>W</td>\n",
              "      <td>10989</td>\n",
              "      <td>360.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>205.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>242.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>282.540009</td>\n",
              "      <td>282.540009</td>\n",
              "      <td>282.540009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663553</td>\n",
              "      <td>18403317</td>\n",
              "      <td>67.95</td>\n",
              "      <td>W</td>\n",
              "      <td>18018</td>\n",
              "      <td>452.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>264.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>183.850006</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>183.850006</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>183.850006</td>\n",
              "      <td>67.949997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 393 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  TransactionDT  TransactionAmt  ... V337  V338  V339\n",
              "0        3663549       18403224           31.95  ...  NaN   NaN   NaN\n",
              "1        3663550       18403263           49.00  ...  NaN   NaN   NaN\n",
              "2        3663551       18403310          171.00  ...  NaN   NaN   NaN\n",
              "3        3663552       18403310          284.95  ...  NaN   NaN   NaN\n",
              "4        3663553       18403317           67.95  ...  NaN   NaN   NaN\n",
              "\n",
              "[5 rows x 393 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoViKsx6cZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID']\n",
        "for column in skip_int_columns:\n",
        "  int_columns_test.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzQZ6nR6wOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_normalization(X, indices, cache_min, cache_max, cache_mean):\n",
        "  X_out = copy.copy(X)\n",
        "  #print(cache_mean, cache_max, cache_min)\n",
        "  X_out[indices] = (X_out[indices] - cache_mean)/(cache_max - cache_min)\n",
        "  X_out[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return X_out.astype('float16')  \n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM75lh_6lhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  test_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjog0oM7p4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  test_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(test_transaction[column].values)):\n",
        "    test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMTT8KB74NL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fd11cfd-184f-4887-93a8-74b7e9924b31"
      },
      "source": [
        "encoded_column = 0\n",
        "for column in obj_columns_test:\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(cache[column])\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.transform(test_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(cache[column])))])\n",
        "  test_transaction = pd.concat([test_transaction, pd_encoded], axis=1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "\n",
        "\n",
        "for column in obj_columns_test:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_OOqFi8HrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "7aef1648-0c15-44f0-c2c2-97cbed3b57b2"
      },
      "source": [
        "# Check if we have the same shape with the X_train\n",
        "print(test_transaction.shape, X_train.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9f1020d5733f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_transaction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9vDvpDZdpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the prediction and submit the output\n",
        "result = new_model.predict(test_transaction)\n",
        "kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"Message\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICp4sPm6brq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3e2nvzrHir4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fe4ae314-d9e2-483a-9baa-e8b9302f14bf"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohc = OneHotEncoder()\n",
        "a = {'a': ['Null', 'A', 'B', 'C', 'D']}\n",
        "df = pd.DataFrame(a)\n",
        "df\n",
        "encoded = ohc.fit_transform(df['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsaGKlzMUlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69213f3a-42cb-4edc-cbc8-5bc6c0bb5a7e"
      },
      "source": [
        "b = {'a': ['Null', 'A', 'B', 'C', 'E']}\n",
        "df_b = pd.DataFrame(b)\n",
        "ohc_b = OneHotEncoder(handle_unknown='ignore')\n",
        "ohc_b.fit(df['a'].values.reshape(-1,1))\n",
        "encoded_b = ohc_b.transform(df_b['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded_b = pd.DataFrame(encoded_b.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvykuaRPMpZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "20c01d2a-8a3b-41b6-a7ae-b6d7a36f327f"
      },
      "source": [
        "for column in obj_columns:\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(dataset_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 5\n",
            "P_emaildomain 60\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_RMIz3NTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2e5b2844-8bdd-45ee-f5ec-83cd54c9cba7"
      },
      "source": [
        "for column in obj_columns_test:\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(test_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 4\n",
            "P_emaildomain 61\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}