{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/master/Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4qbNACq5vY",
        "colab_type": "text"
      },
      "source": [
        "# ***Get the dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28TmZY-0q4mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "30bb4274-0722-4a07-cd85-403288c5dd57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mVq898tzNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "310358aa-4821-4b34-b04c-d1786c927ece"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 63% 33.0M/52.2M [00:01<00:01, 15.2MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 33.8MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 206MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 157MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 70% 41.0M/58.3M [00:01<00:00, 18.8MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 40.3MB/s]\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "\n",
            "Archive:  test_identity.csv.zip\n",
            "  inflating: test_identity.csv       \n",
            "\n",
            "Archive:  train_transaction.csv.zip\n",
            "  inflating: train_transaction.csv   \n",
            "\n",
            "Archive:  test_transaction.csv.zip\n",
            "  inflating: test_transaction.csv    \n",
            "\n",
            "Archive:  train_identity.csv.zip\n",
            "  inflating: train_identity.csv      \n",
            "\n",
            "5 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-VLOPU9zZii",
        "colab_type": "text"
      },
      "source": [
        "# ***Analyzing the dataset and doing the cleansing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYzy-sxDzdFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f099c676-f1e5-4141-f1aa-60d33f2148d7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBOSTwRzj4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ec08b7ac-a3da-4b87-abcb-3dee683cd105"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoApMJ8vz3IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "0b3d9768-b2f6-45c3-eec5-b89dea3c244c"
      },
      "source": [
        "dataset_identity = pd.read_csv('train_identity.csv')\n",
        "dataset_identity.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_03</th>\n",
              "      <th>id_04</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_07</th>\n",
              "      <th>id_08</th>\n",
              "      <th>id_09</th>\n",
              "      <th>id_10</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_14</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-480.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>542.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>Android 7.0</td>\n",
              "      <td>samsung browser 6.2</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2220x1080</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987008</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>98945.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>49.0</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>166.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>621.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>iOS 11.1.2</td>\n",
              "      <td>mobile safari 11.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1334x750</td>\n",
              "      <td>match_status:1</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>iOS Device</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987010</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>191631.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>121.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>410.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>Windows</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987011</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>221832.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>225.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>176.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>166.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>529.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>Mac OS X 10_11_6</td>\n",
              "      <td>chrome 62.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1280x800</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>desktop</td>\n",
              "      <td>MacOS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  id_01  ...  DeviceType                     DeviceInfo\n",
              "0        2987004    0.0  ...      mobile  SAMSUNG SM-G892A Build/NRD90M\n",
              "1        2987008   -5.0  ...      mobile                     iOS Device\n",
              "2        2987010   -5.0  ...     desktop                        Windows\n",
              "3        2987011   -5.0  ...     desktop                            NaN\n",
              "4        2987016    0.0  ...     desktop                          MacOS\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmudmokF4Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ec9227ad-84ef-492c-9ccf-5b2d145c6729"
      },
      "source": [
        "dataset_identity.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
              "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
              "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
              "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
              "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
              "       'DeviceType', 'DeviceInfo'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NesEY-44N6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "40859b37-07be-4663-d5e6-f414d6f05ba3"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDu1rWAkUafP",
        "colab_type": "text"
      },
      "source": [
        "**Check NaN, Null, and OneHotEncoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']\n",
        "cache = dict()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f3a4e456-9dfd-49ce-d443-dd487ae7e241"
      },
      "source": [
        "data_backup = copy.copy(dataset_transaction)\n",
        "data_backup.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# Task 3: Remove the irrelevant columns\n",
        "\n",
        "dataset_transaction = copy.copy(data_backup)\n",
        "\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN column for every features\n",
        "  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d679ac29-a141-4c3d-bff3-b64d85550801"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 770 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 770 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  #if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "  #  dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "a49ee523-9a0b-49cf-8443-cad9cc02aee8"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 770 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 770 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee954f5b-ab31-437e-b3d9-3040da8b028b"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoded_column = 0\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "  cache[column] = dataset_transaction[column].values.reshape(-1,1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuvQmMmLRnM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f77cc314-a21f-4b22-e732-cdc15bd6f65d"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 920 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  M9_0  M9_1  M9_2\n",
              "0        2987000        0      -0.463379  ...     0     1     0\n",
              "1        2987001        0      -0.463379  ...     0     1     0\n",
              "2        2987002        0      -0.463379  ...     1     0     0\n",
              "3        2987003        0      -0.463379  ...     0     1     0\n",
              "4        2987004        0      -0.463379  ...     0     1     0\n",
              "\n",
              "[5 rows x 920 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2163826e-3ca1-4ae6-92d4-33c43ff906ef"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoc4TuIx1zoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2db7b070-1730-464b-e5dd-c27e4707d53e"
      },
      "source": [
        "out = ['isFraud']\n",
        "for column in dataset_transaction.columns:\n",
        "  if column.find('R_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "  if column.find('P_emaildomain') != -1:\n",
        "    out.append(column)\n",
        "print(out)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['isFraud', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'P_emaildomain_5', 'P_emaildomain_6', 'P_emaildomain_7', 'P_emaildomain_8', 'P_emaildomain_9', 'P_emaildomain_10', 'P_emaildomain_11', 'P_emaildomain_12', 'P_emaildomain_13', 'P_emaildomain_14', 'P_emaildomain_15', 'P_emaildomain_16', 'P_emaildomain_17', 'P_emaildomain_18', 'P_emaildomain_19', 'P_emaildomain_20', 'P_emaildomain_21', 'P_emaildomain_22', 'P_emaildomain_23', 'P_emaildomain_24', 'P_emaildomain_25', 'P_emaildomain_26', 'P_emaildomain_27', 'P_emaildomain_28', 'P_emaildomain_29', 'P_emaildomain_30', 'P_emaildomain_31', 'P_emaildomain_32', 'P_emaildomain_33', 'P_emaildomain_34', 'P_emaildomain_35', 'P_emaildomain_36', 'P_emaildomain_37', 'P_emaildomain_38', 'P_emaildomain_39', 'P_emaildomain_40', 'P_emaildomain_41', 'P_emaildomain_42', 'P_emaildomain_43', 'P_emaildomain_44', 'P_emaildomain_45', 'P_emaildomain_46', 'P_emaildomain_47', 'P_emaildomain_48', 'P_emaildomain_49', 'P_emaildomain_50', 'P_emaildomain_51', 'P_emaildomain_52', 'P_emaildomain_53', 'P_emaildomain_54', 'P_emaildomain_55', 'P_emaildomain_56', 'P_emaildomain_57', 'P_emaildomain_58', 'P_emaildomain_59', 'R_emaildomain_0', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'R_emaildomain_4', 'R_emaildomain_5', 'R_emaildomain_6', 'R_emaildomain_7', 'R_emaildomain_8', 'R_emaildomain_9', 'R_emaildomain_10', 'R_emaildomain_11', 'R_emaildomain_12', 'R_emaildomain_13', 'R_emaildomain_14', 'R_emaildomain_15', 'R_emaildomain_16', 'R_emaildomain_17', 'R_emaildomain_18', 'R_emaildomain_19', 'R_emaildomain_20', 'R_emaildomain_21', 'R_emaildomain_22', 'R_emaildomain_23', 'R_emaildomain_24', 'R_emaildomain_25', 'R_emaildomain_26', 'R_emaildomain_27', 'R_emaildomain_28', 'R_emaildomain_29', 'R_emaildomain_30', 'R_emaildomain_31', 'R_emaildomain_32', 'R_emaildomain_33', 'R_emaildomain_34', 'R_emaildomain_35', 'R_emaildomain_36', 'R_emaildomain_37', 'R_emaildomain_38', 'R_emaildomain_39', 'R_emaildomain_40', 'R_emaildomain_41', 'R_emaildomain_42', 'R_emaildomain_43', 'R_emaildomain_44', 'R_emaildomain_45', 'R_emaildomain_46', 'R_emaildomain_47', 'R_emaildomain_48', 'R_emaildomain_49', 'R_emaildomain_50', 'R_emaildomain_51', 'R_emaildomain_52', 'R_emaildomain_53', 'R_emaildomain_54', 'R_emaildomain_55', 'R_emaildomain_56', 'R_emaildomain_57', 'R_emaildomain_58', 'R_emaildomain_59', 'R_emaildomain_60']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#colums_to_analyze = ['isFraud', 'TransactionDT', 'TransactionAmt', 'R_emaildomain_0', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2', 'card1', 'card2', 'card3']\n",
        "columns_to_analyze = out\n",
        "\n",
        "analyzing_data = dataset_transaction[columns_to_analyze]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "to_display = False\n",
        "\n",
        "if to_display:\n",
        "  mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD5CKASq2rzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "86463285-0507-4d84-802b-e91b4e567f29"
      },
      "source": [
        "# Remove the weak correlation features\n",
        "col = corr.columns\n",
        "is_fraud = np.where(col=='isFraud')[0][0]\n",
        "col = col.to_list()\n",
        "col.pop(is_fraud)\n",
        "to_remove = []\n",
        "for each_col in col:\n",
        "  if abs(corr['isFraud'][each_col]) < 0.05: # Weak correlation\n",
        "    to_remove.append(each_col)\n",
        "    a = dataset_transaction.pop(each_col)\n",
        "print(len(to_remove))\n",
        "\n",
        "\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>card4_1</th>\n",
              "      <th>card4_2</th>\n",
              "      <th>card4_3</th>\n",
              "      <th>card4_4</th>\n",
              "      <th>card6_0</th>\n",
              "      <th>card6_1</th>\n",
              "      <th>card6_2</th>\n",
              "      <th>card6_3</th>\n",
              "      <th>card6_4</th>\n",
              "      <th>R_emaildomain_0</th>\n",
              "      <th>R_emaildomain_17</th>\n",
              "      <th>R_emaildomain_20</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 802 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  M9_0  M9_1  M9_2\n",
              "0        2987000        0      -0.463379  ...     0     1     0\n",
              "1        2987001        0      -0.463379  ...     0     1     0\n",
              "2        2987002        0      -0.463379  ...     1     0     0\n",
              "3        2987003        0      -0.463379  ...     0     1     0\n",
              "4        2987004        0      -0.463379  ...     0     1     0\n",
              "\n",
              "[5 rows x 802 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "3d776e95-47c1-442f-d58e-7bb4cdd1cdf7"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "a = dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>card4_1</th>\n",
              "      <th>card4_2</th>\n",
              "      <th>card4_3</th>\n",
              "      <th>card4_4</th>\n",
              "      <th>card6_0</th>\n",
              "      <th>card6_1</th>\n",
              "      <th>card6_2</th>\n",
              "      <th>card6_3</th>\n",
              "      <th>card6_4</th>\n",
              "      <th>R_emaildomain_0</th>\n",
              "      <th>R_emaildomain_17</th>\n",
              "      <th>R_emaildomain_20</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 801 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  TransactionAmt     card1  ...  M8_2  M9_0  M9_1  M9_2\n",
              "0        0      -0.463379       -0.002083  0.231445  ...     0     0     1     0\n",
              "1        0      -0.463379       -0.003321 -0.410645  ...     0     0     1     0\n",
              "2        0      -0.463379       -0.002380 -0.301025  ...     0     1     0     0\n",
              "3        0      -0.463379       -0.002663  0.473389  ...     0     0     1     0\n",
              "4        0      -0.463379       -0.002663 -0.310547  ...     0     0     1     0\n",
              "\n",
              "[5 rows x 801 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "087121db-799a-4f21-d790-19e7180ed43d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)\n",
        "\n",
        "print(X_train.shape, Y_train.shape, X_test.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(584634, 800) (584634,) (5906, 800)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ef6e2ad3-a406-4f9b-c63b-5673992a6c6f"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.5%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling and upsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "20d7137e-6a19-4230-d395-001f0bcabf18"
      },
      "source": [
        "downsampling_factor = 1\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "upsampling_factor = 5\n",
        "indices_1_new = indices_1\n",
        "for i in range(upsampling_factor):\n",
        "  indices_1_new = np.concatenate((indices_1_new, indices_1), axis=0)\n",
        "\n",
        "indices_0_new = np.concatenate((indices_1_new, indices_0_new), axis=0)\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "indices_0_new = tf.random.shuffle(indices_0_new)\n",
        "\n",
        "X_to_train = np.array(X_train)[indices_0_new]\n",
        "Y_to_train = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "\n",
        "X_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_to_train, axis=1)\n",
        "print(X_to_train.shape, Y_to_train.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(564167, 1)\n",
            "(686969, 1)\n",
            "(686969, 800) (686969,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2df1fdb1-a06b-461f-da80-17b0f8fc4cb2"
      },
      "source": [
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 17.86%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  out_model.add(Dense(dense1, activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                      input_shape=(X_train.shape[1],),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense1, activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(dense2, activation=tf.keras.layers.LeakyReLU(alpha=0.01), \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "  out_model.add(Dense(dense2, activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n",
        "                      activity_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
        "\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "  out_model.add(BatchNormalization())\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[METRICS])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a4826eda-52ef-401d-9068-f02a7ca19361"
      },
      "source": [
        "my_model = create_model(dense1=256, dense2=128, dropout_rate=0.4, l1_rate=1e-5, l2_rate=1e-4, init_std=0.1, lr=0.00005)\n",
        "my_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 256)               205056    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 321,921\n",
            "Trainable params: 321,153\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UTsRGUjjzpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cdf869a-d161-4763-ec97-625d76557307"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "NB_EPOCH = 2000\n",
        "PATIENCE = 100\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', patience=PATIENCE, verbose=0, mode='max',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./best_model.h5', monitor='val_auc', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='max')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.2, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.6727 - tp: 68840.0000 - fp: 121811.0000 - tn: 492469.0000 - fn: 64400.0000 - accuracy: 0.7509 - precision: 0.3611 - recall: 0.5167 - auc: 0.7212\n",
            "Epoch 00001: val_auc improved from -inf to 0.83978, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 22s 10ms/step - loss: 0.6727 - tp: 68847.0000 - fp: 121822.0000 - tn: 492557.0000 - fn: 64409.0000 - accuracy: 0.7509 - precision: 0.3611 - recall: 0.5167 - auc: 0.7212 - val_loss: 0.4947 - val_tp: 10189.0000 - val_fp: 2676.0000 - val_tn: 109931.0000 - val_fn: 14577.0000 - val_accuracy: 0.8744 - val_precision: 0.7920 - val_recall: 0.4114 - val_auc: 0.8398\n",
            "Epoch 2/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.4954 - tp: 40099.0000 - fp: 14158.0000 - tn: 437122.0000 - fn: 57741.0000 - accuracy: 0.8691 - precision: 0.7391 - recall: 0.4098 - auc: 0.8200\n",
            "Epoch 00002: val_auc improved from 0.83978 to 0.85448, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.4953 - tp: 40123.0000 - fp: 14161.0000 - tn: 437420.0000 - fn: 57787.0000 - accuracy: 0.8691 - precision: 0.7391 - recall: 0.4098 - auc: 0.8200 - val_loss: 0.4560 - val_tp: 9871.0000 - val_fp: 1826.0000 - val_tn: 110781.0000 - val_fn: 14895.0000 - val_accuracy: 0.8783 - val_precision: 0.8439 - val_recall: 0.3986 - val_auc: 0.8545\n",
            "Epoch 3/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.4618 - tp: 40918.0000 - fp: 11023.0000 - tn: 440460.0000 - fn: 56975.0000 - accuracy: 0.8762 - precision: 0.7878 - recall: 0.4180 - auc: 0.8425\n",
            "Epoch 00003: val_auc improved from 0.85448 to 0.86289, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.4618 - tp: 40927.0000 - fp: 11025.0000 - tn: 440556.0000 - fn: 56983.0000 - accuracy: 0.8762 - precision: 0.7878 - recall: 0.4180 - auc: 0.8425 - val_loss: 0.4384 - val_tp: 9643.0000 - val_fp: 1313.0000 - val_tn: 111294.0000 - val_fn: 15123.0000 - val_accuracy: 0.8804 - val_precision: 0.8802 - val_recall: 0.3894 - val_auc: 0.8629\n",
            "Epoch 4/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.4422 - tp: 42435.0000 - fp: 10654.0000 - tn: 440927.0000 - fn: 55475.0000 - accuracy: 0.8797 - precision: 0.7993 - recall: 0.4334 - auc: 0.8535\n",
            "Epoch 00004: val_auc improved from 0.86289 to 0.86503, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.4422 - tp: 42435.0000 - fp: 10654.0000 - tn: 440927.0000 - fn: 55475.0000 - accuracy: 0.8797 - precision: 0.7993 - recall: 0.4334 - auc: 0.8535 - val_loss: 0.4289 - val_tp: 11924.0000 - val_fp: 3299.0000 - val_tn: 109308.0000 - val_fn: 12842.0000 - val_accuracy: 0.8825 - val_precision: 0.7833 - val_recall: 0.4815 - val_auc: 0.8650\n",
            "Epoch 5/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.4262 - tp: 44135.0000 - fp: 10519.0000 - tn: 440135.0000 - fn: 53563.0000 - accuracy: 0.8831 - precision: 0.8075 - recall: 0.4517 - auc: 0.8624\n",
            "Epoch 00005: val_auc improved from 0.86503 to 0.87540, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.4262 - tp: 44238.0000 - fp: 10543.0000 - tn: 441038.0000 - fn: 53672.0000 - accuracy: 0.8831 - precision: 0.8075 - recall: 0.4518 - auc: 0.8624 - val_loss: 0.4091 - val_tp: 11149.0000 - val_fp: 1865.0000 - val_tn: 110742.0000 - val_fn: 13617.0000 - val_accuracy: 0.8873 - val_precision: 0.8567 - val_recall: 0.4502 - val_auc: 0.8754\n",
            "Epoch 6/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.4134 - tp: 45891.0000 - fp: 10566.0000 - tn: 440715.0000 - fn: 51948.0000 - accuracy: 0.8862 - precision: 0.8128 - recall: 0.4690 - auc: 0.8688\n",
            "Epoch 00006: val_auc improved from 0.87540 to 0.88054, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.4134 - tp: 45916.0000 - fp: 10571.0000 - tn: 441010.0000 - fn: 51994.0000 - accuracy: 0.8861 - precision: 0.8129 - recall: 0.4690 - auc: 0.8688 - val_loss: 0.3984 - val_tp: 12521.0000 - val_fp: 2722.0000 - val_tn: 109885.0000 - val_fn: 12245.0000 - val_accuracy: 0.8910 - val_precision: 0.8214 - val_recall: 0.5056 - val_auc: 0.8805\n",
            "Epoch 7/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.4017 - tp: 47536.0000 - fp: 10571.0000 - tn: 440510.0000 - fn: 50247.0000 - accuracy: 0.8892 - precision: 0.8181 - recall: 0.4861 - auc: 0.8749\n",
            "Epoch 00007: val_auc improved from 0.88054 to 0.88685, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.4017 - tp: 47595.0000 - fp: 10574.0000 - tn: 441007.0000 - fn: 50315.0000 - accuracy: 0.8892 - precision: 0.8182 - recall: 0.4861 - auc: 0.8749 - val_loss: 0.3905 - val_tp: 11193.0000 - val_fp: 1519.0000 - val_tn: 111088.0000 - val_fn: 13573.0000 - val_accuracy: 0.8901 - val_precision: 0.8805 - val_recall: 0.4520 - val_auc: 0.8868\n",
            "Epoch 8/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.3923 - tp: 48828.0000 - fp: 10618.0000 - tn: 440868.0000 - fn: 49062.0000 - accuracy: 0.8914 - precision: 0.8214 - recall: 0.4988 - auc: 0.8797\n",
            "Epoch 00008: val_auc improved from 0.88685 to 0.89049, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3923 - tp: 48840.0000 - fp: 10619.0000 - tn: 440962.0000 - fn: 49070.0000 - accuracy: 0.8914 - precision: 0.8214 - recall: 0.4988 - auc: 0.8797 - val_loss: 0.3795 - val_tp: 12230.0000 - val_fp: 1854.0000 - val_tn: 110753.0000 - val_fn: 12536.0000 - val_accuracy: 0.8952 - val_precision: 0.8684 - val_recall: 0.4938 - val_auc: 0.8905\n",
            "Epoch 9/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.3838 - tp: 49947.0000 - fp: 10703.0000 - tn: 440782.0000 - fn: 47944.0000 - accuracy: 0.8932 - precision: 0.8235 - recall: 0.5102 - auc: 0.8842\n",
            "Epoch 00009: val_auc improved from 0.89049 to 0.89510, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3838 - tp: 49958.0000 - fp: 10707.0000 - tn: 440874.0000 - fn: 47952.0000 - accuracy: 0.8932 - precision: 0.8235 - recall: 0.5102 - auc: 0.8842 - val_loss: 0.3736 - val_tp: 12011.0000 - val_fp: 1645.0000 - val_tn: 110962.0000 - val_fn: 12755.0000 - val_accuracy: 0.8952 - val_precision: 0.8795 - val_recall: 0.4850 - val_auc: 0.8951\n",
            "Epoch 10/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.3761 - tp: 50976.0000 - fp: 10711.0000 - tn: 440870.0000 - fn: 46934.0000 - accuracy: 0.8951 - precision: 0.8264 - recall: 0.5206 - auc: 0.8887\n",
            "Epoch 00010: val_auc improved from 0.89510 to 0.89585, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3761 - tp: 50976.0000 - fp: 10711.0000 - tn: 440870.0000 - fn: 46934.0000 - accuracy: 0.8951 - precision: 0.8264 - recall: 0.5206 - auc: 0.8887 - val_loss: 0.3678 - val_tp: 13721.0000 - val_fp: 3046.0000 - val_tn: 109561.0000 - val_fn: 11045.0000 - val_accuracy: 0.8974 - val_precision: 0.8183 - val_recall: 0.5540 - val_auc: 0.8959\n",
            "Epoch 11/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.3682 - tp: 51947.0000 - fp: 10565.0000 - tn: 439894.0000 - fn: 45690.0000 - accuracy: 0.8974 - precision: 0.8310 - recall: 0.5320 - auc: 0.8930\n",
            "Epoch 00011: val_auc improved from 0.89585 to 0.90216, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.3682 - tp: 52088.0000 - fp: 10587.0000 - tn: 440994.0000 - fn: 45822.0000 - accuracy: 0.8973 - precision: 0.8311 - recall: 0.5320 - auc: 0.8930 - val_loss: 0.3596 - val_tp: 12573.0000 - val_fp: 1748.0000 - val_tn: 110859.0000 - val_fn: 12193.0000 - val_accuracy: 0.8985 - val_precision: 0.8779 - val_recall: 0.5077 - val_auc: 0.9022\n",
            "Epoch 12/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.3621 - tp: 52915.0000 - fp: 10547.0000 - tn: 440092.0000 - fn: 44798.0000 - accuracy: 0.8991 - precision: 0.8338 - recall: 0.5415 - auc: 0.8965\n",
            "Epoch 00012: val_auc improved from 0.90216 to 0.90643, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3621 - tp: 53021.0000 - fp: 10573.0000 - tn: 441008.0000 - fn: 44889.0000 - accuracy: 0.8991 - precision: 0.8337 - recall: 0.5415 - auc: 0.8966 - val_loss: 0.3514 - val_tp: 13882.0000 - val_fp: 2651.0000 - val_tn: 109956.0000 - val_fn: 10884.0000 - val_accuracy: 0.9015 - val_precision: 0.8397 - val_recall: 0.5605 - val_auc: 0.9064\n",
            "Epoch 13/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.3555 - tp: 53837.0000 - fp: 10579.0000 - tn: 440483.0000 - fn: 43965.0000 - accuracy: 0.9006 - precision: 0.8358 - recall: 0.5505 - auc: 0.9006\n",
            "Epoch 00013: val_auc improved from 0.90643 to 0.91001, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.3555 - tp: 53895.0000 - fp: 10588.0000 - tn: 440993.0000 - fn: 44015.0000 - accuracy: 0.9006 - precision: 0.8358 - recall: 0.5505 - auc: 0.9006 - val_loss: 0.3447 - val_tp: 14283.0000 - val_fp: 2706.0000 - val_tn: 109901.0000 - val_fn: 10483.0000 - val_accuracy: 0.9040 - val_precision: 0.8407 - val_recall: 0.5767 - val_auc: 0.9100\n",
            "Epoch 14/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.3503 - tp: 54503.0000 - fp: 10516.0000 - tn: 440763.0000 - fn: 43338.0000 - accuracy: 0.9019 - precision: 0.8383 - recall: 0.5571 - auc: 0.9033\n",
            "Epoch 00014: val_auc improved from 0.91001 to 0.91203, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3503 - tp: 54540.0000 - fp: 10518.0000 - tn: 441063.0000 - fn: 43370.0000 - accuracy: 0.9019 - precision: 0.8383 - recall: 0.5570 - auc: 0.9033 - val_loss: 0.3446 - val_tp: 13007.0000 - val_fp: 1591.0000 - val_tn: 111016.0000 - val_fn: 11759.0000 - val_accuracy: 0.9028 - val_precision: 0.8910 - val_recall: 0.5252 - val_auc: 0.9120\n",
            "Epoch 15/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.3446 - tp: 55246.0000 - fp: 10592.0000 - tn: 440061.0000 - fn: 42453.0000 - accuracy: 0.9033 - precision: 0.8391 - recall: 0.5655 - auc: 0.9071\n",
            "Epoch 00015: val_auc improved from 0.91203 to 0.91594, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.3447 - tp: 55364.0000 - fp: 10618.0000 - tn: 440963.0000 - fn: 42546.0000 - accuracy: 0.9032 - precision: 0.8391 - recall: 0.5655 - auc: 0.9071 - val_loss: 0.3333 - val_tp: 15000.0000 - val_fp: 2882.0000 - val_tn: 109725.0000 - val_fn: 9766.0000 - val_accuracy: 0.9079 - val_precision: 0.8388 - val_recall: 0.6057 - val_auc: 0.9159\n",
            "Epoch 16/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.3399 - tp: 56043.0000 - fp: 10558.0000 - tn: 440088.0000 - fn: 41663.0000 - accuracy: 0.9048 - precision: 0.8415 - recall: 0.5736 - auc: 0.9096\n",
            "Epoch 00016: val_auc improved from 0.91594 to 0.91805, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3399 - tp: 56167.0000 - fp: 10580.0000 - tn: 441001.0000 - fn: 41743.0000 - accuracy: 0.9048 - precision: 0.8415 - recall: 0.5737 - auc: 0.9096 - val_loss: 0.3303 - val_tp: 14580.0000 - val_fp: 2513.0000 - val_tn: 110094.0000 - val_fn: 10186.0000 - val_accuracy: 0.9076 - val_precision: 0.8530 - val_recall: 0.5887 - val_auc: 0.9181\n",
            "Epoch 17/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.3351 - tp: 56876.0000 - fp: 10455.0000 - tn: 441126.0000 - fn: 41034.0000 - accuracy: 0.9063 - precision: 0.8447 - recall: 0.5809 - auc: 0.9125\n",
            "Epoch 00017: val_auc improved from 0.91805 to 0.92193, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3351 - tp: 56876.0000 - fp: 10455.0000 - tn: 441126.0000 - fn: 41034.0000 - accuracy: 0.9063 - precision: 0.8447 - recall: 0.5809 - auc: 0.9125 - val_loss: 0.3248 - val_tp: 15122.0000 - val_fp: 2648.0000 - val_tn: 109959.0000 - val_fn: 9644.0000 - val_accuracy: 0.9105 - val_precision: 0.8510 - val_recall: 0.6106 - val_auc: 0.9219\n",
            "Epoch 18/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.3305 - tp: 57349.0000 - fp: 10367.0000 - tn: 440488.0000 - fn: 40404.0000 - accuracy: 0.9075 - precision: 0.8469 - recall: 0.5867 - auc: 0.9151\n",
            "Epoch 00018: val_auc improved from 0.92193 to 0.92319, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3305 - tp: 57447.0000 - fp: 10386.0000 - tn: 441195.0000 - fn: 40463.0000 - accuracy: 0.9075 - precision: 0.8469 - recall: 0.5867 - auc: 0.9151 - val_loss: 0.3212 - val_tp: 15293.0000 - val_fp: 2810.0000 - val_tn: 109797.0000 - val_fn: 9473.0000 - val_accuracy: 0.9106 - val_precision: 0.8448 - val_recall: 0.6175 - val_auc: 0.9232\n",
            "Epoch 19/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.3265 - tp: 57993.0000 - fp: 10388.0000 - tn: 440885.0000 - fn: 39854.0000 - accuracy: 0.9085 - precision: 0.8481 - recall: 0.5927 - auc: 0.9175\n",
            "Epoch 00019: val_auc improved from 0.92319 to 0.92761, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3265 - tp: 58027.0000 - fp: 10394.0000 - tn: 441187.0000 - fn: 39883.0000 - accuracy: 0.9085 - precision: 0.8481 - recall: 0.5927 - auc: 0.9175 - val_loss: 0.3154 - val_tp: 15430.0000 - val_fp: 2593.0000 - val_tn: 110014.0000 - val_fn: 9336.0000 - val_accuracy: 0.9132 - val_precision: 0.8561 - val_recall: 0.6230 - val_auc: 0.9276\n",
            "Epoch 20/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.3221 - tp: 58652.0000 - fp: 10381.0000 - tn: 441200.0000 - fn: 39258.0000 - accuracy: 0.9097 - precision: 0.8496 - recall: 0.5990 - auc: 0.9201\n",
            "Epoch 00020: val_auc improved from 0.92761 to 0.92848, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3221 - tp: 58652.0000 - fp: 10381.0000 - tn: 441200.0000 - fn: 39258.0000 - accuracy: 0.9097 - precision: 0.8496 - recall: 0.5990 - auc: 0.9201 - val_loss: 0.3111 - val_tp: 15111.0000 - val_fp: 2105.0000 - val_tn: 110502.0000 - val_fn: 9655.0000 - val_accuracy: 0.9144 - val_precision: 0.8777 - val_recall: 0.6102 - val_auc: 0.9285\n",
            "Epoch 21/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.3182 - tp: 59329.0000 - fp: 10275.0000 - tn: 440983.0000 - fn: 38533.0000 - accuracy: 0.9111 - precision: 0.8524 - recall: 0.6063 - auc: 0.9222\n",
            "Epoch 00021: val_auc improved from 0.92848 to 0.93148, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3182 - tp: 59361.0000 - fp: 10286.0000 - tn: 441295.0000 - fn: 38549.0000 - accuracy: 0.9111 - precision: 0.8523 - recall: 0.6063 - auc: 0.9222 - val_loss: 0.3068 - val_tp: 15617.0000 - val_fp: 2547.0000 - val_tn: 110060.0000 - val_fn: 9149.0000 - val_accuracy: 0.9149 - val_precision: 0.8598 - val_recall: 0.6306 - val_auc: 0.9315\n",
            "Epoch 22/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.3146 - tp: 59886.0000 - fp: 10258.0000 - tn: 441323.0000 - fn: 38024.0000 - accuracy: 0.9121 - precision: 0.8538 - recall: 0.6116 - auc: 0.9244\n",
            "Epoch 00022: val_auc improved from 0.93148 to 0.93245, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3146 - tp: 59886.0000 - fp: 10258.0000 - tn: 441323.0000 - fn: 38024.0000 - accuracy: 0.9121 - precision: 0.8538 - recall: 0.6116 - auc: 0.9244 - val_loss: 0.3048 - val_tp: 16089.0000 - val_fp: 2932.0000 - val_tn: 109675.0000 - val_fn: 8677.0000 - val_accuracy: 0.9155 - val_precision: 0.8459 - val_recall: 0.6496 - val_auc: 0.9325\n",
            "Epoch 23/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.3107 - tp: 60299.0000 - fp: 10109.0000 - tn: 440751.0000 - fn: 37449.0000 - accuracy: 0.9133 - precision: 0.8564 - recall: 0.6169 - auc: 0.9267\n",
            "Epoch 00023: val_auc improved from 0.93245 to 0.93514, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3107 - tp: 60396.0000 - fp: 10126.0000 - tn: 441455.0000 - fn: 37514.0000 - accuracy: 0.9133 - precision: 0.8564 - recall: 0.6169 - auc: 0.9267 - val_loss: 0.2992 - val_tp: 15530.0000 - val_fp: 2115.0000 - val_tn: 110492.0000 - val_fn: 9236.0000 - val_accuracy: 0.9174 - val_precision: 0.8801 - val_recall: 0.6271 - val_auc: 0.9351\n",
            "Epoch 24/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.3076 - tp: 60925.0000 - fp: 10173.0000 - tn: 441408.0000 - fn: 36985.0000 - accuracy: 0.9142 - precision: 0.8569 - recall: 0.6223 - auc: 0.9283\n",
            "Epoch 00024: val_auc improved from 0.93514 to 0.93736, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3076 - tp: 60925.0000 - fp: 10173.0000 - tn: 441408.0000 - fn: 36985.0000 - accuracy: 0.9142 - precision: 0.8569 - recall: 0.6223 - auc: 0.9283 - val_loss: 0.2974 - val_tp: 15277.0000 - val_fp: 1810.0000 - val_tn: 110797.0000 - val_fn: 9489.0000 - val_accuracy: 0.9177 - val_precision: 0.8941 - val_recall: 0.6169 - val_auc: 0.9374\n",
            "Epoch 25/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.3046 - tp: 61205.0000 - fp: 10176.0000 - tn: 441311.0000 - fn: 36684.0000 - accuracy: 0.9147 - precision: 0.8574 - recall: 0.6252 - auc: 0.9303\n",
            "Epoch 00025: val_auc improved from 0.93736 to 0.93929, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3046 - tp: 61219.0000 - fp: 10178.0000 - tn: 441403.0000 - fn: 36691.0000 - accuracy: 0.9147 - precision: 0.8574 - recall: 0.6253 - auc: 0.9303 - val_loss: 0.2941 - val_tp: 16363.0000 - val_fp: 2646.0000 - val_tn: 109961.0000 - val_fn: 8403.0000 - val_accuracy: 0.9196 - val_precision: 0.8608 - val_recall: 0.6607 - val_auc: 0.9393\n",
            "Epoch 26/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.3011 - tp: 61755.0000 - fp: 9925.0000 - tn: 440708.0000 - fn: 35964.0000 - accuracy: 0.9163 - precision: 0.8615 - recall: 0.6320 - auc: 0.9318\n",
            "Epoch 00026: val_auc improved from 0.93929 to 0.94174, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.3011 - tp: 61875.0000 - fp: 9952.0000 - tn: 441629.0000 - fn: 36035.0000 - accuracy: 0.9163 - precision: 0.8614 - recall: 0.6320 - auc: 0.9318 - val_loss: 0.2887 - val_tp: 15996.0000 - val_fp: 2201.0000 - val_tn: 110406.0000 - val_fn: 8770.0000 - val_accuracy: 0.9201 - val_precision: 0.8790 - val_recall: 0.6459 - val_auc: 0.9417\n",
            "Epoch 27/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2986 - tp: 62083.0000 - fp: 10020.0000 - tn: 441561.0000 - fn: 35827.0000 - accuracy: 0.9166 - precision: 0.8610 - recall: 0.6341 - auc: 0.9335\n",
            "Epoch 00027: val_auc improved from 0.94174 to 0.94228, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2986 - tp: 62083.0000 - fp: 10020.0000 - tn: 441561.0000 - fn: 35827.0000 - accuracy: 0.9166 - precision: 0.8610 - recall: 0.6341 - auc: 0.9335 - val_loss: 0.2866 - val_tp: 15822.0000 - val_fp: 1957.0000 - val_tn: 110650.0000 - val_fn: 8944.0000 - val_accuracy: 0.9206 - val_precision: 0.8899 - val_recall: 0.6389 - val_auc: 0.9423\n",
            "Epoch 28/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2953 - tp: 62864.0000 - fp: 10037.0000 - tn: 441447.0000 - fn: 35028.0000 - accuracy: 0.9180 - precision: 0.8623 - recall: 0.6422 - auc: 0.9352\n",
            "Epoch 00028: val_auc improved from 0.94228 to 0.94243, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2953 - tp: 62875.0000 - fp: 10039.0000 - tn: 441542.0000 - fn: 35035.0000 - accuracy: 0.9180 - precision: 0.8623 - recall: 0.6422 - auc: 0.9352 - val_loss: 0.2874 - val_tp: 16482.0000 - val_fp: 2564.0000 - val_tn: 110043.0000 - val_fn: 8284.0000 - val_accuracy: 0.9210 - val_precision: 0.8654 - val_recall: 0.6655 - val_auc: 0.9424\n",
            "Epoch 29/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2929 - tp: 63033.0000 - fp: 9986.0000 - tn: 441495.0000 - fn: 34862.0000 - accuracy: 0.9184 - precision: 0.8632 - recall: 0.6439 - auc: 0.9363\n",
            "Epoch 00029: val_auc improved from 0.94243 to 0.94420, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2929 - tp: 63045.0000 - fp: 9989.0000 - tn: 441592.0000 - fn: 34865.0000 - accuracy: 0.9184 - precision: 0.8632 - recall: 0.6439 - auc: 0.9363 - val_loss: 0.2853 - val_tp: 15310.0000 - val_fp: 1621.0000 - val_tn: 110986.0000 - val_fn: 9456.0000 - val_accuracy: 0.9194 - val_precision: 0.9043 - val_recall: 0.6182 - val_auc: 0.9442\n",
            "Epoch 30/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2893 - tp: 63608.0000 - fp: 9907.0000 - tn: 440950.0000 - fn: 34143.0000 - accuracy: 0.9197 - precision: 0.8652 - recall: 0.6507 - auc: 0.9384\n",
            "Epoch 00030: val_auc improved from 0.94420 to 0.94644, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2893 - tp: 63707.0000 - fp: 9922.0000 - tn: 441659.0000 - fn: 34203.0000 - accuracy: 0.9197 - precision: 0.8652 - recall: 0.6507 - auc: 0.9384 - val_loss: 0.2809 - val_tp: 15313.0000 - val_fp: 1381.0000 - val_tn: 111226.0000 - val_fn: 9453.0000 - val_accuracy: 0.9211 - val_precision: 0.9173 - val_recall: 0.6183 - val_auc: 0.9464\n",
            "Epoch 31/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2869 - tp: 63969.0000 - fp: 9905.0000 - tn: 441155.0000 - fn: 33835.0000 - accuracy: 0.9203 - precision: 0.8659 - recall: 0.6541 - auc: 0.9397\n",
            "Epoch 00031: val_auc improved from 0.94644 to 0.94826, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2869 - tp: 64036.0000 - fp: 9913.0000 - tn: 441668.0000 - fn: 33874.0000 - accuracy: 0.9203 - precision: 0.8659 - recall: 0.6540 - auc: 0.9397 - val_loss: 0.2763 - val_tp: 16360.0000 - val_fp: 1975.0000 - val_tn: 110632.0000 - val_fn: 8406.0000 - val_accuracy: 0.9244 - val_precision: 0.8923 - val_recall: 0.6606 - val_auc: 0.9483\n",
            "Epoch 32/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2848 - tp: 64245.0000 - fp: 9868.0000 - tn: 441198.0000 - fn: 33553.0000 - accuracy: 0.9209 - precision: 0.8669 - recall: 0.6569 - auc: 0.9409\n",
            "Epoch 00032: val_auc improved from 0.94826 to 0.94941, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2848 - tp: 64319.0000 - fp: 9876.0000 - tn: 441705.0000 - fn: 33591.0000 - accuracy: 0.9209 - precision: 0.8669 - recall: 0.6569 - auc: 0.9409 - val_loss: 0.2750 - val_tp: 17491.0000 - val_fp: 3002.0000 - val_tn: 109605.0000 - val_fn: 7275.0000 - val_accuracy: 0.9252 - val_precision: 0.8535 - val_recall: 0.7063 - val_auc: 0.9494\n",
            "Epoch 33/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2827 - tp: 64631.0000 - fp: 9833.0000 - tn: 441448.0000 - fn: 33208.0000 - accuracy: 0.9216 - precision: 0.8679 - recall: 0.6606 - auc: 0.9418\n",
            "Epoch 00033: val_auc improved from 0.94941 to 0.95072, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2827 - tp: 64675.0000 - fp: 9840.0000 - tn: 441741.0000 - fn: 33235.0000 - accuracy: 0.9216 - precision: 0.8679 - recall: 0.6606 - auc: 0.9418 - val_loss: 0.2716 - val_tp: 16391.0000 - val_fp: 1993.0000 - val_tn: 110614.0000 - val_fn: 8375.0000 - val_accuracy: 0.9245 - val_precision: 0.8916 - val_recall: 0.6618 - val_auc: 0.9507\n",
            "Epoch 34/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2803 - tp: 65195.0000 - fp: 9702.0000 - tn: 441782.0000 - fn: 32697.0000 - accuracy: 0.9228 - precision: 0.8705 - recall: 0.6660 - auc: 0.9431\n",
            "Epoch 00034: val_auc improved from 0.95072 to 0.95231, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2803 - tp: 65209.0000 - fp: 9703.0000 - tn: 441878.0000 - fn: 32701.0000 - accuracy: 0.9228 - precision: 0.8705 - recall: 0.6660 - auc: 0.9431 - val_loss: 0.2693 - val_tp: 16662.0000 - val_fp: 1946.0000 - val_tn: 110661.0000 - val_fn: 8104.0000 - val_accuracy: 0.9268 - val_precision: 0.8954 - val_recall: 0.6728 - val_auc: 0.9523\n",
            "Epoch 35/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2779 - tp: 65346.0000 - fp: 9719.0000 - tn: 441340.0000 - fn: 32459.0000 - accuracy: 0.9232 - precision: 0.8705 - recall: 0.6681 - auc: 0.9446\n",
            "Epoch 00035: val_auc improved from 0.95231 to 0.95260, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2779 - tp: 65418.0000 - fp: 9726.0000 - tn: 441855.0000 - fn: 32492.0000 - accuracy: 0.9232 - precision: 0.8706 - recall: 0.6681 - auc: 0.9446 - val_loss: 0.2691 - val_tp: 16198.0000 - val_fp: 1638.0000 - val_tn: 110969.0000 - val_fn: 8568.0000 - val_accuracy: 0.9257 - val_precision: 0.9082 - val_recall: 0.6540 - val_auc: 0.9526\n",
            "Epoch 36/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2762 - tp: 65643.0000 - fp: 9871.0000 - tn: 441189.0000 - fn: 32161.0000 - accuracy: 0.9234 - precision: 0.8693 - recall: 0.6712 - auc: 0.9455\n",
            "Epoch 00036: val_auc improved from 0.95260 to 0.95346, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2761 - tp: 65718.0000 - fp: 9879.0000 - tn: 441702.0000 - fn: 32192.0000 - accuracy: 0.9234 - precision: 0.8693 - recall: 0.6712 - auc: 0.9455 - val_loss: 0.2650 - val_tp: 16570.0000 - val_fp: 1928.0000 - val_tn: 110679.0000 - val_fn: 8196.0000 - val_accuracy: 0.9263 - val_precision: 0.8958 - val_recall: 0.6691 - val_auc: 0.9535\n",
            "Epoch 37/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2735 - tp: 66196.0000 - fp: 9887.0000 - tn: 440961.0000 - fn: 31564.0000 - accuracy: 0.9244 - precision: 0.8700 - recall: 0.6771 - auc: 0.9469\n",
            "Epoch 00037: val_auc improved from 0.95346 to 0.95466, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2735 - tp: 66301.0000 - fp: 9900.0000 - tn: 441681.0000 - fn: 31609.0000 - accuracy: 0.9245 - precision: 0.8701 - recall: 0.6772 - auc: 0.9469 - val_loss: 0.2649 - val_tp: 17259.0000 - val_fp: 2454.0000 - val_tn: 110153.0000 - val_fn: 7507.0000 - val_accuracy: 0.9275 - val_precision: 0.8755 - val_recall: 0.6969 - val_auc: 0.9547\n",
            "Epoch 38/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2713 - tp: 66303.0000 - fp: 9746.0000 - tn: 441523.0000 - fn: 31548.0000 - accuracy: 0.9248 - precision: 0.8718 - recall: 0.6776 - auc: 0.9479\n",
            "Epoch 00038: val_auc improved from 0.95466 to 0.95831, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2713 - tp: 66344.0000 - fp: 9755.0000 - tn: 441826.0000 - fn: 31566.0000 - accuracy: 0.9248 - precision: 0.8718 - recall: 0.6776 - auc: 0.9479 - val_loss: 0.2576 - val_tp: 16924.0000 - val_fp: 1829.0000 - val_tn: 110778.0000 - val_fn: 7842.0000 - val_accuracy: 0.9296 - val_precision: 0.9025 - val_recall: 0.6834 - val_auc: 0.9583\n",
            "Epoch 39/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2696 - tp: 66581.0000 - fp: 9819.0000 - tn: 441240.0000 - fn: 31224.0000 - accuracy: 0.9252 - precision: 0.8715 - recall: 0.6808 - auc: 0.9490\n",
            "Epoch 00039: val_auc did not improve from 0.95831\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2696 - tp: 66660.0000 - fp: 9838.0000 - tn: 441743.0000 - fn: 31250.0000 - accuracy: 0.9252 - precision: 0.8714 - recall: 0.6808 - auc: 0.9490 - val_loss: 0.2606 - val_tp: 17425.0000 - val_fp: 2588.0000 - val_tn: 110019.0000 - val_fn: 7341.0000 - val_accuracy: 0.9277 - val_precision: 0.8707 - val_recall: 0.7036 - val_auc: 0.9557\n",
            "Epoch 40/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2678 - tp: 67058.0000 - fp: 9809.0000 - tn: 441464.0000 - fn: 30789.0000 - accuracy: 0.9261 - precision: 0.8724 - recall: 0.6853 - auc: 0.9496\n",
            "Epoch 00040: val_auc improved from 0.95831 to 0.95871, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2678 - tp: 67104.0000 - fp: 9813.0000 - tn: 441768.0000 - fn: 30806.0000 - accuracy: 0.9261 - precision: 0.8724 - recall: 0.6854 - auc: 0.9496 - val_loss: 0.2560 - val_tp: 17585.0000 - val_fp: 2425.0000 - val_tn: 110182.0000 - val_fn: 7181.0000 - val_accuracy: 0.9301 - val_precision: 0.8788 - val_recall: 0.7100 - val_auc: 0.9587\n",
            "Epoch 41/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2657 - tp: 67228.0000 - fp: 9664.0000 - tn: 440985.0000 - fn: 30475.0000 - accuracy: 0.9268 - precision: 0.8743 - recall: 0.6881 - auc: 0.9506\n",
            "Epoch 00041: val_auc did not improve from 0.95871\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2657 - tp: 67368.0000 - fp: 9683.0000 - tn: 441898.0000 - fn: 30542.0000 - accuracy: 0.9268 - precision: 0.8743 - recall: 0.6881 - auc: 0.9506 - val_loss: 0.2617 - val_tp: 15658.0000 - val_fp: 1243.0000 - val_tn: 111364.0000 - val_fn: 9108.0000 - val_accuracy: 0.9247 - val_precision: 0.9265 - val_recall: 0.6322 - val_auc: 0.9574\n",
            "Epoch 42/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2639 - tp: 67638.0000 - fp: 9698.0000 - tn: 441883.0000 - fn: 30272.0000 - accuracy: 0.9273 - precision: 0.8746 - recall: 0.6908 - auc: 0.9517\n",
            "Epoch 00042: val_auc did not improve from 0.95871\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2639 - tp: 67638.0000 - fp: 9698.0000 - tn: 441883.0000 - fn: 30272.0000 - accuracy: 0.9273 - precision: 0.8746 - recall: 0.6908 - auc: 0.9517 - val_loss: 0.2553 - val_tp: 17419.0000 - val_fp: 2320.0000 - val_tn: 110287.0000 - val_fn: 7347.0000 - val_accuracy: 0.9296 - val_precision: 0.8825 - val_recall: 0.7033 - val_auc: 0.9577\n",
            "Epoch 43/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2620 - tp: 67893.0000 - fp: 9727.0000 - tn: 441372.0000 - fn: 29872.0000 - accuracy: 0.9279 - precision: 0.8747 - recall: 0.6945 - auc: 0.9526\n",
            "Epoch 00043: val_auc improved from 0.95871 to 0.96107, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2620 - tp: 67984.0000 - fp: 9733.0000 - tn: 441848.0000 - fn: 29926.0000 - accuracy: 0.9278 - precision: 0.8748 - recall: 0.6944 - auc: 0.9526 - val_loss: 0.2498 - val_tp: 17447.0000 - val_fp: 2077.0000 - val_tn: 110530.0000 - val_fn: 7319.0000 - val_accuracy: 0.9316 - val_precision: 0.8936 - val_recall: 0.7045 - val_auc: 0.9611\n",
            "Epoch 44/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.2601 - tp: 67961.0000 - fp: 9592.0000 - tn: 440835.0000 - fn: 29708.0000 - accuracy: 0.9283 - precision: 0.8763 - recall: 0.6958 - auc: 0.9535\n",
            "Epoch 00044: val_auc improved from 0.96107 to 0.96179, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2601 - tp: 68130.0000 - fp: 9619.0000 - tn: 441962.0000 - fn: 29780.0000 - accuracy: 0.9283 - precision: 0.8763 - recall: 0.6958 - auc: 0.9535 - val_loss: 0.2476 - val_tp: 17701.0000 - val_fp: 2206.0000 - val_tn: 110401.0000 - val_fn: 7065.0000 - val_accuracy: 0.9325 - val_precision: 0.8892 - val_recall: 0.7147 - val_auc: 0.9618\n",
            "Epoch 45/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2585 - tp: 68586.0000 - fp: 9683.0000 - tn: 441898.0000 - fn: 29324.0000 - accuracy: 0.9290 - precision: 0.8763 - recall: 0.7005 - auc: 0.9543\n",
            "Epoch 00045: val_auc improved from 0.96179 to 0.96385, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2585 - tp: 68586.0000 - fp: 9683.0000 - tn: 441898.0000 - fn: 29324.0000 - accuracy: 0.9290 - precision: 0.8763 - recall: 0.7005 - auc: 0.9543 - val_loss: 0.2471 - val_tp: 16793.0000 - val_fp: 1540.0000 - val_tn: 111067.0000 - val_fn: 7973.0000 - val_accuracy: 0.9308 - val_precision: 0.9160 - val_recall: 0.6781 - val_auc: 0.9639\n",
            "Epoch 46/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2570 - tp: 68721.0000 - fp: 9560.0000 - tn: 441925.0000 - fn: 29170.0000 - accuracy: 0.9295 - precision: 0.8779 - recall: 0.7020 - auc: 0.9549\n",
            "Epoch 00046: val_auc did not improve from 0.96385\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2570 - tp: 68735.0000 - fp: 9565.0000 - tn: 442016.0000 - fn: 29175.0000 - accuracy: 0.9295 - precision: 0.8778 - recall: 0.7020 - auc: 0.9549 - val_loss: 0.2463 - val_tp: 17962.0000 - val_fp: 2447.0000 - val_tn: 110160.0000 - val_fn: 6804.0000 - val_accuracy: 0.9327 - val_precision: 0.8801 - val_recall: 0.7253 - val_auc: 0.9619\n",
            "Epoch 47/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2550 - tp: 69037.0000 - fp: 9628.0000 - tn: 441434.0000 - fn: 28765.0000 - accuracy: 0.9301 - precision: 0.8776 - recall: 0.7059 - auc: 0.9560\n",
            "Epoch 00047: val_auc did not improve from 0.96385\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2550 - tp: 69107.0000 - fp: 9630.0000 - tn: 441951.0000 - fn: 28803.0000 - accuracy: 0.9301 - precision: 0.8777 - recall: 0.7058 - auc: 0.9560 - val_loss: 0.2448 - val_tp: 17797.0000 - val_fp: 2256.0000 - val_tn: 110351.0000 - val_fn: 6969.0000 - val_accuracy: 0.9328 - val_precision: 0.8875 - val_recall: 0.7186 - val_auc: 0.9629\n",
            "Epoch 48/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2538 - tp: 69188.0000 - fp: 9682.0000 - tn: 441809.0000 - fn: 28697.0000 - accuracy: 0.9301 - precision: 0.8772 - recall: 0.7068 - auc: 0.9566\n",
            "Epoch 00048: val_auc improved from 0.96385 to 0.96580, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2538 - tp: 69204.0000 - fp: 9685.0000 - tn: 441896.0000 - fn: 28706.0000 - accuracy: 0.9301 - precision: 0.8772 - recall: 0.7068 - auc: 0.9566 - val_loss: 0.2395 - val_tp: 17771.0000 - val_fp: 1894.0000 - val_tn: 110713.0000 - val_fn: 6995.0000 - val_accuracy: 0.9353 - val_precision: 0.9037 - val_recall: 0.7176 - val_auc: 0.9658\n",
            "Epoch 49/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2518 - tp: 69722.0000 - fp: 9626.0000 - tn: 441955.0000 - fn: 28188.0000 - accuracy: 0.9312 - precision: 0.8787 - recall: 0.7121 - auc: 0.9574\n",
            "Epoch 00049: val_auc improved from 0.96580 to 0.96607, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2518 - tp: 69722.0000 - fp: 9626.0000 - tn: 441955.0000 - fn: 28188.0000 - accuracy: 0.9312 - precision: 0.8787 - recall: 0.7121 - auc: 0.9574 - val_loss: 0.2419 - val_tp: 16985.0000 - val_fp: 1451.0000 - val_tn: 111156.0000 - val_fn: 7781.0000 - val_accuracy: 0.9328 - val_precision: 0.9213 - val_recall: 0.6858 - val_auc: 0.9661\n",
            "Epoch 50/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2517 - tp: 69665.0000 - fp: 9576.0000 - tn: 441905.0000 - fn: 28230.0000 - accuracy: 0.9312 - precision: 0.8792 - recall: 0.7116 - auc: 0.9574\n",
            "Epoch 00050: val_auc improved from 0.96607 to 0.96630, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2517 - tp: 69676.0000 - fp: 9581.0000 - tn: 442000.0000 - fn: 28234.0000 - accuracy: 0.9312 - precision: 0.8791 - recall: 0.7116 - auc: 0.9574 - val_loss: 0.2388 - val_tp: 18134.0000 - val_fp: 2259.0000 - val_tn: 110348.0000 - val_fn: 6632.0000 - val_accuracy: 0.9353 - val_precision: 0.8892 - val_recall: 0.7322 - val_auc: 0.9663\n",
            "Epoch 51/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2497 - tp: 69833.0000 - fp: 9540.0000 - tn: 441517.0000 - fn: 27974.0000 - accuracy: 0.9317 - precision: 0.8798 - recall: 0.7140 - auc: 0.9584\n",
            "Epoch 00051: val_auc did not improve from 0.96630\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2497 - tp: 69909.0000 - fp: 9546.0000 - tn: 442035.0000 - fn: 28001.0000 - accuracy: 0.9317 - precision: 0.8799 - recall: 0.7140 - auc: 0.9584 - val_loss: 0.2432 - val_tp: 16604.0000 - val_fp: 1133.0000 - val_tn: 111474.0000 - val_fn: 8162.0000 - val_accuracy: 0.9323 - val_precision: 0.9361 - val_recall: 0.6704 - val_auc: 0.9660\n",
            "Epoch 52/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2485 - tp: 70270.0000 - fp: 9621.0000 - tn: 441960.0000 - fn: 27640.0000 - accuracy: 0.9322 - precision: 0.8796 - recall: 0.7177 - auc: 0.9589\n",
            "Epoch 00052: val_auc improved from 0.96630 to 0.96707, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2485 - tp: 70270.0000 - fp: 9621.0000 - tn: 441960.0000 - fn: 27640.0000 - accuracy: 0.9322 - precision: 0.8796 - recall: 0.7177 - auc: 0.9589 - val_loss: 0.2375 - val_tp: 17383.0000 - val_fp: 1527.0000 - val_tn: 111080.0000 - val_fn: 7383.0000 - val_accuracy: 0.9351 - val_precision: 0.9192 - val_recall: 0.7019 - val_auc: 0.9671\n",
            "Epoch 53/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2470 - tp: 70595.0000 - fp: 9601.0000 - tn: 441887.0000 - fn: 27293.0000 - accuracy: 0.9328 - precision: 0.8803 - recall: 0.7212 - auc: 0.9598\n",
            "Epoch 00053: val_auc did not improve from 0.96707\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2469 - tp: 70614.0000 - fp: 9602.0000 - tn: 441979.0000 - fn: 27296.0000 - accuracy: 0.9329 - precision: 0.8803 - recall: 0.7212 - auc: 0.9598 - val_loss: 0.2390 - val_tp: 19207.0000 - val_fp: 3262.0000 - val_tn: 109345.0000 - val_fn: 5559.0000 - val_accuracy: 0.9358 - val_precision: 0.8548 - val_recall: 0.7755 - val_auc: 0.9669\n",
            "Epoch 54/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2455 - tp: 70827.0000 - fp: 9661.0000 - tn: 441615.0000 - fn: 27017.0000 - accuracy: 0.9332 - precision: 0.8800 - recall: 0.7239 - auc: 0.9604\n",
            "Epoch 00054: val_auc improved from 0.96707 to 0.96879, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2455 - tp: 70880.0000 - fp: 9667.0000 - tn: 441914.0000 - fn: 27030.0000 - accuracy: 0.9332 - precision: 0.8800 - recall: 0.7239 - auc: 0.9604 - val_loss: 0.2325 - val_tp: 18253.0000 - val_fp: 2073.0000 - val_tn: 110534.0000 - val_fn: 6513.0000 - val_accuracy: 0.9375 - val_precision: 0.8980 - val_recall: 0.7370 - val_auc: 0.9688\n",
            "Epoch 55/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2447 - tp: 70722.0000 - fp: 9560.0000 - tn: 441311.0000 - fn: 27015.0000 - accuracy: 0.9333 - precision: 0.8809 - recall: 0.7236 - auc: 0.9606\n",
            "Epoch 00055: val_auc improved from 0.96879 to 0.96981, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2447 - tp: 70846.0000 - fp: 9579.0000 - tn: 442002.0000 - fn: 27064.0000 - accuracy: 0.9333 - precision: 0.8809 - recall: 0.7236 - auc: 0.9606 - val_loss: 0.2311 - val_tp: 18634.0000 - val_fp: 2394.0000 - val_tn: 110213.0000 - val_fn: 6132.0000 - val_accuracy: 0.9379 - val_precision: 0.8862 - val_recall: 0.7524 - val_auc: 0.9698\n",
            "Epoch 56/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2428 - tp: 71005.0000 - fp: 9385.0000 - tn: 441471.0000 - fn: 26747.0000 - accuracy: 0.9341 - precision: 0.8833 - recall: 0.7264 - auc: 0.9615\n",
            "Epoch 00056: val_auc improved from 0.96981 to 0.97049, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2428 - tp: 71125.0000 - fp: 9398.0000 - tn: 442183.0000 - fn: 26785.0000 - accuracy: 0.9342 - precision: 0.8833 - recall: 0.7264 - auc: 0.9615 - val_loss: 0.2297 - val_tp: 17657.0000 - val_fp: 1510.0000 - val_tn: 111097.0000 - val_fn: 7109.0000 - val_accuracy: 0.9373 - val_precision: 0.9212 - val_recall: 0.7130 - val_auc: 0.9705\n",
            "Epoch 57/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2411 - tp: 71273.0000 - fp: 9469.0000 - tn: 441196.0000 - fn: 26414.0000 - accuracy: 0.9346 - precision: 0.8827 - recall: 0.7296 - auc: 0.9624\n",
            "Epoch 00057: val_auc did not improve from 0.97049\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2411 - tp: 71437.0000 - fp: 9482.0000 - tn: 442099.0000 - fn: 26473.0000 - accuracy: 0.9346 - precision: 0.8828 - recall: 0.7296 - auc: 0.9624 - val_loss: 0.2299 - val_tp: 18649.0000 - val_fp: 2359.0000 - val_tn: 110248.0000 - val_fn: 6117.0000 - val_accuracy: 0.9383 - val_precision: 0.8877 - val_recall: 0.7530 - val_auc: 0.9693\n",
            "Epoch 58/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2400 - tp: 71586.0000 - fp: 9408.0000 - tn: 441445.0000 - fn: 26169.0000 - accuracy: 0.9352 - precision: 0.8838 - recall: 0.7323 - auc: 0.9628\n",
            "Epoch 00058: val_auc improved from 0.97049 to 0.97135, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2399 - tp: 71701.0000 - fp: 9425.0000 - tn: 442156.0000 - fn: 26209.0000 - accuracy: 0.9352 - precision: 0.8838 - recall: 0.7323 - auc: 0.9628 - val_loss: 0.2266 - val_tp: 18144.0000 - val_fp: 1724.0000 - val_tn: 110883.0000 - val_fn: 6622.0000 - val_accuracy: 0.9392 - val_precision: 0.9132 - val_recall: 0.7326 - val_auc: 0.9713\n",
            "Epoch 59/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2391 - tp: 71780.0000 - fp: 9451.0000 - tn: 442033.0000 - fn: 26112.0000 - accuracy: 0.9353 - precision: 0.8837 - recall: 0.7333 - auc: 0.9631\n",
            "Epoch 00059: val_auc did not improve from 0.97135\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2391 - tp: 71796.0000 - fp: 9455.0000 - tn: 442126.0000 - fn: 26114.0000 - accuracy: 0.9353 - precision: 0.8836 - recall: 0.7333 - auc: 0.9631 - val_loss: 0.2294 - val_tp: 18830.0000 - val_fp: 2528.0000 - val_tn: 110079.0000 - val_fn: 5936.0000 - val_accuracy: 0.9384 - val_precision: 0.8816 - val_recall: 0.7603 - val_auc: 0.9698\n",
            "Epoch 60/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2376 - tp: 71983.0000 - fp: 9385.0000 - tn: 441670.0000 - fn: 25826.0000 - accuracy: 0.9358 - precision: 0.8847 - recall: 0.7360 - auc: 0.9639\n",
            "Epoch 00060: val_auc improved from 0.97135 to 0.97346, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2376 - tp: 72054.0000 - fp: 9395.0000 - tn: 442186.0000 - fn: 25856.0000 - accuracy: 0.9358 - precision: 0.8847 - recall: 0.7359 - auc: 0.9639 - val_loss: 0.2220 - val_tp: 18399.0000 - val_fp: 1741.0000 - val_tn: 110866.0000 - val_fn: 6367.0000 - val_accuracy: 0.9410 - val_precision: 0.9136 - val_recall: 0.7429 - val_auc: 0.9735\n",
            "Epoch 61/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2364 - tp: 72122.0000 - fp: 9513.0000 - tn: 441769.0000 - fn: 25716.0000 - accuracy: 0.9358 - precision: 0.8835 - recall: 0.7372 - auc: 0.9645\n",
            "Epoch 00061: val_auc did not improve from 0.97346\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2364 - tp: 72175.0000 - fp: 9518.0000 - tn: 442063.0000 - fn: 25735.0000 - accuracy: 0.9358 - precision: 0.8835 - recall: 0.7372 - auc: 0.9645 - val_loss: 0.2261 - val_tp: 18509.0000 - val_fp: 2052.0000 - val_tn: 110555.0000 - val_fn: 6257.0000 - val_accuracy: 0.9395 - val_precision: 0.9002 - val_recall: 0.7474 - val_auc: 0.9704\n",
            "Epoch 62/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2354 - tp: 72283.0000 - fp: 9361.0000 - tn: 441508.0000 - fn: 25456.0000 - accuracy: 0.9365 - precision: 0.8853 - recall: 0.7396 - auc: 0.9648\n",
            "Epoch 00062: val_auc did not improve from 0.97346\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2354 - tp: 72407.0000 - fp: 9379.0000 - tn: 442202.0000 - fn: 25503.0000 - accuracy: 0.9365 - precision: 0.8853 - recall: 0.7395 - auc: 0.9648 - val_loss: 0.2241 - val_tp: 18375.0000 - val_fp: 1899.0000 - val_tn: 110708.0000 - val_fn: 6391.0000 - val_accuracy: 0.9397 - val_precision: 0.9063 - val_recall: 0.7419 - val_auc: 0.9719\n",
            "Epoch 63/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.2342 - tp: 72412.0000 - fp: 9381.0000 - tn: 441053.0000 - fn: 25250.0000 - accuracy: 0.9368 - precision: 0.8853 - recall: 0.7415 - auc: 0.9653\n",
            "Epoch 00063: val_auc did not improve from 0.97346\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2342 - tp: 72600.0000 - fp: 9407.0000 - tn: 442174.0000 - fn: 25310.0000 - accuracy: 0.9368 - precision: 0.8853 - recall: 0.7415 - auc: 0.9654 - val_loss: 0.2400 - val_tp: 19590.0000 - val_fp: 3883.0000 - val_tn: 108724.0000 - val_fn: 5176.0000 - val_accuracy: 0.9341 - val_precision: 0.8346 - val_recall: 0.7910 - val_auc: 0.9679\n",
            "Epoch 64/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.2333 - tp: 72583.0000 - fp: 9447.0000 - tn: 440985.0000 - fn: 25081.0000 - accuracy: 0.9370 - precision: 0.8848 - recall: 0.7432 - auc: 0.9657\n",
            "Epoch 00064: val_auc did not improve from 0.97346\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2334 - tp: 72756.0000 - fp: 9486.0000 - tn: 442095.0000 - fn: 25154.0000 - accuracy: 0.9370 - precision: 0.8847 - recall: 0.7431 - auc: 0.9657 - val_loss: 0.2245 - val_tp: 19455.0000 - val_fp: 2747.0000 - val_tn: 109860.0000 - val_fn: 5311.0000 - val_accuracy: 0.9413 - val_precision: 0.8763 - val_recall: 0.7856 - val_auc: 0.9734\n",
            "Epoch 65/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2322 - tp: 72892.0000 - fp: 9568.0000 - tn: 441490.0000 - fn: 24914.0000 - accuracy: 0.9372 - precision: 0.8840 - recall: 0.7453 - auc: 0.9663\n",
            "Epoch 00065: val_auc improved from 0.97346 to 0.97514, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2322 - tp: 72975.0000 - fp: 9578.0000 - tn: 442003.0000 - fn: 24935.0000 - accuracy: 0.9372 - precision: 0.8840 - recall: 0.7453 - auc: 0.9663 - val_loss: 0.2169 - val_tp: 18953.0000 - val_fp: 1986.0000 - val_tn: 110621.0000 - val_fn: 5813.0000 - val_accuracy: 0.9432 - val_precision: 0.9052 - val_recall: 0.7653 - val_auc: 0.9751\n",
            "Epoch 66/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2310 - tp: 73114.0000 - fp: 9363.0000 - tn: 441688.0000 - fn: 24699.0000 - accuracy: 0.9379 - precision: 0.8865 - recall: 0.7475 - auc: 0.9666\n",
            "Epoch 00066: val_auc did not improve from 0.97514\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2310 - tp: 73182.0000 - fp: 9380.0000 - tn: 442201.0000 - fn: 24728.0000 - accuracy: 0.9379 - precision: 0.8864 - recall: 0.7474 - auc: 0.9666 - val_loss: 0.2215 - val_tp: 18017.0000 - val_fp: 1426.0000 - val_tn: 111181.0000 - val_fn: 6749.0000 - val_accuracy: 0.9405 - val_precision: 0.9267 - val_recall: 0.7275 - val_auc: 0.9735\n",
            "Epoch 67/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2298 - tp: 73341.0000 - fp: 9349.0000 - tn: 442147.0000 - fn: 24539.0000 - accuracy: 0.9383 - precision: 0.8869 - recall: 0.7493 - auc: 0.9672\n",
            "Epoch 00067: val_auc did not improve from 0.97514\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2298 - tp: 73365.0000 - fp: 9349.0000 - tn: 442232.0000 - fn: 24545.0000 - accuracy: 0.9383 - precision: 0.8870 - recall: 0.7493 - auc: 0.9672 - val_loss: 0.2184 - val_tp: 18452.0000 - val_fp: 1639.0000 - val_tn: 110968.0000 - val_fn: 6314.0000 - val_accuracy: 0.9421 - val_precision: 0.9184 - val_recall: 0.7451 - val_auc: 0.9742\n",
            "Epoch 68/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2287 - tp: 73490.0000 - fp: 9306.0000 - tn: 442275.0000 - fn: 24420.0000 - accuracy: 0.9386 - precision: 0.8876 - recall: 0.7506 - auc: 0.9677\n",
            "Epoch 00068: val_auc improved from 0.97514 to 0.97518, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2287 - tp: 73490.0000 - fp: 9306.0000 - tn: 442275.0000 - fn: 24420.0000 - accuracy: 0.9386 - precision: 0.8876 - recall: 0.7506 - auc: 0.9677 - val_loss: 0.2164 - val_tp: 19358.0000 - val_fp: 2292.0000 - val_tn: 110315.0000 - val_fn: 5408.0000 - val_accuracy: 0.9439 - val_precision: 0.8941 - val_recall: 0.7816 - val_auc: 0.9752\n",
            "Epoch 69/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2284 - tp: 73504.0000 - fp: 9305.0000 - tn: 442276.0000 - fn: 24406.0000 - accuracy: 0.9387 - precision: 0.8876 - recall: 0.7507 - auc: 0.9678\n",
            "Epoch 00069: val_auc did not improve from 0.97518\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2284 - tp: 73504.0000 - fp: 9305.0000 - tn: 442276.0000 - fn: 24406.0000 - accuracy: 0.9387 - precision: 0.8876 - recall: 0.7507 - auc: 0.9678 - val_loss: 0.2193 - val_tp: 18284.0000 - val_fp: 1651.0000 - val_tn: 110956.0000 - val_fn: 6482.0000 - val_accuracy: 0.9408 - val_precision: 0.9172 - val_recall: 0.7383 - val_auc: 0.9737\n",
            "Epoch 70/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2276 - tp: 73546.0000 - fp: 9383.0000 - tn: 441463.0000 - fn: 24216.0000 - accuracy: 0.9388 - precision: 0.8869 - recall: 0.7523 - auc: 0.9682\n",
            "Epoch 00070: val_auc improved from 0.97518 to 0.97644, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2276 - tp: 73658.0000 - fp: 9404.0000 - tn: 442177.0000 - fn: 24252.0000 - accuracy: 0.9388 - precision: 0.8868 - recall: 0.7523 - auc: 0.9682 - val_loss: 0.2124 - val_tp: 19290.0000 - val_fp: 2072.0000 - val_tn: 110535.0000 - val_fn: 5476.0000 - val_accuracy: 0.9451 - val_precision: 0.9030 - val_recall: 0.7789 - val_auc: 0.9764\n",
            "Epoch 71/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2257 - tp: 73829.0000 - fp: 9427.0000 - tn: 442060.0000 - fn: 24060.0000 - accuracy: 0.9390 - precision: 0.8868 - recall: 0.7542 - auc: 0.9689\n",
            "Epoch 00071: val_auc did not improve from 0.97644\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2257 - tp: 73848.0000 - fp: 9428.0000 - tn: 442153.0000 - fn: 24062.0000 - accuracy: 0.9391 - precision: 0.8868 - recall: 0.7542 - auc: 0.9690 - val_loss: 0.2212 - val_tp: 18171.0000 - val_fp: 1608.0000 - val_tn: 110999.0000 - val_fn: 6595.0000 - val_accuracy: 0.9403 - val_precision: 0.9187 - val_recall: 0.7337 - val_auc: 0.9728\n",
            "Epoch 72/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2252 - tp: 74038.0000 - fp: 9274.0000 - tn: 441575.0000 - fn: 23721.0000 - accuracy: 0.9399 - precision: 0.8887 - recall: 0.7574 - auc: 0.9690\n",
            "Epoch 00072: val_auc did not improve from 0.97644\n",
            "2147/2147 [==============================] - 22s 10ms/step - loss: 0.2252 - tp: 74155.0000 - fp: 9296.0000 - tn: 442285.0000 - fn: 23755.0000 - accuracy: 0.9399 - precision: 0.8886 - recall: 0.7574 - auc: 0.9690 - val_loss: 0.2127 - val_tp: 19045.0000 - val_fp: 2003.0000 - val_tn: 110604.0000 - val_fn: 5721.0000 - val_accuracy: 0.9438 - val_precision: 0.9048 - val_recall: 0.7690 - val_auc: 0.9759\n",
            "Epoch 73/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2243 - tp: 74144.0000 - fp: 9173.0000 - tn: 442408.0000 - fn: 23766.0000 - accuracy: 0.9401 - precision: 0.8899 - recall: 0.7573 - auc: 0.9694\n",
            "Epoch 00073: val_auc improved from 0.97644 to 0.97686, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2243 - tp: 74144.0000 - fp: 9173.0000 - tn: 442408.0000 - fn: 23766.0000 - accuracy: 0.9401 - precision: 0.8899 - recall: 0.7573 - auc: 0.9694 - val_loss: 0.2149 - val_tp: 19503.0000 - val_fp: 2410.0000 - val_tn: 110197.0000 - val_fn: 5263.0000 - val_accuracy: 0.9441 - val_precision: 0.8900 - val_recall: 0.7875 - val_auc: 0.9769\n",
            "Epoch 74/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2231 - tp: 74289.0000 - fp: 9200.0000 - tn: 441464.0000 - fn: 23399.0000 - accuracy: 0.9406 - precision: 0.8898 - recall: 0.7605 - auc: 0.9699\n",
            "Epoch 00074: val_auc improved from 0.97686 to 0.97736, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2230 - tp: 74467.0000 - fp: 9217.0000 - tn: 442364.0000 - fn: 23443.0000 - accuracy: 0.9406 - precision: 0.8899 - recall: 0.7606 - auc: 0.9699 - val_loss: 0.2098 - val_tp: 19083.0000 - val_fp: 1927.0000 - val_tn: 110680.0000 - val_fn: 5683.0000 - val_accuracy: 0.9446 - val_precision: 0.9083 - val_recall: 0.7705 - val_auc: 0.9774\n",
            "Epoch 75/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2224 - tp: 74418.0000 - fp: 9292.0000 - tn: 441357.0000 - fn: 23285.0000 - accuracy: 0.9406 - precision: 0.8890 - recall: 0.7617 - auc: 0.9703\n",
            "Epoch 00075: val_auc improved from 0.97736 to 0.97748, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2224 - tp: 74575.0000 - fp: 9315.0000 - tn: 442266.0000 - fn: 23335.0000 - accuracy: 0.9406 - precision: 0.8890 - recall: 0.7617 - auc: 0.9703 - val_loss: 0.2082 - val_tp: 19392.0000 - val_fp: 2063.0000 - val_tn: 110544.0000 - val_fn: 5374.0000 - val_accuracy: 0.9459 - val_precision: 0.9038 - val_recall: 0.7830 - val_auc: 0.9775\n",
            "Epoch 76/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2210 - tp: 74759.0000 - fp: 9198.0000 - tn: 442063.0000 - fn: 23100.0000 - accuracy: 0.9412 - precision: 0.8904 - recall: 0.7639 - auc: 0.9708\n",
            "Epoch 00076: val_auc did not improve from 0.97748\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2210 - tp: 74799.0000 - fp: 9209.0000 - tn: 442372.0000 - fn: 23111.0000 - accuracy: 0.9412 - precision: 0.8904 - recall: 0.7640 - auc: 0.9708 - val_loss: 0.2121 - val_tp: 19827.0000 - val_fp: 2741.0000 - val_tn: 109866.0000 - val_fn: 4939.0000 - val_accuracy: 0.9441 - val_precision: 0.8785 - val_recall: 0.8006 - val_auc: 0.9767\n",
            "Epoch 77/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2205 - tp: 74753.0000 - fp: 9240.0000 - tn: 441414.0000 - fn: 22945.0000 - accuracy: 0.9413 - precision: 0.8900 - recall: 0.7651 - auc: 0.9710\n",
            "Epoch 00077: val_auc improved from 0.97748 to 0.97806, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2206 - tp: 74901.0000 - fp: 9262.0000 - tn: 442319.0000 - fn: 23009.0000 - accuracy: 0.9413 - precision: 0.8900 - recall: 0.7650 - auc: 0.9710 - val_loss: 0.2071 - val_tp: 19693.0000 - val_fp: 2316.0000 - val_tn: 110291.0000 - val_fn: 5073.0000 - val_accuracy: 0.9462 - val_precision: 0.8948 - val_recall: 0.7952 - val_auc: 0.9781\n",
            "Epoch 78/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2206 - tp: 74982.0000 - fp: 9261.0000 - tn: 442230.0000 - fn: 22903.0000 - accuracy: 0.9415 - precision: 0.8901 - recall: 0.7660 - auc: 0.9709\n",
            "Epoch 00078: val_auc did not improve from 0.97806\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2206 - tp: 75002.0000 - fp: 9262.0000 - tn: 442319.0000 - fn: 22908.0000 - accuracy: 0.9415 - precision: 0.8901 - recall: 0.7660 - auc: 0.9709 - val_loss: 0.2188 - val_tp: 20431.0000 - val_fp: 3437.0000 - val_tn: 109170.0000 - val_fn: 4335.0000 - val_accuracy: 0.9434 - val_precision: 0.8560 - val_recall: 0.8250 - val_auc: 0.9765\n",
            "Epoch 79/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2197 - tp: 75144.0000 - fp: 9269.0000 - tn: 441799.0000 - fn: 22652.0000 - accuracy: 0.9418 - precision: 0.8902 - recall: 0.7684 - auc: 0.9713\n",
            "Epoch 00079: val_auc did not improve from 0.97806\n",
            "2147/2147 [==============================] - 22s 10ms/step - loss: 0.2197 - tp: 75230.0000 - fp: 9275.0000 - tn: 442306.0000 - fn: 22680.0000 - accuracy: 0.9418 - precision: 0.8902 - recall: 0.7684 - auc: 0.9713 - val_loss: 0.2092 - val_tp: 18610.0000 - val_fp: 1438.0000 - val_tn: 111169.0000 - val_fn: 6156.0000 - val_accuracy: 0.9447 - val_precision: 0.9283 - val_recall: 0.7514 - val_auc: 0.9779\n",
            "Epoch 80/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2186 - tp: 75121.0000 - fp: 9196.0000 - tn: 441643.0000 - fn: 22648.0000 - accuracy: 0.9420 - precision: 0.8909 - recall: 0.7684 - auc: 0.9717\n",
            "Epoch 00080: val_auc improved from 0.97806 to 0.97858, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2185 - tp: 75235.0000 - fp: 9209.0000 - tn: 442372.0000 - fn: 22675.0000 - accuracy: 0.9420 - precision: 0.8909 - recall: 0.7684 - auc: 0.9717 - val_loss: 0.2046 - val_tp: 19716.0000 - val_fp: 2106.0000 - val_tn: 110501.0000 - val_fn: 5050.0000 - val_accuracy: 0.9479 - val_precision: 0.9035 - val_recall: 0.7961 - val_auc: 0.9786\n",
            "Epoch 81/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2177 - tp: 75286.0000 - fp: 9258.0000 - tn: 441816.0000 - fn: 22504.0000 - accuracy: 0.9421 - precision: 0.8905 - recall: 0.7699 - auc: 0.9721\n",
            "Epoch 00081: val_auc improved from 0.97858 to 0.97925, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2177 - tp: 75377.0000 - fp: 9269.0000 - tn: 442312.0000 - fn: 22533.0000 - accuracy: 0.9421 - precision: 0.8905 - recall: 0.7699 - auc: 0.9721 - val_loss: 0.2042 - val_tp: 19043.0000 - val_fp: 1642.0000 - val_tn: 110965.0000 - val_fn: 5723.0000 - val_accuracy: 0.9464 - val_precision: 0.9206 - val_recall: 0.7689 - val_auc: 0.9793\n",
            "Epoch 82/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2169 - tp: 75372.0000 - fp: 9070.0000 - tn: 442426.0000 - fn: 22508.0000 - accuracy: 0.9425 - precision: 0.8926 - recall: 0.7700 - auc: 0.9723\n",
            "Epoch 00082: val_auc did not improve from 0.97925\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2169 - tp: 75395.0000 - fp: 9071.0000 - tn: 442510.0000 - fn: 22515.0000 - accuracy: 0.9425 - precision: 0.8926 - recall: 0.7700 - auc: 0.9723 - val_loss: 0.2050 - val_tp: 19761.0000 - val_fp: 2271.0000 - val_tn: 110336.0000 - val_fn: 5005.0000 - val_accuracy: 0.9470 - val_precision: 0.8969 - val_recall: 0.7979 - val_auc: 0.9789\n",
            "Epoch 83/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2167 - tp: 75463.0000 - fp: 9162.0000 - tn: 441481.0000 - fn: 22246.0000 - accuracy: 0.9427 - precision: 0.8917 - recall: 0.7723 - auc: 0.9724\n",
            "Epoch 00083: val_auc did not improve from 0.97925\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2167 - tp: 75624.0000 - fp: 9188.0000 - tn: 442393.0000 - fn: 22286.0000 - accuracy: 0.9427 - precision: 0.8917 - recall: 0.7724 - auc: 0.9724 - val_loss: 0.2201 - val_tp: 20845.0000 - val_fp: 3939.0000 - val_tn: 108668.0000 - val_fn: 3921.0000 - val_accuracy: 0.9428 - val_precision: 0.8411 - val_recall: 0.8417 - val_auc: 0.9770\n",
            "Epoch 84/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2164 - tp: 75522.0000 - fp: 9196.0000 - tn: 441473.0000 - fn: 22161.0000 - accuracy: 0.9428 - precision: 0.8915 - recall: 0.7731 - auc: 0.9725\n",
            "Epoch 00084: val_auc improved from 0.97925 to 0.97960, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2164 - tp: 75693.0000 - fp: 9212.0000 - tn: 442369.0000 - fn: 22217.0000 - accuracy: 0.9428 - precision: 0.8915 - recall: 0.7731 - auc: 0.9725 - val_loss: 0.2039 - val_tp: 19985.0000 - val_fp: 2315.0000 - val_tn: 110292.0000 - val_fn: 4781.0000 - val_accuracy: 0.9483 - val_precision: 0.8962 - val_recall: 0.8070 - val_auc: 0.9796\n",
            "Epoch 85/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2146 - tp: 75921.0000 - fp: 9072.0000 - tn: 441785.0000 - fn: 21830.0000 - accuracy: 0.9437 - precision: 0.8933 - recall: 0.7767 - auc: 0.9731\n",
            "Epoch 00085: val_auc did not improve from 0.97960\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2147 - tp: 76036.0000 - fp: 9082.0000 - tn: 442499.0000 - fn: 21874.0000 - accuracy: 0.9437 - precision: 0.8933 - recall: 0.7766 - auc: 0.9731 - val_loss: 0.2079 - val_tp: 19207.0000 - val_fp: 1847.0000 - val_tn: 110760.0000 - val_fn: 5559.0000 - val_accuracy: 0.9461 - val_precision: 0.9123 - val_recall: 0.7755 - val_auc: 0.9777\n",
            "Epoch 86/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2137 - tp: 75983.0000 - fp: 9140.0000 - tn: 442131.0000 - fn: 21866.0000 - accuracy: 0.9435 - precision: 0.8926 - recall: 0.7765 - auc: 0.9736\n",
            "Epoch 00086: val_auc did not improve from 0.97960\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.2137 - tp: 76033.0000 - fp: 9148.0000 - tn: 442433.0000 - fn: 21877.0000 - accuracy: 0.9435 - precision: 0.8926 - recall: 0.7766 - auc: 0.9736 - val_loss: 0.2044 - val_tp: 19834.0000 - val_fp: 2209.0000 - val_tn: 110398.0000 - val_fn: 4932.0000 - val_accuracy: 0.9480 - val_precision: 0.8998 - val_recall: 0.8009 - val_auc: 0.9789\n",
            "Epoch 87/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2136 - tp: 76152.0000 - fp: 9147.0000 - tn: 441907.0000 - fn: 21658.0000 - accuracy: 0.9439 - precision: 0.8928 - recall: 0.7786 - auc: 0.9736\n",
            "Epoch 00087: val_auc improved from 0.97960 to 0.98170, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.2136 - tp: 76227.0000 - fp: 9157.0000 - tn: 442424.0000 - fn: 21683.0000 - accuracy: 0.9439 - precision: 0.8928 - recall: 0.7785 - auc: 0.9736 - val_loss: 0.2014 - val_tp: 18540.0000 - val_fp: 1132.0000 - val_tn: 111475.0000 - val_fn: 6226.0000 - val_accuracy: 0.9464 - val_precision: 0.9425 - val_recall: 0.7486 - val_auc: 0.9817\n",
            "Epoch 88/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2129 - tp: 76206.0000 - fp: 9145.0000 - tn: 442339.0000 - fn: 21686.0000 - accuracy: 0.9439 - precision: 0.8929 - recall: 0.7785 - auc: 0.9739\n",
            "Epoch 00088: val_auc did not improve from 0.98170\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2129 - tp: 76217.0000 - fp: 9149.0000 - tn: 442432.0000 - fn: 21693.0000 - accuracy: 0.9439 - precision: 0.8928 - recall: 0.7784 - auc: 0.9739 - val_loss: 0.1992 - val_tp: 19985.0000 - val_fp: 2137.0000 - val_tn: 110470.0000 - val_fn: 4781.0000 - val_accuracy: 0.9496 - val_precision: 0.9034 - val_recall: 0.8070 - val_auc: 0.9808\n",
            "Epoch 89/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2122 - tp: 76331.0000 - fp: 9031.0000 - tn: 442463.0000 - fn: 21551.0000 - accuracy: 0.9443 - precision: 0.8942 - recall: 0.7798 - auc: 0.9740\n",
            "Epoch 00089: val_auc did not improve from 0.98170\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2122 - tp: 76351.0000 - fp: 9032.0000 - tn: 442549.0000 - fn: 21559.0000 - accuracy: 0.9443 - precision: 0.8942 - recall: 0.7798 - auc: 0.9740 - val_loss: 0.2002 - val_tp: 19783.0000 - val_fp: 2068.0000 - val_tn: 110539.0000 - val_fn: 4983.0000 - val_accuracy: 0.9487 - val_precision: 0.9054 - val_recall: 0.7988 - val_auc: 0.9806\n",
            "Epoch 90/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2110 - tp: 76465.0000 - fp: 9040.0000 - tn: 442541.0000 - fn: 21445.0000 - accuracy: 0.9445 - precision: 0.8943 - recall: 0.7810 - auc: 0.9746\n",
            "Epoch 00090: val_auc did not improve from 0.98170\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2110 - tp: 76465.0000 - fp: 9040.0000 - tn: 442541.0000 - fn: 21445.0000 - accuracy: 0.9445 - precision: 0.8943 - recall: 0.7810 - auc: 0.9746 - val_loss: 0.2049 - val_tp: 19245.0000 - val_fp: 1818.0000 - val_tn: 110789.0000 - val_fn: 5521.0000 - val_accuracy: 0.9466 - val_precision: 0.9137 - val_recall: 0.7771 - val_auc: 0.9797\n",
            "Epoch 91/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2109 - tp: 76498.0000 - fp: 9030.0000 - tn: 442037.0000 - fn: 21299.0000 - accuracy: 0.9447 - precision: 0.8944 - recall: 0.7822 - auc: 0.9747\n",
            "Epoch 00091: val_auc did not improve from 0.98170\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2109 - tp: 76582.0000 - fp: 9041.0000 - tn: 442540.0000 - fn: 21328.0000 - accuracy: 0.9447 - precision: 0.8944 - recall: 0.7822 - auc: 0.9747 - val_loss: 0.2030 - val_tp: 18611.0000 - val_fp: 1143.0000 - val_tn: 111464.0000 - val_fn: 6155.0000 - val_accuracy: 0.9469 - val_precision: 0.9421 - val_recall: 0.7515 - val_auc: 0.9805\n",
            "Epoch 92/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2099 - tp: 76618.0000 - fp: 9050.0000 - tn: 441592.0000 - fn: 21092.0000 - accuracy: 0.9450 - precision: 0.8944 - recall: 0.7841 - auc: 0.9749\n",
            "Epoch 00092: val_auc did not improve from 0.98170\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2099 - tp: 76774.0000 - fp: 9065.0000 - tn: 442516.0000 - fn: 21136.0000 - accuracy: 0.9450 - precision: 0.8944 - recall: 0.7841 - auc: 0.9749 - val_loss: 0.1996 - val_tp: 20409.0000 - val_fp: 2495.0000 - val_tn: 110112.0000 - val_fn: 4357.0000 - val_accuracy: 0.9501 - val_precision: 0.8911 - val_recall: 0.8241 - val_auc: 0.9811\n",
            "Epoch 93/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2093 - tp: 76847.0000 - fp: 9028.0000 - tn: 442553.0000 - fn: 21063.0000 - accuracy: 0.9452 - precision: 0.8949 - recall: 0.7849 - auc: 0.9752\n",
            "Epoch 00093: val_auc did not improve from 0.98170\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2093 - tp: 76847.0000 - fp: 9028.0000 - tn: 442553.0000 - fn: 21063.0000 - accuracy: 0.9452 - precision: 0.8949 - recall: 0.7849 - auc: 0.9752 - val_loss: 0.1996 - val_tp: 20480.0000 - val_fp: 2564.0000 - val_tn: 110043.0000 - val_fn: 4286.0000 - val_accuracy: 0.9501 - val_precision: 0.8887 - val_recall: 0.8269 - val_auc: 0.9810\n",
            "Epoch 94/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2091 - tp: 76819.0000 - fp: 9097.0000 - tn: 442383.0000 - fn: 21077.0000 - accuracy: 0.9451 - precision: 0.8941 - recall: 0.7847 - auc: 0.9752\n",
            "Epoch 00094: val_auc improved from 0.98170 to 0.98228, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2092 - tp: 76830.0000 - fp: 9102.0000 - tn: 442479.0000 - fn: 21080.0000 - accuracy: 0.9451 - precision: 0.8941 - recall: 0.7847 - auc: 0.9752 - val_loss: 0.1944 - val_tp: 19635.0000 - val_fp: 1700.0000 - val_tn: 110907.0000 - val_fn: 5131.0000 - val_accuracy: 0.9503 - val_precision: 0.9203 - val_recall: 0.7928 - val_auc: 0.9823\n",
            "Epoch 95/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2084 - tp: 76931.0000 - fp: 8985.0000 - tn: 441882.0000 - fn: 20810.0000 - accuracy: 0.9457 - precision: 0.8954 - recall: 0.7871 - auc: 0.9754\n",
            "Epoch 00095: val_auc did not improve from 0.98228\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2084 - tp: 77068.0000 - fp: 8996.0000 - tn: 442585.0000 - fn: 20842.0000 - accuracy: 0.9457 - precision: 0.8955 - recall: 0.7871 - auc: 0.9754 - val_loss: 0.1958 - val_tp: 19943.0000 - val_fp: 2073.0000 - val_tn: 110534.0000 - val_fn: 4823.0000 - val_accuracy: 0.9498 - val_precision: 0.9058 - val_recall: 0.8053 - val_auc: 0.9813\n",
            "Epoch 96/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2078 - tp: 77151.0000 - fp: 9016.0000 - tn: 442476.0000 - fn: 20733.0000 - accuracy: 0.9458 - precision: 0.8954 - recall: 0.7882 - auc: 0.9756\n",
            "Epoch 00096: val_auc did not improve from 0.98228\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2078 - tp: 77172.0000 - fp: 9017.0000 - tn: 442564.0000 - fn: 20738.0000 - accuracy: 0.9458 - precision: 0.8954 - recall: 0.7882 - auc: 0.9756 - val_loss: 0.2054 - val_tp: 20496.0000 - val_fp: 2820.0000 - val_tn: 109787.0000 - val_fn: 4270.0000 - val_accuracy: 0.9484 - val_precision: 0.8791 - val_recall: 0.8276 - val_auc: 0.9800\n",
            "Epoch 97/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2073 - tp: 77145.0000 - fp: 9023.0000 - tn: 441832.0000 - fn: 20608.0000 - accuracy: 0.9460 - precision: 0.8953 - recall: 0.7892 - auc: 0.9759\n",
            "Epoch 00097: val_auc did not improve from 0.98228\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2073 - tp: 77268.0000 - fp: 9038.0000 - tn: 442543.0000 - fn: 20642.0000 - accuracy: 0.9460 - precision: 0.8953 - recall: 0.7892 - auc: 0.9759 - val_loss: 0.1980 - val_tp: 20113.0000 - val_fp: 2174.0000 - val_tn: 110433.0000 - val_fn: 4653.0000 - val_accuracy: 0.9503 - val_precision: 0.9025 - val_recall: 0.8121 - val_auc: 0.9805\n",
            "Epoch 98/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2064 - tp: 77258.0000 - fp: 8921.0000 - tn: 442660.0000 - fn: 20652.0000 - accuracy: 0.9462 - precision: 0.8965 - recall: 0.7891 - auc: 0.9761\n",
            "Epoch 00098: val_auc did not improve from 0.98228\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2064 - tp: 77258.0000 - fp: 8921.0000 - tn: 442660.0000 - fn: 20652.0000 - accuracy: 0.9462 - precision: 0.8965 - recall: 0.7891 - auc: 0.9761 - val_loss: 0.1994 - val_tp: 20598.0000 - val_fp: 2717.0000 - val_tn: 109890.0000 - val_fn: 4168.0000 - val_accuracy: 0.9499 - val_precision: 0.8835 - val_recall: 0.8317 - val_auc: 0.9809\n",
            "Epoch 99/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2059 - tp: 77385.0000 - fp: 8961.0000 - tn: 441887.0000 - fn: 20375.0000 - accuracy: 0.9465 - precision: 0.8962 - recall: 0.7916 - auc: 0.9763\n",
            "Epoch 00099: val_auc improved from 0.98228 to 0.98302, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2058 - tp: 77505.0000 - fp: 8974.0000 - tn: 442607.0000 - fn: 20405.0000 - accuracy: 0.9465 - precision: 0.8962 - recall: 0.7916 - auc: 0.9763 - val_loss: 0.1922 - val_tp: 19664.0000 - val_fp: 1640.0000 - val_tn: 110967.0000 - val_fn: 5102.0000 - val_accuracy: 0.9509 - val_precision: 0.9230 - val_recall: 0.7940 - val_auc: 0.9830\n",
            "Epoch 100/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2053 - tp: 77416.0000 - fp: 8912.0000 - tn: 441929.0000 - fn: 20351.0000 - accuracy: 0.9467 - precision: 0.8968 - recall: 0.7918 - auc: 0.9765\n",
            "Epoch 00100: val_auc did not improve from 0.98302\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2054 - tp: 77535.0000 - fp: 8937.0000 - tn: 442644.0000 - fn: 20375.0000 - accuracy: 0.9467 - precision: 0.8966 - recall: 0.7919 - auc: 0.9765 - val_loss: 0.1944 - val_tp: 20373.0000 - val_fp: 2470.0000 - val_tn: 110137.0000 - val_fn: 4393.0000 - val_accuracy: 0.9500 - val_precision: 0.8919 - val_recall: 0.8226 - val_auc: 0.9823\n",
            "Epoch 101/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.2050 - tp: 77475.0000 - fp: 9043.0000 - tn: 441608.0000 - fn: 20226.0000 - accuracy: 0.9466 - precision: 0.8955 - recall: 0.7930 - auc: 0.9768\n",
            "Epoch 00101: val_auc did not improve from 0.98302\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.2050 - tp: 77647.0000 - fp: 9060.0000 - tn: 442521.0000 - fn: 20263.0000 - accuracy: 0.9466 - precision: 0.8955 - recall: 0.7930 - auc: 0.9767 - val_loss: 0.1952 - val_tp: 20187.0000 - val_fp: 2194.0000 - val_tn: 110413.0000 - val_fn: 4579.0000 - val_accuracy: 0.9507 - val_precision: 0.9020 - val_recall: 0.8151 - val_auc: 0.9814\n",
            "Epoch 102/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2036 - tp: 77783.0000 - fp: 8915.0000 - tn: 442571.0000 - fn: 20107.0000 - accuracy: 0.9472 - precision: 0.8972 - recall: 0.7946 - auc: 0.9771\n",
            "Epoch 00102: val_auc improved from 0.98302 to 0.98345, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2036 - tp: 77800.0000 - fp: 8918.0000 - tn: 442663.0000 - fn: 20110.0000 - accuracy: 0.9472 - precision: 0.8972 - recall: 0.7946 - auc: 0.9771 - val_loss: 0.1895 - val_tp: 20754.0000 - val_fp: 2407.0000 - val_tn: 110200.0000 - val_fn: 4012.0000 - val_accuracy: 0.9533 - val_precision: 0.8961 - val_recall: 0.8380 - val_auc: 0.9834\n",
            "Epoch 103/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2033 - tp: 77851.0000 - fp: 8847.0000 - tn: 442004.0000 - fn: 19906.0000 - accuracy: 0.9476 - precision: 0.8980 - recall: 0.7964 - auc: 0.9772\n",
            "Epoch 00103: val_auc did not improve from 0.98345\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2033 - tp: 77955.0000 - fp: 8858.0000 - tn: 442723.0000 - fn: 19955.0000 - accuracy: 0.9476 - precision: 0.8980 - recall: 0.7962 - auc: 0.9772 - val_loss: 0.1952 - val_tp: 20463.0000 - val_fp: 2306.0000 - val_tn: 110301.0000 - val_fn: 4303.0000 - val_accuracy: 0.9519 - val_precision: 0.8987 - val_recall: 0.8263 - val_auc: 0.9825\n",
            "Epoch 104/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.2029 - tp: 78010.0000 - fp: 8885.0000 - tn: 442601.0000 - fn: 19880.0000 - accuracy: 0.9476 - precision: 0.8978 - recall: 0.7969 - auc: 0.9774\n",
            "Epoch 00104: val_auc did not improve from 0.98345\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.2029 - tp: 78026.0000 - fp: 8886.0000 - tn: 442695.0000 - fn: 19884.0000 - accuracy: 0.9476 - precision: 0.8978 - recall: 0.7969 - auc: 0.9774 - val_loss: 0.1922 - val_tp: 20110.0000 - val_fp: 1938.0000 - val_tn: 110669.0000 - val_fn: 4656.0000 - val_accuracy: 0.9520 - val_precision: 0.9121 - val_recall: 0.8120 - val_auc: 0.9830\n",
            "Epoch 105/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2019 - tp: 78022.0000 - fp: 8993.0000 - tn: 442068.0000 - fn: 19781.0000 - accuracy: 0.9476 - precision: 0.8967 - recall: 0.7977 - auc: 0.9778\n",
            "Epoch 00105: val_auc did not improve from 0.98345\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.2019 - tp: 78110.0000 - fp: 9009.0000 - tn: 442572.0000 - fn: 19800.0000 - accuracy: 0.9476 - precision: 0.8966 - recall: 0.7978 - auc: 0.9778 - val_loss: 0.1984 - val_tp: 21010.0000 - val_fp: 3211.0000 - val_tn: 109396.0000 - val_fn: 3756.0000 - val_accuracy: 0.9493 - val_precision: 0.8674 - val_recall: 0.8483 - val_auc: 0.9819\n",
            "Epoch 106/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.2023 - tp: 77860.0000 - fp: 8784.0000 - tn: 441669.0000 - fn: 19783.0000 - accuracy: 0.9479 - precision: 0.8986 - recall: 0.7974 - auc: 0.9775\n",
            "Epoch 00106: val_auc did not improve from 0.98345\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2023 - tp: 78062.0000 - fp: 8791.0000 - tn: 442790.0000 - fn: 19848.0000 - accuracy: 0.9479 - precision: 0.8988 - recall: 0.7973 - auc: 0.9775 - val_loss: 0.1927 - val_tp: 20324.0000 - val_fp: 2005.0000 - val_tn: 110602.0000 - val_fn: 4442.0000 - val_accuracy: 0.9531 - val_precision: 0.9102 - val_recall: 0.8206 - val_auc: 0.9833\n",
            "Epoch 107/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.2010 - tp: 78287.0000 - fp: 8986.0000 - tn: 442281.0000 - fn: 19566.0000 - accuracy: 0.9480 - precision: 0.8970 - recall: 0.8000 - auc: 0.9781\n",
            "Epoch 00107: val_auc did not improve from 0.98345\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.2010 - tp: 78334.0000 - fp: 8996.0000 - tn: 442585.0000 - fn: 19576.0000 - accuracy: 0.9480 - precision: 0.8970 - recall: 0.8001 - auc: 0.9781 - val_loss: 0.1971 - val_tp: 19123.0000 - val_fp: 1518.0000 - val_tn: 111089.0000 - val_fn: 5643.0000 - val_accuracy: 0.9479 - val_precision: 0.9265 - val_recall: 0.7721 - val_auc: 0.9819\n",
            "Epoch 108/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.2008 - tp: 78109.0000 - fp: 8672.0000 - tn: 442178.0000 - fn: 19649.0000 - accuracy: 0.9484 - precision: 0.9001 - recall: 0.7990 - auc: 0.9781\n",
            "Epoch 00108: val_auc did not improve from 0.98345\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.2008 - tp: 78232.0000 - fp: 8684.0000 - tn: 442897.0000 - fn: 19678.0000 - accuracy: 0.9484 - precision: 0.9001 - recall: 0.7990 - auc: 0.9781 - val_loss: 0.1953 - val_tp: 21044.0000 - val_fp: 2866.0000 - val_tn: 109741.0000 - val_fn: 3722.0000 - val_accuracy: 0.9520 - val_precision: 0.8801 - val_recall: 0.8497 - val_auc: 0.9832\n",
            "Epoch 109/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.2004 - tp: 78452.0000 - fp: 8892.0000 - tn: 442689.0000 - fn: 19458.0000 - accuracy: 0.9484 - precision: 0.8982 - recall: 0.8013 - auc: 0.9782\n",
            "Epoch 00109: val_auc improved from 0.98345 to 0.98390, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.2004 - tp: 78452.0000 - fp: 8892.0000 - tn: 442689.0000 - fn: 19458.0000 - accuracy: 0.9484 - precision: 0.8982 - recall: 0.8013 - auc: 0.9782 - val_loss: 0.1881 - val_tp: 20414.0000 - val_fp: 2020.0000 - val_tn: 110587.0000 - val_fn: 4352.0000 - val_accuracy: 0.9536 - val_precision: 0.9100 - val_recall: 0.8243 - val_auc: 0.9839\n",
            "Epoch 110/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.2000 - tp: 78370.0000 - fp: 8873.0000 - tn: 442206.0000 - fn: 19415.0000 - accuracy: 0.9485 - precision: 0.8983 - recall: 0.8015 - auc: 0.9784\n",
            "Epoch 00110: val_auc improved from 0.98390 to 0.98412, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.2000 - tp: 78463.0000 - fp: 8886.0000 - tn: 442695.0000 - fn: 19447.0000 - accuracy: 0.9484 - precision: 0.8983 - recall: 0.8014 - auc: 0.9784 - val_loss: 0.1897 - val_tp: 20796.0000 - val_fp: 2314.0000 - val_tn: 110293.0000 - val_fn: 3970.0000 - val_accuracy: 0.9543 - val_precision: 0.8999 - val_recall: 0.8397 - val_auc: 0.9841\n",
            "Epoch 111/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1990 - tp: 78458.0000 - fp: 8705.0000 - tn: 442777.0000 - fn: 19436.0000 - accuracy: 0.9488 - precision: 0.9001 - recall: 0.8015 - auc: 0.9786\n",
            "Epoch 00111: val_auc improved from 0.98412 to 0.98439, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1990 - tp: 78472.0000 - fp: 8705.0000 - tn: 442876.0000 - fn: 19438.0000 - accuracy: 0.9488 - precision: 0.9001 - recall: 0.8015 - auc: 0.9786 - val_loss: 0.1879 - val_tp: 20332.0000 - val_fp: 1900.0000 - val_tn: 110707.0000 - val_fn: 4434.0000 - val_accuracy: 0.9539 - val_precision: 0.9145 - val_recall: 0.8210 - val_auc: 0.9844\n",
            "Epoch 112/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1986 - tp: 78506.0000 - fp: 8717.0000 - tn: 442574.0000 - fn: 19323.0000 - accuracy: 0.9489 - precision: 0.9001 - recall: 0.8025 - auc: 0.9788\n",
            "Epoch 00112: val_auc did not improve from 0.98439\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1986 - tp: 78571.0000 - fp: 8726.0000 - tn: 442855.0000 - fn: 19339.0000 - accuracy: 0.9489 - precision: 0.9000 - recall: 0.8025 - auc: 0.9788 - val_loss: 0.1900 - val_tp: 20068.0000 - val_fp: 1784.0000 - val_tn: 110823.0000 - val_fn: 4698.0000 - val_accuracy: 0.9528 - val_precision: 0.9184 - val_recall: 0.8103 - val_auc: 0.9835\n",
            "Epoch 113/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1980 - tp: 78550.0000 - fp: 8765.0000 - tn: 441865.0000 - fn: 19172.0000 - accuracy: 0.9491 - precision: 0.8996 - recall: 0.8038 - auc: 0.9790\n",
            "Epoch 00113: val_auc improved from 0.98439 to 0.98452, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1980 - tp: 78693.0000 - fp: 8784.0000 - tn: 442797.0000 - fn: 19217.0000 - accuracy: 0.9490 - precision: 0.8996 - recall: 0.8037 - auc: 0.9790 - val_loss: 0.1888 - val_tp: 20355.0000 - val_fp: 1935.0000 - val_tn: 110672.0000 - val_fn: 4411.0000 - val_accuracy: 0.9538 - val_precision: 0.9132 - val_recall: 0.8219 - val_auc: 0.9845\n",
            "Epoch 114/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1974 - tp: 78680.0000 - fp: 8714.0000 - tn: 442867.0000 - fn: 19230.0000 - accuracy: 0.9491 - precision: 0.9003 - recall: 0.8036 - auc: 0.9792\n",
            "Epoch 00114: val_auc did not improve from 0.98452\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1974 - tp: 78680.0000 - fp: 8714.0000 - tn: 442867.0000 - fn: 19230.0000 - accuracy: 0.9491 - precision: 0.9003 - recall: 0.8036 - auc: 0.9792 - val_loss: 0.1883 - val_tp: 20279.0000 - val_fp: 1960.0000 - val_tn: 110647.0000 - val_fn: 4487.0000 - val_accuracy: 0.9531 - val_precision: 0.9119 - val_recall: 0.8188 - val_auc: 0.9838\n",
            "Epoch 115/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1971 - tp: 78834.0000 - fp: 8774.0000 - tn: 442310.0000 - fn: 18946.0000 - accuracy: 0.9495 - precision: 0.8998 - recall: 0.8062 - auc: 0.9793\n",
            "Epoch 00115: val_auc did not improve from 0.98452\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1971 - tp: 78938.0000 - fp: 8780.0000 - tn: 442801.0000 - fn: 18972.0000 - accuracy: 0.9495 - precision: 0.8999 - recall: 0.8062 - auc: 0.9793 - val_loss: 0.1906 - val_tp: 20719.0000 - val_fp: 2503.0000 - val_tn: 110104.0000 - val_fn: 4047.0000 - val_accuracy: 0.9523 - val_precision: 0.8922 - val_recall: 0.8366 - val_auc: 0.9842\n",
            "Epoch 116/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1970 - tp: 78700.0000 - fp: 8675.0000 - tn: 441747.0000 - fn: 18974.0000 - accuracy: 0.9496 - precision: 0.9007 - recall: 0.8057 - auc: 0.9793\n",
            "Epoch 00116: val_auc did not improve from 0.98452\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1970 - tp: 78900.0000 - fp: 8691.0000 - tn: 442890.0000 - fn: 19010.0000 - accuracy: 0.9496 - precision: 0.9008 - recall: 0.8058 - auc: 0.9793 - val_loss: 0.1865 - val_tp: 20355.0000 - val_fp: 1930.0000 - val_tn: 110677.0000 - val_fn: 4411.0000 - val_accuracy: 0.9538 - val_precision: 0.9134 - val_recall: 0.8219 - val_auc: 0.9842\n",
            "Epoch 117/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1965 - tp: 78800.0000 - fp: 8778.0000 - tn: 442077.0000 - fn: 18953.0000 - accuracy: 0.9495 - precision: 0.8998 - recall: 0.8061 - auc: 0.9795\n",
            "Epoch 00117: val_auc improved from 0.98452 to 0.98466, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1965 - tp: 78927.0000 - fp: 8795.0000 - tn: 442786.0000 - fn: 18983.0000 - accuracy: 0.9494 - precision: 0.8997 - recall: 0.8061 - auc: 0.9795 - val_loss: 0.1853 - val_tp: 20260.0000 - val_fp: 1824.0000 - val_tn: 110783.0000 - val_fn: 4506.0000 - val_accuracy: 0.9539 - val_precision: 0.9174 - val_recall: 0.8181 - val_auc: 0.9847\n",
            "Epoch 118/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1961 - tp: 79000.0000 - fp: 8743.0000 - tn: 442740.0000 - fn: 18893.0000 - accuracy: 0.9497 - precision: 0.9004 - recall: 0.8070 - auc: 0.9796\n",
            "Epoch 00118: val_auc did not improve from 0.98466\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1960 - tp: 79014.0000 - fp: 8746.0000 - tn: 442835.0000 - fn: 18896.0000 - accuracy: 0.9497 - precision: 0.9003 - recall: 0.8070 - auc: 0.9796 - val_loss: 0.1881 - val_tp: 21090.0000 - val_fp: 2597.0000 - val_tn: 110010.0000 - val_fn: 3676.0000 - val_accuracy: 0.9543 - val_precision: 0.8904 - val_recall: 0.8516 - val_auc: 0.9846\n",
            "Epoch 119/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1950 - tp: 79032.0000 - fp: 8675.0000 - tn: 441739.0000 - fn: 18650.0000 - accuracy: 0.9501 - precision: 0.9011 - recall: 0.8091 - auc: 0.9801\n",
            "Epoch 00119: val_auc did not improve from 0.98466\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1950 - tp: 79214.0000 - fp: 8697.0000 - tn: 442884.0000 - fn: 18696.0000 - accuracy: 0.9501 - precision: 0.9011 - recall: 0.8090 - auc: 0.9801 - val_loss: 0.1909 - val_tp: 20845.0000 - val_fp: 2593.0000 - val_tn: 110014.0000 - val_fn: 3921.0000 - val_accuracy: 0.9526 - val_precision: 0.8894 - val_recall: 0.8417 - val_auc: 0.9839\n",
            "Epoch 120/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1947 - tp: 79221.0000 - fp: 8662.0000 - tn: 442620.0000 - fn: 18617.0000 - accuracy: 0.9503 - precision: 0.9014 - recall: 0.8097 - auc: 0.9801\n",
            "Epoch 00120: val_auc did not improve from 0.98466\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1947 - tp: 79276.0000 - fp: 8667.0000 - tn: 442914.0000 - fn: 18634.0000 - accuracy: 0.9503 - precision: 0.9014 - recall: 0.8097 - auc: 0.9801 - val_loss: 0.1912 - val_tp: 20809.0000 - val_fp: 2457.0000 - val_tn: 110150.0000 - val_fn: 3957.0000 - val_accuracy: 0.9533 - val_precision: 0.8944 - val_recall: 0.8402 - val_auc: 0.9836\n",
            "Epoch 121/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1942 - tp: 79511.0000 - fp: 8601.0000 - tn: 442884.0000 - fn: 18380.0000 - accuracy: 0.9509 - precision: 0.9024 - recall: 0.8122 - auc: 0.9802\n",
            "Epoch 00121: val_auc improved from 0.98466 to 0.98503, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1942 - tp: 79528.0000 - fp: 8601.0000 - tn: 442980.0000 - fn: 18382.0000 - accuracy: 0.9509 - precision: 0.9024 - recall: 0.8123 - auc: 0.9802 - val_loss: 0.1831 - val_tp: 20396.0000 - val_fp: 1814.0000 - val_tn: 110793.0000 - val_fn: 4370.0000 - val_accuracy: 0.9550 - val_precision: 0.9183 - val_recall: 0.8235 - val_auc: 0.9850\n",
            "Epoch 122/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1939 - tp: 79354.0000 - fp: 8658.0000 - tn: 442923.0000 - fn: 18556.0000 - accuracy: 0.9505 - precision: 0.9016 - recall: 0.8105 - auc: 0.9803\n",
            "Epoch 00122: val_auc did not improve from 0.98503\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1939 - tp: 79354.0000 - fp: 8658.0000 - tn: 442923.0000 - fn: 18556.0000 - accuracy: 0.9505 - precision: 0.9016 - recall: 0.8105 - auc: 0.9803 - val_loss: 0.1909 - val_tp: 21303.0000 - val_fp: 2847.0000 - val_tn: 109760.0000 - val_fn: 3463.0000 - val_accuracy: 0.9541 - val_precision: 0.8821 - val_recall: 0.8602 - val_auc: 0.9847\n",
            "Epoch 123/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1938 - tp: 79272.0000 - fp: 8625.0000 - tn: 442453.0000 - fn: 18514.0000 - accuracy: 0.9506 - precision: 0.9019 - recall: 0.8107 - auc: 0.9804\n",
            "Epoch 00123: val_auc did not improve from 0.98503\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1938 - tp: 79366.0000 - fp: 8633.0000 - tn: 442948.0000 - fn: 18544.0000 - accuracy: 0.9505 - precision: 0.9019 - recall: 0.8106 - auc: 0.9804 - val_loss: 0.1857 - val_tp: 20875.0000 - val_fp: 2376.0000 - val_tn: 110231.0000 - val_fn: 3891.0000 - val_accuracy: 0.9544 - val_precision: 0.8978 - val_recall: 0.8429 - val_auc: 0.9846\n",
            "Epoch 124/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1933 - tp: 79568.0000 - fp: 8581.0000 - tn: 442905.0000 - fn: 18322.0000 - accuracy: 0.9510 - precision: 0.9027 - recall: 0.8128 - auc: 0.9804\n",
            "Epoch 00124: val_auc improved from 0.98503 to 0.98513, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1932 - tp: 79585.0000 - fp: 8581.0000 - tn: 443000.0000 - fn: 18325.0000 - accuracy: 0.9510 - precision: 0.9027 - recall: 0.8128 - auc: 0.9804 - val_loss: 0.1857 - val_tp: 20656.0000 - val_fp: 2184.0000 - val_tn: 110423.0000 - val_fn: 4110.0000 - val_accuracy: 0.9542 - val_precision: 0.9044 - val_recall: 0.8340 - val_auc: 0.9851\n",
            "Epoch 125/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1928 - tp: 79399.0000 - fp: 8554.0000 - tn: 441874.0000 - fn: 18269.0000 - accuracy: 0.9511 - precision: 0.9027 - recall: 0.8129 - auc: 0.9806\n",
            "Epoch 00125: val_auc improved from 0.98513 to 0.98551, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1927 - tp: 79604.0000 - fp: 8586.0000 - tn: 442995.0000 - fn: 18306.0000 - accuracy: 0.9511 - precision: 0.9026 - recall: 0.8130 - auc: 0.9806 - val_loss: 0.1825 - val_tp: 20895.0000 - val_fp: 2133.0000 - val_tn: 110474.0000 - val_fn: 3871.0000 - val_accuracy: 0.9563 - val_precision: 0.9074 - val_recall: 0.8437 - val_auc: 0.9855\n",
            "Epoch 126/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1917 - tp: 79576.0000 - fp: 8534.0000 - tn: 441890.0000 - fn: 18096.0000 - accuracy: 0.9514 - precision: 0.9031 - recall: 0.8147 - auc: 0.9811\n",
            "Epoch 00126: val_auc did not improve from 0.98551\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1917 - tp: 79777.0000 - fp: 8558.0000 - tn: 443023.0000 - fn: 18133.0000 - accuracy: 0.9514 - precision: 0.9031 - recall: 0.8148 - auc: 0.9811 - val_loss: 0.1934 - val_tp: 21205.0000 - val_fp: 3006.0000 - val_tn: 109601.0000 - val_fn: 3561.0000 - val_accuracy: 0.9522 - val_precision: 0.8758 - val_recall: 0.8562 - val_auc: 0.9836\n",
            "Epoch 127/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1921 - tp: 79606.0000 - fp: 8507.0000 - tn: 442543.0000 - fn: 18208.0000 - accuracy: 0.9513 - precision: 0.9035 - recall: 0.8139 - auc: 0.9809\n",
            "Epoch 00127: val_auc did not improve from 0.98551\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1921 - tp: 79691.0000 - fp: 8526.0000 - tn: 443055.0000 - fn: 18219.0000 - accuracy: 0.9513 - precision: 0.9034 - recall: 0.8139 - auc: 0.9808 - val_loss: 0.1844 - val_tp: 20831.0000 - val_fp: 2185.0000 - val_tn: 110422.0000 - val_fn: 3935.0000 - val_accuracy: 0.9554 - val_precision: 0.9051 - val_recall: 0.8411 - val_auc: 0.9852\n",
            "Epoch 128/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1914 - tp: 79675.0000 - fp: 8560.0000 - tn: 442286.0000 - fn: 18087.0000 - accuracy: 0.9514 - precision: 0.9030 - recall: 0.8150 - auc: 0.9812\n",
            "Epoch 00128: val_auc improved from 0.98551 to 0.98593, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1913 - tp: 79794.0000 - fp: 8573.0000 - tn: 443008.0000 - fn: 18116.0000 - accuracy: 0.9514 - precision: 0.9030 - recall: 0.8150 - auc: 0.9812 - val_loss: 0.1809 - val_tp: 20414.0000 - val_fp: 1695.0000 - val_tn: 110912.0000 - val_fn: 4352.0000 - val_accuracy: 0.9560 - val_precision: 0.9233 - val_recall: 0.8243 - val_auc: 0.9859\n",
            "Epoch 129/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1915 - tp: 79771.0000 - fp: 8571.0000 - tn: 443010.0000 - fn: 18139.0000 - accuracy: 0.9514 - precision: 0.9030 - recall: 0.8147 - auc: 0.9809\n",
            "Epoch 00129: val_auc did not improve from 0.98593\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1915 - tp: 79771.0000 - fp: 8571.0000 - tn: 443010.0000 - fn: 18139.0000 - accuracy: 0.9514 - precision: 0.9030 - recall: 0.8147 - auc: 0.9809 - val_loss: 0.1849 - val_tp: 20378.0000 - val_fp: 1859.0000 - val_tn: 110748.0000 - val_fn: 4388.0000 - val_accuracy: 0.9545 - val_precision: 0.9164 - val_recall: 0.8228 - val_auc: 0.9851\n",
            "Epoch 130/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1917 - tp: 79769.0000 - fp: 8659.0000 - tn: 442412.0000 - fn: 18024.0000 - accuracy: 0.9514 - precision: 0.9021 - recall: 0.8157 - auc: 0.9810\n",
            "Epoch 00130: val_auc did not improve from 0.98593\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1917 - tp: 79861.0000 - fp: 8671.0000 - tn: 442910.0000 - fn: 18049.0000 - accuracy: 0.9514 - precision: 0.9021 - recall: 0.8157 - auc: 0.9810 - val_loss: 0.1894 - val_tp: 20758.0000 - val_fp: 2399.0000 - val_tn: 110208.0000 - val_fn: 4008.0000 - val_accuracy: 0.9534 - val_precision: 0.8964 - val_recall: 0.8382 - val_auc: 0.9839\n",
            "Epoch 131/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1902 - tp: 79980.0000 - fp: 8608.0000 - tn: 442973.0000 - fn: 17930.0000 - accuracy: 0.9517 - precision: 0.9028 - recall: 0.8169 - auc: 0.9815\n",
            "Epoch 00131: val_auc did not improve from 0.98593\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1902 - tp: 79980.0000 - fp: 8608.0000 - tn: 442973.0000 - fn: 17930.0000 - accuracy: 0.9517 - precision: 0.9028 - recall: 0.8169 - auc: 0.9815 - val_loss: 0.1852 - val_tp: 20777.0000 - val_fp: 2228.0000 - val_tn: 110379.0000 - val_fn: 3989.0000 - val_accuracy: 0.9547 - val_precision: 0.9032 - val_recall: 0.8389 - val_auc: 0.9849\n",
            "Epoch 132/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1897 - tp: 80089.0000 - fp: 8519.0000 - tn: 443062.0000 - fn: 17821.0000 - accuracy: 0.9521 - precision: 0.9039 - recall: 0.8180 - auc: 0.9816\n",
            "Epoch 00132: val_auc did not improve from 0.98593\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1897 - tp: 80089.0000 - fp: 8519.0000 - tn: 443062.0000 - fn: 17821.0000 - accuracy: 0.9521 - precision: 0.9039 - recall: 0.8180 - auc: 0.9816 - val_loss: 0.1875 - val_tp: 21257.0000 - val_fp: 2781.0000 - val_tn: 109826.0000 - val_fn: 3509.0000 - val_accuracy: 0.9542 - val_precision: 0.8843 - val_recall: 0.8583 - val_auc: 0.9851\n",
            "Epoch 133/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1896 - tp: 80126.0000 - fp: 8440.0000 - tn: 442846.0000 - fn: 17708.0000 - accuracy: 0.9524 - precision: 0.9047 - recall: 0.8190 - auc: 0.9815\n",
            "Epoch 00133: val_auc improved from 0.98593 to 0.98605, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1897 - tp: 80186.0000 - fp: 8448.0000 - tn: 443133.0000 - fn: 17724.0000 - accuracy: 0.9524 - precision: 0.9047 - recall: 0.8190 - auc: 0.9815 - val_loss: 0.1858 - val_tp: 21755.0000 - val_fp: 3128.0000 - val_tn: 109479.0000 - val_fn: 3011.0000 - val_accuracy: 0.9553 - val_precision: 0.8743 - val_recall: 0.8784 - val_auc: 0.9861\n",
            "Epoch 134/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1893 - tp: 80125.0000 - fp: 8515.0000 - tn: 442748.0000 - fn: 17732.0000 - accuracy: 0.9522 - precision: 0.9039 - recall: 0.8188 - auc: 0.9818\n",
            "Epoch 00134: val_auc improved from 0.98605 to 0.98654, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1893 - tp: 80174.0000 - fp: 8526.0000 - tn: 443055.0000 - fn: 17736.0000 - accuracy: 0.9522 - precision: 0.9039 - recall: 0.8189 - auc: 0.9818 - val_loss: 0.1782 - val_tp: 21026.0000 - val_fp: 2169.0000 - val_tn: 110438.0000 - val_fn: 3740.0000 - val_accuracy: 0.9570 - val_precision: 0.9065 - val_recall: 0.8490 - val_auc: 0.9865\n",
            "Epoch 135/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1891 - tp: 80161.0000 - fp: 8425.0000 - tn: 442641.0000 - fn: 17637.0000 - accuracy: 0.9525 - precision: 0.9049 - recall: 0.8197 - auc: 0.9817\n",
            "Epoch 00135: val_auc did not improve from 0.98654\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1891 - tp: 80253.0000 - fp: 8432.0000 - tn: 443149.0000 - fn: 17657.0000 - accuracy: 0.9525 - precision: 0.9049 - recall: 0.8197 - auc: 0.9817 - val_loss: 0.1832 - val_tp: 21091.0000 - val_fp: 2418.0000 - val_tn: 110189.0000 - val_fn: 3675.0000 - val_accuracy: 0.9556 - val_precision: 0.8971 - val_recall: 0.8516 - val_auc: 0.9857\n",
            "Epoch 136/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1885 - tp: 80191.0000 - fp: 8503.0000 - tn: 442975.0000 - fn: 17707.0000 - accuracy: 0.9523 - precision: 0.9041 - recall: 0.8191 - auc: 0.9819\n",
            "Epoch 00136: val_auc did not improve from 0.98654\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1885 - tp: 80202.0000 - fp: 8505.0000 - tn: 443076.0000 - fn: 17708.0000 - accuracy: 0.9523 - precision: 0.9041 - recall: 0.8191 - auc: 0.9819 - val_loss: 0.1844 - val_tp: 20624.0000 - val_fp: 2051.0000 - val_tn: 110556.0000 - val_fn: 4142.0000 - val_accuracy: 0.9549 - val_precision: 0.9095 - val_recall: 0.8328 - val_auc: 0.9853\n",
            "Epoch 137/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1884 - tp: 80377.0000 - fp: 8641.0000 - tn: 442843.0000 - fn: 17515.0000 - accuracy: 0.9524 - precision: 0.9029 - recall: 0.8211 - auc: 0.9821\n",
            "Epoch 00137: val_auc improved from 0.98654 to 0.98697, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1884 - tp: 80391.0000 - fp: 8641.0000 - tn: 442940.0000 - fn: 17519.0000 - accuracy: 0.9524 - precision: 0.9029 - recall: 0.8211 - auc: 0.9821 - val_loss: 0.1778 - val_tp: 20799.0000 - val_fp: 1897.0000 - val_tn: 110710.0000 - val_fn: 3967.0000 - val_accuracy: 0.9573 - val_precision: 0.9164 - val_recall: 0.8398 - val_auc: 0.9870\n",
            "Epoch 138/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1873 - tp: 80479.0000 - fp: 8514.0000 - tn: 442770.0000 - fn: 17357.0000 - accuracy: 0.9529 - precision: 0.9043 - recall: 0.8226 - auc: 0.9824\n",
            "Epoch 00138: val_auc did not improve from 0.98697\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1873 - tp: 80542.0000 - fp: 8521.0000 - tn: 443060.0000 - fn: 17368.0000 - accuracy: 0.9529 - precision: 0.9043 - recall: 0.8226 - auc: 0.9824 - val_loss: 0.1930 - val_tp: 21382.0000 - val_fp: 3228.0000 - val_tn: 109379.0000 - val_fn: 3384.0000 - val_accuracy: 0.9519 - val_precision: 0.8688 - val_recall: 0.8634 - val_auc: 0.9845\n",
            "Epoch 139/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1876 - tp: 80409.0000 - fp: 8518.0000 - tn: 442964.0000 - fn: 17485.0000 - accuracy: 0.9527 - precision: 0.9042 - recall: 0.8214 - auc: 0.9823\n",
            "Epoch 00139: val_auc did not improve from 0.98697\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1876 - tp: 80424.0000 - fp: 8522.0000 - tn: 443059.0000 - fn: 17486.0000 - accuracy: 0.9527 - precision: 0.9042 - recall: 0.8214 - auc: 0.9823 - val_loss: 0.1850 - val_tp: 21101.0000 - val_fp: 2487.0000 - val_tn: 110120.0000 - val_fn: 3665.0000 - val_accuracy: 0.9552 - val_precision: 0.8946 - val_recall: 0.8520 - val_auc: 0.9854\n",
            "Epoch 140/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1874 - tp: 80361.0000 - fp: 8501.0000 - tn: 441947.0000 - fn: 17287.0000 - accuracy: 0.9529 - precision: 0.9043 - recall: 0.8230 - auc: 0.9823\n",
            "Epoch 00140: val_auc did not improve from 0.98697\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1873 - tp: 80577.0000 - fp: 8514.0000 - tn: 443067.0000 - fn: 17333.0000 - accuracy: 0.9530 - precision: 0.9044 - recall: 0.8230 - auc: 0.9823 - val_loss: 0.1791 - val_tp: 21089.0000 - val_fp: 2266.0000 - val_tn: 110341.0000 - val_fn: 3677.0000 - val_accuracy: 0.9567 - val_precision: 0.9030 - val_recall: 0.8515 - val_auc: 0.9867\n",
            "Epoch 141/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1868 - tp: 80371.0000 - fp: 8459.0000 - tn: 441967.0000 - fn: 17299.0000 - accuracy: 0.9530 - precision: 0.9048 - recall: 0.8229 - auc: 0.9824\n",
            "Epoch 00141: val_auc did not improve from 0.98697\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1869 - tp: 80567.0000 - fp: 8487.0000 - tn: 443094.0000 - fn: 17343.0000 - accuracy: 0.9530 - precision: 0.9047 - recall: 0.8229 - auc: 0.9824 - val_loss: 0.1902 - val_tp: 21529.0000 - val_fp: 3166.0000 - val_tn: 109441.0000 - val_fn: 3237.0000 - val_accuracy: 0.9534 - val_precision: 0.8718 - val_recall: 0.8693 - val_auc: 0.9850\n",
            "Epoch 142/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1866 - tp: 80596.0000 - fp: 8351.0000 - tn: 443130.0000 - fn: 17299.0000 - accuracy: 0.9533 - precision: 0.9061 - recall: 0.8233 - auc: 0.9825\n",
            "Epoch 00142: val_auc did not improve from 0.98697\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1865 - tp: 80610.0000 - fp: 8353.0000 - tn: 443228.0000 - fn: 17300.0000 - accuracy: 0.9533 - precision: 0.9061 - recall: 0.8233 - auc: 0.9825 - val_loss: 0.1837 - val_tp: 20873.0000 - val_fp: 2277.0000 - val_tn: 110330.0000 - val_fn: 3893.0000 - val_accuracy: 0.9551 - val_precision: 0.9016 - val_recall: 0.8428 - val_auc: 0.9860\n",
            "Epoch 143/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1862 - tp: 80542.0000 - fp: 8403.0000 - tn: 442248.0000 - fn: 17159.0000 - accuracy: 0.9534 - precision: 0.9055 - recall: 0.8244 - auc: 0.9826\n",
            "Epoch 00143: val_auc improved from 0.98697 to 0.98734, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1862 - tp: 80710.0000 - fp: 8421.0000 - tn: 443160.0000 - fn: 17200.0000 - accuracy: 0.9534 - precision: 0.9055 - recall: 0.8243 - auc: 0.9826 - val_loss: 0.1771 - val_tp: 20895.0000 - val_fp: 2048.0000 - val_tn: 110559.0000 - val_fn: 3871.0000 - val_accuracy: 0.9569 - val_precision: 0.9107 - val_recall: 0.8437 - val_auc: 0.9873\n",
            "Epoch 144/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1858 - tp: 80637.0000 - fp: 8483.0000 - tn: 442375.0000 - fn: 17113.0000 - accuracy: 0.9533 - precision: 0.9048 - recall: 0.8249 - auc: 0.9827\n",
            "Epoch 00144: val_auc did not improve from 0.98734\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1857 - tp: 80772.0000 - fp: 8503.0000 - tn: 443078.0000 - fn: 17138.0000 - accuracy: 0.9533 - precision: 0.9048 - recall: 0.8250 - auc: 0.9827 - val_loss: 0.1810 - val_tp: 21053.0000 - val_fp: 2355.0000 - val_tn: 110252.0000 - val_fn: 3713.0000 - val_accuracy: 0.9558 - val_precision: 0.8994 - val_recall: 0.8501 - val_auc: 0.9865\n",
            "Epoch 145/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1859 - tp: 80606.0000 - fp: 8470.0000 - tn: 442597.0000 - fn: 17191.0000 - accuracy: 0.9532 - precision: 0.9049 - recall: 0.8242 - auc: 0.9828\n",
            "Epoch 00145: val_auc did not improve from 0.98734\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1859 - tp: 80707.0000 - fp: 8476.0000 - tn: 443105.0000 - fn: 17203.0000 - accuracy: 0.9533 - precision: 0.9050 - recall: 0.8243 - auc: 0.9828 - val_loss: 0.1771 - val_tp: 21365.0000 - val_fp: 2365.0000 - val_tn: 110242.0000 - val_fn: 3401.0000 - val_accuracy: 0.9580 - val_precision: 0.9003 - val_recall: 0.8627 - val_auc: 0.9869\n",
            "Epoch 146/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1855 - tp: 80647.0000 - fp: 8335.0000 - tn: 442501.0000 - fn: 17125.0000 - accuracy: 0.9536 - precision: 0.9063 - recall: 0.8248 - auc: 0.9828\n",
            "Epoch 00146: val_auc did not improve from 0.98734\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1855 - tp: 80768.0000 - fp: 8350.0000 - tn: 443231.0000 - fn: 17142.0000 - accuracy: 0.9536 - precision: 0.9063 - recall: 0.8249 - auc: 0.9828 - val_loss: 0.1791 - val_tp: 21273.0000 - val_fp: 2503.0000 - val_tn: 110104.0000 - val_fn: 3493.0000 - val_accuracy: 0.9564 - val_precision: 0.8947 - val_recall: 0.8590 - val_auc: 0.9865\n",
            "Epoch 147/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1844 - tp: 80950.0000 - fp: 8480.0000 - tn: 443006.0000 - fn: 16940.0000 - accuracy: 0.9537 - precision: 0.9052 - recall: 0.8269 - auc: 0.9832\n",
            "Epoch 00147: val_auc did not improve from 0.98734\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1844 - tp: 80966.0000 - fp: 8482.0000 - tn: 443099.0000 - fn: 16944.0000 - accuracy: 0.9537 - precision: 0.9052 - recall: 0.8269 - auc: 0.9832 - val_loss: 0.1893 - val_tp: 21521.0000 - val_fp: 3180.0000 - val_tn: 109427.0000 - val_fn: 3245.0000 - val_accuracy: 0.9532 - val_precision: 0.8713 - val_recall: 0.8690 - val_auc: 0.9854\n",
            "Epoch 148/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1846 - tp: 80913.0000 - fp: 8391.0000 - tn: 442666.0000 - fn: 16894.0000 - accuracy: 0.9539 - precision: 0.9060 - recall: 0.8273 - auc: 0.9831\n",
            "Epoch 00148: val_auc improved from 0.98734 to 0.98820, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1846 - tp: 81001.0000 - fp: 8405.0000 - tn: 443176.0000 - fn: 16909.0000 - accuracy: 0.9539 - precision: 0.9060 - recall: 0.8273 - auc: 0.9831 - val_loss: 0.1722 - val_tp: 21154.0000 - val_fp: 1930.0000 - val_tn: 110677.0000 - val_fn: 3612.0000 - val_accuracy: 0.9597 - val_precision: 0.9164 - val_recall: 0.8542 - val_auc: 0.9882\n",
            "Epoch 149/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1843 - tp: 80903.0000 - fp: 8356.0000 - tn: 443130.0000 - fn: 16987.0000 - accuracy: 0.9539 - precision: 0.9064 - recall: 0.8265 - auc: 0.9831\n",
            "Epoch 00149: val_auc did not improve from 0.98820\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1843 - tp: 80920.0000 - fp: 8358.0000 - tn: 443223.0000 - fn: 16990.0000 - accuracy: 0.9539 - precision: 0.9064 - recall: 0.8265 - auc: 0.9831 - val_loss: 0.1762 - val_tp: 21459.0000 - val_fp: 2479.0000 - val_tn: 110128.0000 - val_fn: 3307.0000 - val_accuracy: 0.9579 - val_precision: 0.8964 - val_recall: 0.8665 - val_auc: 0.9877\n",
            "Epoch 150/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1840 - tp: 80968.0000 - fp: 8389.0000 - tn: 442890.0000 - fn: 16873.0000 - accuracy: 0.9540 - precision: 0.9061 - recall: 0.8275 - auc: 0.9833\n",
            "Epoch 00150: val_auc did not improve from 0.98820\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1840 - tp: 81028.0000 - fp: 8397.0000 - tn: 443184.0000 - fn: 16882.0000 - accuracy: 0.9540 - precision: 0.9061 - recall: 0.8276 - auc: 0.9833 - val_loss: 0.1798 - val_tp: 20857.0000 - val_fp: 2098.0000 - val_tn: 110509.0000 - val_fn: 3909.0000 - val_accuracy: 0.9563 - val_precision: 0.9086 - val_recall: 0.8422 - val_auc: 0.9863\n",
            "Epoch 151/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1832 - tp: 81139.0000 - fp: 8307.0000 - tn: 442758.0000 - fn: 16660.0000 - accuracy: 0.9545 - precision: 0.9071 - recall: 0.8297 - auc: 0.9834\n",
            "Epoch 00151: val_auc did not improve from 0.98820\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1832 - tp: 81230.0000 - fp: 8310.0000 - tn: 443271.0000 - fn: 16680.0000 - accuracy: 0.9545 - precision: 0.9072 - recall: 0.8296 - auc: 0.9834 - val_loss: 0.1790 - val_tp: 20919.0000 - val_fp: 2051.0000 - val_tn: 110556.0000 - val_fn: 3847.0000 - val_accuracy: 0.9571 - val_precision: 0.9107 - val_recall: 0.8447 - val_auc: 0.9869\n",
            "Epoch 152/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1839 - tp: 81127.0000 - fp: 8449.0000 - tn: 443037.0000 - fn: 16763.0000 - accuracy: 0.9541 - precision: 0.9057 - recall: 0.8288 - auc: 0.9834\n",
            "Epoch 00152: val_auc did not improve from 0.98820\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1839 - tp: 81143.0000 - fp: 8453.0000 - tn: 443128.0000 - fn: 16767.0000 - accuracy: 0.9541 - precision: 0.9057 - recall: 0.8288 - auc: 0.9834 - val_loss: 0.1815 - val_tp: 21442.0000 - val_fp: 2649.0000 - val_tn: 109958.0000 - val_fn: 3324.0000 - val_accuracy: 0.9565 - val_precision: 0.8900 - val_recall: 0.8658 - val_auc: 0.9867\n",
            "Epoch 153/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1832 - tp: 81197.0000 - fp: 8522.0000 - tn: 442531.0000 - fn: 16614.0000 - accuracy: 0.9542 - precision: 0.9050 - recall: 0.8301 - auc: 0.9834\n",
            "Epoch 00153: val_auc improved from 0.98820 to 0.98831, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1831 - tp: 81280.0000 - fp: 8535.0000 - tn: 443046.0000 - fn: 16630.0000 - accuracy: 0.9542 - precision: 0.9050 - recall: 0.8302 - auc: 0.9834 - val_loss: 0.1711 - val_tp: 21104.0000 - val_fp: 1789.0000 - val_tn: 110818.0000 - val_fn: 3662.0000 - val_accuracy: 0.9603 - val_precision: 0.9219 - val_recall: 0.8521 - val_auc: 0.9883\n",
            "Epoch 154/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1822 - tp: 81183.0000 - fp: 8299.0000 - tn: 442975.0000 - fn: 16663.0000 - accuracy: 0.9545 - precision: 0.9073 - recall: 0.8297 - auc: 0.9838\n",
            "Epoch 00154: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1821 - tp: 81236.0000 - fp: 8302.0000 - tn: 443279.0000 - fn: 16674.0000 - accuracy: 0.9545 - precision: 0.9073 - recall: 0.8297 - auc: 0.9838 - val_loss: 0.1820 - val_tp: 21075.0000 - val_fp: 2349.0000 - val_tn: 110258.0000 - val_fn: 3691.0000 - val_accuracy: 0.9560 - val_precision: 0.8997 - val_recall: 0.8510 - val_auc: 0.9857\n",
            "Epoch 155/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1827 - tp: 81115.0000 - fp: 8397.0000 - tn: 443091.0000 - fn: 16773.0000 - accuracy: 0.9542 - precision: 0.9062 - recall: 0.8287 - auc: 0.9836\n",
            "Epoch 00155: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1827 - tp: 81133.0000 - fp: 8399.0000 - tn: 443182.0000 - fn: 16777.0000 - accuracy: 0.9542 - precision: 0.9062 - recall: 0.8286 - auc: 0.9836 - val_loss: 0.1782 - val_tp: 21627.0000 - val_fp: 2627.0000 - val_tn: 109980.0000 - val_fn: 3139.0000 - val_accuracy: 0.9580 - val_precision: 0.8917 - val_recall: 0.8733 - val_auc: 0.9872\n",
            "Epoch 156/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1816 - tp: 81379.0000 - fp: 8216.0000 - tn: 443063.0000 - fn: 16462.0000 - accuracy: 0.9551 - precision: 0.9083 - recall: 0.8317 - auc: 0.9839\n",
            "Epoch 00156: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1816 - tp: 81436.0000 - fp: 8227.0000 - tn: 443354.0000 - fn: 16474.0000 - accuracy: 0.9550 - precision: 0.9082 - recall: 0.8317 - auc: 0.9839 - val_loss: 0.1742 - val_tp: 21401.0000 - val_fp: 2305.0000 - val_tn: 110302.0000 - val_fn: 3365.0000 - val_accuracy: 0.9587 - val_precision: 0.9028 - val_recall: 0.8641 - val_auc: 0.9878\n",
            "Epoch 157/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1818 - tp: 81436.0000 - fp: 8353.0000 - tn: 443132.0000 - fn: 16455.0000 - accuracy: 0.9548 - precision: 0.9070 - recall: 0.8319 - auc: 0.9839\n",
            "Epoch 00157: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1818 - tp: 81451.0000 - fp: 8353.0000 - tn: 443228.0000 - fn: 16459.0000 - accuracy: 0.9548 - precision: 0.9070 - recall: 0.8319 - auc: 0.9839 - val_loss: 0.1738 - val_tp: 21219.0000 - val_fp: 2008.0000 - val_tn: 110599.0000 - val_fn: 3547.0000 - val_accuracy: 0.9596 - val_precision: 0.9135 - val_recall: 0.8568 - val_auc: 0.9877\n",
            "Epoch 158/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1815 - tp: 81266.0000 - fp: 8218.0000 - tn: 442425.0000 - fn: 16443.0000 - accuracy: 0.9550 - precision: 0.9082 - recall: 0.8317 - auc: 0.9840\n",
            "Epoch 00158: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1815 - tp: 81431.0000 - fp: 8232.0000 - tn: 443349.0000 - fn: 16479.0000 - accuracy: 0.9550 - precision: 0.9082 - recall: 0.8317 - auc: 0.9840 - val_loss: 0.1761 - val_tp: 21644.0000 - val_fp: 2772.0000 - val_tn: 109835.0000 - val_fn: 3122.0000 - val_accuracy: 0.9571 - val_precision: 0.8865 - val_recall: 0.8739 - val_auc: 0.9874\n",
            "Epoch 159/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1811 - tp: 81177.0000 - fp: 8137.0000 - tn: 442304.0000 - fn: 16478.0000 - accuracy: 0.9551 - precision: 0.9089 - recall: 0.8313 - auc: 0.9840\n",
            "Epoch 00159: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1812 - tp: 81382.0000 - fp: 8157.0000 - tn: 443424.0000 - fn: 16528.0000 - accuracy: 0.9551 - precision: 0.9089 - recall: 0.8312 - auc: 0.9840 - val_loss: 0.1755 - val_tp: 21745.0000 - val_fp: 2766.0000 - val_tn: 109841.0000 - val_fn: 3021.0000 - val_accuracy: 0.9579 - val_precision: 0.8872 - val_recall: 0.8780 - val_auc: 0.9880\n",
            "Epoch 160/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1808 - tp: 81534.0000 - fp: 8427.0000 - tn: 443154.0000 - fn: 16376.0000 - accuracy: 0.9549 - precision: 0.9063 - recall: 0.8327 - auc: 0.9843\n",
            "Epoch 00160: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1808 - tp: 81534.0000 - fp: 8427.0000 - tn: 443154.0000 - fn: 16376.0000 - accuracy: 0.9549 - precision: 0.9063 - recall: 0.8327 - auc: 0.9843 - val_loss: 0.1747 - val_tp: 20500.0000 - val_fp: 1667.0000 - val_tn: 110940.0000 - val_fn: 4266.0000 - val_accuracy: 0.9568 - val_precision: 0.9248 - val_recall: 0.8277 - val_auc: 0.9874\n",
            "Epoch 161/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1809 - tp: 81470.0000 - fp: 8220.0000 - tn: 443270.0000 - fn: 16416.0000 - accuracy: 0.9552 - precision: 0.9084 - recall: 0.8323 - auc: 0.9842\n",
            "Epoch 00161: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1809 - tp: 81489.0000 - fp: 8221.0000 - tn: 443360.0000 - fn: 16421.0000 - accuracy: 0.9552 - precision: 0.9084 - recall: 0.8323 - auc: 0.9842 - val_loss: 0.1812 - val_tp: 21488.0000 - val_fp: 2777.0000 - val_tn: 109830.0000 - val_fn: 3278.0000 - val_accuracy: 0.9559 - val_precision: 0.8856 - val_recall: 0.8676 - val_auc: 0.9865\n",
            "Epoch 162/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1805 - tp: 81346.0000 - fp: 8252.0000 - tn: 442203.0000 - fn: 16295.0000 - accuracy: 0.9552 - precision: 0.9079 - recall: 0.8331 - auc: 0.9842\n",
            "Epoch 00162: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1805 - tp: 81565.0000 - fp: 8267.0000 - tn: 443314.0000 - fn: 16345.0000 - accuracy: 0.9552 - precision: 0.9080 - recall: 0.8331 - auc: 0.9842 - val_loss: 0.1839 - val_tp: 21956.0000 - val_fp: 3306.0000 - val_tn: 109301.0000 - val_fn: 2810.0000 - val_accuracy: 0.9555 - val_precision: 0.8691 - val_recall: 0.8865 - val_auc: 0.9868\n",
            "Epoch 163/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1801 - tp: 81702.0000 - fp: 8271.0000 - tn: 442573.0000 - fn: 16062.0000 - accuracy: 0.9556 - precision: 0.9081 - recall: 0.8357 - auc: 0.9844\n",
            "Epoch 00163: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1800 - tp: 81832.0000 - fp: 8279.0000 - tn: 443302.0000 - fn: 16078.0000 - accuracy: 0.9557 - precision: 0.9081 - recall: 0.8358 - auc: 0.9844 - val_loss: 0.1772 - val_tp: 20960.0000 - val_fp: 2084.0000 - val_tn: 110523.0000 - val_fn: 3806.0000 - val_accuracy: 0.9571 - val_precision: 0.9096 - val_recall: 0.8463 - val_auc: 0.9874\n",
            "Epoch 164/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1787 - tp: 81696.0000 - fp: 8145.0000 - tn: 442317.0000 - fn: 15938.0000 - accuracy: 0.9561 - precision: 0.9093 - recall: 0.8368 - auc: 0.9848\n",
            "Epoch 00164: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1787 - tp: 81909.0000 - fp: 8156.0000 - tn: 443425.0000 - fn: 16001.0000 - accuracy: 0.9560 - precision: 0.9094 - recall: 0.8366 - auc: 0.9848 - val_loss: 0.1752 - val_tp: 21177.0000 - val_fp: 2198.0000 - val_tn: 110409.0000 - val_fn: 3589.0000 - val_accuracy: 0.9579 - val_precision: 0.9060 - val_recall: 0.8551 - val_auc: 0.9876\n",
            "Epoch 165/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1796 - tp: 81653.0000 - fp: 8209.0000 - tn: 442867.0000 - fn: 16135.0000 - accuracy: 0.9556 - precision: 0.9086 - recall: 0.8350 - auc: 0.9845\n",
            "Epoch 00165: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1795 - tp: 81754.0000 - fp: 8220.0000 - tn: 443361.0000 - fn: 16156.0000 - accuracy: 0.9556 - precision: 0.9086 - recall: 0.8350 - auc: 0.9845 - val_loss: 0.1745 - val_tp: 21402.0000 - val_fp: 2399.0000 - val_tn: 110208.0000 - val_fn: 3364.0000 - val_accuracy: 0.9580 - val_precision: 0.8992 - val_recall: 0.8642 - val_auc: 0.9875\n",
            "Epoch 166/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1791 - tp: 81719.0000 - fp: 8236.0000 - tn: 443345.0000 - fn: 16191.0000 - accuracy: 0.9555 - precision: 0.9084 - recall: 0.8346 - auc: 0.9846\n",
            "Epoch 00166: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1791 - tp: 81719.0000 - fp: 8236.0000 - tn: 443345.0000 - fn: 16191.0000 - accuracy: 0.9555 - precision: 0.9084 - recall: 0.8346 - auc: 0.9846 - val_loss: 0.1713 - val_tp: 21180.0000 - val_fp: 1974.0000 - val_tn: 110633.0000 - val_fn: 3586.0000 - val_accuracy: 0.9595 - val_precision: 0.9147 - val_recall: 0.8552 - val_auc: 0.9883\n",
            "Epoch 167/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1787 - tp: 81850.0000 - fp: 8193.0000 - tn: 443388.0000 - fn: 16060.0000 - accuracy: 0.9559 - precision: 0.9090 - recall: 0.8360 - auc: 0.9847\n",
            "Epoch 00167: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1787 - tp: 81850.0000 - fp: 8193.0000 - tn: 443388.0000 - fn: 16060.0000 - accuracy: 0.9559 - precision: 0.9090 - recall: 0.8360 - auc: 0.9847 - val_loss: 0.1800 - val_tp: 21503.0000 - val_fp: 2611.0000 - val_tn: 109996.0000 - val_fn: 3263.0000 - val_accuracy: 0.9572 - val_precision: 0.8917 - val_recall: 0.8682 - val_auc: 0.9873\n",
            "Epoch 168/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1787 - tp: 81584.0000 - fp: 8162.0000 - tn: 442493.0000 - fn: 16113.0000 - accuracy: 0.9557 - precision: 0.9091 - recall: 0.8351 - auc: 0.9847\n",
            "Epoch 00168: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1788 - tp: 81758.0000 - fp: 8176.0000 - tn: 443405.0000 - fn: 16152.0000 - accuracy: 0.9557 - precision: 0.9091 - recall: 0.8350 - auc: 0.9847 - val_loss: 0.1756 - val_tp: 21104.0000 - val_fp: 2107.0000 - val_tn: 110500.0000 - val_fn: 3662.0000 - val_accuracy: 0.9580 - val_precision: 0.9092 - val_recall: 0.8521 - val_auc: 0.9875\n",
            "Epoch 169/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1785 - tp: 81881.0000 - fp: 8160.0000 - tn: 443330.0000 - fn: 16005.0000 - accuracy: 0.9560 - precision: 0.9094 - recall: 0.8365 - auc: 0.9848\n",
            "Epoch 00169: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1785 - tp: 81903.0000 - fp: 8160.0000 - tn: 443421.0000 - fn: 16007.0000 - accuracy: 0.9560 - precision: 0.9094 - recall: 0.8365 - auc: 0.9848 - val_loss: 0.1878 - val_tp: 22208.0000 - val_fp: 3803.0000 - val_tn: 108804.0000 - val_fn: 2558.0000 - val_accuracy: 0.9537 - val_precision: 0.8538 - val_recall: 0.8967 - val_auc: 0.9868\n",
            "Epoch 170/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1780 - tp: 81883.0000 - fp: 8200.0000 - tn: 443286.0000 - fn: 16007.0000 - accuracy: 0.9559 - precision: 0.9090 - recall: 0.8365 - auc: 0.9848\n",
            "Epoch 00170: val_auc did not improve from 0.98831\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1780 - tp: 81902.0000 - fp: 8200.0000 - tn: 443381.0000 - fn: 16008.0000 - accuracy: 0.9559 - precision: 0.9090 - recall: 0.8365 - auc: 0.9848 - val_loss: 0.1769 - val_tp: 21582.0000 - val_fp: 2548.0000 - val_tn: 110059.0000 - val_fn: 3184.0000 - val_accuracy: 0.9583 - val_precision: 0.8944 - val_recall: 0.8714 - val_auc: 0.9878\n",
            "Epoch 171/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1781 - tp: 81762.0000 - fp: 8189.0000 - tn: 442474.0000 - fn: 15927.0000 - accuracy: 0.9560 - precision: 0.9090 - recall: 0.8370 - auc: 0.9850\n",
            "Epoch 00171: val_auc improved from 0.98831 to 0.98921, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1781 - tp: 81947.0000 - fp: 8202.0000 - tn: 443379.0000 - fn: 15963.0000 - accuracy: 0.9560 - precision: 0.9090 - recall: 0.8370 - auc: 0.9850 - val_loss: 0.1670 - val_tp: 21793.0000 - val_fp: 2338.0000 - val_tn: 110269.0000 - val_fn: 2973.0000 - val_accuracy: 0.9613 - val_precision: 0.9031 - val_recall: 0.8800 - val_auc: 0.9892\n",
            "Epoch 172/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1777 - tp: 82053.0000 - fp: 8259.0000 - tn: 443226.0000 - fn: 15838.0000 - accuracy: 0.9561 - precision: 0.9086 - recall: 0.8382 - auc: 0.9850\n",
            "Epoch 00172: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1777 - tp: 82069.0000 - fp: 8260.0000 - tn: 443321.0000 - fn: 15841.0000 - accuracy: 0.9561 - precision: 0.9086 - recall: 0.8382 - auc: 0.9850 - val_loss: 0.1817 - val_tp: 21449.0000 - val_fp: 2730.0000 - val_tn: 109877.0000 - val_fn: 3317.0000 - val_accuracy: 0.9560 - val_precision: 0.8871 - val_recall: 0.8661 - val_auc: 0.9866\n",
            "Epoch 173/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1778 - tp: 81866.0000 - fp: 8134.0000 - tn: 442715.0000 - fn: 15893.0000 - accuracy: 0.9562 - precision: 0.9096 - recall: 0.8374 - auc: 0.9850\n",
            "Epoch 00173: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1779 - tp: 81995.0000 - fp: 8152.0000 - tn: 443429.0000 - fn: 15915.0000 - accuracy: 0.9562 - precision: 0.9096 - recall: 0.8375 - auc: 0.9850 - val_loss: 0.1763 - val_tp: 21865.0000 - val_fp: 2833.0000 - val_tn: 109774.0000 - val_fn: 2901.0000 - val_accuracy: 0.9583 - val_precision: 0.8853 - val_recall: 0.8829 - val_auc: 0.9876\n",
            "Epoch 174/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1768 - tp: 82131.0000 - fp: 8055.0000 - tn: 443204.0000 - fn: 15730.0000 - accuracy: 0.9567 - precision: 0.9107 - recall: 0.8393 - auc: 0.9852\n",
            "Epoch 00174: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1768 - tp: 82173.0000 - fp: 8068.0000 - tn: 443513.0000 - fn: 15737.0000 - accuracy: 0.9567 - precision: 0.9106 - recall: 0.8393 - auc: 0.9852 - val_loss: 0.1734 - val_tp: 21670.0000 - val_fp: 2434.0000 - val_tn: 110173.0000 - val_fn: 3096.0000 - val_accuracy: 0.9597 - val_precision: 0.8990 - val_recall: 0.8750 - val_auc: 0.9880\n",
            "Epoch 175/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1769 - tp: 82075.0000 - fp: 8109.0000 - tn: 443377.0000 - fn: 15815.0000 - accuracy: 0.9565 - precision: 0.9101 - recall: 0.8384 - auc: 0.9852\n",
            "Epoch 00175: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1769 - tp: 82090.0000 - fp: 8110.0000 - tn: 443471.0000 - fn: 15820.0000 - accuracy: 0.9565 - precision: 0.9101 - recall: 0.8384 - auc: 0.9852 - val_loss: 0.1728 - val_tp: 21766.0000 - val_fp: 2516.0000 - val_tn: 110091.0000 - val_fn: 3000.0000 - val_accuracy: 0.9598 - val_precision: 0.8964 - val_recall: 0.8789 - val_auc: 0.9887\n",
            "Epoch 176/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1762 - tp: 82080.0000 - fp: 7906.0000 - tn: 443370.0000 - fn: 15764.0000 - accuracy: 0.9569 - precision: 0.9121 - recall: 0.8389 - auc: 0.9853\n",
            "Epoch 00176: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1762 - tp: 82138.0000 - fp: 7908.0000 - tn: 443673.0000 - fn: 15772.0000 - accuracy: 0.9569 - precision: 0.9122 - recall: 0.8389 - auc: 0.9853 - val_loss: 0.1796 - val_tp: 21703.0000 - val_fp: 2886.0000 - val_tn: 109721.0000 - val_fn: 3063.0000 - val_accuracy: 0.9567 - val_precision: 0.8826 - val_recall: 0.8763 - val_auc: 0.9869\n",
            "Epoch 177/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1758 - tp: 82379.0000 - fp: 8040.0000 - tn: 443019.0000 - fn: 15426.0000 - accuracy: 0.9572 - precision: 0.9111 - recall: 0.8423 - auc: 0.9854\n",
            "Epoch 00177: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1758 - tp: 82468.0000 - fp: 8055.0000 - tn: 443526.0000 - fn: 15442.0000 - accuracy: 0.9572 - precision: 0.9110 - recall: 0.8423 - auc: 0.9854 - val_loss: 0.1755 - val_tp: 21642.0000 - val_fp: 2697.0000 - val_tn: 109910.0000 - val_fn: 3124.0000 - val_accuracy: 0.9576 - val_precision: 0.8892 - val_recall: 0.8739 - val_auc: 0.9879\n",
            "Epoch 178/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1769 - tp: 82001.0000 - fp: 8059.0000 - tn: 442573.0000 - fn: 15719.0000 - accuracy: 0.9566 - precision: 0.9105 - recall: 0.8391 - auc: 0.9851\n",
            "Epoch 00178: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1769 - tp: 82172.0000 - fp: 8078.0000 - tn: 443503.0000 - fn: 15738.0000 - accuracy: 0.9567 - precision: 0.9105 - recall: 0.8393 - auc: 0.9852 - val_loss: 0.1902 - val_tp: 22103.0000 - val_fp: 3852.0000 - val_tn: 108755.0000 - val_fn: 2663.0000 - val_accuracy: 0.9526 - val_precision: 0.8516 - val_recall: 0.8925 - val_auc: 0.9864\n",
            "Epoch 179/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1759 - tp: 82134.0000 - fp: 8042.0000 - tn: 442814.0000 - fn: 15618.0000 - accuracy: 0.9569 - precision: 0.9108 - recall: 0.8402 - auc: 0.9854\n",
            "Epoch 00179: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1759 - tp: 82281.0000 - fp: 8052.0000 - tn: 443529.0000 - fn: 15629.0000 - accuracy: 0.9569 - precision: 0.9109 - recall: 0.8404 - auc: 0.9854 - val_loss: 0.1771 - val_tp: 21868.0000 - val_fp: 2800.0000 - val_tn: 109807.0000 - val_fn: 2898.0000 - val_accuracy: 0.9585 - val_precision: 0.8865 - val_recall: 0.8830 - val_auc: 0.9882\n",
            "Epoch 180/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1758 - tp: 82165.0000 - fp: 7961.0000 - tn: 442695.0000 - fn: 15531.0000 - accuracy: 0.9572 - precision: 0.9117 - recall: 0.8410 - auc: 0.9854\n",
            "Epoch 00180: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1758 - tp: 82348.0000 - fp: 7979.0000 - tn: 443602.0000 - fn: 15562.0000 - accuracy: 0.9572 - precision: 0.9117 - recall: 0.8411 - auc: 0.9854 - val_loss: 0.1771 - val_tp: 21786.0000 - val_fp: 2772.0000 - val_tn: 109835.0000 - val_fn: 2980.0000 - val_accuracy: 0.9581 - val_precision: 0.8871 - val_recall: 0.8797 - val_auc: 0.9880\n",
            "Epoch 181/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1756 - tp: 82336.0000 - fp: 8051.0000 - tn: 443436.0000 - fn: 15553.0000 - accuracy: 0.9570 - precision: 0.9109 - recall: 0.8411 - auc: 0.9855\n",
            "Epoch 00181: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1756 - tp: 82352.0000 - fp: 8053.0000 - tn: 443528.0000 - fn: 15558.0000 - accuracy: 0.9570 - precision: 0.9109 - recall: 0.8411 - auc: 0.9855 - val_loss: 0.1848 - val_tp: 22506.0000 - val_fp: 4093.0000 - val_tn: 108514.0000 - val_fn: 2260.0000 - val_accuracy: 0.9538 - val_precision: 0.8461 - val_recall: 0.9087 - val_auc: 0.9876\n",
            "Epoch 182/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1750 - tp: 82253.0000 - fp: 8012.0000 - tn: 442834.0000 - fn: 15509.0000 - accuracy: 0.9571 - precision: 0.9112 - recall: 0.8414 - auc: 0.9856\n",
            "Epoch 00182: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1750 - tp: 82377.0000 - fp: 8021.0000 - tn: 443560.0000 - fn: 15533.0000 - accuracy: 0.9571 - precision: 0.9113 - recall: 0.8414 - auc: 0.9856 - val_loss: 0.1763 - val_tp: 21248.0000 - val_fp: 2447.0000 - val_tn: 110160.0000 - val_fn: 3518.0000 - val_accuracy: 0.9566 - val_precision: 0.8967 - val_recall: 0.8580 - val_auc: 0.9867\n",
            "Epoch 183/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1744 - tp: 82495.0000 - fp: 8024.0000 - tn: 443461.0000 - fn: 15396.0000 - accuracy: 0.9574 - precision: 0.9114 - recall: 0.8427 - auc: 0.9859\n",
            "Epoch 00183: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1744 - tp: 82512.0000 - fp: 8025.0000 - tn: 443556.0000 - fn: 15398.0000 - accuracy: 0.9574 - precision: 0.9114 - recall: 0.8427 - auc: 0.9859 - val_loss: 0.1717 - val_tp: 21906.0000 - val_fp: 2617.0000 - val_tn: 109990.0000 - val_fn: 2860.0000 - val_accuracy: 0.9601 - val_precision: 0.8933 - val_recall: 0.8845 - val_auc: 0.9886\n",
            "Epoch 184/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1747 - tp: 82250.0000 - fp: 7992.0000 - tn: 443088.0000 - fn: 15534.0000 - accuracy: 0.9571 - precision: 0.9114 - recall: 0.8411 - auc: 0.9858\n",
            "Epoch 00184: val_auc did not improve from 0.98921\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1747 - tp: 82353.0000 - fp: 7999.0000 - tn: 443582.0000 - fn: 15557.0000 - accuracy: 0.9571 - precision: 0.9115 - recall: 0.8411 - auc: 0.9858 - val_loss: 0.1699 - val_tp: 21671.0000 - val_fp: 2329.0000 - val_tn: 110278.0000 - val_fn: 3095.0000 - val_accuracy: 0.9605 - val_precision: 0.9030 - val_recall: 0.8750 - val_auc: 0.9889\n",
            "Epoch 185/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1740 - tp: 82418.0000 - fp: 8018.0000 - tn: 442850.0000 - fn: 15322.0000 - accuracy: 0.9575 - precision: 0.9113 - recall: 0.8432 - auc: 0.9860\n",
            "Epoch 00185: val_auc improved from 0.98921 to 0.98944, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1740 - tp: 82560.0000 - fp: 8032.0000 - tn: 443549.0000 - fn: 15350.0000 - accuracy: 0.9574 - precision: 0.9113 - recall: 0.8432 - auc: 0.9860 - val_loss: 0.1663 - val_tp: 21671.0000 - val_fp: 2240.0000 - val_tn: 110367.0000 - val_fn: 3095.0000 - val_accuracy: 0.9612 - val_precision: 0.9063 - val_recall: 0.8750 - val_auc: 0.9894\n",
            "Epoch 186/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1744 - tp: 82442.0000 - fp: 8091.0000 - tn: 443395.0000 - fn: 15448.0000 - accuracy: 0.9572 - precision: 0.9106 - recall: 0.8422 - auc: 0.9859\n",
            "Epoch 00186: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1744 - tp: 82461.0000 - fp: 8092.0000 - tn: 443489.0000 - fn: 15449.0000 - accuracy: 0.9572 - precision: 0.9106 - recall: 0.8422 - auc: 0.9859 - val_loss: 0.1703 - val_tp: 21533.0000 - val_fp: 2267.0000 - val_tn: 110340.0000 - val_fn: 3233.0000 - val_accuracy: 0.9600 - val_precision: 0.9047 - val_recall: 0.8695 - val_auc: 0.9883\n",
            "Epoch 187/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1739 - tp: 82639.0000 - fp: 8093.0000 - tn: 443398.0000 - fn: 15246.0000 - accuracy: 0.9575 - precision: 0.9108 - recall: 0.8442 - auc: 0.9859\n",
            "Epoch 00187: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1739 - tp: 82661.0000 - fp: 8094.0000 - tn: 443487.0000 - fn: 15249.0000 - accuracy: 0.9575 - precision: 0.9108 - recall: 0.8443 - auc: 0.9859 - val_loss: 0.1685 - val_tp: 21080.0000 - val_fp: 1885.0000 - val_tn: 110722.0000 - val_fn: 3686.0000 - val_accuracy: 0.9594 - val_precision: 0.9179 - val_recall: 0.8512 - val_auc: 0.9889\n",
            "Epoch 188/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1742 - tp: 82608.0000 - fp: 8064.0000 - tn: 443429.0000 - fn: 15275.0000 - accuracy: 0.9575 - precision: 0.9111 - recall: 0.8439 - auc: 0.9858\n",
            "Epoch 00188: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1742 - tp: 82630.0000 - fp: 8066.0000 - tn: 443515.0000 - fn: 15280.0000 - accuracy: 0.9575 - precision: 0.9111 - recall: 0.8439 - auc: 0.9858 - val_loss: 0.1667 - val_tp: 21081.0000 - val_fp: 1704.0000 - val_tn: 110903.0000 - val_fn: 3685.0000 - val_accuracy: 0.9608 - val_precision: 0.9252 - val_recall: 0.8512 - val_auc: 0.9891\n",
            "Epoch 189/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1742 - tp: 82436.0000 - fp: 7927.0000 - tn: 442714.0000 - fn: 15275.0000 - accuracy: 0.9577 - precision: 0.9123 - recall: 0.8437 - auc: 0.9859\n",
            "Epoch 00189: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1742 - tp: 82601.0000 - fp: 7948.0000 - tn: 443633.0000 - fn: 15309.0000 - accuracy: 0.9577 - precision: 0.9122 - recall: 0.8436 - auc: 0.9859 - val_loss: 0.1744 - val_tp: 22042.0000 - val_fp: 2818.0000 - val_tn: 109789.0000 - val_fn: 2724.0000 - val_accuracy: 0.9597 - val_precision: 0.8866 - val_recall: 0.8900 - val_auc: 0.9888\n",
            "Epoch 190/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1728 - tp: 82570.0000 - fp: 7880.0000 - tn: 442983.0000 - fn: 15175.0000 - accuracy: 0.9580 - precision: 0.9129 - recall: 0.8447 - auc: 0.9862\n",
            "Epoch 00190: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1728 - tp: 82705.0000 - fp: 7893.0000 - tn: 443688.0000 - fn: 15205.0000 - accuracy: 0.9580 - precision: 0.9129 - recall: 0.8447 - auc: 0.9862 - val_loss: 0.1682 - val_tp: 21933.0000 - val_fp: 2479.0000 - val_tn: 110128.0000 - val_fn: 2833.0000 - val_accuracy: 0.9613 - val_precision: 0.8985 - val_recall: 0.8856 - val_auc: 0.9894\n",
            "Epoch 191/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1727 - tp: 82701.0000 - fp: 7934.0000 - tn: 442903.0000 - fn: 15070.0000 - accuracy: 0.9581 - precision: 0.9125 - recall: 0.8459 - auc: 0.9863\n",
            "Epoch 00191: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1727 - tp: 82818.0000 - fp: 7950.0000 - tn: 443631.0000 - fn: 15092.0000 - accuracy: 0.9581 - precision: 0.9124 - recall: 0.8459 - auc: 0.9863 - val_loss: 0.1676 - val_tp: 21474.0000 - val_fp: 2083.0000 - val_tn: 110524.0000 - val_fn: 3292.0000 - val_accuracy: 0.9609 - val_precision: 0.9116 - val_recall: 0.8671 - val_auc: 0.9892\n",
            "Epoch 192/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1732 - tp: 82644.0000 - fp: 8043.0000 - tn: 443232.0000 - fn: 15201.0000 - accuracy: 0.9577 - precision: 0.9113 - recall: 0.8446 - auc: 0.9861\n",
            "Epoch 00192: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1732 - tp: 82702.0000 - fp: 8050.0000 - tn: 443531.0000 - fn: 15208.0000 - accuracy: 0.9577 - precision: 0.9113 - recall: 0.8447 - auc: 0.9861 - val_loss: 0.1757 - val_tp: 21878.0000 - val_fp: 2764.0000 - val_tn: 109843.0000 - val_fn: 2888.0000 - val_accuracy: 0.9589 - val_precision: 0.8878 - val_recall: 0.8834 - val_auc: 0.9883\n",
            "Epoch 193/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1722 - tp: 82781.0000 - fp: 7927.0000 - tn: 443348.0000 - fn: 15064.0000 - accuracy: 0.9581 - precision: 0.9126 - recall: 0.8460 - auc: 0.9864\n",
            "Epoch 00193: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1722 - tp: 82839.0000 - fp: 7933.0000 - tn: 443648.0000 - fn: 15071.0000 - accuracy: 0.9581 - precision: 0.9126 - recall: 0.8461 - auc: 0.9864 - val_loss: 0.1757 - val_tp: 22210.0000 - val_fp: 3224.0000 - val_tn: 109383.0000 - val_fn: 2556.0000 - val_accuracy: 0.9579 - val_precision: 0.8732 - val_recall: 0.8968 - val_auc: 0.9885\n",
            "Epoch 194/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1716 - tp: 82671.0000 - fp: 7959.0000 - tn: 442704.0000 - fn: 15018.0000 - accuracy: 0.9581 - precision: 0.9122 - recall: 0.8463 - auc: 0.9866\n",
            "Epoch 00194: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1716 - tp: 82856.0000 - fp: 7970.0000 - tn: 443611.0000 - fn: 15054.0000 - accuracy: 0.9581 - precision: 0.9122 - recall: 0.8462 - auc: 0.9866 - val_loss: 0.1715 - val_tp: 22216.0000 - val_fp: 3064.0000 - val_tn: 109543.0000 - val_fn: 2550.0000 - val_accuracy: 0.9591 - val_precision: 0.8788 - val_recall: 0.8970 - val_auc: 0.9889\n",
            "Epoch 195/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1717 - tp: 82986.0000 - fp: 7989.0000 - tn: 443592.0000 - fn: 14924.0000 - accuracy: 0.9583 - precision: 0.9122 - recall: 0.8476 - auc: 0.9865\n",
            "Epoch 00195: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1717 - tp: 82986.0000 - fp: 7989.0000 - tn: 443592.0000 - fn: 14924.0000 - accuracy: 0.9583 - precision: 0.9122 - recall: 0.8476 - auc: 0.9865 - val_loss: 0.1679 - val_tp: 21702.0000 - val_fp: 2237.0000 - val_tn: 110370.0000 - val_fn: 3064.0000 - val_accuracy: 0.9614 - val_precision: 0.9066 - val_recall: 0.8763 - val_auc: 0.9892\n",
            "Epoch 196/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1721 - tp: 82814.0000 - fp: 7843.0000 - tn: 443639.0000 - fn: 15080.0000 - accuracy: 0.9583 - precision: 0.9135 - recall: 0.8460 - auc: 0.9864\n",
            "Epoch 00196: val_auc did not improve from 0.98944\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1721 - tp: 82827.0000 - fp: 7844.0000 - tn: 443737.0000 - fn: 15083.0000 - accuracy: 0.9583 - precision: 0.9135 - recall: 0.8460 - auc: 0.9864 - val_loss: 0.1721 - val_tp: 21633.0000 - val_fp: 2504.0000 - val_tn: 110103.0000 - val_fn: 3133.0000 - val_accuracy: 0.9590 - val_precision: 0.8963 - val_recall: 0.8735 - val_auc: 0.9886\n",
            "Epoch 197/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1718 - tp: 82820.0000 - fp: 7853.0000 - tn: 443221.0000 - fn: 14970.0000 - accuracy: 0.9584 - precision: 0.9134 - recall: 0.8469 - auc: 0.9865\n",
            "Epoch 00197: val_auc improved from 0.98944 to 0.99026, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1718 - tp: 82918.0000 - fp: 7862.0000 - tn: 443719.0000 - fn: 14992.0000 - accuracy: 0.9584 - precision: 0.9134 - recall: 0.8469 - auc: 0.9865 - val_loss: 0.1633 - val_tp: 21940.0000 - val_fp: 2337.0000 - val_tn: 110270.0000 - val_fn: 2826.0000 - val_accuracy: 0.9624 - val_precision: 0.9037 - val_recall: 0.8859 - val_auc: 0.9903\n",
            "Epoch 198/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1718 - tp: 82868.0000 - fp: 7998.0000 - tn: 443073.0000 - fn: 14925.0000 - accuracy: 0.9582 - precision: 0.9120 - recall: 0.8474 - auc: 0.9864\n",
            "Epoch 00198: val_auc did not improve from 0.99026\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1719 - tp: 82963.0000 - fp: 8009.0000 - tn: 443572.0000 - fn: 14947.0000 - accuracy: 0.9582 - precision: 0.9120 - recall: 0.8473 - auc: 0.9864 - val_loss: 0.1679 - val_tp: 21678.0000 - val_fp: 2261.0000 - val_tn: 110346.0000 - val_fn: 3088.0000 - val_accuracy: 0.9611 - val_precision: 0.9056 - val_recall: 0.8753 - val_auc: 0.9892\n",
            "Epoch 199/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1715 - tp: 82821.0000 - fp: 7910.0000 - tn: 442951.0000 - fn: 14926.0000 - accuracy: 0.9584 - precision: 0.9128 - recall: 0.8473 - auc: 0.9866\n",
            "Epoch 00199: val_auc did not improve from 0.99026\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1716 - tp: 82957.0000 - fp: 7923.0000 - tn: 443658.0000 - fn: 14953.0000 - accuracy: 0.9584 - precision: 0.9128 - recall: 0.8473 - auc: 0.9866 - val_loss: 0.1868 - val_tp: 22101.0000 - val_fp: 3829.0000 - val_tn: 108778.0000 - val_fn: 2665.0000 - val_accuracy: 0.9527 - val_precision: 0.8523 - val_recall: 0.8924 - val_auc: 0.9864\n",
            "Epoch 200/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1707 - tp: 82960.0000 - fp: 7876.0000 - tn: 442977.0000 - fn: 14795.0000 - accuracy: 0.9587 - precision: 0.9133 - recall: 0.8487 - auc: 0.9868\n",
            "Epoch 00200: val_auc improved from 0.99026 to 0.99069, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1707 - tp: 83082.0000 - fp: 7889.0000 - tn: 443692.0000 - fn: 14828.0000 - accuracy: 0.9587 - precision: 0.9133 - recall: 0.8486 - auc: 0.9868 - val_loss: 0.1606 - val_tp: 21969.0000 - val_fp: 2132.0000 - val_tn: 110475.0000 - val_fn: 2797.0000 - val_accuracy: 0.9641 - val_precision: 0.9115 - val_recall: 0.8871 - val_auc: 0.9907\n",
            "Epoch 201/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1703 - tp: 83190.0000 - fp: 7736.0000 - tn: 443754.0000 - fn: 14696.0000 - accuracy: 0.9592 - precision: 0.9149 - recall: 0.8499 - auc: 0.9868\n",
            "Epoch 00201: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1703 - tp: 83211.0000 - fp: 7737.0000 - tn: 443844.0000 - fn: 14699.0000 - accuracy: 0.9592 - precision: 0.9149 - recall: 0.8499 - auc: 0.9868 - val_loss: 0.1801 - val_tp: 21873.0000 - val_fp: 3093.0000 - val_tn: 109514.0000 - val_fn: 2893.0000 - val_accuracy: 0.9564 - val_precision: 0.8761 - val_recall: 0.8832 - val_auc: 0.9875\n",
            "Epoch 202/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1708 - tp: 82961.0000 - fp: 7837.0000 - tn: 443444.0000 - fn: 14878.0000 - accuracy: 0.9586 - precision: 0.9137 - recall: 0.8479 - auc: 0.9867\n",
            "Epoch 00202: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1708 - tp: 83020.0000 - fp: 7843.0000 - tn: 443738.0000 - fn: 14890.0000 - accuracy: 0.9586 - precision: 0.9137 - recall: 0.8479 - auc: 0.9867 - val_loss: 0.1637 - val_tp: 21587.0000 - val_fp: 1914.0000 - val_tn: 110693.0000 - val_fn: 3179.0000 - val_accuracy: 0.9629 - val_precision: 0.9186 - val_recall: 0.8716 - val_auc: 0.9901\n",
            "Epoch 203/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1704 - tp: 83053.0000 - fp: 7831.0000 - tn: 443750.0000 - fn: 14857.0000 - accuracy: 0.9587 - precision: 0.9138 - recall: 0.8483 - auc: 0.9868\n",
            "Epoch 00203: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1704 - tp: 83053.0000 - fp: 7831.0000 - tn: 443750.0000 - fn: 14857.0000 - accuracy: 0.9587 - precision: 0.9138 - recall: 0.8483 - auc: 0.9868 - val_loss: 0.1652 - val_tp: 21286.0000 - val_fp: 1768.0000 - val_tn: 110839.0000 - val_fn: 3480.0000 - val_accuracy: 0.9618 - val_precision: 0.9233 - val_recall: 0.8595 - val_auc: 0.9896\n",
            "Epoch 204/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1692 - tp: 83345.0000 - fp: 7807.0000 - tn: 443774.0000 - fn: 14565.0000 - accuracy: 0.9593 - precision: 0.9144 - recall: 0.8512 - auc: 0.9872\n",
            "Epoch 00204: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1692 - tp: 83345.0000 - fp: 7807.0000 - tn: 443774.0000 - fn: 14565.0000 - accuracy: 0.9593 - precision: 0.9144 - recall: 0.8512 - auc: 0.9872 - val_loss: 0.1699 - val_tp: 21558.0000 - val_fp: 2302.0000 - val_tn: 110305.0000 - val_fn: 3208.0000 - val_accuracy: 0.9599 - val_precision: 0.9035 - val_recall: 0.8705 - val_auc: 0.9887\n",
            "Epoch 205/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1699 - tp: 83230.0000 - fp: 7843.0000 - tn: 443648.0000 - fn: 14655.0000 - accuracy: 0.9590 - precision: 0.9139 - recall: 0.8503 - auc: 0.9871\n",
            "Epoch 00205: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1698 - tp: 83251.0000 - fp: 7843.0000 - tn: 443738.0000 - fn: 14659.0000 - accuracy: 0.9590 - precision: 0.9139 - recall: 0.8503 - auc: 0.9871 - val_loss: 0.1684 - val_tp: 21933.0000 - val_fp: 2461.0000 - val_tn: 110146.0000 - val_fn: 2833.0000 - val_accuracy: 0.9615 - val_precision: 0.8991 - val_recall: 0.8856 - val_auc: 0.9892\n",
            "Epoch 206/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1703 - tp: 83094.0000 - fp: 7926.0000 - tn: 443368.0000 - fn: 14732.0000 - accuracy: 0.9587 - precision: 0.9129 - recall: 0.8494 - auc: 0.9870\n",
            "Epoch 00206: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1703 - tp: 83166.0000 - fp: 7928.0000 - tn: 443653.0000 - fn: 14744.0000 - accuracy: 0.9587 - precision: 0.9130 - recall: 0.8494 - auc: 0.9870 - val_loss: 0.1717 - val_tp: 22209.0000 - val_fp: 3034.0000 - val_tn: 109573.0000 - val_fn: 2557.0000 - val_accuracy: 0.9593 - val_precision: 0.8798 - val_recall: 0.8968 - val_auc: 0.9890\n",
            "Epoch 207/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1694 - tp: 83258.0000 - fp: 7808.0000 - tn: 443469.0000 - fn: 14585.0000 - accuracy: 0.9592 - precision: 0.9143 - recall: 0.8509 - auc: 0.9870\n",
            "Epoch 00207: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1694 - tp: 83314.0000 - fp: 7813.0000 - tn: 443768.0000 - fn: 14596.0000 - accuracy: 0.9592 - precision: 0.9143 - recall: 0.8509 - auc: 0.9870 - val_loss: 0.1657 - val_tp: 21116.0000 - val_fp: 1706.0000 - val_tn: 110901.0000 - val_fn: 3650.0000 - val_accuracy: 0.9610 - val_precision: 0.9252 - val_recall: 0.8526 - val_auc: 0.9896\n",
            "Epoch 208/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1690 - tp: 83182.0000 - fp: 7846.0000 - tn: 443037.0000 - fn: 14543.0000 - accuracy: 0.9592 - precision: 0.9138 - recall: 0.8512 - auc: 0.9872\n",
            "Epoch 00208: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1691 - tp: 83339.0000 - fp: 7861.0000 - tn: 443720.0000 - fn: 14571.0000 - accuracy: 0.9592 - precision: 0.9138 - recall: 0.8512 - auc: 0.9872 - val_loss: 0.1703 - val_tp: 21886.0000 - val_fp: 2710.0000 - val_tn: 109897.0000 - val_fn: 2880.0000 - val_accuracy: 0.9593 - val_precision: 0.8898 - val_recall: 0.8837 - val_auc: 0.9885\n",
            "Epoch 209/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1689 - tp: 83307.0000 - fp: 7825.0000 - tn: 443657.0000 - fn: 14587.0000 - accuracy: 0.9592 - precision: 0.9141 - recall: 0.8510 - auc: 0.9871\n",
            "Epoch 00209: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1689 - tp: 83321.0000 - fp: 7827.0000 - tn: 443754.0000 - fn: 14589.0000 - accuracy: 0.9592 - precision: 0.9141 - recall: 0.8510 - auc: 0.9871 - val_loss: 0.1697 - val_tp: 22313.0000 - val_fp: 3046.0000 - val_tn: 109561.0000 - val_fn: 2453.0000 - val_accuracy: 0.9600 - val_precision: 0.8799 - val_recall: 0.9010 - val_auc: 0.9899\n",
            "Epoch 210/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1688 - tp: 83297.0000 - fp: 7786.0000 - tn: 443698.0000 - fn: 14595.0000 - accuracy: 0.9593 - precision: 0.9145 - recall: 0.8509 - auc: 0.9872\n",
            "Epoch 00210: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1688 - tp: 83312.0000 - fp: 7789.0000 - tn: 443792.0000 - fn: 14598.0000 - accuracy: 0.9593 - precision: 0.9145 - recall: 0.8509 - auc: 0.9872 - val_loss: 0.1644 - val_tp: 21871.0000 - val_fp: 2236.0000 - val_tn: 110371.0000 - val_fn: 2895.0000 - val_accuracy: 0.9626 - val_precision: 0.9072 - val_recall: 0.8831 - val_auc: 0.9899\n",
            "Epoch 211/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1682 - tp: 83323.0000 - fp: 7772.0000 - tn: 443717.0000 - fn: 14564.0000 - accuracy: 0.9593 - precision: 0.9147 - recall: 0.8512 - auc: 0.9873\n",
            "Epoch 00211: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1682 - tp: 83343.0000 - fp: 7773.0000 - tn: 443808.0000 - fn: 14567.0000 - accuracy: 0.9593 - precision: 0.9147 - recall: 0.8512 - auc: 0.9873 - val_loss: 0.1653 - val_tp: 22137.0000 - val_fp: 2642.0000 - val_tn: 109965.0000 - val_fn: 2629.0000 - val_accuracy: 0.9616 - val_precision: 0.8934 - val_recall: 0.8938 - val_auc: 0.9900\n",
            "Epoch 212/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1689 - tp: 83223.0000 - fp: 7722.0000 - tn: 442709.0000 - fn: 14442.0000 - accuracy: 0.9596 - precision: 0.9151 - recall: 0.8521 - auc: 0.9872\n",
            "Epoch 00212: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1689 - tp: 83430.0000 - fp: 7746.0000 - tn: 443835.0000 - fn: 14480.0000 - accuracy: 0.9596 - precision: 0.9150 - recall: 0.8521 - auc: 0.9872 - val_loss: 0.1637 - val_tp: 21497.0000 - val_fp: 2014.0000 - val_tn: 110593.0000 - val_fn: 3269.0000 - val_accuracy: 0.9615 - val_precision: 0.9143 - val_recall: 0.8680 - val_auc: 0.9896\n",
            "Epoch 213/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1682 - tp: 83376.0000 - fp: 7789.0000 - tn: 443792.0000 - fn: 14534.0000 - accuracy: 0.9594 - precision: 0.9146 - recall: 0.8516 - auc: 0.9874\n",
            "Epoch 00213: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1682 - tp: 83376.0000 - fp: 7789.0000 - tn: 443792.0000 - fn: 14534.0000 - accuracy: 0.9594 - precision: 0.9146 - recall: 0.8516 - auc: 0.9874 - val_loss: 0.1664 - val_tp: 22123.0000 - val_fp: 2713.0000 - val_tn: 109894.0000 - val_fn: 2643.0000 - val_accuracy: 0.9610 - val_precision: 0.8908 - val_recall: 0.8933 - val_auc: 0.9900\n",
            "Epoch 214/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1675 - tp: 83380.0000 - fp: 7681.0000 - tn: 442954.0000 - fn: 14337.0000 - accuracy: 0.9598 - precision: 0.9156 - recall: 0.8533 - auc: 0.9875\n",
            "Epoch 00214: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1675 - tp: 83547.0000 - fp: 7704.0000 - tn: 443877.0000 - fn: 14363.0000 - accuracy: 0.9598 - precision: 0.9156 - recall: 0.8533 - auc: 0.9875 - val_loss: 0.1756 - val_tp: 22156.0000 - val_fp: 3255.0000 - val_tn: 109352.0000 - val_fn: 2610.0000 - val_accuracy: 0.9573 - val_precision: 0.8719 - val_recall: 0.8946 - val_auc: 0.9884\n",
            "Epoch 215/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1678 - tp: 83516.0000 - fp: 7721.0000 - tn: 443325.0000 - fn: 14302.0000 - accuracy: 0.9599 - precision: 0.9154 - recall: 0.8538 - auc: 0.9874\n",
            "Epoch 00215: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1678 - tp: 83589.0000 - fp: 7732.0000 - tn: 443849.0000 - fn: 14321.0000 - accuracy: 0.9599 - precision: 0.9153 - recall: 0.8537 - auc: 0.9874 - val_loss: 0.1682 - val_tp: 21906.0000 - val_fp: 2535.0000 - val_tn: 110072.0000 - val_fn: 2860.0000 - val_accuracy: 0.9607 - val_precision: 0.8963 - val_recall: 0.8845 - val_auc: 0.9893\n",
            "Epoch 216/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1677 - tp: 83435.0000 - fp: 7797.0000 - tn: 443277.0000 - fn: 14355.0000 - accuracy: 0.9596 - precision: 0.9145 - recall: 0.8532 - auc: 0.9875\n",
            "Epoch 00216: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1677 - tp: 83528.0000 - fp: 7809.0000 - tn: 443772.0000 - fn: 14382.0000 - accuracy: 0.9596 - precision: 0.9145 - recall: 0.8531 - auc: 0.9875 - val_loss: 0.1779 - val_tp: 22169.0000 - val_fp: 3259.0000 - val_tn: 109348.0000 - val_fn: 2597.0000 - val_accuracy: 0.9574 - val_precision: 0.8718 - val_recall: 0.8951 - val_auc: 0.9883\n",
            "Epoch 217/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1677 - tp: 83410.0000 - fp: 7649.0000 - tn: 442974.0000 - fn: 14319.0000 - accuracy: 0.9599 - precision: 0.9160 - recall: 0.8535 - auc: 0.9874\n",
            "Epoch 00217: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1677 - tp: 83563.0000 - fp: 7666.0000 - tn: 443915.0000 - fn: 14347.0000 - accuracy: 0.9599 - precision: 0.9160 - recall: 0.8535 - auc: 0.9874 - val_loss: 0.1640 - val_tp: 21743.0000 - val_fp: 2166.0000 - val_tn: 110441.0000 - val_fn: 3023.0000 - val_accuracy: 0.9622 - val_precision: 0.9094 - val_recall: 0.8779 - val_auc: 0.9902\n",
            "Epoch 218/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1675 - tp: 83392.0000 - fp: 7681.0000 - tn: 443194.0000 - fn: 14341.0000 - accuracy: 0.9599 - precision: 0.9157 - recall: 0.8533 - auc: 0.9875\n",
            "Epoch 00218: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1675 - tp: 83545.0000 - fp: 7694.0000 - tn: 443887.0000 - fn: 14365.0000 - accuracy: 0.9599 - precision: 0.9157 - recall: 0.8533 - auc: 0.9875 - val_loss: 0.1711 - val_tp: 22216.0000 - val_fp: 2948.0000 - val_tn: 109659.0000 - val_fn: 2550.0000 - val_accuracy: 0.9600 - val_precision: 0.8828 - val_recall: 0.8970 - val_auc: 0.9894\n",
            "Epoch 219/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1668 - tp: 83553.0000 - fp: 7780.0000 - tn: 442649.0000 - fn: 14114.0000 - accuracy: 0.9601 - precision: 0.9148 - recall: 0.8555 - auc: 0.9877\n",
            "Epoch 00219: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1668 - tp: 83764.0000 - fp: 7805.0000 - tn: 443776.0000 - fn: 14146.0000 - accuracy: 0.9601 - precision: 0.9148 - recall: 0.8555 - auc: 0.9877 - val_loss: 0.1661 - val_tp: 22231.0000 - val_fp: 2772.0000 - val_tn: 109835.0000 - val_fn: 2535.0000 - val_accuracy: 0.9614 - val_precision: 0.8891 - val_recall: 0.8976 - val_auc: 0.9899\n",
            "Epoch 220/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1669 - tp: 83738.0000 - fp: 7817.0000 - tn: 443764.0000 - fn: 14172.0000 - accuracy: 0.9600 - precision: 0.9146 - recall: 0.8553 - auc: 0.9877\n",
            "Epoch 00220: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1669 - tp: 83738.0000 - fp: 7817.0000 - tn: 443764.0000 - fn: 14172.0000 - accuracy: 0.9600 - precision: 0.9146 - recall: 0.8553 - auc: 0.9877 - val_loss: 0.1674 - val_tp: 21821.0000 - val_fp: 2406.0000 - val_tn: 110201.0000 - val_fn: 2945.0000 - val_accuracy: 0.9610 - val_precision: 0.9007 - val_recall: 0.8811 - val_auc: 0.9898\n",
            "Epoch 221/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1662 - tp: 83739.0000 - fp: 7750.0000 - tn: 443525.0000 - fn: 14106.0000 - accuracy: 0.9602 - precision: 0.9153 - recall: 0.8558 - auc: 0.9878\n",
            "Epoch 00221: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1661 - tp: 83799.0000 - fp: 7752.0000 - tn: 443829.0000 - fn: 14111.0000 - accuracy: 0.9602 - precision: 0.9153 - recall: 0.8559 - auc: 0.9879 - val_loss: 0.1679 - val_tp: 22438.0000 - val_fp: 3145.0000 - val_tn: 109462.0000 - val_fn: 2328.0000 - val_accuracy: 0.9602 - val_precision: 0.8771 - val_recall: 0.9060 - val_auc: 0.9897\n",
            "Epoch 222/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1666 - tp: 83481.0000 - fp: 7618.0000 - tn: 442827.0000 - fn: 14170.0000 - accuracy: 0.9602 - precision: 0.9164 - recall: 0.8549 - auc: 0.9876\n",
            "Epoch 00222: val_auc did not improve from 0.99069\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1667 - tp: 83681.0000 - fp: 7638.0000 - tn: 443943.0000 - fn: 14229.0000 - accuracy: 0.9602 - precision: 0.9164 - recall: 0.8547 - auc: 0.9876 - val_loss: 0.1645 - val_tp: 21981.0000 - val_fp: 2407.0000 - val_tn: 110200.0000 - val_fn: 2785.0000 - val_accuracy: 0.9622 - val_precision: 0.9013 - val_recall: 0.8875 - val_auc: 0.9902\n",
            "Epoch 223/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1667 - tp: 83722.0000 - fp: 7778.0000 - tn: 443266.0000 - fn: 14098.0000 - accuracy: 0.9601 - precision: 0.9150 - recall: 0.8559 - auc: 0.9877\n",
            "Epoch 00223: val_auc improved from 0.99069 to 0.99090, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1667 - tp: 83799.0000 - fp: 7788.0000 - tn: 443793.0000 - fn: 14111.0000 - accuracy: 0.9601 - precision: 0.9150 - recall: 0.8559 - auc: 0.9877 - val_loss: 0.1587 - val_tp: 21668.0000 - val_fp: 1874.0000 - val_tn: 110733.0000 - val_fn: 3098.0000 - val_accuracy: 0.9638 - val_precision: 0.9204 - val_recall: 0.8749 - val_auc: 0.9909\n",
            "Epoch 224/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1660 - tp: 83525.0000 - fp: 7618.0000 - tn: 442806.0000 - fn: 14147.0000 - accuracy: 0.9603 - precision: 0.9164 - recall: 0.8552 - auc: 0.9879\n",
            "Epoch 00224: val_auc did not improve from 0.99090\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1660 - tp: 83731.0000 - fp: 7644.0000 - tn: 443937.0000 - fn: 14179.0000 - accuracy: 0.9603 - precision: 0.9163 - recall: 0.8552 - auc: 0.9879 - val_loss: 0.1702 - val_tp: 22392.0000 - val_fp: 3101.0000 - val_tn: 109506.0000 - val_fn: 2374.0000 - val_accuracy: 0.9601 - val_precision: 0.8784 - val_recall: 0.9041 - val_auc: 0.9898\n",
            "Epoch 225/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1665 - tp: 83607.0000 - fp: 7633.0000 - tn: 443856.0000 - fn: 14280.0000 - accuracy: 0.9601 - precision: 0.9163 - recall: 0.8541 - auc: 0.9877\n",
            "Epoch 00225: val_auc did not improve from 0.99090\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1665 - tp: 83626.0000 - fp: 7635.0000 - tn: 443946.0000 - fn: 14284.0000 - accuracy: 0.9601 - precision: 0.9163 - recall: 0.8541 - auc: 0.9877 - val_loss: 0.1638 - val_tp: 21897.0000 - val_fp: 2259.0000 - val_tn: 110348.0000 - val_fn: 2869.0000 - val_accuracy: 0.9627 - val_precision: 0.9065 - val_recall: 0.8842 - val_auc: 0.9903\n",
            "Epoch 226/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1657 - tp: 83772.0000 - fp: 7738.0000 - tn: 443321.0000 - fn: 14033.0000 - accuracy: 0.9603 - precision: 0.9154 - recall: 0.8565 - auc: 0.9880\n",
            "Epoch 00226: val_auc did not improve from 0.99090\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1658 - tp: 83861.0000 - fp: 7752.0000 - tn: 443829.0000 - fn: 14049.0000 - accuracy: 0.9603 - precision: 0.9154 - recall: 0.8565 - auc: 0.9880 - val_loss: 0.1664 - val_tp: 21669.0000 - val_fp: 2186.0000 - val_tn: 110421.0000 - val_fn: 3097.0000 - val_accuracy: 0.9615 - val_precision: 0.9084 - val_recall: 0.8749 - val_auc: 0.9893\n",
            "Epoch 227/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1654 - tp: 83786.0000 - fp: 7572.0000 - tn: 443702.0000 - fn: 14060.0000 - accuracy: 0.9606 - precision: 0.9171 - recall: 0.8563 - auc: 0.9879\n",
            "Epoch 00227: val_auc did not improve from 0.99090\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1654 - tp: 83836.0000 - fp: 7575.0000 - tn: 444006.0000 - fn: 14074.0000 - accuracy: 0.9606 - precision: 0.9171 - recall: 0.8563 - auc: 0.9879 - val_loss: 0.1629 - val_tp: 21932.0000 - val_fp: 2265.0000 - val_tn: 110342.0000 - val_fn: 2834.0000 - val_accuracy: 0.9629 - val_precision: 0.9064 - val_recall: 0.8856 - val_auc: 0.9906\n",
            "Epoch 228/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1648 - tp: 83975.0000 - fp: 7548.0000 - tn: 443528.0000 - fn: 13813.0000 - accuracy: 0.9611 - precision: 0.9175 - recall: 0.8587 - auc: 0.9881\n",
            "Epoch 00228: val_auc did not improve from 0.99090\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1648 - tp: 84075.0000 - fp: 7557.0000 - tn: 444024.0000 - fn: 13835.0000 - accuracy: 0.9611 - precision: 0.9175 - recall: 0.8587 - auc: 0.9881 - val_loss: 0.1752 - val_tp: 22624.0000 - val_fp: 3606.0000 - val_tn: 109001.0000 - val_fn: 2142.0000 - val_accuracy: 0.9582 - val_precision: 0.8625 - val_recall: 0.9135 - val_auc: 0.9896\n",
            "Epoch 229/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1643 - tp: 83880.0000 - fp: 7677.0000 - tn: 443387.0000 - fn: 13920.0000 - accuracy: 0.9607 - precision: 0.9162 - recall: 0.8577 - auc: 0.9884\n",
            "Epoch 00229: val_auc did not improve from 0.99090\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1643 - tp: 83976.0000 - fp: 7684.0000 - tn: 443897.0000 - fn: 13934.0000 - accuracy: 0.9607 - precision: 0.9162 - recall: 0.8577 - auc: 0.9884 - val_loss: 0.1695 - val_tp: 22331.0000 - val_fp: 3096.0000 - val_tn: 109511.0000 - val_fn: 2435.0000 - val_accuracy: 0.9597 - val_precision: 0.8782 - val_recall: 0.9017 - val_auc: 0.9899\n",
            "Epoch 230/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1651 - tp: 83913.0000 - fp: 7694.0000 - tn: 443791.0000 - fn: 13978.0000 - accuracy: 0.9606 - precision: 0.9160 - recall: 0.8572 - auc: 0.9881\n",
            "Epoch 00230: val_auc improved from 0.99090 to 0.99095, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1651 - tp: 83929.0000 - fp: 7698.0000 - tn: 443883.0000 - fn: 13981.0000 - accuracy: 0.9605 - precision: 0.9160 - recall: 0.8572 - auc: 0.9881 - val_loss: 0.1592 - val_tp: 22112.0000 - val_fp: 2372.0000 - val_tn: 110235.0000 - val_fn: 2654.0000 - val_accuracy: 0.9634 - val_precision: 0.9031 - val_recall: 0.8928 - val_auc: 0.9909\n",
            "Epoch 231/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1641 - tp: 84108.0000 - fp: 7574.0000 - tn: 443698.0000 - fn: 13740.0000 - accuracy: 0.9612 - precision: 0.9174 - recall: 0.8596 - auc: 0.9883\n",
            "Epoch 00231: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1641 - tp: 84159.0000 - fp: 7580.0000 - tn: 444001.0000 - fn: 13751.0000 - accuracy: 0.9612 - precision: 0.9174 - recall: 0.8596 - auc: 0.9883 - val_loss: 0.1643 - val_tp: 21957.0000 - val_fp: 2417.0000 - val_tn: 110190.0000 - val_fn: 2809.0000 - val_accuracy: 0.9620 - val_precision: 0.9008 - val_recall: 0.8866 - val_auc: 0.9899\n",
            "Epoch 232/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1652 - tp: 83912.0000 - fp: 7700.0000 - tn: 443347.0000 - fn: 13905.0000 - accuracy: 0.9606 - precision: 0.9159 - recall: 0.8578 - auc: 0.9881\n",
            "Epoch 00232: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1652 - tp: 83994.0000 - fp: 7709.0000 - tn: 443872.0000 - fn: 13916.0000 - accuracy: 0.9606 - precision: 0.9159 - recall: 0.8579 - auc: 0.9881 - val_loss: 0.1620 - val_tp: 21730.0000 - val_fp: 2112.0000 - val_tn: 110495.0000 - val_fn: 3036.0000 - val_accuracy: 0.9625 - val_precision: 0.9114 - val_recall: 0.8774 - val_auc: 0.9899\n",
            "Epoch 233/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1633 - tp: 84157.0000 - fp: 7613.0000 - tn: 443241.0000 - fn: 13597.0000 - accuracy: 0.9613 - precision: 0.9170 - recall: 0.8609 - auc: 0.9885\n",
            "Epoch 00233: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1633 - tp: 84289.0000 - fp: 7619.0000 - tn: 443962.0000 - fn: 13621.0000 - accuracy: 0.9613 - precision: 0.9171 - recall: 0.8609 - auc: 0.9885 - val_loss: 0.1706 - val_tp: 22356.0000 - val_fp: 3035.0000 - val_tn: 109572.0000 - val_fn: 2410.0000 - val_accuracy: 0.9604 - val_precision: 0.8805 - val_recall: 0.9027 - val_auc: 0.9896\n",
            "Epoch 234/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1645 - tp: 83858.0000 - fp: 7659.0000 - tn: 443192.0000 - fn: 13899.0000 - accuracy: 0.9607 - precision: 0.9163 - recall: 0.8578 - auc: 0.9883\n",
            "Epoch 00234: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1645 - tp: 83992.0000 - fp: 7666.0000 - tn: 443915.0000 - fn: 13918.0000 - accuracy: 0.9607 - precision: 0.9164 - recall: 0.8578 - auc: 0.9883 - val_loss: 0.1672 - val_tp: 22169.0000 - val_fp: 2699.0000 - val_tn: 109908.0000 - val_fn: 2597.0000 - val_accuracy: 0.9614 - val_precision: 0.8915 - val_recall: 0.8951 - val_auc: 0.9897\n",
            "Epoch 235/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1635 - tp: 84027.0000 - fp: 7559.0000 - tn: 442850.0000 - fn: 13660.0000 - accuracy: 0.9613 - precision: 0.9175 - recall: 0.8602 - auc: 0.9885\n",
            "Epoch 00235: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1635 - tp: 84224.0000 - fp: 7583.0000 - tn: 443998.0000 - fn: 13686.0000 - accuracy: 0.9613 - precision: 0.9174 - recall: 0.8602 - auc: 0.9885 - val_loss: 0.1642 - val_tp: 22215.0000 - val_fp: 2708.0000 - val_tn: 109899.0000 - val_fn: 2551.0000 - val_accuracy: 0.9617 - val_precision: 0.8913 - val_recall: 0.8970 - val_auc: 0.9902\n",
            "Epoch 236/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1639 - tp: 84010.0000 - fp: 7644.0000 - tn: 443003.0000 - fn: 13695.0000 - accuracy: 0.9611 - precision: 0.9166 - recall: 0.8598 - auc: 0.9883\n",
            "Epoch 00236: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1639 - tp: 84183.0000 - fp: 7649.0000 - tn: 443932.0000 - fn: 13727.0000 - accuracy: 0.9611 - precision: 0.9167 - recall: 0.8598 - auc: 0.9883 - val_loss: 0.1611 - val_tp: 21691.0000 - val_fp: 2067.0000 - val_tn: 110540.0000 - val_fn: 3075.0000 - val_accuracy: 0.9626 - val_precision: 0.9130 - val_recall: 0.8758 - val_auc: 0.9904\n",
            "Epoch 237/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1638 - tp: 84023.0000 - fp: 7667.0000 - tn: 442984.0000 - fn: 13678.0000 - accuracy: 0.9611 - precision: 0.9164 - recall: 0.8600 - auc: 0.9883\n",
            "Epoch 00237: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1638 - tp: 84198.0000 - fp: 7677.0000 - tn: 443904.0000 - fn: 13712.0000 - accuracy: 0.9611 - precision: 0.9164 - recall: 0.8600 - auc: 0.9883 - val_loss: 0.1833 - val_tp: 22643.0000 - val_fp: 4062.0000 - val_tn: 108545.0000 - val_fn: 2123.0000 - val_accuracy: 0.9550 - val_precision: 0.8479 - val_recall: 0.9143 - val_auc: 0.9888\n",
            "Epoch 238/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1643 - tp: 83855.0000 - fp: 7758.0000 - tn: 442694.0000 - fn: 13789.0000 - accuracy: 0.9607 - precision: 0.9153 - recall: 0.8588 - auc: 0.9882\n",
            "Epoch 00238: val_auc did not improve from 0.99095\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1644 - tp: 84076.0000 - fp: 7786.0000 - tn: 443795.0000 - fn: 13834.0000 - accuracy: 0.9607 - precision: 0.9152 - recall: 0.8587 - auc: 0.9882 - val_loss: 0.1714 - val_tp: 21725.0000 - val_fp: 2573.0000 - val_tn: 110034.0000 - val_fn: 3041.0000 - val_accuracy: 0.9591 - val_precision: 0.8941 - val_recall: 0.8772 - val_auc: 0.9887\n",
            "Epoch 239/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1634 - tp: 84188.0000 - fp: 7572.0000 - tn: 444009.0000 - fn: 13722.0000 - accuracy: 0.9612 - precision: 0.9175 - recall: 0.8599 - auc: 0.9885\n",
            "Epoch 00239: val_auc improved from 0.99095 to 0.99133, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1634 - tp: 84188.0000 - fp: 7572.0000 - tn: 444009.0000 - fn: 13722.0000 - accuracy: 0.9612 - precision: 0.9175 - recall: 0.8599 - auc: 0.9885 - val_loss: 0.1572 - val_tp: 21932.0000 - val_fp: 2028.0000 - val_tn: 110579.0000 - val_fn: 2834.0000 - val_accuracy: 0.9646 - val_precision: 0.9154 - val_recall: 0.8856 - val_auc: 0.9913\n",
            "Epoch 240/2000\n",
            "2141/2147 [============================>.] - ETA: 0s - loss: 0.1632 - tp: 84002.0000 - fp: 7696.0000 - tn: 442745.0000 - fn: 13653.0000 - accuracy: 0.9610 - precision: 0.9161 - recall: 0.8602 - auc: 0.9885\n",
            "Epoch 00240: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1632 - tp: 84223.0000 - fp: 7714.0000 - tn: 443867.0000 - fn: 13687.0000 - accuracy: 0.9611 - precision: 0.9161 - recall: 0.8602 - auc: 0.9885 - val_loss: 0.1634 - val_tp: 21910.0000 - val_fp: 2345.0000 - val_tn: 110262.0000 - val_fn: 2856.0000 - val_accuracy: 0.9621 - val_precision: 0.9033 - val_recall: 0.8847 - val_auc: 0.9902\n",
            "Epoch 241/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1627 - tp: 84196.0000 - fp: 7531.0000 - tn: 443118.0000 - fn: 13507.0000 - accuracy: 0.9616 - precision: 0.9179 - recall: 0.8618 - auc: 0.9887\n",
            "Epoch 00241: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1627 - tp: 84372.0000 - fp: 7548.0000 - tn: 444033.0000 - fn: 13538.0000 - accuracy: 0.9616 - precision: 0.9179 - recall: 0.8617 - auc: 0.9887 - val_loss: 0.1613 - val_tp: 22087.0000 - val_fp: 2387.0000 - val_tn: 110220.0000 - val_fn: 2679.0000 - val_accuracy: 0.9631 - val_precision: 0.9025 - val_recall: 0.8918 - val_auc: 0.9906\n",
            "Epoch 242/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1626 - tp: 84168.0000 - fp: 7470.0000 - tn: 443383.0000 - fn: 13587.0000 - accuracy: 0.9616 - precision: 0.9185 - recall: 0.8610 - auc: 0.9886\n",
            "Epoch 00242: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1626 - tp: 84306.0000 - fp: 7489.0000 - tn: 444092.0000 - fn: 13604.0000 - accuracy: 0.9616 - precision: 0.9184 - recall: 0.8611 - auc: 0.9886 - val_loss: 0.1608 - val_tp: 22443.0000 - val_fp: 2738.0000 - val_tn: 109869.0000 - val_fn: 2323.0000 - val_accuracy: 0.9632 - val_precision: 0.8913 - val_recall: 0.9062 - val_auc: 0.9912\n",
            "Epoch 243/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1625 - tp: 84380.0000 - fp: 7606.0000 - tn: 443975.0000 - fn: 13530.0000 - accuracy: 0.9615 - precision: 0.9173 - recall: 0.8618 - auc: 0.9886\n",
            "Epoch 00243: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1625 - tp: 84380.0000 - fp: 7606.0000 - tn: 443975.0000 - fn: 13530.0000 - accuracy: 0.9615 - precision: 0.9173 - recall: 0.8618 - auc: 0.9886 - val_loss: 0.1657 - val_tp: 22591.0000 - val_fp: 3153.0000 - val_tn: 109454.0000 - val_fn: 2175.0000 - val_accuracy: 0.9612 - val_precision: 0.8775 - val_recall: 0.9122 - val_auc: 0.9908\n",
            "Epoch 244/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1631 - tp: 84199.0000 - fp: 7684.0000 - tn: 443381.0000 - fn: 13600.0000 - accuracy: 0.9612 - precision: 0.9164 - recall: 0.8609 - auc: 0.9885\n",
            "Epoch 00244: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1631 - tp: 84301.0000 - fp: 7692.0000 - tn: 443889.0000 - fn: 13609.0000 - accuracy: 0.9612 - precision: 0.9164 - recall: 0.8610 - auc: 0.9885 - val_loss: 0.1744 - val_tp: 22673.0000 - val_fp: 3563.0000 - val_tn: 109044.0000 - val_fn: 2093.0000 - val_accuracy: 0.9588 - val_precision: 0.8642 - val_recall: 0.9155 - val_auc: 0.9900\n",
            "Epoch 245/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1627 - tp: 84192.0000 - fp: 7614.0000 - tn: 443878.0000 - fn: 13692.0000 - accuracy: 0.9612 - precision: 0.9171 - recall: 0.8601 - auc: 0.9886\n",
            "Epoch 00245: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1627 - tp: 84214.0000 - fp: 7616.0000 - tn: 443965.0000 - fn: 13696.0000 - accuracy: 0.9612 - precision: 0.9171 - recall: 0.8601 - auc: 0.9886 - val_loss: 0.1701 - val_tp: 22644.0000 - val_fp: 3449.0000 - val_tn: 109158.0000 - val_fn: 2122.0000 - val_accuracy: 0.9594 - val_precision: 0.8678 - val_recall: 0.9143 - val_auc: 0.9905\n",
            "Epoch 246/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1621 - tp: 84447.0000 - fp: 7608.0000 - tn: 443973.0000 - fn: 13463.0000 - accuracy: 0.9617 - precision: 0.9174 - recall: 0.8625 - auc: 0.9887\n",
            "Epoch 00246: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1621 - tp: 84447.0000 - fp: 7608.0000 - tn: 443973.0000 - fn: 13463.0000 - accuracy: 0.9617 - precision: 0.9174 - recall: 0.8625 - auc: 0.9887 - val_loss: 0.1771 - val_tp: 22188.0000 - val_fp: 3390.0000 - val_tn: 109217.0000 - val_fn: 2578.0000 - val_accuracy: 0.9566 - val_precision: 0.8675 - val_recall: 0.8959 - val_auc: 0.9883\n",
            "Epoch 247/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1623 - tp: 84126.0000 - fp: 7645.0000 - tn: 443004.0000 - fn: 13577.0000 - accuracy: 0.9613 - precision: 0.9167 - recall: 0.8610 - auc: 0.9887\n",
            "Epoch 00247: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1624 - tp: 84300.0000 - fp: 7658.0000 - tn: 443923.0000 - fn: 13610.0000 - accuracy: 0.9613 - precision: 0.9167 - recall: 0.8610 - auc: 0.9887 - val_loss: 0.1620 - val_tp: 21603.0000 - val_fp: 2041.0000 - val_tn: 110566.0000 - val_fn: 3163.0000 - val_accuracy: 0.9621 - val_precision: 0.9137 - val_recall: 0.8723 - val_auc: 0.9902\n",
            "Epoch 248/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1613 - tp: 84362.0000 - fp: 7468.0000 - tn: 444012.0000 - fn: 13534.0000 - accuracy: 0.9618 - precision: 0.9187 - recall: 0.8618 - auc: 0.9889\n",
            "Epoch 00248: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1613 - tp: 84375.0000 - fp: 7470.0000 - tn: 444111.0000 - fn: 13535.0000 - accuracy: 0.9618 - precision: 0.9187 - recall: 0.8618 - auc: 0.9889 - val_loss: 0.1645 - val_tp: 22277.0000 - val_fp: 2741.0000 - val_tn: 109866.0000 - val_fn: 2489.0000 - val_accuracy: 0.9619 - val_precision: 0.8904 - val_recall: 0.8995 - val_auc: 0.9901\n",
            "Epoch 249/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1614 - tp: 84460.0000 - fp: 7570.0000 - tn: 443916.0000 - fn: 13430.0000 - accuracy: 0.9618 - precision: 0.9177 - recall: 0.8628 - auc: 0.9889\n",
            "Epoch 00249: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1614 - tp: 84479.0000 - fp: 7571.0000 - tn: 444010.0000 - fn: 13431.0000 - accuracy: 0.9618 - precision: 0.9178 - recall: 0.8628 - auc: 0.9889 - val_loss: 0.1681 - val_tp: 22447.0000 - val_fp: 3160.0000 - val_tn: 109447.0000 - val_fn: 2319.0000 - val_accuracy: 0.9601 - val_precision: 0.8766 - val_recall: 0.9064 - val_auc: 0.9900\n",
            "Epoch 250/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1614 - tp: 84429.0000 - fp: 7551.0000 - tn: 443736.0000 - fn: 13404.0000 - accuracy: 0.9618 - precision: 0.9179 - recall: 0.8630 - auc: 0.9889\n",
            "Epoch 00250: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1614 - tp: 84497.0000 - fp: 7558.0000 - tn: 444023.0000 - fn: 13413.0000 - accuracy: 0.9618 - precision: 0.9179 - recall: 0.8630 - auc: 0.9889 - val_loss: 0.1683 - val_tp: 21803.0000 - val_fp: 2621.0000 - val_tn: 109986.0000 - val_fn: 2963.0000 - val_accuracy: 0.9594 - val_precision: 0.8927 - val_recall: 0.8804 - val_auc: 0.9890\n",
            "Epoch 251/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1614 - tp: 84375.0000 - fp: 7511.0000 - tn: 443567.0000 - fn: 13411.0000 - accuracy: 0.9619 - precision: 0.9183 - recall: 0.8629 - auc: 0.9889\n",
            "Epoch 00251: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1615 - tp: 84479.0000 - fp: 7520.0000 - tn: 444061.0000 - fn: 13431.0000 - accuracy: 0.9619 - precision: 0.9183 - recall: 0.8628 - auc: 0.9889 - val_loss: 0.1780 - val_tp: 22508.0000 - val_fp: 3633.0000 - val_tn: 108974.0000 - val_fn: 2258.0000 - val_accuracy: 0.9571 - val_precision: 0.8610 - val_recall: 0.9088 - val_auc: 0.9889\n",
            "Epoch 252/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1609 - tp: 84586.0000 - fp: 7483.0000 - tn: 443575.0000 - fn: 13220.0000 - accuracy: 0.9623 - precision: 0.9187 - recall: 0.8648 - auc: 0.9889\n",
            "Epoch 00252: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1610 - tp: 84669.0000 - fp: 7490.0000 - tn: 444091.0000 - fn: 13241.0000 - accuracy: 0.9623 - precision: 0.9187 - recall: 0.8648 - auc: 0.9889 - val_loss: 0.1601 - val_tp: 22219.0000 - val_fp: 2405.0000 - val_tn: 110202.0000 - val_fn: 2547.0000 - val_accuracy: 0.9640 - val_precision: 0.9023 - val_recall: 0.8972 - val_auc: 0.9910\n",
            "Epoch 253/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1611 - tp: 84597.0000 - fp: 7378.0000 - tn: 444203.0000 - fn: 13313.0000 - accuracy: 0.9623 - precision: 0.9198 - recall: 0.8640 - auc: 0.9890\n",
            "Epoch 00253: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1611 - tp: 84597.0000 - fp: 7378.0000 - tn: 444203.0000 - fn: 13313.0000 - accuracy: 0.9623 - precision: 0.9198 - recall: 0.8640 - auc: 0.9890 - val_loss: 0.1701 - val_tp: 22296.0000 - val_fp: 3154.0000 - val_tn: 109453.0000 - val_fn: 2470.0000 - val_accuracy: 0.9591 - val_precision: 0.8761 - val_recall: 0.9003 - val_auc: 0.9896\n",
            "Epoch 254/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1604 - tp: 84699.0000 - fp: 7586.0000 - tn: 443902.0000 - fn: 13189.0000 - accuracy: 0.9622 - precision: 0.9178 - recall: 0.8653 - auc: 0.9891\n",
            "Epoch 00254: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1604 - tp: 84718.0000 - fp: 7587.0000 - tn: 443994.0000 - fn: 13192.0000 - accuracy: 0.9622 - precision: 0.9178 - recall: 0.8653 - auc: 0.9891 - val_loss: 0.1691 - val_tp: 22078.0000 - val_fp: 2765.0000 - val_tn: 109842.0000 - val_fn: 2688.0000 - val_accuracy: 0.9603 - val_precision: 0.8887 - val_recall: 0.8915 - val_auc: 0.9895\n",
            "Epoch 255/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1609 - tp: 84373.0000 - fp: 7641.0000 - tn: 443197.0000 - fn: 13397.0000 - accuracy: 0.9617 - precision: 0.9170 - recall: 0.8630 - auc: 0.9890\n",
            "Epoch 00255: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1609 - tp: 84496.0000 - fp: 7657.0000 - tn: 443924.0000 - fn: 13414.0000 - accuracy: 0.9617 - precision: 0.9169 - recall: 0.8630 - auc: 0.9890 - val_loss: 0.1629 - val_tp: 22413.0000 - val_fp: 2852.0000 - val_tn: 109755.0000 - val_fn: 2353.0000 - val_accuracy: 0.9621 - val_precision: 0.8871 - val_recall: 0.9050 - val_auc: 0.9905\n",
            "Epoch 256/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1605 - tp: 84392.0000 - fp: 7420.0000 - tn: 443217.0000 - fn: 13323.0000 - accuracy: 0.9622 - precision: 0.9192 - recall: 0.8637 - auc: 0.9890\n",
            "Epoch 00256: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1605 - tp: 84570.0000 - fp: 7443.0000 - tn: 444138.0000 - fn: 13340.0000 - accuracy: 0.9622 - precision: 0.9191 - recall: 0.8638 - auc: 0.9890 - val_loss: 0.1598 - val_tp: 22027.0000 - val_fp: 2210.0000 - val_tn: 110397.0000 - val_fn: 2739.0000 - val_accuracy: 0.9640 - val_precision: 0.9088 - val_recall: 0.8894 - val_auc: 0.9910\n",
            "Epoch 257/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1595 - tp: 84814.0000 - fp: 7435.0000 - tn: 444146.0000 - fn: 13096.0000 - accuracy: 0.9626 - precision: 0.9194 - recall: 0.8662 - auc: 0.9893\n",
            "Epoch 00257: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1595 - tp: 84814.0000 - fp: 7435.0000 - tn: 444146.0000 - fn: 13096.0000 - accuracy: 0.9626 - precision: 0.9194 - recall: 0.8662 - auc: 0.9893 - val_loss: 0.1600 - val_tp: 22204.0000 - val_fp: 2388.0000 - val_tn: 110219.0000 - val_fn: 2562.0000 - val_accuracy: 0.9640 - val_precision: 0.9029 - val_recall: 0.8966 - val_auc: 0.9909\n",
            "Epoch 258/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1606 - tp: 84445.0000 - fp: 7430.0000 - tn: 443418.0000 - fn: 13315.0000 - accuracy: 0.9622 - precision: 0.9191 - recall: 0.8638 - auc: 0.9891\n",
            "Epoch 00258: val_auc did not improve from 0.99133\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1606 - tp: 84563.0000 - fp: 7442.0000 - tn: 444139.0000 - fn: 13347.0000 - accuracy: 0.9622 - precision: 0.9191 - recall: 0.8637 - auc: 0.9890 - val_loss: 0.1649 - val_tp: 21989.0000 - val_fp: 2528.0000 - val_tn: 110079.0000 - val_fn: 2777.0000 - val_accuracy: 0.9614 - val_precision: 0.8969 - val_recall: 0.8879 - val_auc: 0.9896\n",
            "Epoch 259/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1600 - tp: 84522.0000 - fp: 7481.0000 - tn: 443583.0000 - fn: 13278.0000 - accuracy: 0.9622 - precision: 0.9187 - recall: 0.8642 - auc: 0.9892\n",
            "Epoch 00259: val_auc improved from 0.99133 to 0.99140, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1600 - tp: 84618.0000 - fp: 7485.0000 - tn: 444096.0000 - fn: 13292.0000 - accuracy: 0.9622 - precision: 0.9187 - recall: 0.8642 - auc: 0.9892 - val_loss: 0.1567 - val_tp: 21147.0000 - val_fp: 1473.0000 - val_tn: 111134.0000 - val_fn: 3619.0000 - val_accuracy: 0.9629 - val_precision: 0.9349 - val_recall: 0.8539 - val_auc: 0.9914\n",
            "Epoch 260/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1603 - tp: 84599.0000 - fp: 7514.0000 - tn: 443762.0000 - fn: 13245.0000 - accuracy: 0.9622 - precision: 0.9184 - recall: 0.8646 - auc: 0.9891\n",
            "Epoch 00260: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1602 - tp: 84659.0000 - fp: 7519.0000 - tn: 444062.0000 - fn: 13251.0000 - accuracy: 0.9622 - precision: 0.9184 - recall: 0.8647 - auc: 0.9891 - val_loss: 0.1613 - val_tp: 22350.0000 - val_fp: 2689.0000 - val_tn: 109918.0000 - val_fn: 2416.0000 - val_accuracy: 0.9628 - val_precision: 0.8926 - val_recall: 0.9024 - val_auc: 0.9906\n",
            "Epoch 261/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1598 - tp: 84542.0000 - fp: 7447.0000 - tn: 443215.0000 - fn: 13148.0000 - accuracy: 0.9624 - precision: 0.9190 - recall: 0.8654 - auc: 0.9893\n",
            "Epoch 00261: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1598 - tp: 84737.0000 - fp: 7471.0000 - tn: 444110.0000 - fn: 13173.0000 - accuracy: 0.9624 - precision: 0.9190 - recall: 0.8655 - auc: 0.9893 - val_loss: 0.1702 - val_tp: 22584.0000 - val_fp: 3437.0000 - val_tn: 109170.0000 - val_fn: 2182.0000 - val_accuracy: 0.9591 - val_precision: 0.8679 - val_recall: 0.9119 - val_auc: 0.9898\n",
            "Epoch 262/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1602 - tp: 84694.0000 - fp: 7598.0000 - tn: 443885.0000 - fn: 13199.0000 - accuracy: 0.9621 - precision: 0.9177 - recall: 0.8652 - auc: 0.9892\n",
            "Epoch 00262: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1602 - tp: 84711.0000 - fp: 7602.0000 - tn: 443979.0000 - fn: 13199.0000 - accuracy: 0.9621 - precision: 0.9176 - recall: 0.8652 - auc: 0.9892 - val_loss: 0.1654 - val_tp: 22563.0000 - val_fp: 3070.0000 - val_tn: 109537.0000 - val_fn: 2203.0000 - val_accuracy: 0.9616 - val_precision: 0.8802 - val_recall: 0.9110 - val_auc: 0.9905\n",
            "Epoch 263/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1595 - tp: 84792.0000 - fp: 7434.0000 - tn: 443845.0000 - fn: 13049.0000 - accuracy: 0.9627 - precision: 0.9194 - recall: 0.8666 - auc: 0.9892\n",
            "Epoch 00263: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1595 - tp: 84851.0000 - fp: 7436.0000 - tn: 444145.0000 - fn: 13059.0000 - accuracy: 0.9627 - precision: 0.9194 - recall: 0.8666 - auc: 0.9892 - val_loss: 0.1727 - val_tp: 22529.0000 - val_fp: 3499.0000 - val_tn: 109108.0000 - val_fn: 2237.0000 - val_accuracy: 0.9582 - val_precision: 0.8656 - val_recall: 0.9097 - val_auc: 0.9894\n",
            "Epoch 264/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1596 - tp: 84732.0000 - fp: 7607.0000 - tn: 443880.0000 - fn: 13157.0000 - accuracy: 0.9622 - precision: 0.9176 - recall: 0.8656 - auc: 0.9893\n",
            "Epoch 00264: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1596 - tp: 84752.0000 - fp: 7607.0000 - tn: 443974.0000 - fn: 13158.0000 - accuracy: 0.9622 - precision: 0.9176 - recall: 0.8656 - auc: 0.9893 - val_loss: 0.1611 - val_tp: 21944.0000 - val_fp: 2342.0000 - val_tn: 110265.0000 - val_fn: 2822.0000 - val_accuracy: 0.9624 - val_precision: 0.9036 - val_recall: 0.8861 - val_auc: 0.9903\n",
            "Epoch 265/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1598 - tp: 84727.0000 - fp: 7493.0000 - tn: 444088.0000 - fn: 13183.0000 - accuracy: 0.9624 - precision: 0.9187 - recall: 0.8654 - auc: 0.9892\n",
            "Epoch 00265: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1598 - tp: 84727.0000 - fp: 7493.0000 - tn: 444088.0000 - fn: 13183.0000 - accuracy: 0.9624 - precision: 0.9187 - recall: 0.8654 - auc: 0.9892 - val_loss: 0.1604 - val_tp: 22343.0000 - val_fp: 2616.0000 - val_tn: 109991.0000 - val_fn: 2423.0000 - val_accuracy: 0.9633 - val_precision: 0.8952 - val_recall: 0.9022 - val_auc: 0.9909\n",
            "Epoch 266/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1589 - tp: 84836.0000 - fp: 7385.0000 - tn: 444105.0000 - fn: 13050.0000 - accuracy: 0.9628 - precision: 0.9199 - recall: 0.8667 - auc: 0.9894\n",
            "Epoch 00266: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1589 - tp: 84857.0000 - fp: 7386.0000 - tn: 444195.0000 - fn: 13053.0000 - accuracy: 0.9628 - precision: 0.9199 - recall: 0.8667 - auc: 0.9894 - val_loss: 0.1615 - val_tp: 22393.0000 - val_fp: 2729.0000 - val_tn: 109878.0000 - val_fn: 2373.0000 - val_accuracy: 0.9629 - val_precision: 0.8914 - val_recall: 0.9042 - val_auc: 0.9909\n",
            "Epoch 267/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1586 - tp: 84842.0000 - fp: 7448.0000 - tn: 443823.0000 - fn: 13007.0000 - accuracy: 0.9627 - precision: 0.9193 - recall: 0.8671 - auc: 0.9895\n",
            "Epoch 00267: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1586 - tp: 84897.0000 - fp: 7453.0000 - tn: 444128.0000 - fn: 13013.0000 - accuracy: 0.9628 - precision: 0.9193 - recall: 0.8671 - auc: 0.9895 - val_loss: 0.1625 - val_tp: 22591.0000 - val_fp: 2913.0000 - val_tn: 109694.0000 - val_fn: 2175.0000 - val_accuracy: 0.9630 - val_precision: 0.8858 - val_recall: 0.9122 - val_auc: 0.9909\n",
            "Epoch 268/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1586 - tp: 84882.0000 - fp: 7409.0000 - tn: 444172.0000 - fn: 13028.0000 - accuracy: 0.9628 - precision: 0.9197 - recall: 0.8669 - auc: 0.9894\n",
            "Epoch 00268: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1586 - tp: 84882.0000 - fp: 7409.0000 - tn: 444172.0000 - fn: 13028.0000 - accuracy: 0.9628 - precision: 0.9197 - recall: 0.8669 - auc: 0.9894 - val_loss: 0.1682 - val_tp: 22410.0000 - val_fp: 2982.0000 - val_tn: 109625.0000 - val_fn: 2356.0000 - val_accuracy: 0.9611 - val_precision: 0.8826 - val_recall: 0.9049 - val_auc: 0.9901\n",
            "Epoch 269/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1586 - tp: 84874.0000 - fp: 7298.0000 - tn: 444192.0000 - fn: 13012.0000 - accuracy: 0.9630 - precision: 0.9208 - recall: 0.8671 - auc: 0.9895\n",
            "Epoch 00269: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1586 - tp: 84894.0000 - fp: 7299.0000 - tn: 444282.0000 - fn: 13016.0000 - accuracy: 0.9630 - precision: 0.9208 - recall: 0.8671 - auc: 0.9895 - val_loss: 0.1577 - val_tp: 22482.0000 - val_fp: 2668.0000 - val_tn: 109939.0000 - val_fn: 2284.0000 - val_accuracy: 0.9640 - val_precision: 0.8939 - val_recall: 0.9078 - val_auc: 0.9914\n",
            "Epoch 270/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1585 - tp: 84766.0000 - fp: 7432.0000 - tn: 443639.0000 - fn: 13027.0000 - accuracy: 0.9627 - precision: 0.9194 - recall: 0.8668 - auc: 0.9895\n",
            "Epoch 00270: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1585 - tp: 84863.0000 - fp: 7437.0000 - tn: 444144.0000 - fn: 13047.0000 - accuracy: 0.9627 - precision: 0.9194 - recall: 0.8667 - auc: 0.9895 - val_loss: 0.1671 - val_tp: 22513.0000 - val_fp: 3051.0000 - val_tn: 109556.0000 - val_fn: 2253.0000 - val_accuracy: 0.9614 - val_precision: 0.8807 - val_recall: 0.9090 - val_auc: 0.9902\n",
            "Epoch 271/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1579 - tp: 84767.0000 - fp: 7334.0000 - tn: 443305.0000 - fn: 12946.0000 - accuracy: 0.9630 - precision: 0.9204 - recall: 0.8675 - auc: 0.9897\n",
            "Epoch 00271: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1578 - tp: 84942.0000 - fp: 7346.0000 - tn: 444235.0000 - fn: 12968.0000 - accuracy: 0.9630 - precision: 0.9204 - recall: 0.8676 - auc: 0.9897 - val_loss: 0.1572 - val_tp: 22217.0000 - val_fp: 2336.0000 - val_tn: 110271.0000 - val_fn: 2549.0000 - val_accuracy: 0.9644 - val_precision: 0.9049 - val_recall: 0.8971 - val_auc: 0.9913\n",
            "Epoch 272/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1577 - tp: 84925.0000 - fp: 7378.0000 - tn: 443694.0000 - fn: 12867.0000 - accuracy: 0.9631 - precision: 0.9201 - recall: 0.8684 - auc: 0.9896\n",
            "Epoch 00272: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1577 - tp: 85025.0000 - fp: 7382.0000 - tn: 444199.0000 - fn: 12885.0000 - accuracy: 0.9631 - precision: 0.9201 - recall: 0.8684 - auc: 0.9896 - val_loss: 0.1577 - val_tp: 22036.0000 - val_fp: 2225.0000 - val_tn: 110382.0000 - val_fn: 2730.0000 - val_accuracy: 0.9639 - val_precision: 0.9083 - val_recall: 0.8898 - val_auc: 0.9907\n",
            "Epoch 273/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1585 - tp: 84885.0000 - fp: 7456.0000 - tn: 443827.0000 - fn: 12952.0000 - accuracy: 0.9628 - precision: 0.9193 - recall: 0.8676 - auc: 0.9895\n",
            "Epoch 00273: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1585 - tp: 84952.0000 - fp: 7461.0000 - tn: 444120.0000 - fn: 12958.0000 - accuracy: 0.9628 - precision: 0.9193 - recall: 0.8677 - auc: 0.9895 - val_loss: 0.1618 - val_tp: 22526.0000 - val_fp: 2903.0000 - val_tn: 109704.0000 - val_fn: 2240.0000 - val_accuracy: 0.9626 - val_precision: 0.8858 - val_recall: 0.9096 - val_auc: 0.9909\n",
            "Epoch 274/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1579 - tp: 84909.0000 - fp: 7388.0000 - tn: 443267.0000 - fn: 12788.0000 - accuracy: 0.9632 - precision: 0.9200 - recall: 0.8691 - auc: 0.9897\n",
            "Epoch 00274: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1579 - tp: 85101.0000 - fp: 7402.0000 - tn: 444179.0000 - fn: 12809.0000 - accuracy: 0.9632 - precision: 0.9200 - recall: 0.8692 - auc: 0.9897 - val_loss: 0.1621 - val_tp: 21920.0000 - val_fp: 2445.0000 - val_tn: 110162.0000 - val_fn: 2846.0000 - val_accuracy: 0.9615 - val_precision: 0.8997 - val_recall: 0.8851 - val_auc: 0.9902\n",
            "Epoch 275/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1583 - tp: 84819.0000 - fp: 7436.0000 - tn: 443431.0000 - fn: 12922.0000 - accuracy: 0.9629 - precision: 0.9194 - recall: 0.8678 - auc: 0.9896\n",
            "Epoch 00275: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1584 - tp: 84969.0000 - fp: 7452.0000 - tn: 444129.0000 - fn: 12941.0000 - accuracy: 0.9629 - precision: 0.9194 - recall: 0.8678 - auc: 0.9896 - val_loss: 0.1566 - val_tp: 22350.0000 - val_fp: 2467.0000 - val_tn: 110140.0000 - val_fn: 2416.0000 - val_accuracy: 0.9645 - val_precision: 0.9006 - val_recall: 0.9024 - val_auc: 0.9913\n",
            "Epoch 276/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1578 - tp: 85094.0000 - fp: 7470.0000 - tn: 444017.0000 - fn: 12795.0000 - accuracy: 0.9631 - precision: 0.9193 - recall: 0.8693 - auc: 0.9897\n",
            "Epoch 00276: val_auc did not improve from 0.99140\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1578 - tp: 85111.0000 - fp: 7472.0000 - tn: 444109.0000 - fn: 12799.0000 - accuracy: 0.9631 - precision: 0.9193 - recall: 0.8693 - auc: 0.9897 - val_loss: 0.1651 - val_tp: 22401.0000 - val_fp: 3018.0000 - val_tn: 109589.0000 - val_fn: 2365.0000 - val_accuracy: 0.9608 - val_precision: 0.8813 - val_recall: 0.9045 - val_auc: 0.9904\n",
            "Epoch 277/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1575 - tp: 85047.0000 - fp: 7371.0000 - tn: 444112.0000 - fn: 12846.0000 - accuracy: 0.9632 - precision: 0.9202 - recall: 0.8688 - auc: 0.9897\n",
            "Epoch 00277: val_auc improved from 0.99140 to 0.99162, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1575 - tp: 85062.0000 - fp: 7373.0000 - tn: 444208.0000 - fn: 12848.0000 - accuracy: 0.9632 - precision: 0.9202 - recall: 0.8688 - auc: 0.9897 - val_loss: 0.1613 - val_tp: 22735.0000 - val_fp: 3056.0000 - val_tn: 109551.0000 - val_fn: 2031.0000 - val_accuracy: 0.9630 - val_precision: 0.8815 - val_recall: 0.9180 - val_auc: 0.9916\n",
            "Epoch 278/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1581 - tp: 84940.0000 - fp: 7467.0000 - tn: 443814.0000 - fn: 12899.0000 - accuracy: 0.9629 - precision: 0.9192 - recall: 0.8682 - auc: 0.9896\n",
            "Epoch 00278: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 22s 10ms/step - loss: 0.1581 - tp: 85002.0000 - fp: 7474.0000 - tn: 444107.0000 - fn: 12908.0000 - accuracy: 0.9629 - precision: 0.9192 - recall: 0.8682 - auc: 0.9896 - val_loss: 0.1594 - val_tp: 21849.0000 - val_fp: 2121.0000 - val_tn: 110486.0000 - val_fn: 2917.0000 - val_accuracy: 0.9633 - val_precision: 0.9115 - val_recall: 0.8822 - val_auc: 0.9909\n",
            "Epoch 279/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1577 - tp: 84800.0000 - fp: 7511.0000 - tn: 443342.0000 - fn: 12955.0000 - accuracy: 0.9627 - precision: 0.9186 - recall: 0.8675 - auc: 0.9897\n",
            "Epoch 00279: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1577 - tp: 84935.0000 - fp: 7522.0000 - tn: 444059.0000 - fn: 12975.0000 - accuracy: 0.9627 - precision: 0.9186 - recall: 0.8675 - auc: 0.9897 - val_loss: 0.1651 - val_tp: 22679.0000 - val_fp: 3222.0000 - val_tn: 109385.0000 - val_fn: 2087.0000 - val_accuracy: 0.9614 - val_precision: 0.8756 - val_recall: 0.9157 - val_auc: 0.9906\n",
            "Epoch 280/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1570 - tp: 85142.0000 - fp: 7481.0000 - tn: 443797.0000 - fn: 12700.0000 - accuracy: 0.9632 - precision: 0.9192 - recall: 0.8702 - auc: 0.9898\n",
            "Epoch 00280: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1570 - tp: 85198.0000 - fp: 7484.0000 - tn: 444097.0000 - fn: 12712.0000 - accuracy: 0.9632 - precision: 0.9193 - recall: 0.8702 - auc: 0.9898 - val_loss: 0.1626 - val_tp: 22145.0000 - val_fp: 2587.0000 - val_tn: 110020.0000 - val_fn: 2621.0000 - val_accuracy: 0.9621 - val_precision: 0.8954 - val_recall: 0.8942 - val_auc: 0.9906\n",
            "Epoch 281/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1576 - tp: 84960.0000 - fp: 7478.0000 - tn: 443589.0000 - fn: 12837.0000 - accuracy: 0.9630 - precision: 0.9191 - recall: 0.8687 - auc: 0.9898\n",
            "Epoch 00281: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1576 - tp: 85060.0000 - fp: 7494.0000 - tn: 444087.0000 - fn: 12850.0000 - accuracy: 0.9630 - precision: 0.9190 - recall: 0.8688 - auc: 0.9898 - val_loss: 0.1666 - val_tp: 22544.0000 - val_fp: 3228.0000 - val_tn: 109379.0000 - val_fn: 2222.0000 - val_accuracy: 0.9603 - val_precision: 0.8747 - val_recall: 0.9103 - val_auc: 0.9902\n",
            "Epoch 282/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1567 - tp: 85144.0000 - fp: 7330.0000 - tn: 443524.0000 - fn: 12610.0000 - accuracy: 0.9637 - precision: 0.9207 - recall: 0.8710 - auc: 0.9898\n",
            "Epoch 00282: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1567 - tp: 85283.0000 - fp: 7351.0000 - tn: 444230.0000 - fn: 12627.0000 - accuracy: 0.9636 - precision: 0.9206 - recall: 0.8710 - auc: 0.9898 - val_loss: 0.1689 - val_tp: 22744.0000 - val_fp: 3566.0000 - val_tn: 109041.0000 - val_fn: 2022.0000 - val_accuracy: 0.9593 - val_precision: 0.8645 - val_recall: 0.9184 - val_auc: 0.9907\n",
            "Epoch 283/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1572 - tp: 84961.0000 - fp: 7377.0000 - tn: 443685.0000 - fn: 12841.0000 - accuracy: 0.9632 - precision: 0.9201 - recall: 0.8687 - auc: 0.9898\n",
            "Epoch 00283: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1572 - tp: 85056.0000 - fp: 7385.0000 - tn: 444196.0000 - fn: 12854.0000 - accuracy: 0.9632 - precision: 0.9201 - recall: 0.8687 - auc: 0.9898 - val_loss: 0.1540 - val_tp: 21698.0000 - val_fp: 1699.0000 - val_tn: 110908.0000 - val_fn: 3068.0000 - val_accuracy: 0.9653 - val_precision: 0.9274 - val_recall: 0.8761 - val_auc: 0.9916\n",
            "Epoch 284/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1566 - tp: 85103.0000 - fp: 7377.0000 - tn: 443686.0000 - fn: 12698.0000 - accuracy: 0.9634 - precision: 0.9202 - recall: 0.8702 - auc: 0.9900\n",
            "Epoch 00284: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1566 - tp: 85197.0000 - fp: 7387.0000 - tn: 444194.0000 - fn: 12713.0000 - accuracy: 0.9634 - precision: 0.9202 - recall: 0.8702 - auc: 0.9900 - val_loss: 0.1620 - val_tp: 22363.0000 - val_fp: 2823.0000 - val_tn: 109784.0000 - val_fn: 2403.0000 - val_accuracy: 0.9620 - val_precision: 0.8879 - val_recall: 0.9030 - val_auc: 0.9909\n",
            "Epoch 285/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1561 - tp: 85278.0000 - fp: 7248.0000 - tn: 444236.0000 - fn: 12614.0000 - accuracy: 0.9638 - precision: 0.9217 - recall: 0.8711 - auc: 0.9900\n",
            "Epoch 00285: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 21s 10ms/step - loss: 0.1561 - tp: 85295.0000 - fp: 7248.0000 - tn: 444333.0000 - fn: 12615.0000 - accuracy: 0.9639 - precision: 0.9217 - recall: 0.8712 - auc: 0.9900 - val_loss: 0.1566 - val_tp: 22103.0000 - val_fp: 2293.0000 - val_tn: 110314.0000 - val_fn: 2663.0000 - val_accuracy: 0.9639 - val_precision: 0.9060 - val_recall: 0.8925 - val_auc: 0.9916\n",
            "Epoch 286/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1559 - tp: 85340.0000 - fp: 7316.0000 - tn: 444171.0000 - fn: 12549.0000 - accuracy: 0.9638 - precision: 0.9210 - recall: 0.8718 - auc: 0.9900\n",
            "Epoch 00286: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 20s 10ms/step - loss: 0.1559 - tp: 85360.0000 - fp: 7317.0000 - tn: 444264.0000 - fn: 12550.0000 - accuracy: 0.9638 - precision: 0.9210 - recall: 0.8718 - auc: 0.9900 - val_loss: 0.1628 - val_tp: 22581.0000 - val_fp: 2991.0000 - val_tn: 109616.0000 - val_fn: 2185.0000 - val_accuracy: 0.9623 - val_precision: 0.8830 - val_recall: 0.9118 - val_auc: 0.9909\n",
            "Epoch 287/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1566 - tp: 85058.0000 - fp: 7362.0000 - tn: 443707.0000 - fn: 12737.0000 - accuracy: 0.9634 - precision: 0.9203 - recall: 0.8698 - auc: 0.9900\n",
            "Epoch 00287: val_auc did not improve from 0.99162\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1566 - tp: 85159.0000 - fp: 7368.0000 - tn: 444213.0000 - fn: 12751.0000 - accuracy: 0.9634 - precision: 0.9204 - recall: 0.8698 - auc: 0.9900 - val_loss: 0.1640 - val_tp: 22462.0000 - val_fp: 2988.0000 - val_tn: 109619.0000 - val_fn: 2304.0000 - val_accuracy: 0.9615 - val_precision: 0.8826 - val_recall: 0.9070 - val_auc: 0.9907\n",
            "Epoch 288/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1560 - tp: 85278.0000 - fp: 7307.0000 - tn: 444274.0000 - fn: 12632.0000 - accuracy: 0.9637 - precision: 0.9211 - recall: 0.8710 - auc: 0.9900\n",
            "Epoch 00288: val_auc improved from 0.99162 to 0.99185, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1560 - tp: 85278.0000 - fp: 7307.0000 - tn: 444274.0000 - fn: 12632.0000 - accuracy: 0.9637 - precision: 0.9211 - recall: 0.8710 - auc: 0.9900 - val_loss: 0.1555 - val_tp: 22321.0000 - val_fp: 2435.0000 - val_tn: 110172.0000 - val_fn: 2445.0000 - val_accuracy: 0.9645 - val_precision: 0.9016 - val_recall: 0.9013 - val_auc: 0.9919\n",
            "Epoch 289/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1563 - tp: 85139.0000 - fp: 7261.0000 - tn: 443594.0000 - fn: 12614.0000 - accuracy: 0.9638 - precision: 0.9214 - recall: 0.8710 - auc: 0.9899\n",
            "Epoch 00289: val_auc did not improve from 0.99185\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1563 - tp: 85276.0000 - fp: 7268.0000 - tn: 444313.0000 - fn: 12634.0000 - accuracy: 0.9638 - precision: 0.9215 - recall: 0.8710 - auc: 0.9899 - val_loss: 0.1599 - val_tp: 22650.0000 - val_fp: 2951.0000 - val_tn: 109656.0000 - val_fn: 2116.0000 - val_accuracy: 0.9631 - val_precision: 0.8847 - val_recall: 0.9146 - val_auc: 0.9915\n",
            "Epoch 290/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1563 - tp: 85308.0000 - fp: 7379.0000 - tn: 444106.0000 - fn: 12583.0000 - accuracy: 0.9637 - precision: 0.9204 - recall: 0.8715 - auc: 0.9900\n",
            "Epoch 00290: val_auc did not improve from 0.99185\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1563 - tp: 85323.0000 - fp: 7379.0000 - tn: 444202.0000 - fn: 12587.0000 - accuracy: 0.9637 - precision: 0.9204 - recall: 0.8714 - auc: 0.9900 - val_loss: 0.1683 - val_tp: 22808.0000 - val_fp: 3672.0000 - val_tn: 108935.0000 - val_fn: 1958.0000 - val_accuracy: 0.9590 - val_precision: 0.8613 - val_recall: 0.9209 - val_auc: 0.9907\n",
            "Epoch 291/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1555 - tp: 85333.0000 - fp: 7239.0000 - tn: 444038.0000 - fn: 12510.0000 - accuracy: 0.9640 - precision: 0.9218 - recall: 0.8721 - auc: 0.9902\n",
            "Epoch 00291: val_auc did not improve from 0.99185\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1555 - tp: 85389.0000 - fp: 7245.0000 - tn: 444336.0000 - fn: 12521.0000 - accuracy: 0.9640 - precision: 0.9218 - recall: 0.8721 - auc: 0.9902 - val_loss: 0.1591 - val_tp: 22196.0000 - val_fp: 2453.0000 - val_tn: 110154.0000 - val_fn: 2570.0000 - val_accuracy: 0.9634 - val_precision: 0.9005 - val_recall: 0.8962 - val_auc: 0.9907\n",
            "Epoch 292/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1551 - tp: 85443.0000 - fp: 7355.0000 - tn: 443925.0000 - fn: 12397.0000 - accuracy: 0.9640 - precision: 0.9207 - recall: 0.8733 - auc: 0.9903\n",
            "Epoch 00292: val_auc did not improve from 0.99185\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1551 - tp: 85504.0000 - fp: 7360.0000 - tn: 444221.0000 - fn: 12406.0000 - accuracy: 0.9640 - precision: 0.9207 - recall: 0.8733 - auc: 0.9903 - val_loss: 0.1541 - val_tp: 22322.0000 - val_fp: 2437.0000 - val_tn: 110170.0000 - val_fn: 2444.0000 - val_accuracy: 0.9645 - val_precision: 0.9016 - val_recall: 0.9013 - val_auc: 0.9918\n",
            "Epoch 293/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1546 - tp: 85375.0000 - fp: 7253.0000 - tn: 443599.0000 - fn: 12381.0000 - accuracy: 0.9642 - precision: 0.9217 - recall: 0.8733 - auc: 0.9903\n",
            "Epoch 00293: val_auc improved from 0.99185 to 0.99205, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1545 - tp: 85507.0000 - fp: 7260.0000 - tn: 444321.0000 - fn: 12403.0000 - accuracy: 0.9642 - precision: 0.9217 - recall: 0.8733 - auc: 0.9903 - val_loss: 0.1572 - val_tp: 22734.0000 - val_fp: 2808.0000 - val_tn: 109799.0000 - val_fn: 2032.0000 - val_accuracy: 0.9648 - val_precision: 0.8901 - val_recall: 0.9180 - val_auc: 0.9920\n",
            "Epoch 294/2000\n",
            "2144/2147 [============================>.] - ETA: 0s - loss: 0.1552 - tp: 85425.0000 - fp: 7329.0000 - tn: 443728.0000 - fn: 12382.0000 - accuracy: 0.9641 - precision: 0.9210 - recall: 0.8734 - auc: 0.9901\n",
            "Epoch 00294: val_auc did not improve from 0.99205\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1552 - tp: 85514.0000 - fp: 7341.0000 - tn: 444240.0000 - fn: 12396.0000 - accuracy: 0.9641 - precision: 0.9209 - recall: 0.8734 - auc: 0.9901 - val_loss: 0.1639 - val_tp: 22771.0000 - val_fp: 3306.0000 - val_tn: 109301.0000 - val_fn: 1995.0000 - val_accuracy: 0.9614 - val_precision: 0.8732 - val_recall: 0.9194 - val_auc: 0.9911\n",
            "Epoch 295/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1553 - tp: 85412.0000 - fp: 7296.0000 - tn: 444190.0000 - fn: 12478.0000 - accuracy: 0.9640 - precision: 0.9213 - recall: 0.8725 - auc: 0.9901\n",
            "Epoch 00295: val_auc did not improve from 0.99205\n",
            "2147/2147 [==============================] - 20s 9ms/step - loss: 0.1553 - tp: 85430.0000 - fp: 7297.0000 - tn: 444284.0000 - fn: 12480.0000 - accuracy: 0.9640 - precision: 0.9213 - recall: 0.8725 - auc: 0.9901 - val_loss: 0.1622 - val_tp: 22577.0000 - val_fp: 2985.0000 - val_tn: 109622.0000 - val_fn: 2189.0000 - val_accuracy: 0.9623 - val_precision: 0.8832 - val_recall: 0.9116 - val_auc: 0.9909\n",
            "Epoch 296/2000\n",
            "2142/2147 [============================>.] - ETA: 0s - loss: 0.1549 - tp: 85340.0000 - fp: 7219.0000 - tn: 443429.0000 - fn: 12364.0000 - accuracy: 0.9643 - precision: 0.9220 - recall: 0.8735 - auc: 0.9903\n",
            "Epoch 00296: val_auc did not improve from 0.99205\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1549 - tp: 85521.0000 - fp: 7236.0000 - tn: 444345.0000 - fn: 12389.0000 - accuracy: 0.9643 - precision: 0.9220 - recall: 0.8735 - auc: 0.9903 - val_loss: 0.1555 - val_tp: 22266.0000 - val_fp: 2299.0000 - val_tn: 110308.0000 - val_fn: 2500.0000 - val_accuracy: 0.9651 - val_precision: 0.9064 - val_recall: 0.8991 - val_auc: 0.9916\n",
            "Epoch 297/2000\n",
            "2147/2147 [==============================] - ETA: 0s - loss: 0.1552 - tp: 85361.0000 - fp: 7286.0000 - tn: 444295.0000 - fn: 12549.0000 - accuracy: 0.9639 - precision: 0.9214 - recall: 0.8718 - auc: 0.9902\n",
            "Epoch 00297: val_auc did not improve from 0.99205\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1552 - tp: 85361.0000 - fp: 7286.0000 - tn: 444295.0000 - fn: 12549.0000 - accuracy: 0.9639 - precision: 0.9214 - recall: 0.8718 - auc: 0.9902 - val_loss: 0.1533 - val_tp: 22277.0000 - val_fp: 2255.0000 - val_tn: 110352.0000 - val_fn: 2489.0000 - val_accuracy: 0.9655 - val_precision: 0.9081 - val_recall: 0.8995 - val_auc: 0.9918\n",
            "Epoch 298/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1546 - tp: 85366.0000 - fp: 7280.0000 - tn: 443574.0000 - fn: 12388.0000 - accuracy: 0.9641 - precision: 0.9214 - recall: 0.8733 - auc: 0.9903\n",
            "Epoch 00298: val_auc improved from 0.99205 to 0.99220, saving model to ./best_model.h5\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1546 - tp: 85503.0000 - fp: 7288.0000 - tn: 444293.0000 - fn: 12407.0000 - accuracy: 0.9642 - precision: 0.9215 - recall: 0.8733 - auc: 0.9903 - val_loss: 0.1566 - val_tp: 22672.0000 - val_fp: 2753.0000 - val_tn: 109854.0000 - val_fn: 2094.0000 - val_accuracy: 0.9647 - val_precision: 0.8917 - val_recall: 0.9154 - val_auc: 0.9922\n",
            "Epoch 299/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1547 - tp: 85411.0000 - fp: 7312.0000 - tn: 443967.0000 - fn: 12430.0000 - accuracy: 0.9640 - precision: 0.9211 - recall: 0.8730 - auc: 0.9903\n",
            "Epoch 00299: val_auc did not improve from 0.99220\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1547 - tp: 85475.0000 - fp: 7320.0000 - tn: 444261.0000 - fn: 12435.0000 - accuracy: 0.9640 - precision: 0.9211 - recall: 0.8730 - auc: 0.9903 - val_loss: 0.1696 - val_tp: 22842.0000 - val_fp: 3735.0000 - val_tn: 108872.0000 - val_fn: 1924.0000 - val_accuracy: 0.9588 - val_precision: 0.8595 - val_recall: 0.9223 - val_auc: 0.9908\n",
            "Epoch 300/2000\n",
            "2145/2147 [============================>.] - ETA: 0s - loss: 0.1543 - tp: 85509.0000 - fp: 7271.0000 - tn: 443992.0000 - fn: 12348.0000 - accuracy: 0.9643 - precision: 0.9216 - recall: 0.8738 - auc: 0.9903\n",
            "Epoch 00300: val_auc did not improve from 0.99220\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1543 - tp: 85558.0000 - fp: 7278.0000 - tn: 444303.0000 - fn: 12352.0000 - accuracy: 0.9643 - precision: 0.9216 - recall: 0.8738 - auc: 0.9903 - val_loss: 0.1573 - val_tp: 22529.0000 - val_fp: 2657.0000 - val_tn: 109950.0000 - val_fn: 2237.0000 - val_accuracy: 0.9644 - val_precision: 0.8945 - val_recall: 0.9097 - val_auc: 0.9917\n",
            "Epoch 301/2000\n",
            "2143/2147 [============================>.] - ETA: 0s - loss: 0.1544 - tp: 85400.0000 - fp: 7276.0000 - tn: 443578.0000 - fn: 12354.0000 - accuracy: 0.9642 - precision: 0.9215 - recall: 0.8736 - auc: 0.9903\n",
            "Epoch 00301: val_auc did not improve from 0.99220\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1544 - tp: 85537.0000 - fp: 7286.0000 - tn: 444295.0000 - fn: 12373.0000 - accuracy: 0.9642 - precision: 0.9215 - recall: 0.8736 - auc: 0.9903 - val_loss: 0.1632 - val_tp: 22497.0000 - val_fp: 2990.0000 - val_tn: 109617.0000 - val_fn: 2269.0000 - val_accuracy: 0.9617 - val_precision: 0.8827 - val_recall: 0.9084 - val_auc: 0.9908\n",
            "Epoch 302/2000\n",
            "2146/2147 [============================>.] - ETA: 0s - loss: 0.1543 - tp: 85563.0000 - fp: 7303.0000 - tn: 444175.0000 - fn: 12335.0000 - accuracy: 0.9643 - precision: 0.9214 - recall: 0.8740 - auc: 0.9903\n",
            "Epoch 00302: val_auc did not improve from 0.99220\n",
            "2147/2147 [==============================] - 19s 9ms/step - loss: 0.1543 - tp: 85575.0000 - fp: 7304.0000 - tn: 444277.0000 - fn: 12335.0000 - accuracy: 0.9643 - precision: 0.9214 - recall: 0.8740 - auc: 0.9903 - val_loss: 0.1820 - val_tp: 22967.0000 - val_fp: 4515.0000 - val_tn: 108092.0000 - val_fn: 1799.0000 - val_accuracy: 0.9540 - val_precision: 0.8357 - val_recall: 0.9274 - val_auc: 0.9895\n",
            "Epoch 303/2000\n",
            " 359/2147 [====>.........................] - ETA: 13s - loss: 0.1539 - tp: 14147.0000 - fp: 1202.0000 - tn: 74543.0000 - fn: 2012.0000 - accuracy: 0.9650 - precision: 0.9217 - recall: 0.8755 - auc: 0.9905"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsIE6_stkBAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_metrics(history):\n",
        "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color=colors[0], linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "plot_metric(history)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ9tGRoYmd4y",
        "colab_type": "text"
      },
      "source": [
        "**F1 validation (From https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKjbrzEe2ISI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "91808d22-80a7-46fc-cc47-660f38c5a544"
      },
      "source": [
        "# Save model weights to drive\n",
        "!cp -r best_model.h5 '/content/gdrive/My Drive/Kaggle/best_model_20200801_METRICS.h5'\n",
        "\n",
        "new_model = tf.keras.models.load_model('./best_model.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "#new_model = tf.keras.models.load_model('/content/gdrive/My Drive/Kaggle/best_model_20200731_METRICS.h5', \n",
        "#                                        custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU(alpha=0.01)})\n",
        "new_model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 256)               205056    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 321,921\n",
            "Trainable params: 321,153\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAzqpDLIS0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty some RAM space\n",
        "indices_0_new = None\n",
        "data_backup = None\n",
        "dataset_transaction = None\n",
        "X_to_train = None\n",
        "Y_to_train = None\n",
        "Y_train = None\n",
        "X_train = None"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe0Q-5JmbVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a161984-b764-4cad-d47c-2c634daa6cfb"
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n",
        "  test_size = len(Y_test)\n",
        "  y_pred = (model.predict(X_test[:test_size], batch_size=128)>threshold).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        " \n",
        "  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)\n",
        "\n",
        "pre, re, f1 = F1_score(new_model, X_test, Y_test, test_size=len(Y_test), threshold=0.9)\n",
        "print(pre, re, f1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9578313253012049 0.8112244897959183 0.8784530386740331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwhX0d2C_c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2H3PvUGb8KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "76678d21-b626-44cf-a77c-069f21f170c8"
      },
      "source": [
        "#BATCH_SIZE = 256\n",
        "baseline_results = new_model.evaluate(X_test, Y_test,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(new_model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "predictions = new_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "plot_cm(Y_test, predictions, p=0.9)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss :  0.11965393275022507\n",
            "tp :  186.0\n",
            "fp :  124.0\n",
            "tn :  5586.0\n",
            "fn :  10.0\n",
            "accuracy :  0.9773111939430237\n",
            "precision :  0.6000000238418579\n",
            "recall :  0.9489796161651611\n",
            "auc :  0.9969570636749268\n",
            "\n",
            "Legitimate Transactions Detected (True Negatives):  5703\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  7\n",
            "Fraudulent Transactions Missed (False Negatives):  37\n",
            "Fraudulent Transactions Detected (True Positives):  159\n",
            "Total Fraudulent Transactions:  196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUuAUMBb_9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "672bab25-2064-4101-c5ef-48cf7c734a28"
      },
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,20])\n",
        "  plt.ylim([80,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "train_prediction = new_model.predict(X_train, batch_size=BATCH_SIZE, verbose=0)\n",
        "plot_roc(\"Train Baseline\", Y_train, train_prediction, color=colors[0])\n",
        "plot_roc(\"Test Baseline\", Y_test, predictions, color=colors[0], linestyle='--')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Legitimate Transactions Detected (True Negatives):  5626\n",
            "Legitimate Transactions Incorrectly Detected (False Positives):  84\n",
            "Fraudulent Transactions Missed (False Negatives):  11\n",
            "Fraudulent Transactions Detected (True Positives):  185\n",
            "Total Fraudulent Transactions:  196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI3GaND0T5HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "43d7bce2-6a9f-403f-fa51-312ba9f6fb96"
      },
      "source": [
        "prediction = np.squeeze(predictions, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist((prediction>0.9).astype('int'), bins=[0,1,2])\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.85).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.08% 3.32%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9zr52D5agr",
        "colab_type": "text"
      },
      "source": [
        "# ***Output the result into a file for a validation with Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tvlyv5V5fsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "test_transaction = pd.read_csv('test_transaction.csv')\n",
        "test_transaction.head(5)\n",
        "\n",
        "# Remove transaction ID\n",
        "TransactionID = test_transaction.pop('TransactionID')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkoViKsx6cZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns_test = test_transaction.columns[np.where(test_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = []\n",
        "for column in skip_int_columns:\n",
        "  int_columns_test.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrzQZ6nR6wOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_normalization(X, indices, cache_min, cache_max, cache_mean):\n",
        "  X_out = copy.copy(X)\n",
        "  #print(cache_mean, cache_max, cache_min)\n",
        "  X_out[indices] = (X_out[indices] - cache_mean)/(cache_max - cache_min)\n",
        "  X_out[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return X_out.astype('float16')  \n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM75lh_6lhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  test_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjog0oM7p4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  test_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  #test_transaction[column + \"_NaN_Code\"] = np.isnan(test_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = test_transaction[column]\n",
        "  indices = np.where(np.isnan(test_transaction[column]) == False)[0]\n",
        "  test_transaction[column] = apply_normalization(X.to_numpy(), indices, cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMTT8KB74NL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01d6ebfb-8fd3-47f0-fe4e-1c9939b4e441"
      },
      "source": [
        "encoded_column = 0\n",
        "for column in obj_columns_test:\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(cache[column])\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.transform(test_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(cache[column])))])\n",
        "  test_transaction = pd.concat([test_transaction, pd_encoded], axis=1)\n",
        "  encoded_column += len(pd_encoded.columns)\n",
        "\n",
        "print(\"Encoded columns: \" + str(encoded_column))\n",
        "\n",
        "\n",
        "for column in obj_columns_test:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "for column in to_remove:\n",
        "  try:\n",
        "    test_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded columns: 164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_OOqFi8HrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd3ad47e-51a0-44c0-abbe-d6aa1c2af235"
      },
      "source": [
        "# Check if we have the same shape with the X_train\n",
        "print(test_transaction.shape, X_train.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506691, 800) (584634, 800)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9vDvpDZdpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the prediction and submit the output\n",
        "result = (new_model.predict(test_transaction)>0.3).astype('int8')\n",
        "result_pd = pd.DataFrame(result, columns=['isFraud'])\n",
        "data_to_file = pd.concat([TransactionID, result_pd], axis=1)\n",
        "data_to_file.head(5)\n",
        "data_to_file.to_csv(\"./submission.csv\", index=False)\n",
        "data_to_file.to_csv('/content/gdrive/My Drive/Kaggle/submission.csv', index=False)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9099XTi4s2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2ba4464d-1757-47e3-b755-291ea8c223ae"
      },
      "source": [
        "!kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"New submission with model using Keras METRICS and LeakyRelu and with threshold 0.3\""
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 4.83M/4.83M [00:04<00:00, 1.26MB/s]\n",
            "Successfully submitted to IEEE-CIS Fraud Detection"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGezGr2PkCbt",
        "colab_type": "text"
      },
      "source": [
        "# ***Debug zone***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J7VBfnUmND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e00e92-3284-4de2-bb93-bc5ebabd26e6"
      },
      "source": [
        "indices = np.where(np.isnan(a) == False)[0]\n",
        "min_value, max_value, mean_value, normalized_data = normalization_data(a, indices)\n",
        "print(min_value, max_value, mean_value, np.mean(normalized_data), np.min(normalized_data), np.max(normalized_data))\n",
        "dataset_transaction['V331'] = normalized_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 160000.0 721.7418829164045 -2.2733716828843707e-16 -0.004510886768227528 0.9954891132317726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICp4sPm6brq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3e2nvzrHir4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fe4ae314-d9e2-483a-9baa-e8b9302f14bf"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohc = OneHotEncoder()\n",
        "a = {'a': ['Null', 'A', 'B', 'C', 'D']}\n",
        "df = pd.DataFrame(a)\n",
        "df\n",
        "encoded = ohc.fit_transform(df['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsaGKlzMUlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69213f3a-42cb-4edc-cbc8-5bc6c0bb5a7e"
      },
      "source": [
        "b = {'a': ['Null', 'A', 'B', 'C', 'E']}\n",
        "df_b = pd.DataFrame(b)\n",
        "ohc_b = OneHotEncoder(handle_unknown='ignore')\n",
        "ohc_b.fit(df['a'].values.reshape(-1,1))\n",
        "encoded_b = ohc_b.transform(df_b['a'].values.reshape(-1,1)).toarray()    \n",
        "pd_encoded_b = pd.DataFrame(encoded_b.astype('int8'), columns=[\"a\"+\"_\"+str(i) for i in range(len(np.unique(df['a'].astype('str'))))])\n",
        "pd_encoded_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>a_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a_0  a_1  a_2  a_3  a_4\n",
              "0    0    0    0    0    1\n",
              "1    1    0    0    0    0\n",
              "2    0    1    0    0    0\n",
              "3    0    0    1    0    0\n",
              "4    0    0    0    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvykuaRPMpZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "20c01d2a-8a3b-41b6-a7ae-b6d7a36f327f"
      },
      "source": [
        "for column in obj_columns:\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(dataset_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 5\n",
            "P_emaildomain 60\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_RMIz3NTTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2e5b2844-8bdd-45ee-f5ec-83cd54c9cba7"
      },
      "source": [
        "for column in obj_columns_test:\n",
        "  test_transaction.loc[np.where(test_transaction[column].isnull())[0], column] = 'Null'\n",
        "  print(column, len(np.unique(test_transaction[column].astype(\"str\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ProductCD 5\n",
            "card4 5\n",
            "card6 4\n",
            "P_emaildomain 61\n",
            "R_emaildomain 61\n",
            "M1 3\n",
            "M2 3\n",
            "M3 3\n",
            "M4 4\n",
            "M5 3\n",
            "M6 3\n",
            "M7 3\n",
            "M8 3\n",
            "M9 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnTtZ-WUmWS",
        "colab_type": "text"
      },
      "source": [
        "**Train val dataset**"
      ]
    }
  ]
}