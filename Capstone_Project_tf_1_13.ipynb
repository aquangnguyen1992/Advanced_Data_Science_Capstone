{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_Project_tf_1_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnqZ1xfL7RtppGSVTwGQWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquangnguyen1992/Advanced_Data_Science_Capstone/blob/tensorflow_1_13_1/Capstone_Project_tf_1_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0j82KmSwpxA",
        "colab_type": "text"
      },
      "source": [
        "# ***Overview***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWHPdoodww93",
        "colab_type": "text"
      },
      "source": [
        "This work is to test the performance of the system using tensorflow 1.13.1 (as required by the Notebook of Watson Studio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svv1CEbTxGd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "59957385-4bff-412b-e3ad-700a27902a97"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 54kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.30.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.34.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (49.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxEp-QGXwuYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fbff08f5-4c5a-422b-f186-8fbc9f0bf197"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr3q-PRBwtqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "26afd185-963e-418d-8a5f-d6f78c1d9690"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "!kaggle competitions download -c ieee-fraud-detection\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 51.4MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 70% 41.0M/58.3M [00:00<00:00, 65.6MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 108MB/s] \n",
            "500 - Internal Server Error\n",
            "Archive:  train_transaction.csv.zip\n",
            "  inflating: train_transaction.csv   \n",
            "\n",
            "Archive:  test_identity.csv.zip\n",
            "  inflating: test_identity.csv       \n",
            "\n",
            "2 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LaoYLtpwhrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3a2b5e6a-5da0-467b-f015-be3ecb9b481f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhekPII3wpJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ae6c829c-e2db-40d0-b2a6-064289022528"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "dataset_transaction = pd.read_csv('train_transaction.csv')\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4neza27xRMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8b1b5d02-8c9e-47b8-da14-63b803ac3b6a"
      },
      "source": [
        "dataset_transaction.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
              "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
              "       ...\n",
              "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
              "       'V339'],\n",
              "      dtype='object', length=394)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNPHQ2NCbGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\n",
        "int_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\n",
        "obj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n",
        "\n",
        "skip_int_columns = ['TransactionID', 'isFraud']\n",
        "for column in skip_int_columns:\n",
        "  int_columns.remove(column)\n",
        "\n",
        "skip_obj_colums = ['']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4AzwRzqEfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization_data(X, indices):\n",
        "  X_out = copy.copy(X)\n",
        "  X_temp = X[indices]\n",
        "  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n",
        "  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n",
        "  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sce8WEFqWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "633642ce-8ee7-4751-a70e-cc95f77a0085"
      },
      "source": [
        "data_backup = copy.copy(dataset_transaction)\n",
        "data_backup.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIIYOrO74QbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1: Detect the columns with NaN and code it with an extra features\n",
        "# Task 2: Apply normalizationn\n",
        "# To-Do: Task 3: Remove the irrelevant columns\n",
        "\n",
        "cache = dict()\n",
        "dataset_transaction = copy.copy(data_backup)\n",
        "\n",
        "#dataset_transaction.pop()\n",
        "\n",
        "for column in float_columns:\n",
        "  # Set to float 16\n",
        "  dataset_transaction[column].astype('float16')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZY_88yeGGSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e6d89900-79d7-4010-fd29-6797343f1f51"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43g5UKZPg32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in int_columns:\n",
        "  # Set to int 32\n",
        "  dataset_transaction[column].astype('int32')\n",
        "\n",
        "  # Code the NaN feature\n",
        "  if np.any(np.isnan(dataset_transaction[column].values)):\n",
        "    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n",
        "  \n",
        "  # Normalization\n",
        "  X = dataset_transaction[column]\n",
        "  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n",
        "  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7scgn0-mD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "abbfc217-a30d-4693-ce34-fe22a8e1516c"
      },
      "source": [
        "dataset_transaction.head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300_NaN_Code</th>\n",
              "      <th>V301_NaN_Code</th>\n",
              "      <th>V302_NaN_Code</th>\n",
              "      <th>V303_NaN_Code</th>\n",
              "      <th>V304_NaN_Code</th>\n",
              "      <th>V305_NaN_Code</th>\n",
              "      <th>V306_NaN_Code</th>\n",
              "      <th>V307_NaN_Code</th>\n",
              "      <th>V308_NaN_Code</th>\n",
              "      <th>V309_NaN_Code</th>\n",
              "      <th>V310_NaN_Code</th>\n",
              "      <th>V311_NaN_Code</th>\n",
              "      <th>V312_NaN_Code</th>\n",
              "      <th>V313_NaN_Code</th>\n",
              "      <th>V314_NaN_Code</th>\n",
              "      <th>V315_NaN_Code</th>\n",
              "      <th>V316_NaN_Code</th>\n",
              "      <th>V317_NaN_Code</th>\n",
              "      <th>V318_NaN_Code</th>\n",
              "      <th>V319_NaN_Code</th>\n",
              "      <th>V320_NaN_Code</th>\n",
              "      <th>V321_NaN_Code</th>\n",
              "      <th>V322_NaN_Code</th>\n",
              "      <th>V323_NaN_Code</th>\n",
              "      <th>V324_NaN_Code</th>\n",
              "      <th>V325_NaN_Code</th>\n",
              "      <th>V326_NaN_Code</th>\n",
              "      <th>V327_NaN_Code</th>\n",
              "      <th>V328_NaN_Code</th>\n",
              "      <th>V329_NaN_Code</th>\n",
              "      <th>V330_NaN_Code</th>\n",
              "      <th>V331_NaN_Code</th>\n",
              "      <th>V332_NaN_Code</th>\n",
              "      <th>V333_NaN_Code</th>\n",
              "      <th>V334_NaN_Code</th>\n",
              "      <th>V335_NaN_Code</th>\n",
              "      <th>V336_NaN_Code</th>\n",
              "      <th>V337_NaN_Code</th>\n",
              "      <th>V338_NaN_Code</th>\n",
              "      <th>V339_NaN_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>W</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>discover</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>W</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>visa</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>W</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>debit</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>H</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>credit</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  ...  V338_NaN_Code  V339_NaN_Code\n",
              "0        2987000        0  ...              1              1\n",
              "1        2987001        0  ...              1              1\n",
              "2        2987002        0  ...              1              1\n",
              "3        2987003        0  ...              1              1\n",
              "4        2987004        0  ...              0              0\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGnSj678SaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "103b9090-14bf-48e8-d9ae-99928e87157d"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "for column in obj_columns:\n",
        "  ohc = OneHotEncoder()\n",
        "  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n",
        "  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n",
        "  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n",
        "  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n",
        "\n",
        "for column in obj_columns:\n",
        "  try:\n",
        "    dataset_transaction.pop(column)\n",
        "  except KeyError:\n",
        "    pass\n",
        "dataset_transaction.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 905 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  M9_0  M9_1  M9_2\n",
              "0        2987000        0      -0.463379  ...     0     1     0\n",
              "1        2987001        0      -0.463379  ...     0     1     0\n",
              "2        2987002        0      -0.463379  ...     1     0     0\n",
              "3        2987003        0      -0.463379  ...     0     1     0\n",
              "4        2987004        0      -0.463379  ...     0     1     0\n",
              "\n",
              "[5 rows x 905 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e626putLzCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3d064fc-bfd4-43c0-e8ec-a52f617200ed"
      },
      "source": [
        "print(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE2H9ryz7bHU",
        "colab_type": "text"
      },
      "source": [
        "**Apply Seaborn to preliminary analyze the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BKg6gZ8qS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colums_to_analyze = ['isFraud', 'TransactionDT', 'TransactionAmt', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2']\n",
        "analyzing_data = dataset_transaction[colums_to_analyze]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWkHi4N7kKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "f87f0abd-9760-4edf-88ea-d53cc3bf1af1"
      },
      "source": [
        "corr = analyzing_data.corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efeeaeffba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf6--7Dn6PZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Creat the train/val dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-8fmFWoOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "aa9056dd-fe55-4d46-a672-64d1c875383d"
      },
      "source": [
        "# Create a copy\n",
        "dataset = copy.copy(dataset_transaction)\n",
        "\n",
        "# Remove the irrelevant columns\n",
        "dataset.pop('TransactionID')\n",
        "dataset.head(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>...</th>\n",
              "      <th>R_emaildomain_49</th>\n",
              "      <th>R_emaildomain_50</th>\n",
              "      <th>R_emaildomain_51</th>\n",
              "      <th>R_emaildomain_52</th>\n",
              "      <th>R_emaildomain_53</th>\n",
              "      <th>R_emaildomain_54</th>\n",
              "      <th>R_emaildomain_55</th>\n",
              "      <th>R_emaildomain_56</th>\n",
              "      <th>R_emaildomain_57</th>\n",
              "      <th>R_emaildomain_58</th>\n",
              "      <th>R_emaildomain_59</th>\n",
              "      <th>R_emaildomain_60</th>\n",
              "      <th>M1_0</th>\n",
              "      <th>M1_1</th>\n",
              "      <th>M1_2</th>\n",
              "      <th>M2_0</th>\n",
              "      <th>M2_1</th>\n",
              "      <th>M2_2</th>\n",
              "      <th>M3_0</th>\n",
              "      <th>M3_1</th>\n",
              "      <th>M3_2</th>\n",
              "      <th>M4_0</th>\n",
              "      <th>M4_1</th>\n",
              "      <th>M4_2</th>\n",
              "      <th>M4_3</th>\n",
              "      <th>M5_0</th>\n",
              "      <th>M5_1</th>\n",
              "      <th>M5_2</th>\n",
              "      <th>M6_0</th>\n",
              "      <th>M6_1</th>\n",
              "      <th>M6_2</th>\n",
              "      <th>M7_0</th>\n",
              "      <th>M7_1</th>\n",
              "      <th>M7_2</th>\n",
              "      <th>M8_0</th>\n",
              "      <th>M8_1</th>\n",
              "      <th>M8_2</th>\n",
              "      <th>M9_0</th>\n",
              "      <th>M9_1</th>\n",
              "      <th>M9_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>0.231445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.418213</td>\n",
              "      <td>0.055145</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>-0.009674</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.125488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.018738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.126709</td>\n",
              "      <td>-0.184814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.003321</td>\n",
              "      <td>-0.410645</td>\n",
              "      <td>0.082886</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.077881</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.170166</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002380</td>\n",
              "      <td>-0.301025</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.242920</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.016388</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.141235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.141479</td>\n",
              "      <td>0.232910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157227</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>0.473389</td>\n",
              "      <td>0.408936</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.600586</td>\n",
              "      <td>0.421143</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>-0.001804</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.002251</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.016571</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.002584</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>0.027588</td>\n",
              "      <td>-0.089966</td>\n",
              "      <td>-0.034607</td>\n",
              "      <td>-0.046417</td>\n",
              "      <td>-0.051697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045654</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.463379</td>\n",
              "      <td>-0.002663</td>\n",
              "      <td>-0.310547</td>\n",
              "      <td>0.302979</td>\n",
              "      <td>-0.024384</td>\n",
              "      <td>-0.709961</td>\n",
              "      <td>0.293701</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002794</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-0.001817</td>\n",
              "      <td>-0.015961</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.021332</td>\n",
              "      <td>-0.001302</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-0.001279</td>\n",
              "      <td>-0.010811</td>\n",
              "      <td>-0.005104</td>\n",
              "      <td>-0.147461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 904 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   isFraud  TransactionDT  TransactionAmt     card1  ...  M8_2  M9_0  M9_1  M9_2\n",
              "0        0      -0.463379       -0.002083  0.231445  ...     0     0     1     0\n",
              "1        0      -0.463379       -0.003321 -0.410645  ...     0     0     1     0\n",
              "2        0      -0.463379       -0.002380 -0.301025  ...     0     1     0     0\n",
              "3        0      -0.463379       -0.002663  0.473389  ...     0     0     1     0\n",
              "4        0      -0.463379       -0.002663 -0.310547  ...     0     0     1     0\n",
              "\n",
              "[5 rows x 904 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KODCOzZbOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c4d07e0-5bbc-47c4-ded9-8adc86d6c7b5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = dataset['isFraud']\n",
        "dataset.pop('isFraud')\n",
        "X = dataset\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "\n",
        "#X_train = np.expand_dims(X_train, axis=2)\n",
        "#X_test = np.expand_dims(X_test, axis=2)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(472432, 903) (472432,) (118108, 903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZY_7rXajHM",
        "colab_type": "text"
      },
      "source": [
        "**Downsampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_kQE1U9amFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d25bd736-2422-4102-e603-c9e50a4b1142"
      },
      "source": [
        "downsampling_factor = 10\n",
        "indices_1 = np.argwhere(np.array(Y_train)==1)\n",
        "indices_0_new = np.argwhere(np.array(Y_train)==0)\n",
        "indices = np.arange(0,len(indices_0_new),downsampling_factor)\n",
        "indices_0_new = indices_0_new[indices]\n",
        "\n",
        "print(indices_0_new.shape)\n",
        "indices_0_new = np.concatenate((indices_0_new, indices_1), axis=0)\n",
        "print(indices_0_new.shape)\n",
        "\n",
        "\n",
        "X_to_train = np.array(X_train)[indices_0_new]\n",
        "Y_to_train = np.array(Y_train)[indices_0_new]\n",
        "\n",
        "\n",
        "X_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\n",
        "Y_to_train = np.squeeze(Y_to_train, axis=1)\n",
        "print(X_to_train.shape, Y_to_train.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45597, 1)\n",
            "(62064, 1)\n",
            "(62064, 903) (62064,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Foj6lbEvL",
        "colab_type": "text"
      },
      "source": [
        "**Check the imbalane of the train/test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyHSb5S3bDdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d6ab3021-2b9e-4059-90a5-7e5bb94bac72"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(Y_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 3.49%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXWUlEQVR4nO3dbYwd1XnA8f9Tm5fmDQx2U2S7rFEtRaZqBbEIJahNoArGNDFVX2SUNiZ166aBioiqrSlSU6WKSr6UBDVNhQAVpChAyZubQKmLjaoW2bCmgDHUsBin2KLBsR0IikoKffphzpLx7T27d+29sxvv/ydd7cxzztzz+NzxfXZm7p2NzESSpH5+bKYTkCTNXhYJSVKVRUKSVGWRkCRVWSQkSVXzZzqB6bZw4cIcGRmZ6TQk6UfKjh07vpOZi3rjx12RGBkZYXR0dKbTkKQfKRHxrX5xTzdJkqosEpKkKouEJKnquLsmcSxGNn5zplPQcWzvDZfNdArSlHkkIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpKqBi0REzIuIf4+Ib5T1ZRGxPSLGIuKuiDixxE8q62OlfaT1HNeV+O6IuKQVX1ViYxGxsRXvO4YkqRtTOZK4Bni6tf4Z4MbM/GngMLC+xNcDh0v8xtKPiFgBrAXOBlYBf1MKzzzg88ClwArgitJ3ojEkSR0YqEhExBLgMuCWsh7ARcA9pcvtwOVleU1Zp7RfXPqvAe7MzNcy83lgDDivPMYyc09m/gC4E1gzyRiSpA4MeiTxWeCPgf8t66cD383M18v6PmBxWV4MvABQ2l8u/d+M92xTi080hiSpA5MWiYj4ZeClzNzRQT5HJSI2RMRoRIweOHBgptORpOPGIEcS7wU+FBF7aU4FXQR8Djg1IsZvELgE2F+W9wNLAUr7KcDBdrxnm1r84ARjHCEzb87MlZm5ctGi//eHlSRJR2nSIpGZ12XmkswcobnwvCUzPwxsBX6tdFsHfL0sbyrrlPYtmZklvrZ8+mkZsBx4GHgEWF4+yXRiGWNT2aY2hiSpA8fyPYk/Aa6NiDGa6we3lvitwOklfi2wESAzdwF3A08B/whclZlvlGsOVwP303x66u7Sd6IxJEkdmNLfk8jMB4EHy/Iemk8m9fb5b+DXK9t/Gvh0n/i9wL194n3HkCR1w29cS5KqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqomLRIRsTQitkbEUxGxKyKuKfHTImJzRDxbfi4o8YiImyJiLCKeiIhzW8+1rvR/NiLWteLvjoidZZubIiImGkOS1I1BjiReB/4wM1cA5wNXRcQKYCPwQGYuBx4o6wCXAsvLYwPwBWje8IFPAu8BzgM+2XrT/wLwu63tVpV4bQxJUgcmLRKZ+WJmPlqWvwc8DSwG1gC3l263A5eX5TXAHdnYBpwaEWcAlwCbM/NQZh4GNgOrSts7MnNbZiZwR89z9RtDktSBKV2TiIgR4BxgO/DOzHyxNP0X8M6yvBh4obXZvhKbKL6vT5wJxujNa0NEjEbE6IEDB6byT5IkTWDgIhERbwO+DHwiM19pt5UjgJzm3I4w0RiZeXNmrszMlYsWLRpmGpI0pwxUJCLiBJoC8cXM/EoJf7ucKqL8fKnE9wNLW5svKbGJ4kv6xCcaQ5LUgUE+3RTArcDTmflXraZNwPgnlNYBX2/FP1I+5XQ+8HI5ZXQ/8IGIWFAuWH8AuL+0vRIR55exPtLzXP3GkCR1YP4Afd4L/BawMyIeK7E/BW4A7o6I9cC3gN8obfcCq4Ex4PvARwEy81BE/AXwSOn3qcw8VJY/Dvwd8OPAfeXBBGNIkjowaZHIzH8FotJ8cZ/+CVxVea7bgNv6xEeBn+kTP9hvDElSN/zGtSSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkqvkzncBkImIV8DlgHnBLZt4wwylJR2Vk4zdnOgUdx/becNlQnndWH0lExDzg88ClwArgiohYMbNZSdLcMauLBHAeMJaZezLzB8CdwJoZzkmS5ozZfrppMfBCa30f8J7eThGxAdhQVl+NiN1HOd5C4DtHue0wmdfUmNfUmNfUzMq84jPHnNeZ/YKzvUgMJDNvBm4+1ueJiNHMXDkNKU0r85oa85oa85qauZbXbD/dtB9Y2lpfUmKSpA7M9iLxCLA8IpZFxInAWmDTDOckSXPGrD7dlJmvR8TVwP00H4G9LTN3DXHIYz5lNSTmNTXmNTXmNTVzKq/IzGE8ryTpODDbTzdJkmaQRUKSVDVnikRErIqI3RExFhEb+7SfFBF3lfbtETHSaruuxHdHxCUd53VtRDwVEU9ExAMRcWar7Y2IeKw8pvWC/gB5XRkRB1rj/06rbV1EPFse6zrO68ZWTs9ExHdbbUOZr4i4LSJeiognK+0RETeVnJ+IiHNbbcOcq8ny+nDJZ2dEPBQRP9dq21vij0XEaMd5vS8iXm69Vn/Wapvw9R9yXn/UyunJsj+dVtqGOV9LI2JreR/YFRHX9OkzvH0sM4/7B81F7+eAs4ATgceBFT19Pg78bVleC9xVlleU/icBy8rzzOswr/cDbynLvz+eV1l/dQbn60rgr/tsexqwp/xcUJYXdJVXT/8/oPmww7Dn6xeAc4EnK+2rgfuAAM4Htg97rgbM64Lx8WhufbO91bYXWDhD8/U+4BvH+vpPd149fT8IbOlovs4Azi3Lbwee6fP/cWj72Fw5khjk9h5rgNvL8j3AxRERJX5nZr6Wmc8DY+X5OskrM7dm5vfL6jaa74oM27HcDuUSYHNmHsrMw8BmYNUM5XUF8KVpGrsqM/8FODRBlzXAHdnYBpwaEWcw3LmaNK/MfKiMC93tW4PMV81Qb9Mzxbw62bcAMvPFzHy0LH8PeJrmbhRtQ9vH5kqR6Hd7j95JfrNPZr4OvAycPuC2w8yrbT3NbwvjTo6I0YjYFhGXT1NOU8nrV8uh7T0RMf6lx1kxX+W03DJgSys8rPmaTC3vYc7VVPXuWwn8U0TsiOa2N137+Yh4PCLui4izS2xWzFdEvIXmjfbLrXAn8xXNafBzgO09TUPbx2b19yT0QxHxm8BK4Bdb4TMzc39EnAVsiYidmflcRyn9A/ClzHwtIn6P5ijsoo7GHsRa4J7MfKMVm8n5mrUi4v00ReLCVvjCMlc/AWyOiP8ov2l34VGa1+rViFgNfA1Y3tHYg/gg8G+Z2T7qGPp8RcTbaArTJzLzlel87onMlSOJQW7v8WafiJgPnAIcHHDbYeZFRPwScD3wocx8bTyemfvLzz3AgzS/YXSSV2YebOVyC/DuQbcdZl4ta+k5HTDE+ZpMLe8Zv+1MRPwszeu3JjMPjsdbc/US8FWm7xTrpDLzlcx8tSzfC5wQEQuZBfNVTLRvDWW+IuIEmgLxxcz8Sp8uw9vHhnGhZbY9aI6Y9tCcfhi/4HV2T5+rOPLC9d1l+WyOvHC9h+m7cD1IXufQXKxb3hNfAJxUlhcCzzJNF/EGzOuM1vKvANvyhxfKni/5LSjLp3WVV+n3LpoLidHFfJXnHKF+IfYyjryo+PCw52rAvH6K5hrbBT3xtwJvby0/BKzqMK+fHH/taN5s/7PM3UCv/7DyKu2n0Fy3eGtX81X+7XcAn52gz9D2sWmb3Nn+oLn6/wzNG+71JfYpmt/OAU4G/r78p3kYOKu17fVlu93ApR3n9c/At4HHymNTiV8A7Cz/UXYC6zvO6y+BXWX8rcC7Wtv+dpnHMeCjXeZV1v8cuKFnu6HNF81vlS8C/0Nzznc98DHgY6U9aP541nNl7JUdzdVked0CHG7tW6MlflaZp8fLa3x9x3ld3dq3ttEqYv1e/67yKn2upPkgS3u7Yc/XhTTXPJ5ovVaru9rHvC2HJKlqrlyTkCQdBYuEJKnKIiFJqjruviexcOHCHBkZmek0JOlHyo4dO76TmYt648ddkRgZGWF0dFrvryVJx72I+Fa/uKebJElVFglJUpVFQpJUddxdkzgWIxu/OdMp6Di294bLZjoFaco8kpAkVVkkJElVFglJUpVFQpJUZZGQJFVZJCRJVRYJSVLVwEUiIuZFxL9HxDfK+rKI2B4RYxFxV0ScWOInlfWx0j7Seo7rSnx3RFzSiq8qsbGI2NiK9x1DktSNqRxJXAM83Vr/DHBjZv40zZ9AXF/i64HDJX5j6UdErKD529FnA6uAvymFZx7Nn927FFgBXFH6TjSGJKkDAxWJiFhC84e2bynrAVwE3FO63A5cXpbXlHVK+8Wl/xqavw37WmY+T/P3Vs8rj7HM3JOZPwDuBNZMMoYkqQODHkl8Fvhj4H/L+unAdzPz9bK+D1hclhcDLwCU9pdL/zfjPdvU4hONcYSI2BARoxExeuDAgQH/SZKkyUxaJCLil4GXMnNHB/kclcy8OTNXZubKRYv+39/MkCQdpUFu8Pde4EMRsRo4GXgH8Dng1IiYX37TXwLsL/33A0uBfRExHzgFONiKj2tv0y9+cIIxJEkdmPRIIjOvy8wlmTlCc+F5S2Z+GNgK/Frptg74elneVNYp7VsyM0t8bfn00zJgOfAw8AiwvHyS6cQyxqayTW0MSVIHjuV7En8CXBsRYzTXD24t8VuB00v8WmAjQGbuAu4GngL+EbgqM98oRwlXA/fTfHrq7tJ3ojEkSR2Y0t+TyMwHgQfL8h6aTyb19vlv4Ncr238a+HSf+L3AvX3ifceQJHXDb1xLkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqatEhExNKI2BoRT0XEroi4psRPi4jNEfFs+bmgxCMiboqIsYh4IiLObT3XutL/2YhY14q/OyJ2lm1uioiYaAxJUjcGOZJ4HfjDzFwBnA9cFRErgI3AA5m5HHigrANcCiwvjw3AF6B5wwc+CbwHOA/4ZOtN/wvA77a2W1XitTEkSR2YtEhk5ouZ+WhZ/h7wNLAYWAPcXrrdDlxeltcAd2RjG3BqRJwBXAJszsxDmXkY2AysKm3vyMxtmZnAHT3P1W8MSVIHpnRNIiJGgHOA7cA7M/PF0vRfwDvL8mLghdZm+0psovi+PnEmGEOS1IGBi0REvA34MvCJzHyl3VaOAHKaczvCRGNExIaIGI2I0QMHDgwzDUmaUwYqEhFxAk2B+GJmfqWEv11OFVF+vlTi+4Glrc2XlNhE8SV94hONcYTMvDkzV2bmykWLFg3yT5IkDWCQTzcFcCvwdGb+VatpEzD+CaV1wNdb8Y+UTzmdD7xcThndD3wgIhaUC9YfAO4vba9ExPllrI/0PFe/MSRJHZg/QJ/3Ar8F7IyIx0rsT4EbgLsjYj3wLeA3Stu9wGpgDPg+8FGAzDwUEX8BPFL6fSozD5XljwN/B/w4cF95MMEYkqQOTFokMvNfgag0X9ynfwJXVZ7rNuC2PvFR4Gf6xA/2G0OS1A2/cS1JqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqcoiIUmqskhIkqosEpKkKouEJKnKIiFJqrJISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqQqi4QkqWr+TCcgzRUjG7850ynoOLb3hsuG8ryz/kgiIlZFxO6IGIuIjTOdjyTNJbO6SETEPODzwKXACuCKiFgxs1lJ0twxq4sEcB4wlpl7MvMHwJ3AmhnOSZLmjNl+TWIx8EJrfR/wnt5OEbEB2FBWX42I3Uc53kLgO0e57TCZ19SY19SY19TMyrziM8ec15n9grO9SAwkM28Gbj7W54mI0cxcOQ0pTSvzmhrzmhrzmpq5ltdsP920H1jaWl9SYpKkDsz2IvEIsDwilkXEicBaYNMM5yRJc8asPt2Uma9HxNXA/cA84LbM3DXEIY/5lNWQmNfUmNfUmNfUzKm8IjOH8bySpOPAbD/dJEmaQRYJSVLVnCkSk93eIyJOioi7Svv2iBhptV1X4rsj4pKO87o2Ip6KiCci4oGIOLPV9kZEPFYe03pBf4C8royIA63xf6fVti4ini2PdR3ndWMrp2ci4ruttqHMV0TcFhEvRcSTlfaIiJtKzk9ExLmttmHO1WR5fbjkszMiHoqIn2u17S3xxyJitOO83hcRL7deqz9rtQ3tNj0D5PVHrZyeLPvTaaVtmPO1NCK2lveBXRFxTZ8+w9vHMvO4f9Bc9H4OOAs4EXgcWNHT5+PA35bltcBdZXlF6X8SsKw8z7wO83o/8Jay/PvjeZX1V2dwvq4E/rrPtqcBe8rPBWV5QVd59fT/A5oPOwx7vn4BOBd4stK+GrgPCOB8YPuw52rAvC4YH4/m1jfbW217gYUzNF/vA75xrK//dOfV0/eDwJaO5usM4Nyy/HbgmT7/H4e2j82VI4lBbu+xBri9LN8DXBwRUeJ3ZuZrmfk8MFaer5O8MnNrZn6/rG6j+a7IsB3L7VAuATZn5qHMPAxsBlbNUF5XAF+aprGrMvNfgEMTdFkD3JGNbcCpEXEGw52rSfPKzIfKuNDdvjXIfNUM9TY9U8yrk30LIDNfzMxHy/L3gKdp7kbRNrR9bK4UiX639+id5Df7ZObrwMvA6QNuO8y82tbT/LYw7uSIGI2IbRFx+TTlNJW8frUc2t4TEeNfepwV81VOyy0DtrTCw5qvydTyHuZcTVXvvpXAP0XEjmhue9O1n4+IxyPivog4u8RmxXxFxFto3mi/3Ap3Ml/RnAY/B9je0zS0fWxWf09CPxQRvwmsBH6xFT4zM/dHxFnAlojYmZnPdZTSPwBfyszXIuL3aI7CLupo7EGsBe7JzDdasZmcr1krIt5PUyQubIUvLHP1E8DmiPiP8pt2Fx6lea1ejYjVwNeA5R2NPYgPAv+Wme2jjqHPV0S8jaYwfSIzX5nO557IXDmSGOT2Hm/2iYj5wCnAwQG3HWZeRMQvAdcDH8rM18bjmbm//NwDPEjzG0YneWXmwVYutwDvHnTbYebVspae0wFDnK/J1PKe8dvORMTP0rx+azLz4Hi8NVcvAV9l+k6xTiozX8nMV8vyvcAJEbGQWTBfxUT71lDmKyJOoCkQX8zMr/TpMrx9bBgXWmbbg+aIaQ/N6YfxC15n9/S5iiMvXN9dls/myAvXe5i+C9eD5HUOzcW65T3xBcBJZXkh8CzTdBFvwLzOaC3/CrAtf3ih7PmS34KyfFpXeZV+76K5kBhdzFd5zhHqF2Iv48iLig8Pe64GzOunaK6xXdATfyvw9tbyQ8CqDvP6yfHXjubN9j/L3A30+g8rr9J+Cs11i7d2NV/l334H8NkJ+gxtH5u2yZ3tD5qr/8/QvOFeX2KfovntHOBk4O/Lf5qHgbNa215fttsNXNpxXv8MfBt4rDw2lfgFwM7yH2UnsL7jvP4S2FXG3wq8q7Xtb5d5HAM+2mVeZf3PgRt6thvafNH8Vvki8D8053zXAx8DPlbag+aPZz1Xxl7Z0VxNltctwOHWvjVa4meVeXq8vMbXd5zX1a19axutItbv9e8qr9LnSpoPsrS3G/Z8XUhzzeOJ1mu1uqt9zNtySJKq5so1CUnSUbBISJKqLBKSpCqLhCSpyiIhSaqySEiSqiwSkqSq/wM/CPfAO6sN3QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvCbtngmd6iw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d44999f2-83d3-4a18-a0b7-8d298c5b3ffc"
      },
      "source": [
        "plt.hist(Y_to_train, bins=[0,1,2])\n",
        "\n",
        "fraud_count = np.unique(Y_to_train, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 26.53%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geeGh4HLc0Xg",
        "colab_type": "text"
      },
      "source": [
        "# ***The model using NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MD1cOJcye2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "830e5f0d-2905-4a6e-bc08-2a06859cb01a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1eD9xGMidhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "278518e2-0528-4c95-a949-1e473b513f89"
      },
      "source": [
        "import keras\n",
        "\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras import Sequential\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.optimizers import Adam, SGD\n",
        "keras.__version__\n",
        "Adam()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkPujj1hlrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr=0.001):\n",
        "  out_model = Sequential()\n",
        "  out_model.add(Dense(dense1, activation=\"relu\", input_shape=(X_train.shape[1],),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std)))\n",
        "  out_model.add(Dense(dense1, activation=\"relu\",\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "\n",
        "  out_model.add(Dense(dense2, activation=\"relu\", \n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std)))\n",
        "  out_model.add(Dense(dense2, activation=\"relu\",\n",
        "                      kernel_regularizer=tf.keras.regularizers.l1(l1_rate),\n",
        "                      kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_std, maxval=init_std)))\n",
        "  out_model.add(Dropout(dropout_rate))\n",
        "\n",
        "  out_model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  out_model.compile(\n",
        "            optimizer=Adam(learning_rate=lr),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['binary_accuracy'])\n",
        "  \n",
        "  return out_model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B8icGb9id1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "555edda4-0db9-4ae4-b6a6-4a737b9913e6"
      },
      "source": [
        "my_model = create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0, l2_rate=0, init_std=0.05, lr=0.0001)\n",
        "my_model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               115712    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 144,705\n",
            "Trainable params: 144,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEvS5-ysjDIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "485a102a-e18c-4dc9-aada-77c18ceea3b3"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NB_EPOCH = 1000\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_binary_accuracy', patience=50, verbose=0, mode='auto',\n",
        "    baseline=None)\n",
        "\n",
        "best_model_hold = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/best_model.pb', monitor='val_binary_accuracy', verbose=1, save_best_only=True,\n",
        "    save_weights_only=False, mode='auto')\n",
        "\n",
        "history = my_model.fit(X_to_train, Y_to_train, \n",
        "             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
        "             validation_split=0.2, shuffle=True,\n",
        "             callbacks=[early_stop, best_model_hold])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 49651 samples, validate on 12413 samples\n",
            "Epoch 1/1000\n",
            "49651/49651 [==============================] - 4s 84us/step - loss: 0.3318 - binary_accuracy: 0.9093 - val_loss: 2.2885 - val_binary_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_binary_accuracy improved from -inf to 0.00000, saving model to /content/best_model\n",
            "Epoch 2/1000\n",
            "49651/49651 [==============================] - 4s 78us/step - loss: 0.2395 - binary_accuracy: 0.9204 - val_loss: 1.9106 - val_binary_accuracy: 0.1261\n",
            "\n",
            "Epoch 00002: val_binary_accuracy improved from 0.00000 to 0.12608, saving model to /content/best_model\n",
            "Epoch 3/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.2171 - binary_accuracy: 0.9311 - val_loss: 1.9521 - val_binary_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_binary_accuracy improved from 0.12608 to 0.24386, saving model to /content/best_model\n",
            "Epoch 4/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.2084 - binary_accuracy: 0.9343 - val_loss: 1.7289 - val_binary_accuracy: 0.2785\n",
            "\n",
            "Epoch 00004: val_binary_accuracy improved from 0.24386 to 0.27850, saving model to /content/best_model\n",
            "Epoch 5/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.2059 - binary_accuracy: 0.9354 - val_loss: 1.6900 - val_binary_accuracy: 0.3019\n",
            "\n",
            "Epoch 00005: val_binary_accuracy improved from 0.27850 to 0.30194, saving model to /content/best_model\n",
            "Epoch 6/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.2018 - binary_accuracy: 0.9371 - val_loss: 1.8035 - val_binary_accuracy: 0.2828\n",
            "\n",
            "Epoch 00006: val_binary_accuracy did not improve from 0.30194\n",
            "Epoch 7/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.2015 - binary_accuracy: 0.9374 - val_loss: 1.8945 - val_binary_accuracy: 0.3165\n",
            "\n",
            "Epoch 00007: val_binary_accuracy improved from 0.30194 to 0.31652, saving model to /content/best_model\n",
            "Epoch 8/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1981 - binary_accuracy: 0.9379 - val_loss: 1.6513 - val_binary_accuracy: 0.2986\n",
            "\n",
            "Epoch 00008: val_binary_accuracy did not improve from 0.31652\n",
            "Epoch 9/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1955 - binary_accuracy: 0.9387 - val_loss: 1.5396 - val_binary_accuracy: 0.3246\n",
            "\n",
            "Epoch 00009: val_binary_accuracy improved from 0.31652 to 0.32458, saving model to /content/best_model\n",
            "Epoch 10/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1948 - binary_accuracy: 0.9388 - val_loss: 1.7335 - val_binary_accuracy: 0.3363\n",
            "\n",
            "Epoch 00010: val_binary_accuracy improved from 0.32458 to 0.33626, saving model to /content/best_model\n",
            "Epoch 11/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1939 - binary_accuracy: 0.9399 - val_loss: 1.7668 - val_binary_accuracy: 0.3328\n",
            "\n",
            "Epoch 00011: val_binary_accuracy did not improve from 0.33626\n",
            "Epoch 12/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1928 - binary_accuracy: 0.9407 - val_loss: 1.7097 - val_binary_accuracy: 0.3509\n",
            "\n",
            "Epoch 00012: val_binary_accuracy improved from 0.33626 to 0.35092, saving model to /content/best_model\n",
            "Epoch 13/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1912 - binary_accuracy: 0.9407 - val_loss: 1.5747 - val_binary_accuracy: 0.3865\n",
            "\n",
            "Epoch 00013: val_binary_accuracy improved from 0.35092 to 0.38653, saving model to /content/best_model\n",
            "Epoch 14/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1906 - binary_accuracy: 0.9406 - val_loss: 1.7714 - val_binary_accuracy: 0.3287\n",
            "\n",
            "Epoch 00014: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 15/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1898 - binary_accuracy: 0.9409 - val_loss: 1.7084 - val_binary_accuracy: 0.3487\n",
            "\n",
            "Epoch 00015: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 16/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1892 - binary_accuracy: 0.9421 - val_loss: 1.8589 - val_binary_accuracy: 0.3326\n",
            "\n",
            "Epoch 00016: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 17/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1883 - binary_accuracy: 0.9418 - val_loss: 1.8389 - val_binary_accuracy: 0.3328\n",
            "\n",
            "Epoch 00017: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 18/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1860 - binary_accuracy: 0.9424 - val_loss: 1.6813 - val_binary_accuracy: 0.3545\n",
            "\n",
            "Epoch 00018: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 19/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1871 - binary_accuracy: 0.9426 - val_loss: 1.7180 - val_binary_accuracy: 0.3333\n",
            "\n",
            "Epoch 00019: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 20/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1860 - binary_accuracy: 0.9428 - val_loss: 1.6677 - val_binary_accuracy: 0.3563\n",
            "\n",
            "Epoch 00020: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 21/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1846 - binary_accuracy: 0.9429 - val_loss: 1.9372 - val_binary_accuracy: 0.3684\n",
            "\n",
            "Epoch 00021: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 22/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1848 - binary_accuracy: 0.9431 - val_loss: 1.8277 - val_binary_accuracy: 0.3317\n",
            "\n",
            "Epoch 00022: val_binary_accuracy did not improve from 0.38653\n",
            "Epoch 23/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1836 - binary_accuracy: 0.9435 - val_loss: 1.5055 - val_binary_accuracy: 0.4134\n",
            "\n",
            "Epoch 00023: val_binary_accuracy improved from 0.38653 to 0.41336, saving model to /content/best_model\n",
            "Epoch 24/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1826 - binary_accuracy: 0.9434 - val_loss: 1.6404 - val_binary_accuracy: 0.3634\n",
            "\n",
            "Epoch 00024: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 25/1000\n",
            "49651/49651 [==============================] - 4s 76us/step - loss: 0.1823 - binary_accuracy: 0.9436 - val_loss: 1.9007 - val_binary_accuracy: 0.3239\n",
            "\n",
            "Epoch 00025: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 26/1000\n",
            "49651/49651 [==============================] - 4s 80us/step - loss: 0.1811 - binary_accuracy: 0.9443 - val_loss: 1.6024 - val_binary_accuracy: 0.3777\n",
            "\n",
            "Epoch 00026: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 27/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1813 - binary_accuracy: 0.9440 - val_loss: 1.6333 - val_binary_accuracy: 0.3549\n",
            "\n",
            "Epoch 00027: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 28/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1804 - binary_accuracy: 0.9446 - val_loss: 1.6505 - val_binary_accuracy: 0.3639\n",
            "\n",
            "Epoch 00028: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 29/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1796 - binary_accuracy: 0.9450 - val_loss: 1.4718 - val_binary_accuracy: 0.3951\n",
            "\n",
            "Epoch 00029: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 30/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1791 - binary_accuracy: 0.9452 - val_loss: 1.6899 - val_binary_accuracy: 0.3625\n",
            "\n",
            "Epoch 00030: val_binary_accuracy did not improve from 0.41336\n",
            "Epoch 31/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1776 - binary_accuracy: 0.9446 - val_loss: 1.3704 - val_binary_accuracy: 0.4497\n",
            "\n",
            "Epoch 00031: val_binary_accuracy improved from 0.41336 to 0.44969, saving model to /content/best_model\n",
            "Epoch 32/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1780 - binary_accuracy: 0.9454 - val_loss: 1.6688 - val_binary_accuracy: 0.3824\n",
            "\n",
            "Epoch 00032: val_binary_accuracy did not improve from 0.44969\n",
            "Epoch 33/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1755 - binary_accuracy: 0.9465 - val_loss: 1.4553 - val_binary_accuracy: 0.3997\n",
            "\n",
            "Epoch 00033: val_binary_accuracy did not improve from 0.44969\n",
            "Epoch 34/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.1778 - binary_accuracy: 0.9452 - val_loss: 1.8070 - val_binary_accuracy: 0.3649\n",
            "\n",
            "Epoch 00034: val_binary_accuracy did not improve from 0.44969\n",
            "Epoch 35/1000\n",
            "49651/49651 [==============================] - 4s 87us/step - loss: 0.1757 - binary_accuracy: 0.9462 - val_loss: 1.8474 - val_binary_accuracy: 0.3341\n",
            "\n",
            "Epoch 00035: val_binary_accuracy did not improve from 0.44969\n",
            "Epoch 36/1000\n",
            "49651/49651 [==============================] - 4s 77us/step - loss: 0.1749 - binary_accuracy: 0.9461 - val_loss: 1.5235 - val_binary_accuracy: 0.3996\n",
            "\n",
            "Epoch 00036: val_binary_accuracy did not improve from 0.44969\n",
            "Epoch 37/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.1748 - binary_accuracy: 0.9463 - val_loss: 1.5494 - val_binary_accuracy: 0.4155\n",
            "\n",
            "Epoch 00037: val_binary_accuracy did not improve from 0.44969\n",
            "Epoch 38/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1739 - binary_accuracy: 0.9468 - val_loss: 1.4477 - val_binary_accuracy: 0.4598\n",
            "\n",
            "Epoch 00038: val_binary_accuracy improved from 0.44969 to 0.45984, saving model to /content/best_model\n",
            "Epoch 39/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.1722 - binary_accuracy: 0.9468 - val_loss: 1.7099 - val_binary_accuracy: 0.3673\n",
            "\n",
            "Epoch 00039: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 40/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1709 - binary_accuracy: 0.9476 - val_loss: 1.5830 - val_binary_accuracy: 0.3926\n",
            "\n",
            "Epoch 00040: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 41/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1716 - binary_accuracy: 0.9476 - val_loss: 1.5151 - val_binary_accuracy: 0.4097\n",
            "\n",
            "Epoch 00041: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 42/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1708 - binary_accuracy: 0.9476 - val_loss: 1.7891 - val_binary_accuracy: 0.3427\n",
            "\n",
            "Epoch 00042: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 43/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1687 - binary_accuracy: 0.9480 - val_loss: 1.6552 - val_binary_accuracy: 0.4000\n",
            "\n",
            "Epoch 00043: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 44/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1697 - binary_accuracy: 0.9477 - val_loss: 1.4461 - val_binary_accuracy: 0.3748\n",
            "\n",
            "Epoch 00044: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 45/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1688 - binary_accuracy: 0.9479 - val_loss: 1.6156 - val_binary_accuracy: 0.4173\n",
            "\n",
            "Epoch 00045: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 46/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1680 - binary_accuracy: 0.9492 - val_loss: 1.5532 - val_binary_accuracy: 0.4002\n",
            "\n",
            "Epoch 00046: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 47/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1681 - binary_accuracy: 0.9488 - val_loss: 1.5946 - val_binary_accuracy: 0.4261\n",
            "\n",
            "Epoch 00047: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 48/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1659 - binary_accuracy: 0.9493 - val_loss: 1.5723 - val_binary_accuracy: 0.4225\n",
            "\n",
            "Epoch 00048: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 49/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1652 - binary_accuracy: 0.9499 - val_loss: 1.5659 - val_binary_accuracy: 0.4105\n",
            "\n",
            "Epoch 00049: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 50/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1648 - binary_accuracy: 0.9495 - val_loss: 1.7279 - val_binary_accuracy: 0.3847\n",
            "\n",
            "Epoch 00050: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 51/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1635 - binary_accuracy: 0.9497 - val_loss: 1.6940 - val_binary_accuracy: 0.3802\n",
            "\n",
            "Epoch 00051: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 52/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1635 - binary_accuracy: 0.9503 - val_loss: 1.4442 - val_binary_accuracy: 0.4389\n",
            "\n",
            "Epoch 00052: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 53/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1628 - binary_accuracy: 0.9507 - val_loss: 1.4408 - val_binary_accuracy: 0.4297\n",
            "\n",
            "Epoch 00053: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 54/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1617 - binary_accuracy: 0.9509 - val_loss: 1.7577 - val_binary_accuracy: 0.3860\n",
            "\n",
            "Epoch 00054: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 55/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1613 - binary_accuracy: 0.9508 - val_loss: 1.8108 - val_binary_accuracy: 0.3719\n",
            "\n",
            "Epoch 00055: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 56/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1601 - binary_accuracy: 0.9513 - val_loss: 1.8336 - val_binary_accuracy: 0.3725\n",
            "\n",
            "Epoch 00056: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 57/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1597 - binary_accuracy: 0.9510 - val_loss: 1.6344 - val_binary_accuracy: 0.4172\n",
            "\n",
            "Epoch 00057: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 58/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1581 - binary_accuracy: 0.9516 - val_loss: 1.6057 - val_binary_accuracy: 0.4276\n",
            "\n",
            "Epoch 00058: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 59/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1583 - binary_accuracy: 0.9510 - val_loss: 1.6591 - val_binary_accuracy: 0.3920\n",
            "\n",
            "Epoch 00059: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 60/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1576 - binary_accuracy: 0.9518 - val_loss: 1.6863 - val_binary_accuracy: 0.4152\n",
            "\n",
            "Epoch 00060: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 61/1000\n",
            "49651/49651 [==============================] - 4s 78us/step - loss: 0.1565 - binary_accuracy: 0.9517 - val_loss: 1.4320 - val_binary_accuracy: 0.4262\n",
            "\n",
            "Epoch 00061: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 62/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1551 - binary_accuracy: 0.9525 - val_loss: 1.9859 - val_binary_accuracy: 0.3337\n",
            "\n",
            "Epoch 00062: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 63/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1542 - binary_accuracy: 0.9534 - val_loss: 1.6412 - val_binary_accuracy: 0.4301\n",
            "\n",
            "Epoch 00063: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 64/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1549 - binary_accuracy: 0.9527 - val_loss: 1.6971 - val_binary_accuracy: 0.4111\n",
            "\n",
            "Epoch 00064: val_binary_accuracy did not improve from 0.45984\n",
            "Epoch 65/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1545 - binary_accuracy: 0.9533 - val_loss: 1.4036 - val_binary_accuracy: 0.4770\n",
            "\n",
            "Epoch 00065: val_binary_accuracy improved from 0.45984 to 0.47700, saving model to /content/best_model\n",
            "Epoch 66/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1541 - binary_accuracy: 0.9528 - val_loss: 1.8034 - val_binary_accuracy: 0.4005\n",
            "\n",
            "Epoch 00066: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 67/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1520 - binary_accuracy: 0.9541 - val_loss: 1.5025 - val_binary_accuracy: 0.4516\n",
            "\n",
            "Epoch 00067: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 68/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1511 - binary_accuracy: 0.9542 - val_loss: 1.5624 - val_binary_accuracy: 0.4399\n",
            "\n",
            "Epoch 00068: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 69/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1502 - binary_accuracy: 0.9541 - val_loss: 1.6424 - val_binary_accuracy: 0.4054\n",
            "\n",
            "Epoch 00069: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 70/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1507 - binary_accuracy: 0.9545 - val_loss: 1.6436 - val_binary_accuracy: 0.4538\n",
            "\n",
            "Epoch 00070: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 71/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1492 - binary_accuracy: 0.9543 - val_loss: 1.5007 - val_binary_accuracy: 0.4577\n",
            "\n",
            "Epoch 00071: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 72/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1480 - binary_accuracy: 0.9547 - val_loss: 1.4920 - val_binary_accuracy: 0.4407\n",
            "\n",
            "Epoch 00072: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 73/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1484 - binary_accuracy: 0.9550 - val_loss: 1.6447 - val_binary_accuracy: 0.4567\n",
            "\n",
            "Epoch 00073: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 74/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1476 - binary_accuracy: 0.9551 - val_loss: 1.7239 - val_binary_accuracy: 0.4351\n",
            "\n",
            "Epoch 00074: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 75/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1468 - binary_accuracy: 0.9550 - val_loss: 1.7082 - val_binary_accuracy: 0.4047\n",
            "\n",
            "Epoch 00075: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 76/1000\n",
            "49651/49651 [==============================] - 4s 78us/step - loss: 0.1448 - binary_accuracy: 0.9560 - val_loss: 1.7273 - val_binary_accuracy: 0.4470\n",
            "\n",
            "Epoch 00076: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 77/1000\n",
            "49651/49651 [==============================] - 4s 77us/step - loss: 0.1457 - binary_accuracy: 0.9560 - val_loss: 1.4959 - val_binary_accuracy: 0.4661\n",
            "\n",
            "Epoch 00077: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 78/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1447 - binary_accuracy: 0.9565 - val_loss: 1.8135 - val_binary_accuracy: 0.4174\n",
            "\n",
            "Epoch 00078: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 79/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1430 - binary_accuracy: 0.9567 - val_loss: 1.5110 - val_binary_accuracy: 0.4390\n",
            "\n",
            "Epoch 00079: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 80/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1419 - binary_accuracy: 0.9571 - val_loss: 1.8411 - val_binary_accuracy: 0.4029\n",
            "\n",
            "Epoch 00080: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 81/1000\n",
            "49651/49651 [==============================] - 4s 78us/step - loss: 0.1418 - binary_accuracy: 0.9572 - val_loss: 1.8770 - val_binary_accuracy: 0.4179\n",
            "\n",
            "Epoch 00081: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 82/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.1437 - binary_accuracy: 0.9562 - val_loss: 1.5301 - val_binary_accuracy: 0.4606\n",
            "\n",
            "Epoch 00082: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 83/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1404 - binary_accuracy: 0.9572 - val_loss: 1.8205 - val_binary_accuracy: 0.4609\n",
            "\n",
            "Epoch 00083: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 84/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1412 - binary_accuracy: 0.9579 - val_loss: 1.6419 - val_binary_accuracy: 0.4537\n",
            "\n",
            "Epoch 00084: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 85/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1396 - binary_accuracy: 0.9573 - val_loss: 1.8990 - val_binary_accuracy: 0.4313\n",
            "\n",
            "Epoch 00085: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 86/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1408 - binary_accuracy: 0.9578 - val_loss: 1.7615 - val_binary_accuracy: 0.4274\n",
            "\n",
            "Epoch 00086: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 87/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1373 - binary_accuracy: 0.9587 - val_loss: 1.8084 - val_binary_accuracy: 0.4287\n",
            "\n",
            "Epoch 00087: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 88/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1369 - binary_accuracy: 0.9578 - val_loss: 1.6769 - val_binary_accuracy: 0.4405\n",
            "\n",
            "Epoch 00088: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 89/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1369 - binary_accuracy: 0.9587 - val_loss: 1.7328 - val_binary_accuracy: 0.4383\n",
            "\n",
            "Epoch 00089: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 90/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1364 - binary_accuracy: 0.9589 - val_loss: 1.6643 - val_binary_accuracy: 0.4491\n",
            "\n",
            "Epoch 00090: val_binary_accuracy did not improve from 0.47700\n",
            "Epoch 91/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1356 - binary_accuracy: 0.9588 - val_loss: 1.5560 - val_binary_accuracy: 0.4950\n",
            "\n",
            "Epoch 00091: val_binary_accuracy improved from 0.47700 to 0.49505, saving model to /content/best_model\n",
            "Epoch 92/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1347 - binary_accuracy: 0.9590 - val_loss: 1.6816 - val_binary_accuracy: 0.4735\n",
            "\n",
            "Epoch 00092: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 93/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1333 - binary_accuracy: 0.9592 - val_loss: 1.5011 - val_binary_accuracy: 0.4788\n",
            "\n",
            "Epoch 00093: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 94/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1332 - binary_accuracy: 0.9598 - val_loss: 1.7046 - val_binary_accuracy: 0.4687\n",
            "\n",
            "Epoch 00094: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 95/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1331 - binary_accuracy: 0.9600 - val_loss: 1.7861 - val_binary_accuracy: 0.4354\n",
            "\n",
            "Epoch 00095: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 96/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.1327 - binary_accuracy: 0.9595 - val_loss: 1.5219 - val_binary_accuracy: 0.4868\n",
            "\n",
            "Epoch 00096: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 97/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1315 - binary_accuracy: 0.9600 - val_loss: 1.5888 - val_binary_accuracy: 0.4570\n",
            "\n",
            "Epoch 00097: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 98/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1320 - binary_accuracy: 0.9601 - val_loss: 1.6330 - val_binary_accuracy: 0.4700\n",
            "\n",
            "Epoch 00098: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 99/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1321 - binary_accuracy: 0.9596 - val_loss: 1.5205 - val_binary_accuracy: 0.4801\n",
            "\n",
            "Epoch 00099: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 100/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1324 - binary_accuracy: 0.9594 - val_loss: 2.0463 - val_binary_accuracy: 0.3881\n",
            "\n",
            "Epoch 00100: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 101/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1292 - binary_accuracy: 0.9610 - val_loss: 1.8571 - val_binary_accuracy: 0.4529\n",
            "\n",
            "Epoch 00101: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 102/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1298 - binary_accuracy: 0.9607 - val_loss: 1.5442 - val_binary_accuracy: 0.4935\n",
            "\n",
            "Epoch 00102: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 103/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1290 - binary_accuracy: 0.9611 - val_loss: 1.9914 - val_binary_accuracy: 0.4186\n",
            "\n",
            "Epoch 00103: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 104/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1289 - binary_accuracy: 0.9602 - val_loss: 1.8503 - val_binary_accuracy: 0.4370\n",
            "\n",
            "Epoch 00104: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 105/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1260 - binary_accuracy: 0.9619 - val_loss: 1.7165 - val_binary_accuracy: 0.4615\n",
            "\n",
            "Epoch 00105: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 106/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1284 - binary_accuracy: 0.9606 - val_loss: 1.9347 - val_binary_accuracy: 0.4366\n",
            "\n",
            "Epoch 00106: val_binary_accuracy did not improve from 0.49505\n",
            "Epoch 107/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1286 - binary_accuracy: 0.9607 - val_loss: 1.5057 - val_binary_accuracy: 0.5033\n",
            "\n",
            "Epoch 00107: val_binary_accuracy improved from 0.49505 to 0.50326, saving model to /content/best_model\n",
            "Epoch 108/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1238 - binary_accuracy: 0.9625 - val_loss: 1.6605 - val_binary_accuracy: 0.4649\n",
            "\n",
            "Epoch 00108: val_binary_accuracy did not improve from 0.50326\n",
            "Epoch 109/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1252 - binary_accuracy: 0.9622 - val_loss: 1.8581 - val_binary_accuracy: 0.4272\n",
            "\n",
            "Epoch 00109: val_binary_accuracy did not improve from 0.50326\n",
            "Epoch 110/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1263 - binary_accuracy: 0.9618 - val_loss: 1.3593 - val_binary_accuracy: 0.5029\n",
            "\n",
            "Epoch 00110: val_binary_accuracy did not improve from 0.50326\n",
            "Epoch 111/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.1253 - binary_accuracy: 0.9619 - val_loss: 1.5359 - val_binary_accuracy: 0.4983\n",
            "\n",
            "Epoch 00111: val_binary_accuracy did not improve from 0.50326\n",
            "Epoch 112/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1234 - binary_accuracy: 0.9623 - val_loss: 1.8869 - val_binary_accuracy: 0.4507\n",
            "\n",
            "Epoch 00112: val_binary_accuracy did not improve from 0.50326\n",
            "Epoch 113/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1245 - binary_accuracy: 0.9619 - val_loss: 1.6524 - val_binary_accuracy: 0.5041\n",
            "\n",
            "Epoch 00113: val_binary_accuracy improved from 0.50326 to 0.50415, saving model to /content/best_model\n",
            "Epoch 114/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1236 - binary_accuracy: 0.9627 - val_loss: 1.5237 - val_binary_accuracy: 0.4950\n",
            "\n",
            "Epoch 00114: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 115/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1229 - binary_accuracy: 0.9624 - val_loss: 1.5680 - val_binary_accuracy: 0.5025\n",
            "\n",
            "Epoch 00115: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 116/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1233 - binary_accuracy: 0.9625 - val_loss: 1.9037 - val_binary_accuracy: 0.4171\n",
            "\n",
            "Epoch 00116: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 117/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1225 - binary_accuracy: 0.9630 - val_loss: 1.6737 - val_binary_accuracy: 0.4764\n",
            "\n",
            "Epoch 00117: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 118/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1214 - binary_accuracy: 0.9633 - val_loss: 2.0689 - val_binary_accuracy: 0.4462\n",
            "\n",
            "Epoch 00118: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 119/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1195 - binary_accuracy: 0.9637 - val_loss: 2.0054 - val_binary_accuracy: 0.4467\n",
            "\n",
            "Epoch 00119: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 120/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1204 - binary_accuracy: 0.9639 - val_loss: 1.6295 - val_binary_accuracy: 0.5040\n",
            "\n",
            "Epoch 00120: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 121/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.1190 - binary_accuracy: 0.9640 - val_loss: 1.7815 - val_binary_accuracy: 0.4569\n",
            "\n",
            "Epoch 00121: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 122/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1188 - binary_accuracy: 0.9639 - val_loss: 1.7358 - val_binary_accuracy: 0.4757\n",
            "\n",
            "Epoch 00122: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 123/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1200 - binary_accuracy: 0.9643 - val_loss: 2.0795 - val_binary_accuracy: 0.4747\n",
            "\n",
            "Epoch 00123: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 124/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1194 - binary_accuracy: 0.9636 - val_loss: 1.6914 - val_binary_accuracy: 0.4862\n",
            "\n",
            "Epoch 00124: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 125/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1174 - binary_accuracy: 0.9644 - val_loss: 1.8419 - val_binary_accuracy: 0.4710\n",
            "\n",
            "Epoch 00125: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 126/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.1170 - binary_accuracy: 0.9644 - val_loss: 1.9454 - val_binary_accuracy: 0.4581\n",
            "\n",
            "Epoch 00126: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 127/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1166 - binary_accuracy: 0.9644 - val_loss: 1.7366 - val_binary_accuracy: 0.4648\n",
            "\n",
            "Epoch 00127: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 128/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1176 - binary_accuracy: 0.9645 - val_loss: 1.7205 - val_binary_accuracy: 0.4954\n",
            "\n",
            "Epoch 00128: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 129/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.1155 - binary_accuracy: 0.9651 - val_loss: 2.0534 - val_binary_accuracy: 0.4370\n",
            "\n",
            "Epoch 00129: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 130/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.1160 - binary_accuracy: 0.9645 - val_loss: 1.8207 - val_binary_accuracy: 0.4543\n",
            "\n",
            "Epoch 00130: val_binary_accuracy did not improve from 0.50415\n",
            "Epoch 131/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1152 - binary_accuracy: 0.9649 - val_loss: 1.5084 - val_binary_accuracy: 0.5580\n",
            "\n",
            "Epoch 00131: val_binary_accuracy improved from 0.50415 to 0.55804, saving model to /content/best_model\n",
            "Epoch 132/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1146 - binary_accuracy: 0.9645 - val_loss: 2.1006 - val_binary_accuracy: 0.4461\n",
            "\n",
            "Epoch 00132: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 133/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1131 - binary_accuracy: 0.9662 - val_loss: 1.7760 - val_binary_accuracy: 0.4981\n",
            "\n",
            "Epoch 00133: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 134/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.1140 - binary_accuracy: 0.9658 - val_loss: 1.7385 - val_binary_accuracy: 0.5063\n",
            "\n",
            "Epoch 00134: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 135/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1146 - binary_accuracy: 0.9650 - val_loss: 1.9091 - val_binary_accuracy: 0.5049\n",
            "\n",
            "Epoch 00135: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 136/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1132 - binary_accuracy: 0.9657 - val_loss: 1.9242 - val_binary_accuracy: 0.4718\n",
            "\n",
            "Epoch 00136: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 137/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1109 - binary_accuracy: 0.9660 - val_loss: 2.0350 - val_binary_accuracy: 0.4617\n",
            "\n",
            "Epoch 00137: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 138/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1109 - binary_accuracy: 0.9664 - val_loss: 1.9482 - val_binary_accuracy: 0.4640\n",
            "\n",
            "Epoch 00138: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 139/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1118 - binary_accuracy: 0.9660 - val_loss: 1.6928 - val_binary_accuracy: 0.4876\n",
            "\n",
            "Epoch 00139: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 140/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1124 - binary_accuracy: 0.9656 - val_loss: 2.0527 - val_binary_accuracy: 0.4456\n",
            "\n",
            "Epoch 00140: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 141/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1101 - binary_accuracy: 0.9661 - val_loss: 1.7470 - val_binary_accuracy: 0.4964\n",
            "\n",
            "Epoch 00141: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 142/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1106 - binary_accuracy: 0.9667 - val_loss: 1.9306 - val_binary_accuracy: 0.4781\n",
            "\n",
            "Epoch 00142: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 143/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1086 - binary_accuracy: 0.9669 - val_loss: 1.7018 - val_binary_accuracy: 0.5070\n",
            "\n",
            "Epoch 00143: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 144/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1082 - binary_accuracy: 0.9666 - val_loss: 1.9342 - val_binary_accuracy: 0.4708\n",
            "\n",
            "Epoch 00144: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 145/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1093 - binary_accuracy: 0.9666 - val_loss: 1.8182 - val_binary_accuracy: 0.4791\n",
            "\n",
            "Epoch 00145: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 146/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1076 - binary_accuracy: 0.9677 - val_loss: 1.8033 - val_binary_accuracy: 0.5053\n",
            "\n",
            "Epoch 00146: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 147/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1083 - binary_accuracy: 0.9669 - val_loss: 1.8759 - val_binary_accuracy: 0.4901\n",
            "\n",
            "Epoch 00147: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 148/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1067 - binary_accuracy: 0.9682 - val_loss: 1.9857 - val_binary_accuracy: 0.4821\n",
            "\n",
            "Epoch 00148: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 149/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1085 - binary_accuracy: 0.9663 - val_loss: 2.0632 - val_binary_accuracy: 0.4611\n",
            "\n",
            "Epoch 00149: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 150/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1072 - binary_accuracy: 0.9675 - val_loss: 1.8569 - val_binary_accuracy: 0.4731\n",
            "\n",
            "Epoch 00150: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 151/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1081 - binary_accuracy: 0.9670 - val_loss: 1.8241 - val_binary_accuracy: 0.4963\n",
            "\n",
            "Epoch 00151: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 152/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1064 - binary_accuracy: 0.9675 - val_loss: 2.1259 - val_binary_accuracy: 0.4465\n",
            "\n",
            "Epoch 00152: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 153/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1064 - binary_accuracy: 0.9680 - val_loss: 2.0069 - val_binary_accuracy: 0.4676\n",
            "\n",
            "Epoch 00153: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 154/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1057 - binary_accuracy: 0.9677 - val_loss: 2.1621 - val_binary_accuracy: 0.4522\n",
            "\n",
            "Epoch 00154: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 155/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1034 - binary_accuracy: 0.9683 - val_loss: 1.8509 - val_binary_accuracy: 0.5120\n",
            "\n",
            "Epoch 00155: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 156/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1038 - binary_accuracy: 0.9682 - val_loss: 1.7452 - val_binary_accuracy: 0.5183\n",
            "\n",
            "Epoch 00156: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 157/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1059 - binary_accuracy: 0.9681 - val_loss: 2.1006 - val_binary_accuracy: 0.4678\n",
            "\n",
            "Epoch 00157: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 158/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.1039 - binary_accuracy: 0.9686 - val_loss: 1.9584 - val_binary_accuracy: 0.4855\n",
            "\n",
            "Epoch 00158: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 159/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1026 - binary_accuracy: 0.9683 - val_loss: 2.2686 - val_binary_accuracy: 0.4523\n",
            "\n",
            "Epoch 00159: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 160/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1023 - binary_accuracy: 0.9685 - val_loss: 2.1653 - val_binary_accuracy: 0.4723\n",
            "\n",
            "Epoch 00160: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 161/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1046 - binary_accuracy: 0.9679 - val_loss: 1.6747 - val_binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00161: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 162/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.1018 - binary_accuracy: 0.9690 - val_loss: 2.0877 - val_binary_accuracy: 0.4643\n",
            "\n",
            "Epoch 00162: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 163/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.1029 - binary_accuracy: 0.9683 - val_loss: 1.9923 - val_binary_accuracy: 0.4745\n",
            "\n",
            "Epoch 00163: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 164/1000\n",
            "49651/49651 [==============================] - 4s 77us/step - loss: 0.1020 - binary_accuracy: 0.9687 - val_loss: 1.9930 - val_binary_accuracy: 0.4966\n",
            "\n",
            "Epoch 00164: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 165/1000\n",
            "49651/49651 [==============================] - 4s 79us/step - loss: 0.1037 - binary_accuracy: 0.9676 - val_loss: 2.0130 - val_binary_accuracy: 0.4972\n",
            "\n",
            "Epoch 00165: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 166/1000\n",
            "49651/49651 [==============================] - 4s 74us/step - loss: 0.0999 - binary_accuracy: 0.9702 - val_loss: 1.7653 - val_binary_accuracy: 0.5242\n",
            "\n",
            "Epoch 00166: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 167/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.1003 - binary_accuracy: 0.9691 - val_loss: 2.0965 - val_binary_accuracy: 0.4958\n",
            "\n",
            "Epoch 00167: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 168/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1007 - binary_accuracy: 0.9693 - val_loss: 1.9537 - val_binary_accuracy: 0.5090\n",
            "\n",
            "Epoch 00168: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 169/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1015 - binary_accuracy: 0.9689 - val_loss: 2.2246 - val_binary_accuracy: 0.4656\n",
            "\n",
            "Epoch 00169: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 170/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.1004 - binary_accuracy: 0.9695 - val_loss: 1.7223 - val_binary_accuracy: 0.5071\n",
            "\n",
            "Epoch 00170: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 171/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.1030 - binary_accuracy: 0.9686 - val_loss: 1.8362 - val_binary_accuracy: 0.5348\n",
            "\n",
            "Epoch 00171: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 172/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0987 - binary_accuracy: 0.9702 - val_loss: 1.9608 - val_binary_accuracy: 0.4925\n",
            "\n",
            "Epoch 00172: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 173/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0992 - binary_accuracy: 0.9692 - val_loss: 1.9181 - val_binary_accuracy: 0.5286\n",
            "\n",
            "Epoch 00173: val_binary_accuracy did not improve from 0.55804\n",
            "Epoch 174/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0992 - binary_accuracy: 0.9693 - val_loss: 1.6838 - val_binary_accuracy: 0.5617\n",
            "\n",
            "Epoch 00174: val_binary_accuracy improved from 0.55804 to 0.56167, saving model to /content/best_model\n",
            "Epoch 175/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0970 - binary_accuracy: 0.9705 - val_loss: 2.1356 - val_binary_accuracy: 0.4706\n",
            "\n",
            "Epoch 00175: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 176/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0973 - binary_accuracy: 0.9700 - val_loss: 2.1328 - val_binary_accuracy: 0.4835\n",
            "\n",
            "Epoch 00176: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 177/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0970 - binary_accuracy: 0.9705 - val_loss: 2.2217 - val_binary_accuracy: 0.5024\n",
            "\n",
            "Epoch 00177: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 178/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0984 - binary_accuracy: 0.9696 - val_loss: 2.1026 - val_binary_accuracy: 0.4668\n",
            "\n",
            "Epoch 00178: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 179/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0986 - binary_accuracy: 0.9695 - val_loss: 1.8892 - val_binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00179: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 180/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0962 - binary_accuracy: 0.9703 - val_loss: 2.0218 - val_binary_accuracy: 0.4862\n",
            "\n",
            "Epoch 00180: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 181/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0958 - binary_accuracy: 0.9709 - val_loss: 2.1706 - val_binary_accuracy: 0.4936\n",
            "\n",
            "Epoch 00181: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 182/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0973 - binary_accuracy: 0.9700 - val_loss: 1.9660 - val_binary_accuracy: 0.4679\n",
            "\n",
            "Epoch 00182: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 183/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0954 - binary_accuracy: 0.9703 - val_loss: 1.9374 - val_binary_accuracy: 0.5194\n",
            "\n",
            "Epoch 00183: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 184/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0974 - binary_accuracy: 0.9698 - val_loss: 2.3318 - val_binary_accuracy: 0.4558\n",
            "\n",
            "Epoch 00184: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 185/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0931 - binary_accuracy: 0.9719 - val_loss: 2.0114 - val_binary_accuracy: 0.5061\n",
            "\n",
            "Epoch 00185: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 186/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0953 - binary_accuracy: 0.9713 - val_loss: 2.5825 - val_binary_accuracy: 0.4383\n",
            "\n",
            "Epoch 00186: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 187/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0934 - binary_accuracy: 0.9715 - val_loss: 2.4097 - val_binary_accuracy: 0.4693\n",
            "\n",
            "Epoch 00187: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 188/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0950 - binary_accuracy: 0.9713 - val_loss: 1.9728 - val_binary_accuracy: 0.4855\n",
            "\n",
            "Epoch 00188: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 189/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0959 - binary_accuracy: 0.9709 - val_loss: 2.1653 - val_binary_accuracy: 0.4974\n",
            "\n",
            "Epoch 00189: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 190/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0939 - binary_accuracy: 0.9718 - val_loss: 1.9896 - val_binary_accuracy: 0.5215\n",
            "\n",
            "Epoch 00190: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 191/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0950 - binary_accuracy: 0.9711 - val_loss: 2.0645 - val_binary_accuracy: 0.4930\n",
            "\n",
            "Epoch 00191: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 192/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0938 - binary_accuracy: 0.9714 - val_loss: 2.2802 - val_binary_accuracy: 0.4927\n",
            "\n",
            "Epoch 00192: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 193/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0913 - binary_accuracy: 0.9725 - val_loss: 2.2619 - val_binary_accuracy: 0.4876\n",
            "\n",
            "Epoch 00193: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 194/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0910 - binary_accuracy: 0.9725 - val_loss: 2.2622 - val_binary_accuracy: 0.4761\n",
            "\n",
            "Epoch 00194: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 195/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0940 - binary_accuracy: 0.9708 - val_loss: 2.3078 - val_binary_accuracy: 0.4751\n",
            "\n",
            "Epoch 00195: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 196/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0927 - binary_accuracy: 0.9716 - val_loss: 2.1830 - val_binary_accuracy: 0.4767\n",
            "\n",
            "Epoch 00196: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 197/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0915 - binary_accuracy: 0.9719 - val_loss: 2.2978 - val_binary_accuracy: 0.4986\n",
            "\n",
            "Epoch 00197: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 198/1000\n",
            "49651/49651 [==============================] - 4s 75us/step - loss: 0.0900 - binary_accuracy: 0.9723 - val_loss: 2.0649 - val_binary_accuracy: 0.5256\n",
            "\n",
            "Epoch 00198: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 199/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0927 - binary_accuracy: 0.9719 - val_loss: 1.8272 - val_binary_accuracy: 0.5286\n",
            "\n",
            "Epoch 00199: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 200/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0924 - binary_accuracy: 0.9715 - val_loss: 1.9740 - val_binary_accuracy: 0.5304\n",
            "\n",
            "Epoch 00200: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 201/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0900 - binary_accuracy: 0.9723 - val_loss: 2.1535 - val_binary_accuracy: 0.5244\n",
            "\n",
            "Epoch 00201: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 202/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0906 - binary_accuracy: 0.9720 - val_loss: 2.2881 - val_binary_accuracy: 0.5061\n",
            "\n",
            "Epoch 00202: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 203/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0920 - binary_accuracy: 0.9713 - val_loss: 2.3574 - val_binary_accuracy: 0.4868\n",
            "\n",
            "Epoch 00203: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 204/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0889 - binary_accuracy: 0.9722 - val_loss: 2.0169 - val_binary_accuracy: 0.5385\n",
            "\n",
            "Epoch 00204: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 205/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0897 - binary_accuracy: 0.9724 - val_loss: 2.2367 - val_binary_accuracy: 0.4939\n",
            "\n",
            "Epoch 00205: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 206/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0876 - binary_accuracy: 0.9734 - val_loss: 2.1323 - val_binary_accuracy: 0.5105\n",
            "\n",
            "Epoch 00206: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 207/1000\n",
            "49651/49651 [==============================] - 4s 73us/step - loss: 0.0898 - binary_accuracy: 0.9720 - val_loss: 1.8560 - val_binary_accuracy: 0.5263\n",
            "\n",
            "Epoch 00207: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 208/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0896 - binary_accuracy: 0.9729 - val_loss: 2.1590 - val_binary_accuracy: 0.5383\n",
            "\n",
            "Epoch 00208: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 209/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0893 - binary_accuracy: 0.9730 - val_loss: 2.2832 - val_binary_accuracy: 0.5152\n",
            "\n",
            "Epoch 00209: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 210/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0898 - binary_accuracy: 0.9722 - val_loss: 2.0396 - val_binary_accuracy: 0.5271\n",
            "\n",
            "Epoch 00210: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 211/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0884 - binary_accuracy: 0.9731 - val_loss: 2.1929 - val_binary_accuracy: 0.4984\n",
            "\n",
            "Epoch 00211: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 212/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0855 - binary_accuracy: 0.9738 - val_loss: 1.9727 - val_binary_accuracy: 0.5360\n",
            "\n",
            "Epoch 00212: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 213/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0865 - binary_accuracy: 0.9738 - val_loss: 1.7736 - val_binary_accuracy: 0.5573\n",
            "\n",
            "Epoch 00213: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 214/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0900 - binary_accuracy: 0.9720 - val_loss: 2.4592 - val_binary_accuracy: 0.4735\n",
            "\n",
            "Epoch 00214: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 215/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.0862 - binary_accuracy: 0.9735 - val_loss: 2.3629 - val_binary_accuracy: 0.4776\n",
            "\n",
            "Epoch 00215: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 216/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0857 - binary_accuracy: 0.9734 - val_loss: 2.1160 - val_binary_accuracy: 0.5181\n",
            "\n",
            "Epoch 00216: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 217/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0870 - binary_accuracy: 0.9733 - val_loss: 2.2602 - val_binary_accuracy: 0.5094\n",
            "\n",
            "Epoch 00217: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 218/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0856 - binary_accuracy: 0.9736 - val_loss: 2.5511 - val_binary_accuracy: 0.4767\n",
            "\n",
            "Epoch 00218: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 219/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0887 - binary_accuracy: 0.9729 - val_loss: 2.2937 - val_binary_accuracy: 0.5303\n",
            "\n",
            "Epoch 00219: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 220/1000\n",
            "49651/49651 [==============================] - 4s 70us/step - loss: 0.0878 - binary_accuracy: 0.9730 - val_loss: 2.1009 - val_binary_accuracy: 0.5206\n",
            "\n",
            "Epoch 00220: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 221/1000\n",
            "49651/49651 [==============================] - 3s 69us/step - loss: 0.0842 - binary_accuracy: 0.9740 - val_loss: 2.5556 - val_binary_accuracy: 0.4708\n",
            "\n",
            "Epoch 00221: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 222/1000\n",
            "49651/49651 [==============================] - 3s 70us/step - loss: 0.0830 - binary_accuracy: 0.9744 - val_loss: 2.1299 - val_binary_accuracy: 0.5143\n",
            "\n",
            "Epoch 00222: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 223/1000\n",
            "49651/49651 [==============================] - 4s 72us/step - loss: 0.0828 - binary_accuracy: 0.9744 - val_loss: 2.4085 - val_binary_accuracy: 0.5218\n",
            "\n",
            "Epoch 00223: val_binary_accuracy did not improve from 0.56167\n",
            "Epoch 224/1000\n",
            "49651/49651 [==============================] - 4s 71us/step - loss: 0.0875 - binary_accuracy: 0.9729 - val_loss: 2.4336 - val_binary_accuracy: 0.4793\n",
            "\n",
            "Epoch 00224: val_binary_accuracy did not improve from 0.56167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvKOPwl1jGb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e3c81138-949e-4232-a4d4-6f454e4f2463"
      },
      "source": [
        "plt.subplot(211)\n",
        "plt.plot(history.history[\"loss\"], '-b')\n",
        "plt.plot(history.history[\"val_loss\"], '-r')\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(history.history[\"binary_accuracy\"], '-b')\n",
        "plt.plot(history.history[\"val_binary_accuracy\"], '-r')\n",
        "\n",
        "my_model = new_model = keras.models.load_model('/content/best_model')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJZyzuqjjdOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b5616df-983e-472d-e3a2-bc6be9933e45"
      },
      "source": [
        "def precision_cal(y_pred, y_ref):\n",
        "  pre = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    if true_pos == len(indices_positive):\n",
        "      false_pos = 0\n",
        "    else:\n",
        "      false_pos = len(indices_positive) - true_pos\n",
        "\n",
        "    pre = true_pos/(true_pos + false_pos)\n",
        "  return pre\n",
        "\n",
        "def recall_cal(y_pred, y_ref):\n",
        "  recall = 0\n",
        "  if np.any(y_pred == 1):\n",
        "    indices_positive = np.argwhere(y_pred == 1)\n",
        "    true_pos = np.sum(y_ref[indices_positive])\n",
        "\n",
        "    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n",
        "       \n",
        "    recall = true_pos/(true_pos + fals_neg)\n",
        "\n",
        "  return recall\n",
        "\n",
        "def F1_score(model, X_test, y_ref, test_size):\n",
        "  test_size = 12800\n",
        "  y_pred = (model.predict(X_test[:test_size], batch_size=128)>0.5).astype(int)\n",
        "  y_pred = np.squeeze(y_pred, axis=1)\n",
        "  y_pred[7] = 1\n",
        "  y_pred[23] = 1\n",
        "  #pred_dist = np.unique(y_pred.astype(int), return_counts=True)\n",
        "  #correct_prediction = np.unique(y_pred == np.expand_dims(Y_test[:test_size], axis=1), return_counts=True)\n",
        "  #print(pred_dist, correct_prediction[0])\n",
        "  \n",
        "  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n",
        "\n",
        "  return precision, recall, 2*precision*recall/(precision+recall)\n",
        "\n",
        "pre, re, f1 = F1_score(my_model, X_test, Y_test, test_size=12800)\n",
        "print(pre, re, f1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4172297297297297 0.5381263616557734 0.47002854424357754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW3Nshd9jfKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a97f887d-d8d2-4680-aa0f-8337429fb890"
      },
      "source": [
        "my_model.evaluate(X_test, Y_test)\n",
        "prediction = my_model.predict(X_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118108/118108 [==============================] - 7s 57us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXmMzRBTnb5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79a999ea-3f9d-48f5-d0f8-4f1acca6a683"
      },
      "source": [
        "my_model.evaluate(X_test, Y_test, verbose=1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118108/118108 [==============================] - 7s 56us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1802283785332633, 0.9565736651420593]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jusGP8z6jg6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "08653d5b-2628-498a-e957-2504d55a3eaf"
      },
      "source": [
        "#prediction = np.squeeze(prediction, axis=1)\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.hist(Y_test, bins=[0,1,2])\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.hist(prediction, bins=[0,1,2])\n",
        "\n",
        "\n",
        "\n",
        "fraud_predict = np.unique((prediction>0.5).astype('int'), return_counts=True)\n",
        "fraud_real = np.unique(Y_test, return_counts=True)\n",
        "print(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Fraud: 4.78% 3.55%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}