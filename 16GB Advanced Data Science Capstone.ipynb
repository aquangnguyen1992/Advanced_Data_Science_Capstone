{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Overview\n- This notebook is the Watson Studio notebook for the Advanced Data Science Capstone project of Coursera.\n- The topic of this project is the \"IEEE-CIS Fraud Detection\" challenge from Kaggle, more information about the dataset and the competition can be found at https://www.kaggle.com/c/ieee-fraud-detection/\n- The Machine Learning model in this Notebook is developped using the tensorflow/keras integrated in the Watson Studio Notebook. In addition, other libraries are used for the data analysis, graph display, data preprocessing:\n    + numpy\n    + pandas\n    + scikit-learn\n    + matplotlib\n    + seaborn\n- This model is developped using only the data from the train_transaction.csv, meaning just have of the given data.\n- In order to evaluate the performance of the model, F1_score function is developped, "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Load the dataset from the Cloud Data Object"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Import the libraries and cleansing (using OneHotEncoding) and Normalize the data"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport copy\nimport os"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n       ...\n       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n       'V339'],\n      dtype='object', length=394)"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "dataset_transaction.columns"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "float_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('float64'))].to_list()\nint_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('int64'))].to_list()\nobj_columns = dataset_transaction.columns[np.where(dataset_transaction.dtypes == np.dtype('O'))].to_list()\n\nskip_int_columns = ['TransactionID', 'isFraud']\nfor column in skip_int_columns:\n  int_columns.remove(column)\n\nskip_obj_colums = ['']"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "def normalization_data(X, indices):\n  X_out = copy.copy(X)\n  X_temp = X[indices]\n  X_out.iloc[indices] = (X_temp-np.mean(X_temp))/(np.max(X_temp)-np.min(X_temp))\n  X_out.iloc[np.where(np.isnan(X_out))[0]] = 0\n  return np.min(X_temp), np.max(X_temp), np.mean(X_temp), X_out.astype('float16')"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# Task 1: Detect the columns with NaN and code it with an extra features\n# Task 2: Apply normalizationn\n# Task 3: Remove irrelevant columns\n\ncache = dict()\n#dataset_transaction = copy.copy(data_backup)\n\n#dataset_transaction.pop()\n\nfor column in float_columns:\n  # Set to float 16\n  dataset_transaction[column].astype('float16')\n\n  # Code the NaN feature\n  if np.any(np.isnan(dataset_transaction[column].values)):\n    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n  \n  # Normalization\n  X = dataset_transaction[column]\n  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionID</th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card4</th>\n      <th>card5</th>\n      <th>...</th>\n      <th>V330_NaN_Code</th>\n      <th>V331_NaN_Code</th>\n      <th>V332_NaN_Code</th>\n      <th>V333_NaN_Code</th>\n      <th>V334_NaN_Code</th>\n      <th>V335_NaN_Code</th>\n      <th>V336_NaN_Code</th>\n      <th>V337_NaN_Code</th>\n      <th>V338_NaN_Code</th>\n      <th>V339_NaN_Code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2987000</td>\n      <td>0</td>\n      <td>86400</td>\n      <td>-0.002083</td>\n      <td>W</td>\n      <td>13926</td>\n      <td>0.000000</td>\n      <td>-0.024384</td>\n      <td>discover</td>\n      <td>-0.418213</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2987001</td>\n      <td>0</td>\n      <td>86401</td>\n      <td>-0.003321</td>\n      <td>W</td>\n      <td>2755</td>\n      <td>0.082886</td>\n      <td>-0.024384</td>\n      <td>mastercard</td>\n      <td>-0.709961</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2987002</td>\n      <td>0</td>\n      <td>86469</td>\n      <td>-0.002380</td>\n      <td>W</td>\n      <td>4663</td>\n      <td>0.254883</td>\n      <td>-0.024384</td>\n      <td>visa</td>\n      <td>-0.242920</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2987003</td>\n      <td>0</td>\n      <td>86499</td>\n      <td>-0.002663</td>\n      <td>W</td>\n      <td>18132</td>\n      <td>0.408936</td>\n      <td>-0.024384</td>\n      <td>mastercard</td>\n      <td>-0.600586</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2987004</td>\n      <td>0</td>\n      <td>86506</td>\n      <td>-0.002663</td>\n      <td>H</td>\n      <td>4497</td>\n      <td>0.302979</td>\n      <td>-0.024384</td>\n      <td>mastercard</td>\n      <td>-0.709961</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 755 columns</p>\n</div>",
                        "text/plain": "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n0        2987000        0          86400       -0.002083         W  13926   \n1        2987001        0          86401       -0.003321         W   2755   \n2        2987002        0          86469       -0.002380         W   4663   \n3        2987003        0          86499       -0.002663         W  18132   \n4        2987004        0          86506       -0.002663         H   4497   \n\n      card2     card3       card4     card5  ... V330_NaN_Code  V331_NaN_Code  \\\n0  0.000000 -0.024384    discover -0.418213  ...             1              1   \n1  0.082886 -0.024384  mastercard -0.709961  ...             1              1   \n2  0.254883 -0.024384        visa -0.242920  ...             1              1   \n3  0.408936 -0.024384  mastercard -0.600586  ...             1              1   \n4  0.302979 -0.024384  mastercard -0.709961  ...             0              0   \n\n   V332_NaN_Code  V333_NaN_Code  V334_NaN_Code V335_NaN_Code V336_NaN_Code  \\\n0              1              1              1             1             1   \n1              1              1              1             1             1   \n2              1              1              1             1             1   \n3              1              1              1             1             1   \n4              0              0              0             0             0   \n\n   V337_NaN_Code  V338_NaN_Code  V339_NaN_Code  \n0              1              1              1  \n1              1              1              1  \n2              1              1              1  \n3              1              1              1  \n4              0              0              0  \n\n[5 rows x 755 columns]"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "dataset_transaction.head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "for column in int_columns:\n  # Set to int 32\n  dataset_transaction[column].astype('int32')\n\n  # Code the NaN feature\n  if np.any(np.isnan(dataset_transaction[column].values)):\n    dataset_transaction[column + \"_NaN_Code\"] = np.isnan(dataset_transaction[column].values).astype('int8')\n  \n  # Normalization\n  X = dataset_transaction[column]\n  indices = np.where(np.isnan(dataset_transaction[column]) == False)[0]\n  cache[column+'_min'], cache[column+'_max'], cache[column+'_mean'], dataset_transaction[column] = normalization_data(X, indices)"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionID</th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card4</th>\n      <th>card5</th>\n      <th>...</th>\n      <th>V330_NaN_Code</th>\n      <th>V331_NaN_Code</th>\n      <th>V332_NaN_Code</th>\n      <th>V333_NaN_Code</th>\n      <th>V334_NaN_Code</th>\n      <th>V335_NaN_Code</th>\n      <th>V336_NaN_Code</th>\n      <th>V337_NaN_Code</th>\n      <th>V338_NaN_Code</th>\n      <th>V339_NaN_Code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2987000</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002083</td>\n      <td>W</td>\n      <td>0.231445</td>\n      <td>0.000000</td>\n      <td>-0.024384</td>\n      <td>discover</td>\n      <td>-0.418213</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2987001</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.003321</td>\n      <td>W</td>\n      <td>-0.410645</td>\n      <td>0.082886</td>\n      <td>-0.024384</td>\n      <td>mastercard</td>\n      <td>-0.709961</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2987002</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002380</td>\n      <td>W</td>\n      <td>-0.301025</td>\n      <td>0.254883</td>\n      <td>-0.024384</td>\n      <td>visa</td>\n      <td>-0.242920</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2987003</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002663</td>\n      <td>W</td>\n      <td>0.473389</td>\n      <td>0.408936</td>\n      <td>-0.024384</td>\n      <td>mastercard</td>\n      <td>-0.600586</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2987004</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002663</td>\n      <td>H</td>\n      <td>-0.310547</td>\n      <td>0.302979</td>\n      <td>-0.024384</td>\n      <td>mastercard</td>\n      <td>-0.709961</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 755 columns</p>\n</div>",
                        "text/plain": "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD     card1  \\\n0        2987000        0      -0.463379       -0.002083         W  0.231445   \n1        2987001        0      -0.463379       -0.003321         W -0.410645   \n2        2987002        0      -0.463379       -0.002380         W -0.301025   \n3        2987003        0      -0.463379       -0.002663         W  0.473389   \n4        2987004        0      -0.463379       -0.002663         H -0.310547   \n\n      card2     card3       card4     card5  ... V330_NaN_Code  V331_NaN_Code  \\\n0  0.000000 -0.024384    discover -0.418213  ...             1              1   \n1  0.082886 -0.024384  mastercard -0.709961  ...             1              1   \n2  0.254883 -0.024384        visa -0.242920  ...             1              1   \n3  0.408936 -0.024384  mastercard -0.600586  ...             1              1   \n4  0.302979 -0.024384  mastercard -0.709961  ...             0              0   \n\n   V332_NaN_Code  V333_NaN_Code  V334_NaN_Code V335_NaN_Code V336_NaN_Code  \\\n0              1              1              1             1             1   \n1              1              1              1             1             1   \n2              1              1              1             1             1   \n3              1              1              1             1             1   \n4              0              0              0             0             0   \n\n   V337_NaN_Code  V338_NaN_Code  V339_NaN_Code  \n0              1              1              1  \n1              1              1              1  \n2              1              1              1  \n3              1              1              1  \n4              0              0              0  \n\n[5 rows x 755 columns]"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "dataset_transaction.head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionID</th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>addr1</th>\n      <th>addr2</th>\n      <th>...</th>\n      <th>M6_2</th>\n      <th>M7_0</th>\n      <th>M7_1</th>\n      <th>M7_2</th>\n      <th>M8_0</th>\n      <th>M8_1</th>\n      <th>M8_2</th>\n      <th>M9_0</th>\n      <th>M9_1</th>\n      <th>M9_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2987000</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002083</td>\n      <td>0.231445</td>\n      <td>0.000000</td>\n      <td>-0.024384</td>\n      <td>-0.418213</td>\n      <td>0.055145</td>\n      <td>0.002167</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2987001</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.003321</td>\n      <td>-0.410645</td>\n      <td>0.082886</td>\n      <td>-0.024384</td>\n      <td>-0.709961</td>\n      <td>0.077881</td>\n      <td>0.002167</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2987002</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002380</td>\n      <td>-0.301025</td>\n      <td>0.254883</td>\n      <td>-0.024384</td>\n      <td>-0.242920</td>\n      <td>0.089233</td>\n      <td>0.002167</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2987003</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002663</td>\n      <td>0.473389</td>\n      <td>0.408936</td>\n      <td>-0.024384</td>\n      <td>-0.600586</td>\n      <td>0.421143</td>\n      <td>0.002167</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2987004</td>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002663</td>\n      <td>-0.310547</td>\n      <td>0.302979</td>\n      <td>-0.024384</td>\n      <td>-0.709961</td>\n      <td>0.293701</td>\n      <td>0.002167</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 905 columns</p>\n</div>",
                        "text/plain": "   TransactionID  isFraud  TransactionDT  TransactionAmt     card1     card2  \\\n0        2987000        0      -0.463379       -0.002083  0.231445  0.000000   \n1        2987001        0      -0.463379       -0.003321 -0.410645  0.082886   \n2        2987002        0      -0.463379       -0.002380 -0.301025  0.254883   \n3        2987003        0      -0.463379       -0.002663  0.473389  0.408936   \n4        2987004        0      -0.463379       -0.002663 -0.310547  0.302979   \n\n      card3     card5     addr1     addr2  ...  M6_2  M7_0  M7_1  M7_2  M8_0  \\\n0 -0.024384 -0.418213  0.055145  0.002167  ...     1     0     1     0     0   \n1 -0.024384 -0.709961  0.077881  0.002167  ...     1     0     1     0     0   \n2 -0.024384 -0.242920  0.089233  0.002167  ...     0     1     0     0     1   \n3 -0.024384 -0.600586  0.421143  0.002167  ...     0     0     1     0     0   \n4 -0.024384 -0.709961  0.293701  0.002167  ...     0     0     1     0     0   \n\n   M8_1  M8_2  M9_0  M9_1  M9_2  \n0     1     0     0     1     0  \n1     1     0     0     1     0  \n2     0     0     1     0     0  \n3     1     0     0     1     0  \n4     1     0     0     1     0  \n\n[5 rows x 905 columns]"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn.preprocessing import OneHotEncoder\nfor column in obj_columns:\n  ohc = OneHotEncoder()\n  dataset_transaction.loc[np.where(dataset_transaction[column].isnull())[0], column] = 'Null'\n  encoded = ohc.fit_transform(dataset_transaction[column].values.reshape(-1,1)).toarray()    \n  pd_encoded = pd.DataFrame(encoded.astype('int8'), columns=[column+\"_\"+str(i) for i in range(len(np.unique(dataset_transaction[column].astype('str'))))])\n  dataset_transaction = pd.concat([dataset_transaction, pd_encoded], axis=1)\n\nfor column in obj_columns:\n  try:\n    dataset_transaction.pop(column)\n  except KeyError:\n    pass\ndataset_transaction.head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "False False\n"
                }
            ],
            "source": "# Double check again Null and Nan\nprint(np.any(np.isnan(dataset_transaction)), np.any(dataset_transaction.isnull()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Apply Seaborn to analyze the processed data and detect removable features"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "colums_to_analyze = ['isFraud', 'TransactionDT', 'TransactionAmt', 'P_emaildomain_0', 'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'P_emaildomain_4', 'addr1', 'addr2', 'dist1', 'dist2']\nanalyzing_data = dataset_transaction[colums_to_analyze]"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7f6d53fecb70>"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 792x648 with 2 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "corr = analyzing_data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Create the train/val dataset"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>addr1</th>\n      <th>addr2</th>\n      <th>dist1</th>\n      <th>...</th>\n      <th>M6_2</th>\n      <th>M7_0</th>\n      <th>M7_1</th>\n      <th>M7_2</th>\n      <th>M8_0</th>\n      <th>M8_1</th>\n      <th>M8_2</th>\n      <th>M9_0</th>\n      <th>M9_1</th>\n      <th>M9_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002083</td>\n      <td>0.231445</td>\n      <td>0.000000</td>\n      <td>-0.024384</td>\n      <td>-0.418213</td>\n      <td>0.055145</td>\n      <td>0.002167</td>\n      <td>-0.009674</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.003321</td>\n      <td>-0.410645</td>\n      <td>0.082886</td>\n      <td>-0.024384</td>\n      <td>-0.709961</td>\n      <td>0.077881</td>\n      <td>0.002167</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002380</td>\n      <td>-0.301025</td>\n      <td>0.254883</td>\n      <td>-0.024384</td>\n      <td>-0.242920</td>\n      <td>0.089233</td>\n      <td>0.002167</td>\n      <td>0.016388</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002663</td>\n      <td>0.473389</td>\n      <td>0.408936</td>\n      <td>-0.024384</td>\n      <td>-0.600586</td>\n      <td>0.421143</td>\n      <td>0.002167</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>-0.463379</td>\n      <td>-0.002663</td>\n      <td>-0.310547</td>\n      <td>0.302979</td>\n      <td>-0.024384</td>\n      <td>-0.709961</td>\n      <td>0.293701</td>\n      <td>0.002167</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 904 columns</p>\n</div>",
                        "text/plain": "   isFraud  TransactionDT  TransactionAmt     card1     card2     card3  \\\n0        0      -0.463379       -0.002083  0.231445  0.000000 -0.024384   \n1        0      -0.463379       -0.003321 -0.410645  0.082886 -0.024384   \n2        0      -0.463379       -0.002380 -0.301025  0.254883 -0.024384   \n3        0      -0.463379       -0.002663  0.473389  0.408936 -0.024384   \n4        0      -0.463379       -0.002663 -0.310547  0.302979 -0.024384   \n\n      card5     addr1     addr2     dist1  ...  M6_2  M7_0  M7_1  M7_2  M8_0  \\\n0 -0.418213  0.055145  0.002167 -0.009674  ...     1     0     1     0     0   \n1 -0.709961  0.077881  0.002167  0.000000  ...     1     0     1     0     0   \n2 -0.242920  0.089233  0.002167  0.016388  ...     0     1     0     0     1   \n3 -0.600586  0.421143  0.002167  0.000000  ...     0     0     1     0     0   \n4 -0.709961  0.293701  0.002167  0.000000  ...     0     0     1     0     0   \n\n   M8_1  M8_2  M9_0  M9_1  M9_2  \n0     1     0     0     1     0  \n1     1     0     0     1     0  \n2     0     0     1     0     0  \n3     1     0     0     1     0  \n4     1     0     0     1     0  \n\n[5 rows x 904 columns]"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Remove the irrelevant columns\ndataset = copy.copy(dataset_transaction)\ndataset.pop('TransactionID')\ndataset.head(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(472432, 903) (472432,) (118108, 903)\n"
                }
            ],
            "source": "from sklearn.model_selection import train_test_split\n\nY = dataset['isFraud']\ndataset.pop('isFraud')\nX = dataset\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\nprint(X_train.shape, Y_train.shape, X_test.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Check the imbalance\nplt.subplot(211)\nplt.hist(Y_train, bins=[0,1,2])\n\nplt.subplot(212)\nplt.hist(Y_test, bins=[0,1,2])\n\nfraud_count = np.unique(Y_train, return_counts=True)\nprint(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Downsampling/Upsampling to minimize the imbalance "
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(45591, 1)\n(62116, 1)\n(62116, 903) (62116,)\n"
                }
            ],
            "source": "downsampling_factor = 10\nindices_1 = np.argwhere(np.array(Y_train)==1)\nindices_0_new = np.argwhere(np.array(Y_train)==0)\nindices = np.arange(0,len(indices_0_new),downsampling_factor)\nindices_0_new = indices_0_new[indices]\n\nprint(indices_0_new.shape)\nindices_0_new = np.concatenate((indices_0_new, indices_1), axis=0)\nprint(indices_0_new.shape)\n\n\nX_to_train = np.array(X_train)[indices_0_new]\nY_to_train = np.array(Y_train)[indices_0_new]\n\n\nX_to_train = np.reshape(X_to_train, (X_to_train.shape[0], X_to_train.shape[2]))\nY_to_train = np.squeeze(Y_to_train, axis=1)\nprint(X_to_train.shape, Y_to_train.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Percentage of Fraud: 26.6%\n"
                },
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.hist(Y_to_train, bins=[0,1,2])\n\nfraud_count = np.unique(Y_to_train, return_counts=True)\nprint(\"Percentage of Fraud: \" + str(round(fraud_count[1][1]/np.sum(fraud_count[1])*100,2)) + \"%\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Create the model using NN"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using TensorFlow backend.\n"
                }
            ],
            "source": "import keras\nfrom keras.layers import Dense, Flatten, Dropout, BatchNormalization\nfrom keras import Sequential\nfrom keras.regularizers import l1_l2\nfrom keras.optimizers import Adam, SGD"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "def create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0.001, l2_rate=0.001, init_std=0.01, lr_value=0.001):\n  out_model = Sequential()\n  out_model.add(Dense(dense1, activation=\"relu\", input_shape=(X_train.shape[1],),\n                      kernel_initializer=keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n                      kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n  out_model.add(Dense(dense1, activation=\"relu\",\n                      kernel_initializer=keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n                      kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n  out_model.add(Dropout(dropout_rate))\n\n  out_model.add(Dense(dense2, activation=\"relu\", \n                      kernel_initializer=keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n                      kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n  out_model.add(Dense(dense2, activation=\"relu\",\n                      kernel_initializer=keras.initializers.RandomUniform(minval=-init_std, maxval=init_std),\n                      kernel_regularizer=l1_l2(l1=l1_rate, l2=l2_rate)))\n  out_model.add(Dropout(dropout_rate))\n\n  out_model.add(Dense(1, activation=\"sigmoid\"))\n\n  out_model.compile(\n            optimizer=Adam(lr=lr_value, beta_1=0.9, beta_2=0.999, amsgrad=True),\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy'])\n  \n  return out_model"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 128)               115712    \n_________________________________________________________________\ndense_7 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_9 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 1)                 65        \n=================================================================\nTotal params: 144,705\nTrainable params: 144,705\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "my_model = create_model(dense1=128, dense2=64, dropout_rate=0.4, l1_rate=0, l2_rate=0, init_std=0.05, lr_value=0.0001)\nmy_model.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Perform the training and using the callbacks to optimize the performance of the final model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 49692 samples, validate on 12424 samples\nEpoch 1/1000\n49692/49692 [==============================] - 511s 10ms/step - loss: 0.3214 - binary_accuracy: 0.9164 - val_loss: 2.1192 - val_binary_accuracy: 0.0000e+00\n\nEpoch 00001: val_binary_accuracy improved from -inf to 0.00000, saving model to ./best_model.pb\nEpoch 2/1000\n49692/49692 [==============================] - 551s 11ms/step - loss: 0.2387 - binary_accuracy: 0.9186 - val_loss: 1.9917 - val_binary_accuracy: 0.1780\n\nEpoch 00002: val_binary_accuracy improved from 0.00000 to 0.17796, saving model to ./best_model.pb\nEpoch 3/1000\n49692/49692 [==============================] - 553s 11ms/step - loss: 0.2180 - binary_accuracy: 0.9311 - val_loss: 1.7611 - val_binary_accuracy: 0.2603\n\nEpoch 00003: val_binary_accuracy improved from 0.17796 to 0.26030, saving model to ./best_model.pb\nEpoch 4/1000\n49692/49692 [==============================] - 548s 11ms/step - loss: 0.2104 - binary_accuracy: 0.9333 - val_loss: 1.6790 - val_binary_accuracy: 0.3053\n\nEpoch 00004: val_binary_accuracy improved from 0.26030 to 0.30530, saving model to ./best_model.pb\nEpoch 5/1000\n49692/49692 [==============================] - 550s 11ms/step - loss: 0.2055 - binary_accuracy: 0.9350 - val_loss: 1.7383 - val_binary_accuracy: 0.3005\n\nEpoch 00005: val_binary_accuracy did not improve from 0.30530\nEpoch 6/1000\n49692/49692 [==============================] - 555s 11ms/step - loss: 0.2034 - binary_accuracy: 0.9354 - val_loss: 1.7936 - val_binary_accuracy: 0.3083\n\nEpoch 00006: val_binary_accuracy improved from 0.30530 to 0.30827, saving model to ./best_model.pb\nEpoch 7/1000\n47488/49692 [===========================>..] - ETA: 22s - loss: 0.2025 - binary_accuracy: 0.9358"
                }
            ],
            "source": "BATCH_SIZE = 128\nNB_EPOCH = 1000\n\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor='val_binary_accuracy', patience=50, verbose=0, mode='auto',\n    baseline=None)\n\nbest_model_hold = keras.callbacks.ModelCheckpoint(\n    filepath='./best_model.pb', monitor='val_binary_accuracy', verbose=1, save_best_only=True,\n    save_weights_only=False, mode='auto')\n\nhistory = my_model.fit(X_to_train, Y_to_train, \n             batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n             validation_split=0.2, shuffle=True,\n             callbacks=[early_stop, best_model_hold])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# The lost and the training cuvre"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.subplot(211)\nplt.plot(history.history[\"loss\"], '-b')\nplt.plot(history.history[\"val_loss\"], '-r')\n\nplt.subplot(212)\nplt.plot(history.history[\"binary_accuracy\"], '-b')\nplt.plot(history.history[\"val_binary_accuracy\"], '-r')\n\nmy_model = new_model = keras.models.load_model('./best_model.pb')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# F1_score function to tune the threshold"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def precision_cal(y_pred, y_ref):\n  pre = 0\n  if np.any(y_pred == 1):\n    indices_positive = np.argwhere(y_pred == 1)\n    true_pos = np.sum(y_ref[indices_positive])\n\n    if true_pos == len(indices_positive):\n      false_pos = 0\n    else:\n      false_pos = len(indices_positive) - true_pos\n\n    pre = true_pos/(true_pos + false_pos)\n  return pre\n\ndef recall_cal(y_pred, y_ref):\n  recall = 0\n  if np.any(y_pred == 1):\n    indices_positive = np.argwhere(y_pred == 1)\n    true_pos = np.sum(y_ref[indices_positive])\n\n    fals_neg = np.sum(y_ref[np.argwhere(y_pred == 0)])\n       \n    recall = true_pos/(true_pos + fals_neg)\n\n  return recall\n\ndef F1_score(model, X_test, y_ref, test_size, threshold=0.5):\n  test_size = 12800\n  y_pred = (model.predict(X_test[:test_size], batch_size=128)>threshold).astype(int)\n  y_pred = np.squeeze(y_pred, axis=1)\n  y_pred[7] = 1\n  y_pred[23] = 1\n  #pred_dist = np.unique(y_pred.astype(int), return_counts=True)\n  #correct_prediction = np.unique(y_pred == np.expand_dims(Y_test[:test_size], axis=1), return_counts=True)\n  #print(pred_dist, correct_prediction[0])\n  \n  precision = precision_cal(y_pred, np.array(Y_test[:test_size]))\n  recall = recall_cal(y_pred, np.array(Y_test[:test_size]))\n\n  return precision, recall, 2*precision*recall/(precision+recall)\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Tune the threshold values\nf1_final = []\nbest_threshold = 0\nbest_f1 = 0\nfor i in np.arrange(0.2, 0.1, 0.8):\n    pre, re, f1 = F1_score(my_model, X_test, Y_test, test_size=12800, threshold=i)\n    if len(f1_final) > 0:\n        if f1 < best_f1:\n            best_threshold = i\n            best_f1 = f1\n    else:\n        best_f1 = f1\n        best_threshold = i\n        \n    f1_final.append(f1)\n    \nprint(\"Best threshold value: \" + str(best_threshold) + \"; F1 score is: \" + str(best_f1))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.plot(f1_final)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Have a look at the prediction using the X_test"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "my_model.evaluate(X_test, Y_test, verbose=1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "prediction = my_model.predict(X_test)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "prediction = np.squeeze(prediction, axis=1)\n\nplt.subplot(211)\nplt.hist(Y_test, bins=[0,1,2])\n\nplt.subplot(212)\nplt.hist(prediction, bins=[0,1,2])\n\n\n\nfraud_predict = np.unique((prediction>0.5).astype('int'), return_counts=True)\nfraud_real = np.unique(Y_test, return_counts=True)\nprint(\"Percentage of Fraud: \" + str(round(fraud_predict[1][1]/np.sum(fraud_predict[1])*100,2)) + \"% \" + str(round(fraud_real[1][1]/np.sum(fraud_real[1])*100,2)) + \"%\")\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}